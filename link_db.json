{"_default": {"1": {"url": "https://blog.insightdatascience.com/preparing-for-the-transition-to-applied-ai-d41e48403447", "title": "Transitioning from Software Engineering to Artificial Intelligence", "text": "Emmanuel Ameisen and Jeremy Karnowski, Insight AI A significant part of the Software Engineer role requires staying up-to-date with evolving frameworks, standards, and paradigms. Software Engineers strive to constantly learn, in order to always use the best tool for the job. As Machine Learning finds footholds in more applications every day, it has naturally become a topic that many Engineers want to master. Machine Learning, though, is harder to pick up than a new framework. To be an efficient practitioner, you require a solid understanding of the theory of the field, broad knowledge of the current state of the art, and an ability to frame problems in a non deterministic way. Many guides you can find online will simply teach you how to train an out-of-the-box model on a curated data set to achieve good accuracy and call it a day. The truth is that a much more extensive skillset is essential in becoming an effective Machine Learning Engineer. Below is a distillation of the many conversations we\u2019ve had with over 50 top Machine Learning teams all over The Bay Area and New York, who\u2019ve come to Insight to find AI Practitioners poised to tackle their problems and accelerate their expansion into Applied AI. Deploying a Machine Learning solution requires much more than just training an arbitrary model on your data. It requires an understanding of: In other words, in addition to engineering chops, you need to understand the fundamentals of statistics, linear algebra, and optimization theory in order to integrate, deploy, and debug models. Building a custom Machine Learning solution for a problem requires that you consider issues ranging from acquiring, labeling and pre-processing your data to building, updating, and serving an inference model, and everything in between. Finally, building a REST API for a standard web-app is a task that we can deem feasible ahead of time. Machine Learning models, on the other hand are not always guaranteed to converge, or produce usable outputs. The best way to learn how to scope and deliver impactful Machine Learning products, is to understand how their theoretical underpinnings relate to the taxonomy of your data. In order to understand Machine Learning, a solid knowledge of statistics fundamentals is essential. This involves understanding the following: When you are training a neural network, what is actually happening? What makes some tasks doable and others not? A good approach to this might be to first try to understand Machine Learning through graphics and examples, before diving deeper into the theory. Concepts to understand range from how different loss functions work, why back propagation is useful, or what a computational graph is. A deep understanding is crucial both for building a functional model, and to communicate about it efficiently to the rest of the organization. Following are a few resources, starting with high level overviews, and diving deeper. Another fundamental skill is the ability to read, understand and implement research papers. It can seem like a daunting task at first, so a good way to start is to look up a paper that already has code attached to it (on GitXiv for example) and try to understand the implementation in depth. Ask any Data Scientist and they\u2019ll tell you 90% of the work they do is data munging. This is just as important for Applied AI, as the success of your model correlates hugely with the quality (and quantity) of your data. Data work comes in many aspects, and falls within a few categories: The best way to get familiar with data wrangling is to grab a dataset in the wild and try to use it. There are many datasets online and many social media and news outlets sites have great APIs. Following the steps above, a good way to learn is to: Debugging Machine Learning algorithms that fail to converge or to give sensible results involves a very different process from debugging code. In the same vein, finding the right architecture and hyperparameters requires solid theoretical fundamentals, but also good infrastructure work to be able to test different configurations out. Because of the pace at which the fields evolve, the methods to debug models are constantly evolving. Here are a few \u201csanity checks\u201d from our discussions and experience deploying models that mirror in some ways the principles of KISS familiar to many Software Engineers. A lot of those steps can be accelerated significantly by your development skills, which brings us to our last skill. A lot of Applied Machine Learning will allow you to leverage Software Engineering skills, sometimes with a little twist. These skills include: For more details on some of the software skills we recommend acquiring to become a quality Machine Learning Engineer, check out our post dedicated to transitioning to Applied AI from Academia. The resources above will help you approach and tackle actual Machine Learning problems. But the field of Applied AI changes extremely quickly, and the best way to learn, is to get your hands dirty and actually try to build out an end-to-end solution to solve a real problem. Action Items: Find some inspiration, then dive in! Remember that while Machine Learning Engineering is about building products at heart, there is a research aspect to it. You will explore models and paradigms that will prove unsuccessful, and that is perfectly fine, as it will lead you to understand the intricacies of the problem better. AI is an exciting, ever-changing field. The demand for Machine Learning Engineers is strong, and it is easy to get overwhelmed with the amount of news surrounding the topic. We recommend following a few serious sources and newsletters, to be able to separate PR and abstract research from innovations that are immediately relevant to the field. Here are some sources to help out: Want to learn applied Artificial Intelligence from top professionals in Silicon Valley or New York? Learn more about the Artificial Intelligence program.", "links": ["https://blog.insightdatascience.com?source=logo-lo_bb02784013ba---d02e65779d7b", "https://twitter.com/InsightDataSci", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.insightdatascience.com%2Fpreparing-for-the-transition-to-applied-ai-d41e48403447", "https://blog.insightdatascience.com", "https://blog.insightdatascience.com/tagged/about-insight", "https://blog.insightdatascience.com/tagged/insight-data-science", "https://blog.insightdatascience.com/tagged/insight-data-engineering", "https://blog.insightdatascience.com/tagged/insight-health-data", "https://blog.insightdatascience.com/tagged/insight-ai", "https://blog.insightdatascience.com/search", "https://blog.insightdatascience.com/@emmanuelameisen?source=post_header_lockup", "https://blog.insightdatascience.com/@emmanuelameisen?source=post_header_lockup", "https://www.udacity.com/course/deep-learning--ud730", "http://cs231n.stanford.edu/", "http://cs224d.stanford.edu/", "http://www.deeplearningbook.org/", "http://www.gitxiv.com/", "https://data.sfgov.org/", "https://www.data.gov/", "https://dev.twitter.com/streaming/public", "http://developer.nytimes.com/docs", "http://people.apache.org/~fhanik/kiss.html", "http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf", "https://github.com/JasperSnoek/spearmint", "https://blog.insightdatascience.com/preparing-for-the-transition-to-applied-ai-8eaf53624079", "https://news.ycombinator.com/", "https://jack-clark.net/import-ai/", "http://www.insightdatascience.com/blog/", "https://blog.insightdatascience.com/tagged/insight-ai", "https://blog.insightdatascience.com/using-deep-learning-to-reconstruct-high-resolution-audio-29deee8b7ccd", "https://blog.insightdatascience.com/separating-overlapping-chromosomes-with-deep-learning-based-image-segmentation-22f97afd3283", "http://insightdata.ai/notifications-list", "http:insightdata.ai", "https://medium.com/@jkarnows?source=post_page", "https://medium.com/@InsightData?source=post_page", "https://medium.com/@rossfadely?source=post_page", "https://blog.insightdatascience.com/tagged/machine-learning?source=post", "https://blog.insightdatascience.com/tagged/ai?source=post", "https://blog.insightdatascience.com/tagged/software-engineering?source=post", "https://blog.insightdatascience.com/tagged/insight-ai?source=post", "https://blog.insightdatascience.com/tagged/deep-learning?source=post", "https://blog.insightdatascience.com/@emmanuelameisen?source=footer_card", "https://blog.insightdatascience.com/@emmanuelameisen", "http://twitter.com/EmmanuelAmeisen", "https://blog.insightdatascience.com?source=footer_card", "https://blog.insightdatascience.com?source=footer_card", "https://blog.insightdatascience.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "2": {"url": "http://vooban.com/en/tips-articles-geek-stuff/hyperopt-tutorial-for-optimizing-neural-networks-hyperparameters/", "title": "", "text": "Hyperopt is a way to search through an hyperparameter space. For example, it can use the Tree-structured Parzen Estimator (TPE) algorithm, which explore intelligently the search space while narrowing down to the estimated best parameters. It is hence a good method for meta-optimizing a neural network which is itself an optimisation problem: tuning a neural network uses gradient descent methods, and tuning the hyperparameters needs to be done differently since gradient descent can\u2019t apply. Therefore, Hyperopt can be useful not only for tuning hyperparameters such as the learning rate, but also to tune more fancy parameters in a flexible way, such as changing the number of layers of certain types, or the number of neurons in a layer, or even the type of layer to use at a certain place in the network given an array of choices, each with nested tunable hyperparameters. This is an oriented random search, in contrast with a Grid Search where hyperparameters are pre-established with fixed steps increase. Random Search for Hyper-Parameter Optimization (such as what Hyperopt do) has proven to be an effective search technique. The paper about this technique sits among the most cited deep learning papers. To sum up, it is more efficient to search randomly through values and to intelligently narrow the search space rather than looping on fixed sets of values for the hyperparameters. Note that this blog post is also available as a Notebook on GitHub. It contains code that can be run with Jupyter. A parameter is defined with a certain uniformrange or else a probability distribution, such as: There is also a few quantized versions of those functions, which rounds the generated values at each step of \u201cq\u201d: It is also possible to use a \u201cchoice\u201d which can lead to hyperparameter nesting: Visualisations of the parameters for probability distributions can be found below. Then, more details on choices and parameter nesting will come. Note on the above charts (especially for the loguniform and uniform distributions): the blurred line averaging the values fades out toward the ends of the signal since it is zero-padded. The line ideally would not fade out by using techniques such as mirror-padding. Those are the best distributions for modeling the values a learning rate. That\u2019s because we want to observe changes in the learning rate according to changing it with multiplications rather than additions, e.g.: when adjusting the learning rate, we\u2019ll want to try to divide it or multiply it by 2 rather than adding and substracting a finite value. To proove this, let\u2019s generate a loguniform distribution for a multiplier of the learning rate, centered at 1.0. Dividing 1 by those values should yield the same distribution. Let\u2019s now define a simple search space and solve for f(x) = x^2 - x + 1, where x is an hyperparameter. Let\u2019s solve for minimizing f(x, y) = x^2 + y^2 using a space using a python dict as structure. Later, this will neable us to nest hyperparameters with choices in a clean way. Yet, we have defined spaces as a single parameter. But that is 1D. Normally, spaces contain many parameters. Let\u2019s define a more complex one and with one nested hyperparameter choice for an uniform float: This will require us to import a few more things, and return the results with a dict that has a \u201cstatus\u201d and \u201closs\u201d key at least. Let\u2019s keep in our return dict the evaluated space too as this may come in handy if we save results to disk. Note that the optimization could be parallelized by using MongoDB and storing the trials\u2019 state here. Althought this is a built-in feature of hyperopt, let\u2019s keep things simple for our examples here. Indeed, the TPE algorithm used by the fmin function has state which is stored in the trials and which is useful to narrow the search space dynamically once we have a few trials. It is then interesting to pause and resume a training, and to apply that to a real problem. This is what\u2019s done inside the hyperopt_optimize.py file of the GitHub repository for this project. There, as an example, we optimize a convolutional neural network for solving the CIFAR-100 problem. Star Fork You might as well like this other blog post of mine on how to use Git Large File Storage (Git LFS) to handle the versioning of huge files when working with machine learning projects. 510-1015 av. Wilfrid-Pelletier  Quebec, QC, Canada  G1W 0C4 418 800.0027 TOLL FREE: 1 844 800.0027 info@vooban.com", "links": ["http://vooban.com/en/", "http://vooban.com/en/", "http://vooban.com/en/", "http://vooban.com/en/products/", "http://vooban.com/en/products/#tpti", "http://vooban.com/en/products/#tdti", "http://vooban.com/en/approach/", "http://vooban.com/en/tactical-squad/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/", "http://vooban.com/en/tips-articles-geek-stuff/", "https://pdfs.semanticscholar.org/9f2a/efc3821853e963beda011ed770f740385b77.pdf", "http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a", "https://github.com/terryum/awesome-deep-learning-papers#optimization--training-techniques", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100/blob/master/IntroductionToHyperopt.ipynb", "http://jupyter.org/", "http://vooban.com/wp-content/uploads/2017/08/histogram_1_int.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_2_uniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_3_loguniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_4_normal.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_5_lognormal.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_6_inverse_loguniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_7_inverse_lognormal.png", "http://vooban.com/wp-content/uploads/2017/08/fx2.png", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100/fork", "http://vooban.com/en/tips-articles-geek-stuff/migrating-to-git-lfs-for-developing-deep-learning-applications-with-large-files/", "http://vooban.com/en/tips-articles-geek-stuff/discover-structure-behind-data-with-decision-trees/", "http://vooban.com/en/tips-articles-geek-stuff/", "https://www.google.ca/maps/place/Vooban/@46.7694849,-71.3055195,17z/data=!3m1!4b1!4m5!3m4!1s0x4cb890d721a9234b:0x979022270b4a9fa4!8m2!3d46.7694849!4d-71.3033308", "http://vooban.com/en/products/", "http://vooban.com/en/approach/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/", "https://twitter.com/vooban", "https://www.instagram.com/voobanvibe/", "https://www.facebook.com/vooban", "https://www.linkedin.com/company/vooban", "http://vooban.com/en/", "http://vooban.com/en/products/", "http://vooban.com/en/products/#tpti", "http://vooban.com/en/products/#tdti", "http://vooban.com/en/approach/", "http://vooban.com/en/tactical-squad/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/"]}, "3": {"url": "https://elitedatascience.com/learn-python-for-data-science", "title": "How to Learn Python for Data Science in 2017 (Updated)", "text": "In this guide, we\u2019ll cover how to learn Python for data science, including our favorite curriculum for self-study. You see, data science is about problem solving, exploration, and extracting valuable information from data. To do so effectively, you\u2019ll need to wrangle\u00a0datasets, train machine learning models, visualize results, and much more. Enter Python. This is the best time ever to learn Python. In fact, Forbes named it a top 10 technical skill in terms of job demand growth. Let\u2019s discuss why\u2026 Python is one of the most widespread languages in the world, and it has a passionate community of users: Python Popularity, TIOBE Index It has an even more loyal following within the data science profession. Some people judge the quality of a programming language by the simplicity of its \"hello, world!\" program. Python does pretty well by this standard: For comparison, here's the same output in Java: Great, case closed. See you back here\u00a0after you've mastered Python? Well, in all seriousness, simplicity is one of Python's greatest strengths. Thanks to its precise and efficient syntax, Python can accomplish the same tasks with less code than other languages. This makes implementing\u00a0solutions refreshingly fast. In addition, Python's vibrant data science community means you'll be able to find plenty of tutorials, code snippets, and people to commiserate with\u00a0fixes to common bugs. Stackoverflow will be one of your best friends. Finally,\u00a0Python\u00a0has an\u00a0all-star lineup of libraries (a.k.a. packages) for data analysis and machine learning, which drastically reduce the time it takes to produce results. More on these later. Before we go into what you'll need to learn, let's discuss what you won't need. You won't need a C.S. degree. Most data scientists will never deal with topics such as memory leaks, cryptography, or \"Big O\" notation. You'll be fine as long as you can write clean, logical code in a scripting language such as Python or R. You won't need a complete course on Python. Python and data science are\u00a0not synonymous. You won't need to memorize all the syntax. Instead, focus on grasping the intuition, such as when function is appropriate or how conditional statements work. You'll gradually remember the syntax after Googling, reading documentation, and good ol' fashioned practice. We recommend a top-down approach. We advocate\u00a0a top-down approach with the goal of getting results first and then solidifying concepts over time. In fact, we prefer to cut out \"classroom\" study in favor of real-world practice. This approach will allow you to build mastery over time while having more fun. There are many ways to install Python on your computer, but we recommend the Anaconda bundle, which comes with the libraries you'll need for data science. Effective programming is not about memorizing syntax, but rather mastering a new way of thinking. Therefore, take your time in building a solid foundation of core programming concepts. These will help you translate solutions in your head into instructions for a computer. If you are completely new to programming, we recommend the excellent Automate the Boring Stuff with Python book, which has been released for free online under a creative commons license. The book promises \"practical programming for total beginners,\" and it keeps each lesson down-to-earth. Read up to Chapter 6 - Manipulating Strings and complete the practice questions along the way. If you only need to brush up on Python syntax, then we recommend the following video, aptly named \"Learn Python in One Video:\" Again, the goal of this step is not to learn everything about Python and programming. Instead, focus on the intuition. You should be able to answer questions such as: If you'd like more practice with the core programming concepts, check out the following resources. Next, we're going to focus on the for data science part of \"how to learn Python for data science.\" As we mentioned earlier, Python has an all-star lineup of libraries for data science. Libraries are simply bundles of pre-existing functions and objects that you can import into your script to save time. These are the action steps we recommend for efficiently picking up a new library: We don't recommend diving much deeper into a library right now because you'll likely forget most of what you've learned by the time you jump into projects. Instead, aim to discover what each library is capable of. If you installed Python through the Anaconda bundle as we recommended above, it will also come with Jupyter Notebook. Jupyter Notebook is a lightweight IDE that's a favorite among data scientists. We recommend it for your projects. You can open a new notebook through Anaconda Navigator, which came with Anaconda. Check out this short video for instructions. These are the essential libraries you'll need: NumPy allows easy and efficient numeric computation, and many other data science libraries are built on top of it. Pandas is high-performance library for data structures and exploratory analysis. It's built on top of NumPy. Matplotlib is a flexible plotting and visualization library. It's powerful but somewhat cumbersome. You have the option of skipping Matplotlib for now and using Seaborn to get started (see our Seaborn recommendation below). Scikit-Learn is the premier general-purpose machine learning library in Python. It has many popular algorithms and modules for pre-processing, cross-validation, and much more. Seaborn makes it much easier to plot common data visualizations. It's built on top of Matplotlib and offers a more pleasant high-level wrapper. By now, you'll have a basic understanding of programming and a working knowledge of essential libraries. This actually covers most of the Python you'll need to get started with data science. At this point, some students will feel a bit overwhelmed. That's OK, and it's perfectly normal. If you were to take the slow and traditional bottom-up approach, you might feel less overwhelmed, but it would have taken you 10 times as long to get here. Now the key is to dive in immediately and start gluing everything together. Again, our goal up to here has been to just learn enough to get started. Next, it's time to solidify your knowledge through plenty of practice and projects. You have several options. The first option is to participate on Kaggle, a site that hosts data science competitions. The main advantage of Kaggle is that every project is self-contained. You're given the dataset, a goal, and tutorials to get you started. The major disadvantage of competitions is that they're usually not representative of real-world data science. The \"Getting Started\" competitions are way too basic while the standard competitions (i.e. those with prize pools) are usually too tough for beginners. If you're interested in this path, check out our Beginner's Guide to Kaggle. The next option is to structure your own projects and pick datasets that interest you. The main advantage of this approach is that the projects are more representative of real-world data science. You'll likely need to define your own goals, collect data, clean your dataset, engineer features, and so on. The disadvantage of DIY projects is that you'll need to already be familiar with a proper data science workflow. Without one, you could miss important steps or get stuck without knowing how to proceed. If you go with this path, check out our free 7-day crash course on applied machine learning, which covers the key steps in a data science workflow. We also have another article with several\u00a0DIY project ideas. Finally, there are guided end-to-end projects. Proper guided projects should combine the best of both words - they should be representative of real-world data science and allow you to solidify your skills through a carefully planned learning curve. Many data science bootcamps offer this as a main benefit. Bootcamps usually conclude with a \"capstone project\" that allows you to see all the moving pieces together, from start to finish. We've also crafted our own\u00a0Machine Learning Masterclass to solve this exact need. It will provide you over-the-shoulder mentorship for real-world projects while teaching you all of the key concepts in context. The masterclass also includes a comprehensive Python course that gets you up to speed ASAP. In fact, many successful students have enrolled without any prior programming experience. Learn more about it here. [\u2026] In terms of programming languages \u2013 Python is pretty powerful language with integration to all technologies in the CDH stack. For a basic course check out the link below \u2013 Python For Data Science [\u2026] * Denotes Required Field Copyright \u00a9 2017 \u00b7 EliteDataScience.com \u00b7 All Rights Reserved", "links": ["https://elitedatascience.com/", "https://elitedatascience.com/", "https://elitedatascience.com/blog", "https://elitedatascience.com/machine-learning-masterclass", "https://elitedatascience.com/about", "https://elitedatascience.com/about", "https://elitedatascience.com/guest-post-submissions", "https://elitedatascience.com/author/slice2o", "https://elitedatascience.com/learn-python-for-data-science#comments", "https://www.facebook.com/sharer.php?u=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://plus.google.com/share?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://www.linkedin.com/shareArticle?trk=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://twitter.com/intent/tweet?text=How%20to%20Learn%20Python%20for%20Data%20Science%20%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "https://www.forbes.com/sites/jeffkauflin/2017/01/08/the-10-technical-skills-with-explosive-growth-in-job-demand/#6fa6bf344f5c", "http://www.tiobe.com/tiobe-index/", "https://stackoverflow.com/questions/tagged/python?sort=votes&pageSize=15", "https://www.continuum.io/downloads", "https://automatetheboringstuff.com/", "https://automatetheboringstuff.com/", "https://codefights.com/", "http://www.pythonchallenge.com/index.php", "http://www.practicepython.org/", "http://interactivepython.org/runestone/static/thinkcspy/index.html", "http://jupyter.org/", "https://www.youtube.com/watch?v=-MyjG00la2k", "https://docs.scipy.org/doc/numpy/user/", "https://docs.scipy.org/doc/numpy-dev/user/quickstart.html", "https://pandas.pydata.org/pandas-docs/stable/", "https://pandas.pydata.org/pandas-docs/stable/10min.html", "https://matplotlib.org/contents.html", "https://matplotlib.org/users/pyplot_tutorial.html", "http://scikit-learn.org/stable/documentation.html", "http://elitedatascience.com/python-machine-learning-tutorial-scikit-learn", "https://seaborn.pydata.org/", "http://elitedatascience.com/python-seaborn-tutorial", "http://elitedatascience.com/beginner-kaggle", "http://elitedatascience.com/", "http://elitedatascience.com/machine-learning-projects-for-beginners", "http://elitedatascience.com/machine-learning-masterclass", "http://elitedatascience.com/machine-learning-masterclass", "https://www.facebook.com/sharer.php?u=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://plus.google.com/share?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://www.linkedin.com/shareArticle?trk=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://twitter.com/intent/tweet?text=How%20to%20Learn%20Python%20for%20Data%20Science%20%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "https://datascienceandmore.wordpress.com/2017/03/04/getting-the-basics-down/", "https://elitedatascience.com/learn-machine-learning", "https://elitedatascience.com/learn-python-for-data-science", "https://elitedatascience.com/feature-engineering-best-practices", "https://elitedatascience.com/beginner-kaggle", "https://elitedatascience.com/imbalanced-classes", "https://elitedatascience.com/beginner-mistakes", "https://elitedatascience.com/bias-variance-tradeoff", "https://elitedatascience.com/data-science-resources", "https://elitedatascience.com/guest-post-submissions", "https://elitedatascience.com/", "https://elitedatascience.com/terms-of-service", "https://elitedatascience.com/privacy-policy", "https://elitedatascience.com/about", "https://www.linkedin.com/shareArticle?trk=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science", "http://www.facebook.com/sharer.php?u=https://elitedatascience.com/learn-python-for-data-science", "https://plus.google.com/share?text=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science", "https://twitter.com/intent/tweet?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "http://service.weibo.com/share/share.php?url=https://elitedatascience.com/learn-python-for-data-science&title=How to Learn Python for Data Science (Updated)", "https://getpocket.com/save?title=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science"]}, "4": {"url": "https://blog.insightdatascience.com/preparing-for-the-transition-to-applied-ai-d41e48403447", "title": "Transitioning from Software Engineering to Artificial Intelligence", "text": "Emmanuel Ameisen and Jeremy Karnowski, Insight AI A significant part of the Software Engineer role requires staying up-to-date with evolving frameworks, standards, and paradigms. Software Engineers strive to constantly learn, in order to always use the best tool for the job. As Machine Learning finds footholds in more applications every day, it has naturally become a topic that many Engineers want to master. Machine Learning, though, is harder to pick up than a new framework. To be an efficient practitioner, you require a solid understanding of the theory of the field, broad knowledge of the current state of the art, and an ability to frame problems in a non deterministic way. Many guides you can find online will simply teach you how to train an out-of-the-box model on a curated data set to achieve good accuracy and call it a day. The truth is that a much more extensive skillset is essential in becoming an effective Machine Learning Engineer. Below is a distillation of the many conversations we\u2019ve had with over 50 top Machine Learning teams all over The Bay Area and New York, who\u2019ve come to Insight to find AI Practitioners poised to tackle their problems and accelerate their expansion into Applied AI. Deploying a Machine Learning solution requires much more than just training an arbitrary model on your data. It requires an understanding of: In other words, in addition to engineering chops, you need to understand the fundamentals of statistics, linear algebra, and optimization theory in order to integrate, deploy, and debug models. Building a custom Machine Learning solution for a problem requires that you consider issues ranging from acquiring, labeling and pre-processing your data to building, updating, and serving an inference model, and everything in between. Finally, building a REST API for a standard web-app is a task that we can deem feasible ahead of time. Machine Learning models, on the other hand are not always guaranteed to converge, or produce usable outputs. The best way to learn how to scope and deliver impactful Machine Learning products, is to understand how their theoretical underpinnings relate to the taxonomy of your data. In order to understand Machine Learning, a solid knowledge of statistics fundamentals is essential. This involves understanding the following: When you are training a neural network, what is actually happening? What makes some tasks doable and others not? A good approach to this might be to first try to understand Machine Learning through graphics and examples, before diving deeper into the theory. Concepts to understand range from how different loss functions work, why back propagation is useful, or what a computational graph is. A deep understanding is crucial both for building a functional model, and to communicate about it efficiently to the rest of the organization. Following are a few resources, starting with high level overviews, and diving deeper. Another fundamental skill is the ability to read, understand and implement research papers. It can seem like a daunting task at first, so a good way to start is to look up a paper that already has code attached to it (on GitXiv for example) and try to understand the implementation in depth. Ask any Data Scientist and they\u2019ll tell you 90% of the work they do is data munging. This is just as important for Applied AI, as the success of your model correlates hugely with the quality (and quantity) of your data. Data work comes in many aspects, and falls within a few categories: The best way to get familiar with data wrangling is to grab a dataset in the wild and try to use it. There are many datasets online and many social media and news outlets sites have great APIs. Following the steps above, a good way to learn is to: Debugging Machine Learning algorithms that fail to converge or to give sensible results involves a very different process from debugging code. In the same vein, finding the right architecture and hyperparameters requires solid theoretical fundamentals, but also good infrastructure work to be able to test different configurations out. Because of the pace at which the fields evolve, the methods to debug models are constantly evolving. Here are a few \u201csanity checks\u201d from our discussions and experience deploying models that mirror in some ways the principles of KISS familiar to many Software Engineers. A lot of those steps can be accelerated significantly by your development skills, which brings us to our last skill. A lot of Applied Machine Learning will allow you to leverage Software Engineering skills, sometimes with a little twist. These skills include: For more details on some of the software skills we recommend acquiring to become a quality Machine Learning Engineer, check out our post dedicated to transitioning to Applied AI from Academia. The resources above will help you approach and tackle actual Machine Learning problems. But the field of Applied AI changes extremely quickly, and the best way to learn, is to get your hands dirty and actually try to build out an end-to-end solution to solve a real problem. Action Items: Find some inspiration, then dive in! Remember that while Machine Learning Engineering is about building products at heart, there is a research aspect to it. You will explore models and paradigms that will prove unsuccessful, and that is perfectly fine, as it will lead you to understand the intricacies of the problem better. AI is an exciting, ever-changing field. The demand for Machine Learning Engineers is strong, and it is easy to get overwhelmed with the amount of news surrounding the topic. We recommend following a few serious sources and newsletters, to be able to separate PR and abstract research from innovations that are immediately relevant to the field. Here are some sources to help out: Want to learn applied Artificial Intelligence from top professionals in Silicon Valley or New York? Learn more about the Artificial Intelligence program.", "links": ["https://blog.insightdatascience.com?source=logo-lo_a8a03b62bf0d---d02e65779d7b", "https://twitter.com/InsightDataSci", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.insightdatascience.com%2Fpreparing-for-the-transition-to-applied-ai-d41e48403447", "https://blog.insightdatascience.com", "https://blog.insightdatascience.com/tagged/about-insight", "https://blog.insightdatascience.com/tagged/insight-data-science", "https://blog.insightdatascience.com/tagged/insight-data-engineering", "https://blog.insightdatascience.com/tagged/insight-health-data", "https://blog.insightdatascience.com/tagged/insight-ai", "https://blog.insightdatascience.com/search", "https://blog.insightdatascience.com/@emmanuelameisen?source=post_header_lockup", "https://blog.insightdatascience.com/@emmanuelameisen?source=post_header_lockup", "https://www.udacity.com/course/deep-learning--ud730", "http://cs231n.stanford.edu/", "http://cs224d.stanford.edu/", "http://www.deeplearningbook.org/", "http://www.gitxiv.com/", "https://data.sfgov.org/", "https://www.data.gov/", "https://dev.twitter.com/streaming/public", "http://developer.nytimes.com/docs", "http://people.apache.org/~fhanik/kiss.html", "http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf", "https://github.com/JasperSnoek/spearmint", "https://blog.insightdatascience.com/preparing-for-the-transition-to-applied-ai-8eaf53624079", "https://news.ycombinator.com/", "https://jack-clark.net/import-ai/", "http://www.insightdatascience.com/blog/", "https://blog.insightdatascience.com/tagged/insight-ai", "https://blog.insightdatascience.com/using-deep-learning-to-reconstruct-high-resolution-audio-29deee8b7ccd", "https://blog.insightdatascience.com/separating-overlapping-chromosomes-with-deep-learning-based-image-segmentation-22f97afd3283", "http://insightdata.ai/notifications-list", "http:insightdata.ai", "https://medium.com/@jkarnows?source=post_page", "https://medium.com/@InsightData?source=post_page", "https://medium.com/@rossfadely?source=post_page", "https://blog.insightdatascience.com/tagged/machine-learning?source=post", "https://blog.insightdatascience.com/tagged/ai?source=post", "https://blog.insightdatascience.com/tagged/software-engineering?source=post", "https://blog.insightdatascience.com/tagged/insight-ai?source=post", "https://blog.insightdatascience.com/tagged/deep-learning?source=post", "https://blog.insightdatascience.com/@emmanuelameisen?source=footer_card", "https://blog.insightdatascience.com/@emmanuelameisen", "http://twitter.com/EmmanuelAmeisen", "https://blog.insightdatascience.com?source=footer_card", "https://blog.insightdatascience.com?source=footer_card", "https://blog.insightdatascience.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "5": {"url": "http://vooban.com/en/tips-articles-geek-stuff/hyperopt-tutorial-for-optimizing-neural-networks-hyperparameters/", "title": "", "text": "Hyperopt is a way to search through an hyperparameter space. For example, it can use the Tree-structured Parzen Estimator (TPE) algorithm, which explore intelligently the search space while narrowing down to the estimated best parameters. It is hence a good method for meta-optimizing a neural network which is itself an optimisation problem: tuning a neural network uses gradient descent methods, and tuning the hyperparameters needs to be done differently since gradient descent can\u2019t apply. Therefore, Hyperopt can be useful not only for tuning hyperparameters such as the learning rate, but also to tune more fancy parameters in a flexible way, such as changing the number of layers of certain types, or the number of neurons in a layer, or even the type of layer to use at a certain place in the network given an array of choices, each with nested tunable hyperparameters. This is an oriented random search, in contrast with a Grid Search where hyperparameters are pre-established with fixed steps increase. Random Search for Hyper-Parameter Optimization (such as what Hyperopt do) has proven to be an effective search technique. The paper about this technique sits among the most cited deep learning papers. To sum up, it is more efficient to search randomly through values and to intelligently narrow the search space rather than looping on fixed sets of values for the hyperparameters. Note that this blog post is also available as a Notebook on GitHub. It contains code that can be run with Jupyter. A parameter is defined with a certain uniformrange or else a probability distribution, such as: There is also a few quantized versions of those functions, which rounds the generated values at each step of \u201cq\u201d: It is also possible to use a \u201cchoice\u201d which can lead to hyperparameter nesting: Visualisations of the parameters for probability distributions can be found below. Then, more details on choices and parameter nesting will come. Note on the above charts (especially for the loguniform and uniform distributions): the blurred line averaging the values fades out toward the ends of the signal since it is zero-padded. The line ideally would not fade out by using techniques such as mirror-padding. Those are the best distributions for modeling the values a learning rate. That\u2019s because we want to observe changes in the learning rate according to changing it with multiplications rather than additions, e.g.: when adjusting the learning rate, we\u2019ll want to try to divide it or multiply it by 2 rather than adding and substracting a finite value. To proove this, let\u2019s generate a loguniform distribution for a multiplier of the learning rate, centered at 1.0. Dividing 1 by those values should yield the same distribution. Let\u2019s now define a simple search space and solve for f(x) = x^2 - x + 1, where x is an hyperparameter. Let\u2019s solve for minimizing f(x, y) = x^2 + y^2 using a space using a python dict as structure. Later, this will neable us to nest hyperparameters with choices in a clean way. Yet, we have defined spaces as a single parameter. But that is 1D. Normally, spaces contain many parameters. Let\u2019s define a more complex one and with one nested hyperparameter choice for an uniform float: This will require us to import a few more things, and return the results with a dict that has a \u201cstatus\u201d and \u201closs\u201d key at least. Let\u2019s keep in our return dict the evaluated space too as this may come in handy if we save results to disk. Note that the optimization could be parallelized by using MongoDB and storing the trials\u2019 state here. Althought this is a built-in feature of hyperopt, let\u2019s keep things simple for our examples here. Indeed, the TPE algorithm used by the fmin function has state which is stored in the trials and which is useful to narrow the search space dynamically once we have a few trials. It is then interesting to pause and resume a training, and to apply that to a real problem. This is what\u2019s done inside the hyperopt_optimize.py file of the GitHub repository for this project. There, as an example, we optimize a convolutional neural network for solving the CIFAR-100 problem. Star Fork You might as well like this other blog post of mine on how to use Git Large File Storage (Git LFS) to handle the versioning of huge files when working with machine learning projects. 510-1015 av. Wilfrid-Pelletier  Quebec, QC, Canada  G1W 0C4 418 800.0027 TOLL FREE: 1 844 800.0027 info@vooban.com", "links": ["http://vooban.com/en/", "http://vooban.com/en/", "http://vooban.com/en/", "http://vooban.com/en/products/", "http://vooban.com/en/products/#tpti", "http://vooban.com/en/products/#tdti", "http://vooban.com/en/approach/", "http://vooban.com/en/tactical-squad/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/", "http://vooban.com/en/tips-articles-geek-stuff/", "https://pdfs.semanticscholar.org/9f2a/efc3821853e963beda011ed770f740385b77.pdf", "http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a", "https://github.com/terryum/awesome-deep-learning-papers#optimization--training-techniques", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100/blob/master/IntroductionToHyperopt.ipynb", "http://jupyter.org/", "http://vooban.com/wp-content/uploads/2017/08/histogram_1_int.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_2_uniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_3_loguniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_4_normal.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_5_lognormal.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_6_inverse_loguniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_7_inverse_lognormal.png", "http://vooban.com/wp-content/uploads/2017/08/fx2.png", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100/fork", "http://vooban.com/en/tips-articles-geek-stuff/migrating-to-git-lfs-for-developing-deep-learning-applications-with-large-files/", "http://vooban.com/en/tips-articles-geek-stuff/discover-structure-behind-data-with-decision-trees/", "http://vooban.com/en/tips-articles-geek-stuff/", "https://www.google.ca/maps/place/Vooban/@46.7694849,-71.3055195,17z/data=!3m1!4b1!4m5!3m4!1s0x4cb890d721a9234b:0x979022270b4a9fa4!8m2!3d46.7694849!4d-71.3033308", "http://vooban.com/en/products/", "http://vooban.com/en/approach/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/", "https://twitter.com/vooban", "https://www.instagram.com/voobanvibe/", "https://www.facebook.com/vooban", "https://www.linkedin.com/company/vooban", "http://vooban.com/en/", "http://vooban.com/en/products/", "http://vooban.com/en/products/#tpti", "http://vooban.com/en/products/#tdti", "http://vooban.com/en/approach/", "http://vooban.com/en/tactical-squad/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/"]}, "6": {"url": "https://elitedatascience.com/learn-python-for-data-science", "title": "How to Learn Python for Data Science in 2017 (Updated)", "text": "In this guide, we\u2019ll cover how to learn Python for data science, including our favorite curriculum for self-study. You see, data science is about problem solving, exploration, and extracting valuable information from data. To do so effectively, you\u2019ll need to wrangle\u00a0datasets, train machine learning models, visualize results, and much more. Enter Python. This is the best time ever to learn Python. In fact, Forbes named it a top 10 technical skill in terms of job demand growth. Let\u2019s discuss why\u2026 Python is one of the most widespread languages in the world, and it has a passionate community of users: Python Popularity, TIOBE Index It has an even more loyal following within the data science profession. Some people judge the quality of a programming language by the simplicity of its \"hello, world!\" program. Python does pretty well by this standard: For comparison, here's the same output in Java: Great, case closed. See you back here\u00a0after you've mastered Python? Well, in all seriousness, simplicity is one of Python's greatest strengths. Thanks to its precise and efficient syntax, Python can accomplish the same tasks with less code than other languages. This makes implementing\u00a0solutions refreshingly fast. In addition, Python's vibrant data science community means you'll be able to find plenty of tutorials, code snippets, and people to commiserate with\u00a0fixes to common bugs. Stackoverflow will be one of your best friends. Finally,\u00a0Python\u00a0has an\u00a0all-star lineup of libraries (a.k.a. packages) for data analysis and machine learning, which drastically reduce the time it takes to produce results. More on these later. Before we go into what you'll need to learn, let's discuss what you won't need. You won't need a C.S. degree. Most data scientists will never deal with topics such as memory leaks, cryptography, or \"Big O\" notation. You'll be fine as long as you can write clean, logical code in a scripting language such as Python or R. You won't need a complete course on Python. Python and data science are\u00a0not synonymous. You won't need to memorize all the syntax. Instead, focus on grasping the intuition, such as when function is appropriate or how conditional statements work. You'll gradually remember the syntax after Googling, reading documentation, and good ol' fashioned practice. We recommend a top-down approach. We advocate\u00a0a top-down approach with the goal of getting results first and then solidifying concepts over time. In fact, we prefer to cut out \"classroom\" study in favor of real-world practice. This approach will allow you to build mastery over time while having more fun. There are many ways to install Python on your computer, but we recommend the Anaconda bundle, which comes with the libraries you'll need for data science. Effective programming is not about memorizing syntax, but rather mastering a new way of thinking. Therefore, take your time in building a solid foundation of core programming concepts. These will help you translate solutions in your head into instructions for a computer. If you are completely new to programming, we recommend the excellent Automate the Boring Stuff with Python book, which has been released for free online under a creative commons license. The book promises \"practical programming for total beginners,\" and it keeps each lesson down-to-earth. Read up to Chapter 6 - Manipulating Strings and complete the practice questions along the way. If you only need to brush up on Python syntax, then we recommend the following video, aptly named \"Learn Python in One Video:\" Again, the goal of this step is not to learn everything about Python and programming. Instead, focus on the intuition. You should be able to answer questions such as: If you'd like more practice with the core programming concepts, check out the following resources. Next, we're going to focus on the for data science part of \"how to learn Python for data science.\" As we mentioned earlier, Python has an all-star lineup of libraries for data science. Libraries are simply bundles of pre-existing functions and objects that you can import into your script to save time. These are the action steps we recommend for efficiently picking up a new library: We don't recommend diving much deeper into a library right now because you'll likely forget most of what you've learned by the time you jump into projects. Instead, aim to discover what each library is capable of. If you installed Python through the Anaconda bundle as we recommended above, it will also come with Jupyter Notebook. Jupyter Notebook is a lightweight IDE that's a favorite among data scientists. We recommend it for your projects. You can open a new notebook through Anaconda Navigator, which came with Anaconda. Check out this short video for instructions. These are the essential libraries you'll need: NumPy allows easy and efficient numeric computation, and many other data science libraries are built on top of it. Pandas is high-performance library for data structures and exploratory analysis. It's built on top of NumPy. Matplotlib is a flexible plotting and visualization library. It's powerful but somewhat cumbersome. You have the option of skipping Matplotlib for now and using Seaborn to get started (see our Seaborn recommendation below). Scikit-Learn is the premier general-purpose machine learning library in Python. It has many popular algorithms and modules for pre-processing, cross-validation, and much more. Seaborn makes it much easier to plot common data visualizations. It's built on top of Matplotlib and offers a more pleasant high-level wrapper. By now, you'll have a basic understanding of programming and a working knowledge of essential libraries. This actually covers most of the Python you'll need to get started with data science. At this point, some students will feel a bit overwhelmed. That's OK, and it's perfectly normal. If you were to take the slow and traditional bottom-up approach, you might feel less overwhelmed, but it would have taken you 10 times as long to get here. Now the key is to dive in immediately and start gluing everything together. Again, our goal up to here has been to just learn enough to get started. Next, it's time to solidify your knowledge through plenty of practice and projects. You have several options. The first option is to participate on Kaggle, a site that hosts data science competitions. The main advantage of Kaggle is that every project is self-contained. You're given the dataset, a goal, and tutorials to get you started. The major disadvantage of competitions is that they're usually not representative of real-world data science. The \"Getting Started\" competitions are way too basic while the standard competitions (i.e. those with prize pools) are usually too tough for beginners. If you're interested in this path, check out our Beginner's Guide to Kaggle. The next option is to structure your own projects and pick datasets that interest you. The main advantage of this approach is that the projects are more representative of real-world data science. You'll likely need to define your own goals, collect data, clean your dataset, engineer features, and so on. The disadvantage of DIY projects is that you'll need to already be familiar with a proper data science workflow. Without one, you could miss important steps or get stuck without knowing how to proceed. If you go with this path, check out our free 7-day crash course on applied machine learning, which covers the key steps in a data science workflow. We also have another article with several\u00a0DIY project ideas. Finally, there are guided end-to-end projects. Proper guided projects should combine the best of both words - they should be representative of real-world data science and allow you to solidify your skills through a carefully planned learning curve. Many data science bootcamps offer this as a main benefit. Bootcamps usually conclude with a \"capstone project\" that allows you to see all the moving pieces together, from start to finish. We've also crafted our own\u00a0Machine Learning Masterclass to solve this exact need. It will provide you over-the-shoulder mentorship for real-world projects while teaching you all of the key concepts in context. The masterclass also includes a comprehensive Python course that gets you up to speed ASAP. In fact, many successful students have enrolled without any prior programming experience. Learn more about it here. [\u2026] In terms of programming languages \u2013 Python is pretty powerful language with integration to all technologies in the CDH stack. For a basic course check out the link below \u2013 Python For Data Science [\u2026] * Denotes Required Field Copyright \u00a9 2017 \u00b7 EliteDataScience.com \u00b7 All Rights Reserved", "links": ["https://elitedatascience.com/", "https://elitedatascience.com/", "https://elitedatascience.com/blog", "https://elitedatascience.com/machine-learning-masterclass", "https://elitedatascience.com/about", "https://elitedatascience.com/about", "https://elitedatascience.com/guest-post-submissions", "https://elitedatascience.com/author/slice2o", "https://elitedatascience.com/learn-python-for-data-science#comments", "https://www.facebook.com/sharer.php?u=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://plus.google.com/share?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://www.linkedin.com/shareArticle?trk=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://twitter.com/intent/tweet?text=How%20to%20Learn%20Python%20for%20Data%20Science%20%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "https://www.forbes.com/sites/jeffkauflin/2017/01/08/the-10-technical-skills-with-explosive-growth-in-job-demand/#6fa6bf344f5c", "http://www.tiobe.com/tiobe-index/", "https://stackoverflow.com/questions/tagged/python?sort=votes&pageSize=15", "https://www.continuum.io/downloads", "https://automatetheboringstuff.com/", "https://automatetheboringstuff.com/", "https://codefights.com/", "http://www.pythonchallenge.com/index.php", "http://www.practicepython.org/", "http://interactivepython.org/runestone/static/thinkcspy/index.html", "http://jupyter.org/", "https://www.youtube.com/watch?v=-MyjG00la2k", "https://docs.scipy.org/doc/numpy/user/", "https://docs.scipy.org/doc/numpy-dev/user/quickstart.html", "https://pandas.pydata.org/pandas-docs/stable/", "https://pandas.pydata.org/pandas-docs/stable/10min.html", "https://matplotlib.org/contents.html", "https://matplotlib.org/users/pyplot_tutorial.html", "http://scikit-learn.org/stable/documentation.html", "http://elitedatascience.com/python-machine-learning-tutorial-scikit-learn", "https://seaborn.pydata.org/", "http://elitedatascience.com/python-seaborn-tutorial", "http://elitedatascience.com/beginner-kaggle", "http://elitedatascience.com/", "http://elitedatascience.com/machine-learning-projects-for-beginners", "http://elitedatascience.com/machine-learning-masterclass", "http://elitedatascience.com/machine-learning-masterclass", "https://www.facebook.com/sharer.php?u=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://plus.google.com/share?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://www.linkedin.com/shareArticle?trk=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://twitter.com/intent/tweet?text=How%20to%20Learn%20Python%20for%20Data%20Science%20%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "https://datascienceandmore.wordpress.com/2017/03/04/getting-the-basics-down/", "https://elitedatascience.com/learn-machine-learning", "https://elitedatascience.com/learn-python-for-data-science", "https://elitedatascience.com/feature-engineering-best-practices", "https://elitedatascience.com/beginner-kaggle", "https://elitedatascience.com/imbalanced-classes", "https://elitedatascience.com/beginner-mistakes", "https://elitedatascience.com/bias-variance-tradeoff", "https://elitedatascience.com/data-science-resources", "https://elitedatascience.com/guest-post-submissions", "https://elitedatascience.com/", "https://elitedatascience.com/terms-of-service", "https://elitedatascience.com/privacy-policy", "https://elitedatascience.com/about", "https://www.linkedin.com/shareArticle?trk=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science", "http://www.facebook.com/sharer.php?u=https://elitedatascience.com/learn-python-for-data-science", "https://plus.google.com/share?text=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science", "https://twitter.com/intent/tweet?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "http://service.weibo.com/share/share.php?url=https://elitedatascience.com/learn-python-for-data-science&title=How to Learn Python for Data Science (Updated)", "https://getpocket.com/save?title=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science"]}, "7": {"url": "https://stats.stackexchange.com/questions/297380/why-not-just-dump-the-neural-networks-and-deep-learning", "title": "", "text": "Fundamental problem with deep learning and neural networks in general. The solutions that fit training data are infinite. We don't have precise mathematical equation that is satisfied by only a single one and that we can say generalizes best. Simply speaking we don't know which generalizes best. Optimizing weights is not a convex problem, so we never know we end up with a global or a local minimum. So why not just dump the neural networks and instead search for a better ML model? Something that we understand, and something that is consistent with a set of mathematical equations? Linear and SVM do not have this mathematical drawbacks and are fully consistent with a a set of mathematical equations. Why not just think on same lines (need not be linear though) and come up with a new ML model better than Linear and SVM and neural networks and deep learning? Not being able to know what solution generalizes best is an issue, but it shouldn't deter us from otherwise using a good solution. Humans themselves often do not known what generalizes best (consider, for example, competing unifying theories of physics), but that doesn't cause us too many problems. It has been shown that it is extremely rare for training to fail because of local minimums. Most of the local minimums in a deep neural network are close in value to the global minimum, so this is not an issue. source But the broader answer is that you can talk all day about nonconvexity and model selection, and people will still use neural networks simply because they work better than anything else (at least on things like image classification).  Of course there are also people arguing that we shouldn't get too focused on CNNs like the community was focused on SVMs a few decades ago, and instead keep looking for the next big thing. In particular, I think I remember Hinton regretting the effectiveness of CNNs as something which might hinder research. related post As the comments to your question point out, there are a lot of people working on finding something better. I would though like to answer this question by expanding the comment left by @josh All models are wrong but some are useful (Wiki) The above statement is a general truth used to describe the nature of statistical models. Using data that we have available, we can create models that let us do useful things such as approximate a predicted value. Take for example Linear Regression  Using a number of observations, we can fit a model to give us an approximate value for a dependent variable given any value(s) for the independent variable(s).  Burnham, K. P.; Anderson, D. R. (2002), Model Selection and Multimodel > Inference: A Practical Information-Theoretic Approach (2nd ed.): \"A model is a simplification or approximation of reality and hence   will not reflect all of reality. ... Box noted that \u201call models are   wrong, but some are useful.\u201d While a model can never be \u201ctruth,\u201d a   model might be ranked from very useful, to useful, to somewhat useful   to, finally, essentially useless.\" Deviations from our model (as can be seen in the image above) appear random, some observations are below the line and some are above, but our regression line shows a general correlation. Whilst deviations in our model appear random, in realistic scenarios there will be other factors at play which cause this deviation. For example, imagine watching cars as they drove through a junction where they must turn either left or right to continue, the cars turn in no particular pattern. Whilst we could say that the direction the cars turn is completely random, does every driver reach the junction and at that point make a random decision of which way to turn? In reality they are probably heading somewhere specific for a specific reason, and without attempting to stop each car to ask them about their reasoning, we can only describe their actions as random.  Where we are able to fit a model with minimal deviation, how certain can we be that an unknown, unnoticed or immeasurable variable wont at some point throw our model? Does the flap of a butterfly\u2019s wings in Brazil set off a tornado in Texas? The problem with using the Linear and SVN models you mention alone is that we are somewhat required to manually observe our variables and how they each affect each other. We then need to decide what variables are important and write a task-specific algorithm. This can be straight forward if we only have a few variables, but what if we had thousands? What if we wanted to create a generalised image recognition model, could this realistically be achieved with this approach?  Deep Learning and Artificial Neural Networks (ANNs) can help us create useful models for huge data sets containing huge amounts of variables (e.g. image libraries). As you mention, there's an incomprehensible number of solutions which could fit the data using ANNs, but is this number really any different to the amount of solutions we would need to develop ourselves through trial and error? The application of ANNs do much of the work for us, we can specify our inputs and our desired outputs (and tweak them later to make improvements) and leave it up to the ANN to figure out the solution. This is why ANNs are often described as \"black boxes\". From a given input they output an approximation, however (in general terms) these approximations don't include details on how they were approximated. And so it really comes down to what problem you are trying to solve, as the problem will dictate what model approach is more useful. Models are not absolutely accurate and so there is always an element of being 'wrong', however the more accurate your results the more useful they are. Having more detail in the results on how the approximation was made may also be useful, depending on the problem it may even be more useful than increased accuracy. If for example you are calculating a persons credit score, using regression and SVMs provides calculations that can be better explored. Being able to both tweak the model directly and explain to customers the effect separate independent variables have on their overall score is very useful. An ANN may aid in processing larger amounts of variables to achieve a more accurate score, but would this accuracy be more useful? I guess for some problem we care less for the mathematical rigor and simplicity but more for its utility, current status is neural network is better in performing certain task like pattern recognition in image processing.  The global minimum may as well as be useless, so we don't really care if we find it or not. The reason is that, for deep networks, not only the time to find it becomes exponentially longer as the network size increases, but also the global minimum often corresponds to overfitting the training set. Thus the generalization ability of the DNN (which is what we really care about) would suffer. Also, often we prefer flatter minima corresponding to a higher value of the loss function, than sharper minima corresponding to a lower value of the loss function, because the second one will deal very badly with uncertainty in the inputs. This is becoming increasingly clear with the development of Bayesian Deep Learning. Robust Optimization beats Determinist Optimization very often, when applied to real world problems where uncertainty is important. Finally, it's a fact that DNNs just kick the ass of methods such as XGBoost at image classification and NLP. A company which must make a profit out of image classification will correctly select them as modelsto be deployed in production (and invest a significant amount of money on feature engineering, data pipeline, etc. but I digress). This doesn't mean that they dominate all the ML environment: for example, they do worse than XGBoost on structured data (see the last winners of Kaggle competitions) and they seem to not still do as well as particle filters on time series modelling. However, some very recent innovations on RNNs may modify this situation. There are excellent answers, mostly weighing in with the usefulness of DL and ANNs. But I would like to object the OP in a more fundamental way, since the question already takes for granted the mathematical inconsistency of neural networks. First of all, there is a mathematical theory behind (most models of) Neural Networks. You could likewise argue that linear regression does not generalize, unless the underlying model is... well, linear. In neural algorithms, a model is assumed (even if not explicitly) and the fitting error is computed. The fact that algorithms are modified with various heuristics does not void the original mathematical support. BTW, local optimization is also a mathematically consistent, let alone useful, theory. Along this line, if Neural Networks just constitute one class of methods within the whole toolbox of scientists, which is the line that separates Neural Networks from the rest of techniques? In fact, SVMs were once considered a class of NNs and they still appear in the same books. On the other hand, NNs could be regarded as a (nonlinear) regression technique, maybe with some simplification. I agree with the OP that we must search better, well founded, efficient algorithms, regardless you label them as NNs or not. Sign up using Google Sign up using Facebook Sign up using Email and Password   By posting your answer, you agree to the privacy policy and terms of service. asked 3 days ago viewed  7,704 times  active yesterday                                   site design / logo \u00a9 2017 Stack Exchange Inc; user contributions licensed under cc by-sa 3.0                                  with attribution required.                                  rev 2017.8.11.26777", "links": ["https://stats.stackexchange.com", "https://chat.stackexchange.com?tab=site&host=stats.stackexchange.com", "https://stats.stackexchange.com", "https://stats.meta.stackexchange.com", "https://stats.stackexchange.com/users/signup?ssrc=site_switcher&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.stackexchange.com/users/login?ssrc=site_switcher&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stackexchange.com/sites", "https://stackoverflow.blog", "https://stackexchange.com", "https://stats.stackexchange.com/users/signup?ssrc=head&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.stackexchange.com/users/login?ssrc=head&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.meta.stackexchange.com", "https://stackoverflow.com/company/about", "https://www.stackoverflowbusiness.com/?ref=topbar_help", "https://stats.stackexchange.com", "https://arxiv.org/pdf/1406.2572.pdf", "https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/clyohpf/?context=3", "https://arxiv.org/pdf/1406.2572.pdf", "https://arxiv.org/pdf/1412.0233.pdf", "https://en.wikipedia.org/wiki/All_models_are_wrong", "https://en.wikipedia.org/wiki/Butterfly_effect", "https://stats.stackexchange.com/questions/93705/meaning-of-a-neural-network-as-a-black-box", "https://stackexchange.com/legal/privacy-policy", "https://stackexchange.com/legal/terms-of-service", "http://stats.stackexchange.com/election", "http://stats.stackexchange.com/election", "https://stackexchange.com/questions?tab=hot", "https://math.stackexchange.com/questions/2392702/first-30-solutions-of-pells-equation", "https://money.stackexchange.com/questions/83755/do-people-tend-to-spend-less-when-using-cash-than-credit-cards", "https://scifi.stackexchange.com/questions/166822/why-did-the-slytherin-team-opt-for-size-over-skill-given-their-many-defeats", "https://softwareengineering.stackexchange.com/questions/355580/how-many-cores-should-i-utilize-for-calculations-cores-or-cores-1", "https://interpersonal.stackexchange.com/questions/1556/how-to-ask-a-vegan-to-stop-telling-me-about-veganism-because-i-am-not-interested", "https://scifi.stackexchange.com/questions/166938/story-about-humanity-realizing-they-are-in-a-simulation", "https://math.stackexchange.com/questions/2392728/proving-that-the-limit-of-an-abstract-function-with-certain-properties-is-zero", "https://apple.stackexchange.com/questions/295071/c-programming-on-macos", "https://stackoverflow.com/questions/45657517/why-isnt-this-c-program-looping-from-a-negative-origin", "https://cseducators.stackexchange.com/questions/3175/learning-fundamental-differences-between-functional-programming-and-object-orien", "https://tex.stackexchange.com/questions/386053/how-to-check-software-generated-bibliographic-entries-for-errors-and-other-mista", "https://history.stackexchange.com/questions/39549/was-russia-the-only-country-in-wwii-that-succeeded-in-expansionist-war-aims", "https://math.stackexchange.com/questions/2392309/closed-circle-as-a-metric-space", "https://electronics.stackexchange.com/questions/323757/audible-difference-between-audio-grade-capacitors-in-the-same-range", "https://math.stackexchange.com/questions/2392029/why-does-rudin-say-the-rational-number-system-is-inadequate-as-a-field", "https://japanese.stackexchange.com/questions/52280/help-translating-a-sentence-with-%e3%82%aa%e3%83%ac%e3%81%ab%e3%81%82%e3%81%a3%e3%81%9f", "https://writers.stackexchange.com/questions/29683/should-all-books-have-page-numbers", "https://academia.stackexchange.com/questions/94443/are-course-grade-distributions-supposed-to-be-bell-shaped", "https://mathematica.stackexchange.com/questions/153689/accesing-keys-based-on-position-in-an-association", "https://english.stackexchange.com/questions/405054/is-there-an-english-equivalent-for-the-persian-proverb-to-play-with-tail-of-lio", "https://gardening.stackexchange.com/questions/34995/why-did-my-potatoes-not-grow", "https://worldbuilding.stackexchange.com/questions/88602/could-a-wwi-biplane-fly-if-it-were-dropped-from-a-high-altitude-instead-of-using", "https://latin.stackexchange.com/questions/5045/why-is-the-roman-acronym-spqr-and-not-spr", "https://codegolf.stackexchange.com/questions/138840/reliably-broken-sort", "https://chat.stackexchange.com?tab=site&host=stats.stackexchange.com", "https://stats.meta.stackexchange.com", "https://stackoverflow.com/company/about", "https://stackoverflow.com", "https://www.stackoverflowbusiness.com/?utm_source=so-footer&utm_medium=referral&utm_campaign=brand-activation", "https://stackoverflow.com/jobs", "https://stackoverflow.com/company/about", "https://stackoverflow.com/company/press", "https://stackexchange.com/legal", "https://stackexchange.com/legal/privacy-policy", "https://stackexchange.com", "https://stackexchange.com/sites#technology", "https://stackexchange.com/sites#lifearts", "https://stackexchange.com/sites#culturerecreation", "https://stackexchange.com/sites#science", "https://api.stackexchange.com", "https://data.stackexchange.com", "https://stackoverflow.blog?blb=1", "https://www.facebook.com/officialstackoverflow/", "https://twitter.com/stackoverflow", "https://linkedin.com/company/stack-overflow", "https://creativecommons.org/licenses/by-sa/3.0/", "https://stackoverflow.blog/2009/06/25/attribution-required/"]}, "8": {"url": "https://blog.godatadriven.com/practical-airflow-tutorial", "title": "", "text": "Airflow is a scheduler for workflows such as data pipelines, similar to Luigi and Oozie. It's written in Python and we at GoDataDriven have been contributing to it in the last few months. This tutorial is loosely based on the Airflow tutorial in the official documentation. It will walk you through the basics of setting up Airflow and creating an Airflow workflow, and it will give you some practical tips. A (possibly) more up-to-date version of this blog can be found in my git repo. Setting up a basic configuration of Airflow is pretty straightforward. After installing the Python package, we'll need a database to store some data and start the core Airflow services. You can skip this section if Airflow is already set up. Make sure that you can run airflow commands, know where to put your DAGs and have access to the web UI. Airflow is installable with pip via a simple pip install apache-airflow. Either use a separate Python virtual environment or install it in your default python environment. To use the conda virtual environment as defined in environment.yml from my git repo: You should now have an (almost) working Airflow installation. Alternatively, install Airflow yourself by running: Airflow used to be packaged as airflow but is packaged as apache-airflow since version 1.8.1. Make sure that you install any extra packages with the right Python package: e.g. use pip install apache-airflow[dask] if you've installed apache-airflow and do not use pip install airflow[dask]. Leaving out the prefix apache- will install an old version of Airflow next to your current version, leading to a world of hurt. You may run into problems if you don't have the right binaries or Python packages installed for certain backends or operators. When specifying support for e.g. PostgreSQL when installing extra Airflow packages, make sure the database is installed; do a brew install postgresql or apt-get install postgresql before the pip install apache-airflow[postgres]. Similarly, when running into HiveOperator errors, do a pip install apache-airflow[hive] and make sure you can use Hive. Before you can use Airflow you have to initialize its database. The database contains information about historical & running workflows, connections to external data sources,  user management, etc. Once the database is set up, Airflow's UI can be accessed by running a web server and workflows can be started. The default database is a SQLite database, which is fine for this tutorial. In a production setting you'll probably be using something like MySQL or PostgreSQL. You'll probably want to back it up as this database stores the state of everything related to Airflow. Airflow will use the directory set in the environment variable AIRFLOW_HOME to store its configuration and our SQlite database. This directory will be used after your first Airflow command. If you don't set the environment variable AIRFLOW_HOME, Airflow will create the directory ~/airflow/ to put its files in. Set environment variable AIRFLOW_HOME to e.g. your current directory $(pwd): or any other suitable directory. Next, initialize the database: Now start the web server and go to localhost:8080 to check out the UI: It should look something like this:  With the web server running workflows can be started from a new terminal window. Open a new terminal, activate the virtual environment and set the environment variable AIRFLOW_HOME for this terminal as well: Make sure that you're an in the same directory as before when using $(pwd). Run a supplied example: And check in the web UI that it has run by going to Browse -> Task Instances. This concludes all the setting up that you need for this tutorial. We'll create a workflow by specifying actions as a Directed Acyclic Graph (DAG) in Python. The tasks of a workflow make up a Graph; the graph is Directed because the tasks are ordered; and we don't want to get stuck in an eternal loop so the graph also has to be Acyclic. The figure below shows an example of a DAG:  The DAG of this tutorial is a bit easier. It will consist of the following tasks: and we'll plan daily execution of this workflow. Go to the folder that you've designated to be your AIRFLOW_HOME and find the DAGs folder located in subfolder dags/ (if you cannot find, check the setting dags_folder in $AIRFLOW_HOME/airflow.cfg). Create a Python file with the name airflow_tutorial.py that will contain your DAG. Your workflow will automatically be picked up and scheduled to run. First we'll configure settings that are shared by all our tasks. Settings for tasks can be passed as arguments when creating them, but we can also pass a dictionary with default values to the DAG. This allows us to share default arguments for all the tasks in our DAG is the best place to set e.g. the owner and start date of our DAG. Add the following import and dictionary to airflow_tutorial.py to specify the owner, start time, and retry settings that are shared by our tasks: These settings tell Airflow that this workflow is owned by 'me', that the workflow is valid since June 1st of 2017, it should not send emails and it is allowed to retry the workflow once if it fails with a delay of 5 minutes. Other common default arguments are email settings on failure and the end time. We'll now create a DAG object that will contain our tasks. Name it airflow_tutorial_v01 and pass default_args: With schedule_interval='0 * * * *' we've specified a run at every hour 0; the DAG will run each day at 00:00. See crontab.guru for help deciphering cron schedule expressions. Alternatively, you can use strings like '@daily' and '@hourly'. We've used a context manager to create a DAG (new since 1.8). All the tasks for the DAG should be indented to indicate that they are part of this DAG. Without this context manager you'd have to set the dag parameter for each of your tasks. Airflow will generate DAG runs from the start_date with the specified schedule_interval. Once a DAG is active, Airflow continuously checks in the database if all the DAG runs have successfully ran since the start_date. Any missing DAG runs are automatically scheduled. When you initialize on 2016-01-04 a DAG with a start_date at 2016-01-01 and a daily schedule_interval, Airflow will schedule DAG runs for all the days between 2016-01-01 and 2016-01-04. A run starts after the time for the run has passed. The time for which the workflow runs is called the execution_date. The daily workflow for 2016-06-02 runs after 2016-06-02 23:59 and the hourly workflow for 2016-07-03 01:00 starts after 2016-07-03 01:59. From the ETL viewpoint this makes sense: you can only process the daily data for a day after it has passed. This can, however, ask for some juggling with date for other workflows. For Machine Learning models you may want to use all the data up to a given date, you'll have to add the schedule_interval to your execution_date somewhere in the workflow logic. Because Airflow saves all the (scheduled) DAG runs in its database, you should not change the start_date and schedule_interval of a DAG. Instead, up the version number of the DAG (e.g. airflow_tutorial_v02) and avoid running unnecessary tasks by using the web interface or command line tools Timezones and especially daylight savings can mean trouble when scheduling things, so keep your Airflow machine in UTC. You don't want to skip an hour because daylight savings kicks in (or out). Tasks are represented by operators that either perform an action, transfer data, or sense if something has been done. Examples of actions are running a bash script or calling a Python function; of transfers are copying tables between databases or uploading a file; and of sensors are checking if a file exists or data has been added to a database. We'll create a workflow consisting of three tasks: we'll print 'hello', wait for 10 seconds and finally print 'world'.  The first two are done with the BashOperator and the latter with the PythonOperator. Give each operator an unique task ID and something to do: Note how we can pass bash commands in the BashOperator and that the PythonOperator asks for a Python function that can be called. Dependencies in tasks are added by setting other actions as upstream (or downstream).  Link the operations in a chain so that sleep will be run after print_hello and is followed by print_world; print_hello -> sleep -> print_world: After rearranging the code your final DAG should look something like: First check that DAG file contains valid Python code by executing the file with Python: You can manually test a single task for a given execution_date with airflow test: This runs the task locally as if it was for 2017-07-01, ignoring other tasks and without communicating to the database. Now that you're confident that your dag works, turn on the DAG in the web UI and sit back while Airflow starts backfilling the dag runs! You now know the basics of setting up Airflow, creating a DAG and turning it on; time to go deeper!  Address: Wibautstraat 202, 1091 GS Amsterdam, The Netherlands           Phone: +31 (0)35 672 9069           Email: signal@godatadriven.com", "links": ["http://godatadriven.com/contact", "http://godatadriven.com/careers", "http://blog.godatadriven.com", "http://godatadriven.com/business-consultancy", "http://godatadriven.com/big-data-infrastructure", "http://godatadriven.com/predictive-modelling", "http://godatadriven.com/products-overview", "http://godatadriven.com/data-discovery", "http://godatadriven.com/data-science-audit", "http://godatadriven.com/data-science-suite", "http://godatadriven.com/data-science-box", "http://godatadriven.com/divolte", "http://godatadriven.com/customers", "http://godatadriven.com/casestudy-airfranceklm", "http://godatadriven.com/casestudy-bakkersland", "http://godatadriven.com/casestudy-bol", "http://godatadriven.com/casestudy-eneco", "http://godatadriven.com/casestudy-eretail", "http://godatadriven.com/casestudy-ing", "http://godatadriven.com/casestudy-nuon", "http://godatadriven.com/casestudy-npo", "http://godatadriven.com/casestudy-schipholgroup", "http://godatadriven.com/casestudy-transavia", "http://godatadriven.com/technology-overview", "http://godatadriven.com/databricks", "http://godatadriven.com/hadoop", "http://godatadriven.com/python", "http://godatadriven.com/r", "http://godatadriven.com/spark", "http://godatadriven.com/training-overview", "http://godatadriven.com/training-schedule", "http://godatadriven.com/bigdata-training", "http://godatadriven.com/datascience-training", "http://godatadriven.com/data-science-accelerator-program", "http://godatadriven.com/meetups", "http://godatadriven.com/events", "http://blog.godatadriven.com", "http://godatadriven.com/our-story", "http://godatadriven.com/team", "http://godatadriven.com/careers", "http://godatadriven.com/media-resources", "https://blog.godatadriven.com", "http://godatadriven.com/contact", "http://godatadriven.com", "http://blog.godatadriven.com", "https://airflow.incubator.apache.org/", "https://github.com/spotify/luigi", "https://oozie.apache.org/", "https://blog.godatadriven.com/open-source-201702", "https://blog.godatadriven.com/open-source-201703", "https://blog.godatadriven.com/open-source-201704", "https://blog.godatadriven.com/open-source-201705", "https://blog.godatadriven.com/open-source-201707", "https://pythonhosted.org/airflow/tutorial.html", "https://github.com/hgrif/airflow-tutorial", "https://github.com/hgrif/airflow-tutorial", "http://conda.pydata.org/miniconda.html", "http://localhost:8080/", "https://airflow.incubator.apache.org/configuration.html", "https://airflow.incubator.apache.org/security.html", "https://github.com/apache/incubator-airflow/tree/master/scripts", "https://crontab.guru/#0_*_*_*_*", "https://jeffknupp.com/blog/2016/03/07/python-with-context-managers/", "https://airflow.incubator.apache.org/configuration.html#connections", "https://airflow.incubator.apache.org/ui.html#variable-view", "https://airflow.incubator.apache.org/tutorial.html#templating-with-jinja", "https://github.com/apache/incubator-airflow/blob/master/airflow/example_dags/example_bash_operator.py", "https://airflow.incubator.apache.org/code.html#airflow.operators.PythonOperator", "https://pythonhosted.org/airflow/tutorial.html", "https://gtoonstra.github.io/etl-with-airflow/", "https://medium.com/handy-tech/airflow-tips-tricks-and-pitfalls-9ba53fba14eb", "http://godatadriven.com/players/henk-griffioen", "http://blog.godatadriven.com/practical-airflow-tutorial", "http://blog.godatadriven.com/open-source-201707", "http://blog.godatadriven.com/cd-python-eggs-vsts", "http://blog.godatadriven.com/ldappy", "http://blog.godatadriven.com/open-source-201706", "http://blog.godatadriven.com/vendor-free-ds", "https://twitter.com/GoDataDriven", "http://godatadriven.com/business-consultancy", "http://godatadriven.com/big-data-infrastructure", "http://godatadriven.com/predictive-modelling", "http://godatadriven.com/products-overview", "http://godatadriven.com/data-discovery", "http://godatadriven.com/data-science-audit", "http://godatadriven.com/data-science-suite", "http://godatadriven.com/data-science-box", "http://godatadriven.com/divolte", "http://godatadriven.com/customers", "http://godatadriven.com/casestudy-airfranceklm", "http://godatadriven.com/casestudy-bakkersland", "http://godatadriven.com/casestudy-bol", "http://godatadriven.com/casestudy-eneco", "http://godatadriven.com/casestudy-eretail", "http://godatadriven.com/casestudy-ing", "http://godatadriven.com/casestudy-nuon", "http://godatadriven.com/casestudy-npo", "http://godatadriven.com/casestudy-schipholgroup", "http://godatadriven.com/casestudy-transavia", "http://godatadriven.com/technology-overview", "http://godatadriven.com/hadoop", "http://godatadriven.com/python", "http://godatadriven.com/r", "http://godatadriven.com/spark", "http://godatadriven.com/training-overview", "http://godatadriven.com/training-schedule", "http://godatadriven.com/bigdata-training", "http://godatadriven.com/datascience-training", "http://godatadriven.com/data-science-accelerator-program", "http://godatadriven.com/events", "http://godatadriven.com/meetups", "http://blog.godatadriven.com", "http://godatadriven.com/our-story", "http://godatadriven.com/team", "http://godatadriven.com/careers", "http://godatadriven.com/media-resources", "http://godatadriven.com/news", "http://godatadriven.com/contact", "https://cdn.xebia.com/documents/Cookies+Policy+Xebia+20170130.pdf"]}, "9": {"url": "https://concepttoclinic.drivendata.org/", "title": "", "text": "Lung Cancer Early Detection Challenge  We\u2019re calling on a global community of data scientists, engineers, designers, and researchers to build an open source software application that brings advances from machine learning into the clinic. We\u2019re not just optimizing an algorithm for a single metric\u2014we\u2019re collaborating to build tools which put AI in the hands of clinicians. In addition to pushing forward the cutting-edge of open clinical software, top contributors will be eligible for a share of $100,000 in monetary prizes generously provided by the Bonnie J. Addario Lung Cancer Foundation.          Contribute now by grabbing an issue from the project's GitHub repository and submitting a PR!        Lung cancer causes more deaths each year than any other cancer in the US, and early detection makes a big difference. Recent advances in machine learning \u2013 including open source algorithms from the 2017 Data Science Bowl \u2013 help find and interpret early signs of cancer. Using imagery from CT scans, this application will build on cutting-edge algorithms in machine learning to help clinicians identify and interpret lung nodules. Your guide to contributing throughout the challenge. All you need to start contributing is a passion for building great software and a github account.  Contributions of all sizes can earn points as they add value to the project, by building on pre-designated issues, building out new content, and building up the community. $100,000 in cash, as well as other in-kind prizes, will be available for top contributors filling key roles throughout the challenge. A live leaderboard keeps track of points earned throughout the challenge by top contributors filling key roles. Meet the amazing group of of technical, clinical, and domain experts that are helping to guide the end-to-end design of this challenge. The Addario Lung Cancer Foundation (ALCF) has set the audacious goal of making lung cancer a chronically managed disease by 2023.  Ultimately, the goal of this project is to produce a tool that draws on machine intelligence, is useful to clinicians, and makes a lasting difference in the lives of patients. We hope you will find a way to contribute!  Lung Cancer Early Detection Challenge", "links": ["https://www.drivendata.org/termsofuse/", "https://www.drivendata.org/copyrightpolicy/", "https://www.drivendata.org/privacypolicy/", "https://www.drivendata.org/", "http://www.drivendata.co/"]}, "10": {"url": "http://appsilondatascience.com/blog/rstats/2017/08/09/shiny-collections.html", "title": "shiny.collections,  Google Docs-like live collaboration in ShinyAppsilon Data Science Blog", "text": "A few weeks ago, our CTO Marek Rogala gave an engaging talk at the User Conference in Brussels. Despite being one of the last talks, he drew a crowd that filled the room and had significant viewership online. Marek\u2019s talk was entitled shiny.collections: Google Docs-like live collaboration in Shiny. In short, he went over a package we built that allows for persistence and collaboration without losing reactivity in Shiny. Users can enter information and have continuity, knowing that if they exit and return to the app, their inputs will still be there. Let\u2019s go over his talk. Recently, Shiny users have demanded more from their applications. The advent of cloud applications, especially ones like Google Docs, have accustomed users to collaboration. They expect their Shiny apps to be more than just an interactive tool for data visualisation, but rather, a production ready application that works like any other tool. Their data has to be saved automatically. Interactive collaboration is a must in some use cases. And the application must be delivered as fast as possible. You need to use a reactive database such as rethinkDB, Firebase, or mongoDB to achieve this. mongoDB isn\u2019t really a reactive database, but there are ways where it can be used as one. We are going to use rethinkDB in our case. Also, we are going to use rethinker, which is a rethinkDB driver for R. But using rethinker is not the most ideal solution, as callback do not work very well with Shiny and it is quite painful to configure to our needs. To get around this, we created our own package built upon rethinker called shiny.collections.  It lets you easily connect to your shiny app and takes care of all of the trickiness involved. During his talk, Marek showed a live demo that involved creating a chat application. Check out how to do this yourself. I reccomend watching Marek first, as he has some valuable commentary you won\u2019t find in the blog post. This example is just a simple use case of what you can achieve with shiny.collections. We\u2019ve made it a priority to make it convenient and easily integrable with other powerful tools, including DT, leaflet or rhandsontable. Our goals for the future of this package include getting it on cran and diversifying the functionality of the API. Take a look at the our package and contribute. Get in touch with us for all of you other data science needs as well.  Published 09 Aug 2017", "links": ["https://channel9.msdn.com/Events/useR-international-R-User-conferences/useR-International-R-User-2017-Conference/shinycollections-Google-Docs-like-live-collaboration-in-Shiny#comments", "https://cran.r-project.org/web/packages/rethinker/index.html", "https://appsilon.github.io/shiny.collections/", "http://appsilondatascience.com/blog/rstats/2017/07/02/shiny-chat.html", "http://appsilondatascience.com", "http://disqus.com/?ref_noscript"]}, "11": {"url": "https://blog.statsbot.co/introduction-to-imitation-learning-32334c3b1e7a", "title": "Introduction to Imitation Learning", "text": "This post is the first in the series where we will describe what Imitation Learning is. For today\u2019s article, the Statsbot team asked computer scientist Vitaly Kurin to briefly introduce Imitation Learning and outline the basics of Reinforcement Learning. Living organisms are extremely complex. Even the relatively simple ones such as flies or worms. They are able not only to operate successfully in the real world, but are extremely resilient to changes in conditions. And that\u2019s if we\u2019re not even talking about humans. We can plan ahead, we can alter our plans given new information, and we can collaborate with others and execute our plans more effectively. Science and such projects as CERN or a huge Radio Telescope in Arecibo are perfect examples of human collaboration and the art of planning. Yes, we do a lot of stupid stuff, but let\u2019s not talk about that here and instead focus on the good. Yes, we can now beat the best human in chess or Go, we can get a crazy score in Video Pinball for Atari 2600, we can even already make a pure human broke by challenging him in poker. Can we open a bottle of champagne and celebrate the triumph? I\u2019m afraid not. Yes, machine learning has recently made a significant leap forward. The combination of new Deep Learning ideas with old ones has enabled us to advance in many domains, such as computer vision, speech recognition, and text translation. Reinforcement Learning has also benefited greatly from its marriage with Deep Learning. You\u2019ve definitely heard of Deep Reinforcement Learning success such as achieving superhuman score in Atari 2600 games, solving Go, and making robots learn parkour. Though, we must admit, that operating successfully in the real world is much harder than playing Go or Space Invaders. Many of the tasks are much harder than this. Imagine a kid riding a bicycle in the middle of a crowded city center or a man driving a Porsche 911 on an autobahn at 200 miles per hour. Let\u2019s all admit that we are not there yet. The typical machine learning approach is to train a model from scratch. Give it a million images and some time to figure it out. Give it a week and let it play Space Invaders until it reaches some acceptable score. We, as humans, beg to differ. When a typical human starts to play some game he has never seen, he already has a huge amount of prior information. If he sees a door in Montezuma\u2019s Revenge, he realizes that somewhere there should lie a key and he needs to find it. When he finds the key, he remembers that the closed door is back through the two previous rooms and he returns to open it. When he sees a ladder, he realizes that he can climb it because he has done this hundreds of time already. What if we could somehow transfer human knowledge about the world to an agent? How can we extract all this information? How can we create a model out of it? There is such a way. It\u2019s called Imitation Learning. Imitation Learning is not the only name for leveraging human data for good. Some researchers also call it apprenticeship learning, others refer to it as Learning from Demonstration. From our point of view, there is no substantial difference between all of these titles and we will use Imitation Learning from now on. In order to introduce Imitation Learning, we will need to understand the basics of Reinforcement Learning first. Let\u2019s move on. It\u2019s not hard to get the general idea of the Reinforcement Learning setup. There is some agent and we want this agent to learn some task. Let\u2019s say, we have an Atari 2600 console, the agent has access to the joystick and can see what\u2019s happening on the screen. Let\u2019s say, 60 times per second we give our agent a screenshot of the game and ask him what button he wants to press. If our agent does well, he can see that his score is increasing (positive reinforcement), otherwise we can give him a penalty as a negative reward (negative reinforcement). Gradually, by trial and error, the agent starts to understand that it\u2019s better to avoid some of the actions and do those which bring him a reward. Let\u2019s make it more formal and describe the process stated above mathematically. We can describe the RL framework mentioned above (observe -> act -> get the reward and the next state) as a Markov Decision Process (MDP): where: We also need a definition of a policy function for the next section. Policy is a function, that returns an action given the state: And, actually, our final goal when solving an MDP is to learn such a policy in order to maximize the reward for our agent. Let\u2019s take an example of an MDP. The circles represent the states, arrows with green labels are actions, red labels are the rewards for actions, and the square is the terminal state. The green numeric labels are the transition probabilities. Our student starts in a state with the blue circle. He studies, but this is hard and sometimes boring. He decides to open a Facebook app and once he is there, he can either quit or continue scrolling. He then studies more and more, and finally decides to go to the pub. The state is a smaller filled circle, since now there is an element of randomness, based on the amount of knowledge the student forgets after visiting the pub. He can then either study more and pass the exam (+10 in reward), or he can go to sleep and finish the MDP right now. Since we will use DQN and related ideas in the future, let\u2019s briefly understand what is going on here. The whole approach is built upon approximating the so-called Q function and building the agent\u2019s behavior based on it. The idea of the Q function is the following: it returns you the entire expected discounted reward flow for the particular action and the particular state, given that starting from the next state we will be following our policy \ud835\udfb9. It answers the question: \u201cHow good is to press this button in this state?\u201d The Q function obeys the Bellman equation: And, finally, the Bellman principle of optimality is the following: notwithstanding what happened before, we should always take the action with the highest Q to maximize the reward flow: But how do we get such a Q function, you might ask. Let\u2019s look at an example. Imagine, you want to grab a coffee (+20 in reward) and a chocolate in a vending machine (+10 in reward). Your total reward cannot exceed thirty. Moreover, if you have taken the coffee already, it cannot be higher than 10 (the reward for chocolate) from now on. This is the idea: the Q value for the current step and action is equal to the maximum Q value for the next state (since we behave optimally) + the reward we get for the transition. The value of the quadratic objective function becomes: Q-learning itself is not new. Q-learning which uses neural networks as a function approximators is also not new (e.g. neural fitted-q iteration). A DQN paper was the first to use deep convolutional networks to solve this type of problem and introduced a couple of novelties that make the training process much more stable. First of all, experience replay. The vanilla Q-learning point is to make a step, get the reward and the next state, then update the approximation function parameters based on this transition. The DQN idea is to make the transition and save it in a \u201creplay memory\u201d\u200a\u2014\u200aan array that stores the last 10\u2076 (<insert any large number here>) transitions with the information about the reward, states before and after the transition, and if the event is terminal (game over) or not. Having this experience replay we can randomly sample mini-batches from it and learn more effectively. Another thing that makes the algorithm more stable is that DQN uses two neural networks: the first to compute the Q value for the current state and the second to compute the Q value for the next state. You can see that from the equation with the objective: two different Q functions use \ud835\udfb1 and \ud835\udfb1\u2019, respectively. Each 10,000 steps, the parameters \ud835\udfb1\u2019 are copied from the learned parameters \ud835\udfb1 and this helps a lot in increasing the stability. The problem here with using one function is that when we update the weights, both Q(s,a) and Q(s\u2019,a\u2019) increase and this might lead to oscillations or policy divergence. Using two separate networks adds a delay between an update and computation of the target Q value and reduces such cases. If you have further interest in the phenomena, read the Method section in the DQN Nature paper. Okay, everything described above sounds quite simple. If there is still something you do not understand, please, have a look at David Silver\u2019s lecture where he explains everything perfectly! Knowing all of these, can we build an True AI now? I\u2019m sorry, but we can\u2019t. There are several problems that hinder us from building an agent that will beat ByuN at StarCraft II, bring an autonomous car to the market, or give you an opportunity to buy your grandma a robot that will do the dishes for her after lunch. One of these problems is that the rewards our agent gets might be very sparse in time. Let\u2019s say, you play chess. If you lose, how do you know when you made a catastrophic move? Moreover, it\u2019s highly possible that there was not a catastrophic move, but several average ones. Another problem that is closely connected to the previous one, is the sample-efficiency problem. Or, more honestly, sample-inefficiency. Even to master a simple game such as Space Invaders might take a couple of days in-game time. It\u2019s easy to speed up learning in games since we have access to the simulators, but what if we want to learn something in real life? Unfortunately, physics is not there yet and we cannot speed up time. There is an approach that could potentially solve these problems and a bunch of others\u200a\u2014\u200aImitation Learning, as we mentioned at the beginning of this post. As we said, we, humans, rarely learn something without any prior information. Let\u2019s use this data! What should we do? The idea of Imitation Learning is implicitly giving an agent prior information about the world by mimicking human behavior in some sense. Imitation Learning will not only help us solve the sample-inefficiency or computational feasibility problems, it might potentially make the training process safer. We cannot just put an autonomous car in the middle of the street and let it do whatever it wants. We do not want it to kill humans that are around, destroy someone\u2019s property, or the equipment itself. Pretraining it on a human demonstrator\u2019s data might make the training process faster and avoid undesirable situations. Training a model requires some data. Training a Deep Learning model requires even more data. Training a Deep Reinforcement Learning model requires\u2026 Okay, you get the idea. So, this series is only partly describes what we can do with demonstration data. The main point of all of this is to call for human demonstration datasets, because we do not have many, unfortunately, up to this moment. Okay, we should stop here. The key points of this post are: In the next chapter we will write more about Behavior Cloning\u200a\u2014\u200athe simplest approach to an RL problem that can leverage the human demonstration data. Stay tuned!", "links": ["https://blog.statsbot.co?source=logo-lo_88e61bf948c0---cfc9f21a543a", "https://twitter.com/statsbotco", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.statsbot.co%2Fintroduction-to-imitation-learning-32334c3b1e7a", "https://blog.statsbot.co", "https://blog.statsbot.co/analytics/home", "https://blog.statsbot.co/datascience/home", "https://blog.statsbot.co/design/home", "https://blog.statsbot.co/startups/home", "https://blog.statsbot.co/bots/home", "https://blog.statsbot.co/news/home", "https://blog.statsbot.co/statsbot-digest-b0d7372f842a", "http://statsbot.co?utm_source=blog&utm_medium=navigation&utm_campaign=robot_face", "https://blog.statsbot.co/search", "https://blog.statsbot.co/@yobibyte?source=post_header_lockup", "https://blog.statsbot.co/@yobibyte?source=post_header_lockup", "https://atlas.cern/discover/about", "http://statsbot.co?utm_source=blog&utm_medium=article&utm_campaign=imitation_learning", "https://en.wikipedia.org/wiki/CERN", "https://en.wikipedia.org/wiki/Arecibo_Observatory", "https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning/", "https://deepmind.com/research/alphago/", "https://www.theverge.com/tldr/2017/7/10/15946542/deepmind-parkour-agent-reinforcement-learning", "http://www.ceva-dsp.com/ourblog/wp-content/uploads/sites/3/2016/04/AlphaGo-Lee-Se-dol.png", "https://simple.wikipedia.org/wiki/File:Rl_agent.png", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf", "https://link.springer.com/article/10.1007/BF00992698", "https://pdfs.semanticscholar.org/2820/01869bd502c7917db8b32b75593addfbbc68.pdf", "https://link.springer.com/article/10.1007/BF00992699", "http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html?foxtrotcallback=true", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "http://wiki.teamliquid.net/starcraft2/ByuN", "http://toyoutheartist.co.uk/technique/imitation/", "https://mitpress.mit.edu/books/reinforcement-learning", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html", "https://arxiv.org/abs/1312.5602", "https://www.nature.com/nature/journal/v518/n7540/full/nature14236.html", "https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner", "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/google-analytics-audit-checklist-and-tools-fca7df2f2e7a", "https://blog.statsbot.co/google-analytics-audit-checklist-and-tools-fca7df2f2e7a", "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co/tagged/imitation-learning?source=post", "https://blog.statsbot.co/tagged/data-science?source=post", "https://blog.statsbot.co/tagged/machine-learning?source=post", "https://blog.statsbot.co/tagged/ai?source=post", "https://blog.statsbot.co/tagged/reinforcement-learning?source=post", "https://blog.statsbot.co/@yobibyte?source=footer_card", "https://blog.statsbot.co/@yobibyte", "https://github.com/yobibyte/yobiblog", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "12": {"url": "https://axibase.github.io/atsd-use-cases/DataShorts/Dollar-EX/", "title": "", "text": "Data Source: Federal Reserve Economic Data (FRED) Visualizations: ChartLab Structured Query Language: SQL Console from Axibase Download the Community Edition of Axibase Time Series Database here Data is aggregated daily in visualizations, and annually in SQL queries. Favorable dollar exhange rates are shown in green while less favorable exchange rates are shown in red, based on the overall 5-year average exchange rate. Each section is accompanied by a brief analysis to give the data context. Using computed value settings thresholds have been calculated using the underlying data, the upper 25th percentile of a given exchange rate is considered favorable while the lower 25th percentile is considered less than favorable. The data is coded with an ID which is provided in the index as well. Open any ChartLab display below to navigate through time, or change the metric to reflect the desired exchange rate. Europe Asia North America As the amount of debt carried by various EU member countries grew to increasingly worrisome heights, the Euro plunged in 2015  losing ground against the dollar. Further, as oil prices slumped the European Central Bank began to compensate by launching a program of government bond purchases. The combination of these factors and the growth of the dollar post-recession have all contributed to the currently favorable exchange rate enjoyed by the dollar. Query 1.1 Table 1.1 Figure 1.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index During the worst of the European Debt Crisis which saw European Union per capita debt grow at unprecedented rates across the continent, EU investors sought refuge in the stability of the Swiss Franc, backed by the world-renowned Swiss banking system. Further, the Swiss National Bank removed the peg to the Euro unexpectably in 2015 causing a huge surge in 2015, clearly visible in the visualization below as a sudden change from a favorable exchange rate for dollar holders to an all-time low for the observed time period. Query 1.2 Table 1.2 Figure 1.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Largely influenced by the divisive Brexit vote, the value of the Pound has remained relatively low since the referendum in late June of 2016, which aligns perfectly with the Pound\u2019s slight depreciation in value against the dollar on the chart below. Query 1.3 Table 1.3 Average Exchange Rate: 1.50 Figure 1.3   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index After purchasing trillions of Yen worth of Japanese national debt from the private sector three years ago, the Central Bank of Japan has been watching the gradual decrease in value of the Yen against the dollar. Most analysts, see this fall as controlled and predict a bright future for the Yen as the Central Bank can no longer afford to collect more debt than they already have. However, the Yen\u2019s rise much also be managed as the sudden appreciation of the currency could result in an increase in loan defaults because industries which have taken loans at current Yen prices would be paying them back at a much higher rate. Query 2.1 Table 2.1 Figure 2.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index To prevent dramatic internal inflation, the People\u2019s Bank of China closely regulates local, or onshore, trading of the Yuan by fixing the price each day. Although independent officially, the international, or offshore, price of the Yuan usually remains fairly close to its onshore value. Additionally, the PBoC carefully controls the outflow of capital and often hedges their own currency by maintaining positions in international currencies. Query 2.2 Table 2.2 Figure 2.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index The Hong Kong Dollar is officially pegged to the value of the United States Dollar so any change in relative  value is planned. The \u201cMiracle of the Orient\u201d continues to be one of the most dominant financial markets in the world. Query 2.3 Table 2.3 Figure 2.3   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index One of the financial centers of the world, Singapore is home to the second-busiest port in the world and one of the largest oil refining industries worldwide as well. With one of the highest per capita GDP values, Singapore has cemented its place of international importance despite inexplicably loose financial policy whereby the Monetary Authority of Singapore does not regulate their currency\u2019s value by adjusting interest rates, as is common with most central banking systems. Query 2.4 Table 2.4 Figure 2.4   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Relative to the USD, the Canadian dollar has been making a bit of a resurgence in 2017 as United States GDP numbers missed its targets in quarter two. Although dependant on the price of oil due to their large natural reserves, the diverse Canadian economy is one of the few globally-active nations with a significant trade surplus, although that too have been reduced in recent years. Query 3.1 Table 3.1 Figure 3.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Boasting the highest per capita income of Latin America, the Mexican economy has continued to demonstrate healthy growth over the last several decades. North America\u2019s number one producer of automobiles also enjoys a $46 billion trade surplus with the United States, and is the 9th ranked owner of United States government debt. This dependence on America affords Mexico the ability to spur dramatic internal growth that has led to the creation of a booming electronics sector, but at the cost of a  certain amount of reliance on the United States as roughly half of the country\u2019s imports and exports originate or end up  on U.S. soil. Query 3.2 Table 3.2 Figure 3.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index", "links": ["https://github.com/axibase/atsd-use-cases", "https://fred.stlouisfed.org/categories/94", "https://apps.axibase.com", "https://github.com/axibase/atsd/tree/master/api/sql", "https://axibase.com", "https://axibase.com/products/axibase-time-series-database/", "https://github.com/axibase/atsd/blob/master/installation/README.md#installation", "https://axibase.com/products/axibase-time-series-database/visualization/widgets/configuring-the-widgets/", "https://apps.axibse.com", "https://apps.axibase.com/chartlab/424eb6b2/5/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/9/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/6/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/4/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/2/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/3/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/8/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/7/#fullscreen", "https://github.com/axibase/atsd-use-cases", "https://github.com/axibase", "https://pages.github.com"]}, "13": {"url": "http://byteacademy.co/blog/overview-NLG", "title": "", "text": "NLG (Natural Language Generation), a subfield of Artificial Intelligence, is a hot topic in the technology news today. We hear a lot about AI that can soon replace writers and journalists beginning the era of machine creativity. But, what\u2019s all this fuss about? In this article, we unveil what NLG really is and show that it can bring a lot of benefits to businesses and consumers. In a nutshell, NLG is a sub-field of NLP (Natural Language Processing) that studies methods of automatic transformation of structured data into a human-readable text. In practice, there are two major types of NLG applications: template-based NLG and advanced NLG. \u00a0 Template-based NLG is the simplest solution that uses templates with canned text and placeholders to insert data into them. Such systems heavily rely on hard-coded rules, which makes them less flexible than advanced NLG. Since template-based NLG tools have a limited number of templates and require special data representations, they can not be easily reused across different projects and business use cases. \u00a0 Advanced NLG tools are more flexible thanks to the use of supervised and unsupervised Machine Learning (ML). Rather than tying down structured data to the Procrustean bed of templates, advanced NLG uses neural networks that learn morphological, lexical, and grammar patterns from large corpora of written language. Soft probabilistic methods used in the advanced NLG algorithms allow predicting the likelihood of one word appearing after another, and correcting language errors, such as misspellings. ML algorithms used in the advanced NLG are also better in dealing with new words and expressions not included in the original training samples. \u00a0 Modern NLG service providers such as Narrative Science and Automated Insights prefer advanced NLG methods because they allow creating rich data-driven models that produce intelligent insights from data. These algorithms are much more skillful in making right word choices and writing narratives that reflect intentions and business needs of the NLG users. As an added bonus, advanced NLG models can preprocess and analyze data which makes them not just translators of structured data into text, but automatic analysts able to provide actionable insights. \u00a0 Despite the fact that NLG methods have been used since the 1970s, they got a powerful momentum only recently and thanks to the AI/ML revolution. Today, many startups offer cloud-based NLG services to businesses. NLG is also gaining traction in mass media and journalism. Major American newspapers are already experimenting with the automatic storytelling. For example, in 2016 the Washington Post unveiled its automatic storytelling AI named Heliograph AI. Heliograph was used in the coverage of Rio Olympics and the US Presidential election in 2016. \u00a0 Leveraging data mining techniques and ML models the machine reporter can convert structured statistical data, diagrams, graphs, weather forecasts and other data-rich content into excellent descriptive reports that sound if though they were written by the professional reporters. But, isn\u2019t this dangerous for journalism as a profession? Proponents of automatic storytellers say that they actually free up much time for reporters to add analysis and real insights to stories rather than spending countless hours publishing news and descriptive reports[i]. NLG tools may be used in other innovative ways as well: Benefits of NLG, however, go beyond journalism. There is a growing demand for NLG services among major companies. For example, Quill, an NLG system developed by Narrative Science, is used by such companies as Deloitte, Groupon, and Credit Suisse[iv]. These companies opt for NLG solutions for a reason. Growing acceptance of NLG among businesses makes it a promising field to study. If you want to learn more about NLG, \u00a0Byte Academy offers a Natural Language course that covers Natural Language Processing and Natural Language Generation. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0  Your Name (required)    Your Email (required)    \u00a0 \u00a0 References: [i] \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 WashPost PR Blog (August 5, 2016). The Washington Post Experiments With Automated Storytelling to Help Power 2016 Rio Olympics Coverage. WashPost PR Blog. Retrieved from https://www.washingtonpost.com/pr/wp/2016/08/05/the-washington-post-experiments-with-automated-storytelling-to-help-power-2016-rio-olympics-coverage/?utm_term=.bf63b03c4aeb [ii] \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dayan, Zohar (2015). Hearst, USA Today Sports, & Viralnova Partner With Wibbitz For Video Strategy. Wibbitz Blog. Retrieved from http://blog.wibbitz.com/wibbitz-partners-hearst-usa-today-sports-group-and-viralnova-to-expand-video-strategy [iii] \u00a0\u00a0\u00a0\u00a0\u00a0 Keohane, Joe (2017). What News-Writing Bots Mean for the Future of Journalism. Wired. Retrieved from https://www.wired.com/2017/02/robots-wrote-this-story/ [iv] \u00a0\u00a0\u00a0\u00a0 Narrative Science. Turn Your Data Into Better Decisions With Quill. Retrieved from https://narrativescience.com/Platform info.in@byteacademy.co Byte Dev Licensed by the State of New York, New York State Education Department \u00a9 2017 Byte Academy LLC.  All rights reserved.", "links": ["http://byteacademy.co", "http://byteacademy.co/program/", "http://byteacademy.co/courses/", "http://byteacademy.co/data-science-mini-courses/", "http://byteacademy.co/upcoming-classes/", "http://byteacademy.co/students/", "http://byteacademy.co/hiring/", "http://byteacademy.co/events/", "http://byteacademy.co/blog/", "http://byteacademy.co/india/", "https://byteacademy.fluidreview.com", "http://feeds.feedburner.com/ByteAcademy", "http://byteacademy.co/category/data-science/", "http://byteacademy.co/category/career/", "http://byteacademy.co/category/fintech/", "http://byteacademy.co/category/medtech/", "http://byteacademy.co/category/programming-tips/", "http://byteacademy.co/category/startup/", "http://byteacademy.co/category/student-stories/", "http://byteacademy.co/category/blockchain/", "http://byteacademy.co/category/current-events/", "http://byteacademy.co/category/diversity/", "http://byteacademy.co/category/events/", "http://byteacademy.co/category/finovate/", "http://byteacademy.co/category/light-reads/", "http://byteacademy.co/category/millennials/", "http://byteacademy.co/category/quant/", "http://byteacademy.co/2017/", "http://byteacademy.co/2016/", "http://byteacademy.co/2015/", "http://byteacademy.co/category/data-science/", "http://byteacademy.co/blog/artificial-intelligence-future", "https://www.washingtonpost.com/pr/wp/2016/08/05/the-washington-post-experiments-with-automated-storytelling-to-help-power-2016-rio-olympics-coverage/?utm_term=.bf63b03c4aeb", "http://www.reddit.com/submit?url=http://byteacademy.co/blog/overview-NLG", "http://www.tumblr.com/share?v=3&u=http://byteacademy.co/blog/overview-NLG&t=Overview of Natural Language Generation (NLG)", "http://byteacademy.co/blog/artificial-intelligence-future", "http://byteacademy.co/blog/artificial-intelligence-future", "http://byteacademy.co/blog/data-science-podcasts", "http://byteacademy.co/blog/data-science-podcasts", "http://byteacademy.co/blog/data-science-fintech/", "http://byteacademy.co/blog/data-science-fintech/", "http://byteacademy.co/financial-aid/", "http://byteacademy.co/upcoming-classes/", "http://byteacademy.co/contact-us/", "http://byteacademy.co/corporate-training/", "http://byteacademy.co/faq/", "http://byteacademy.co/press-page/", "http://byteacademy.co/careers-at-byte/", "http://byteacademy.co/brand-ambassador/", "https://www.meetup.com/Byte-Academy-Finance-and-Technology-community/", "http://www.bytedev.co", "https://www.facebook.com/byteacademy/", "https://twitter.com/ByteAcademyCo", "https://www.linkedin.com/edu/school?id=171001", "https://github.com/ByteAcademyCo", "https://www.instagram.com/byteacademy/", "https://www.meetup.com/Byte-Academy-Finance-and-Technology-community/", "https://www.quora.com/topic/Byte-Academy", "https://www.youtube.com/channel/UCrMcJALnO748TSK27bZQzSg", "http://byteacademy.co/privacy-policy/"]}, "14": {"url": "http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "title": "", "text": "", "links": ["http://www.getrevue.co/?ref=Revue+Profile", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profileimage&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profiledate&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profilename&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Mm6Kn?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Mm6Kn?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.datasciencecentral.com/profiles/blogs/automated-machine-learning-for-professionals?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lXBy8?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lXBy8?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.nature.com/articles/s41467-017-00181-8?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/d0nAm?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/d0nAm?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/4mO6D?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/4mO6D?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/eK78N?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/eK78N?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://sourcedexter.com/amazing-tensorflow-github-projects/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xejEQ?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xejEQ?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://techblog.appnexus.com/a-keras-multithreaded-dataframe-generator-for-millions-of-image-files-84d3027f6f43?gi=79cece23ee37&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/6nA72?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/6nA72?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://blog.revolutionanalytics.com/2017/08/a-modern-database-interface-for-r.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/5kyK3?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/5kyK3?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://ruder.io/deep-learning-nlp-best-practices/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/7yxNV?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/7yxNV?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://hyperparameter.space/blog/when-not-to-use-deep-learning/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/8A1NP?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/8A1NP?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278?gi=8e0386afa61e&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/1bKwn?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/1bKwn?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://adeshpande3.github.io/How-I-Used-Deep-Learning-to-Train-a-Chatbot-to-Talk-Like-Me?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lX0x4?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lX0x4?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.youtube.com/channel/UC0n76gicaarsN_Y9YShWwhw/playlists?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/qYVyK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/qYVyK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.youtube.com/watch?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest&v=0fhUJT21-bs", "http://rev.vu/Nkeov?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Nkeov?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%239%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "https://www.getrevue.co/?utm_source=Data Science Digest&utm_medium=email&utm_content=footerlink&utm_campaign=Issue", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=forwarded&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%239%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "http://www.getrevue.co/?ref=Revue+Profile"]}, "15": {"url": "http://rpsychologist.com/d3/bayes/", "title": "", "text": "Created by Kristoffer Magnusson The visualization shows a Bayesian two-sample t test, for simplicity the variance is assumed to be known. It illustrates both Bayesian estimation via the posterior distribution for the effect, and Bayesian hypothesis testing via Bayes factor. The frequentist p-value is also shown. The null hypothesis, H0 is that the effect \u03b4 = 0, and the alternative H1: \u03b4 \u2260 0, just like a two-tailed t test. You can use the sliders to vary the observed effect (Cohen's d), sample size (n per group) and the prior on \u03b4. The prior on the effect is a scaled unit-information prior. The black, and red circle on the curves represents the likelihood of 0 under the prior and posterior. Their likelihood ratio is the Savage-Dickey density ratio, which I use here as to compute Bayes factor. The p-value is the traditional p-value for a two-sample t test with known variance (i.e. a Z test).        HDI is the posterior highest density interval, which in this case is analogous a credible interval. And CI is the traditional frequentist confidence interval.  Check out Alexander Etz's blog series \"Understanding Bayes\" for a really good introduction to Bayes factor. Fabian Dablander also wrote a really good post, \"Bayesian statistics: why and how\", which introduces Bayesian inference in general. If you're interesting in an easy way to perform a Bayesian t test check out JASP, or BayesFactor if you use R. Interactive visualization of Cohen's d effect size Interactive visualization of statistical power and significance testing Interactive visualization of Confidence Intervals Have any suggestion? Or found any bugs? Send them to me, my contact info can be found here. Designed and built by Kristoffer Magnusson. Built with D3.js, jStat and Bootstrap.", "links": ["http://rpsychologist.com", "http://rpsychologist.com", "https://twitter.com/krstoffr", "https://se.linkedin.com/pub/kristoffer-magnusson/b5/133/1b6", "https://twitter.com/share", "http://alexanderetz.com/understanding-bayes/", "http://blog.efpsa.org/2015/08/03/bayesian-statistics-why-and-how/", "https://jasp-stats.org", "http://bayesfactorpcl.r-forge.r-project.org", "http://rpsychologist.com/d3/cohend/", "http://rpsychologist.com/d3/NHST/", "http://rpsychologist.com/d3/CI/", "http://rpsychologist.com/tag/d3js.html", "http://rpsychologist.com/about", "http://rpsychologist.com", "http://d3js.org/", "http://jstat.org/", "http://getbootstrap.com/", "http://twitter.com/krstoffr", "https://github.com/rpsychologist"]}, "16": {"url": "https://blog.datasyndrome.com/generalists-dominate-data-science-f01882f25347", "title": "Generalists Dominate Data\u00a0Science", "text": "Analytics products and systems are best built by small teams of generalists. Large teams of specialists become dominated by communication overhead, and the effect of \u201cChinese whispers\u201d distorts the flow of tasks and stagnates creativity. Data scientists should develop generalist skills to become more efficient members of a data science team. Building data products takes a team covering a broad and diverse skillset. From the customer representative at one end, to the operations engineer at the other, the spectrum of roles in a product analytics team looks like this: Large companies often fill each role with a pair of shoes, resulting in a twelve person team like the one below. The problem with this setup is that it becomes much more difficult to achieve consensus and to perform any task that spans roles. And in data science, most tasks span roles. To look at a particular example, lets focus on the creation of a chart as part of a data product. To begin, a product manager creates a specification, then an interaction designer mocks up the chart, handing it off to a data scientist to fill with data (and hopefully to explore the data and find a chart worth producing), then a back-end engineer to setup an API to grab that data, a front-end web developer to create a web page using the data that matches the mock, and an experience designer to ensure the entire thing feels right and makes sense. Charts take iteration, so this cycle of communication could happen repeatedly for each chart. You can see how communication overhead starts to predominate. A meeting of six people is a full-blown, formal meeting. It is hard to get things done in formal meetings. In the next figure, we see how a data product team might be composed of four generalists: a data engineer, a data scientist/back-end developer, a designer who can build front ends and a product manager that can write marketing copy and cut deals. This is how a startup team would span the skill spectrum, and you can probably see how this makes them more efficient. Revisiting the chart example, creating a chart becomes a collaboration between the product manager, a designer who codes, and a data scientist. This is the kind of ad hoc meeting of 2\u20133 people where \u201cshit gets done\u201d efficiently. This group will be more efficient than the six man group. Put another way: this small team will kick the large team\u2019s ass. In the big company system, sometimes the only way to get anything done efficiently is to go \u201cguerilla generalist\u201d and work with other generalists to cut people out of the chain. This is bad politically, and is part of what drives effective people from big companies. We\u2019ve shown that small teams generalists outperform large teams of specialists. In fact, generalist skills are something every data scientists should work to develop. That doesn\u2019t mean you can\u2019t specialize, but should combine specialization with generalization in order to develop \u201cT-shaped skills.\u201d The T-shaped employee is one that can lend deep expertise across projects while fulfilling multiple roles in his own. It takes time to develop general skills, and that is why the path to becoming a data scientist is not a six month bootcamp, but a ten year journey. Along this path, remember to try to be T-Shaped! Need help building an analytics product or platform? The Data Syndrome team of data scientists and data engineers is available to build your data products and systems as a service. We also offer training in Agile Data Science for all members of data science teams.", "links": ["https://blog.datasyndrome.com?source=logo-lo_6f7c758f7fca---500653fb51a1", "https://twitter.com/datasyndrome", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.datasyndrome.com%2Fgeneralists-dominate-data-science-f01882f25347", "https://blog.datasyndrome.com/@rjurney?source=post_header_lockup", "https://blog.datasyndrome.com/@rjurney?source=post_header_lockup", "https://personalmba.com/communication-overhead/", "https://en.wikipedia.org/wiki/Chinese_whispers", "https://en.wikipedia.org/wiki/T-shaped_skills", "http://datasyndrome.com", "http://datasyndrome.com/training", "https://blog.datasyndrome.com/tagged/data-science?source=post", "https://blog.datasyndrome.com/tagged/agile?source=post", "https://blog.datasyndrome.com/tagged/generalists?source=post", "https://blog.datasyndrome.com/tagged/analytics?source=post", "https://blog.datasyndrome.com/tagged/software-engineering?source=post", "https://blog.datasyndrome.com/@rjurney?source=footer_card", "https://blog.datasyndrome.com/@rjurney", "https://blog.datasyndrome.com?source=footer_card", "https://blog.datasyndrome.com?source=footer_card", "https://blog.datasyndrome.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "17": {"url": "https://github.com/axibase/atsd-use-cases/tree/master/FED_FORDSR#the-average-american-debt-profile", "title": "", "text": "Debt is a complicated concept. After the sub-prime mortgage crisis of the late 2000s, modern Americans are all too familiar with the problems of irresponsible spending on credit. Student loan recipients who queue up to drop off another application for a job in a field they did not study are quick to point to the trappings of deficit spending as a means of wealth creation. Politicians and voters on both sides of the aisle point to the ever-growing United States Government debt with anxiety for the future. And yet despite all the doom and gloom, the American financial system is one of the most stable and robust in the world, in no small part thanks to ingenious monetary policy and hegemonic economic position organized over the entire course of the country's history, modern American consumers are among the wealthiest on the planet. The United States Federal Reserve is the central banking system of the United States, responsible for monitoring the global financial climate and enacting policy that supports the American economy and American consumers. They maintain a number of statistics about these consumers and their monetary practices to better inform their decisions and practices. Provided by the Federal Reserve, this dataset must be correctly parsed during import. The quarterly date format needs to be converted into a monthly format that ATSD can interprete (Q/q letter is not supported). We also need to discard metadata lines contained in the multi-line header. This can be accompilshed with a schema-based parser that provides granular control over the document's rows and columns using RFC 7111 selectors and Javascript: Script 1.1 For step-by-step instructions on data customization with schema based parsing, see this support tutorial. The Financial Obligation Ratio (FOR) is an estimate of the ratio of required debt payments to disposable income. This is a broad calculation and includes all kinds of debt:  mortgage payments, credit cards, property tax and lease payments. Each of these metrics can be expanded further to include associated costs, such as homeowner's insurance for example. The Federal Reserve releases this number each quarter. Figure 1.1   Use the dropdown menus at the top of the visualization screen to navigate through time, selecting the starttime and endtime values to observe a desired period. The data can also be queried with a structured query language in the SQL Console. The data will be aggregated annually, derived from the average value of each quarter within a given year: Query 1.1 Table 1.1 All values are shown as a percent of one hundred, where the whole is representative of the total income of the average person. The Debt Service Ratio (DSR) is more specific than the Financial Obligation Ratio in that it typically does not include non-essential debt payments. Here, it has been parsed into two categories, mortgage debt and consumer debt. These numbers represent the average percent of a person's earned salary each month which much be used to make the required payments associated with consumer credit and mortgage. Typically the DSR is an initial calculation performed to determine a person's eligibility to receive a mortgage. A DSR value of less than 48% is generally preferred, meaning that with a particular mortgage plus other credit obligations at least 52% of a person's gross monthly earning would still be available to them after making the required payments. Figure 2.1   Query 2.1 Table 2.1 Because the FOR value includes the DSR value plus additional non-essential credit values, and the DSR value is parsed into both consumer and mortgage related debt, these three values can be shown in a new visualization that creates a typical consumer profile of the average American. By using the calculated value setting shown below, additional data not specifically included in the set can be displayed: Script 2.1 Shown below is the debt profile of the average American consumer from 1980 to 2017, navigate through time using the dropdown menus at the top of the screen to select a desired span of time and compare how bearing debt has changed over the course of the last three decades. Figure 3.1   The visualization can also be organized to show the amount of each type of debt as it relates to the others: Figure 3.2   Additionally, these values can be compared on an annual basis as shown in the visualization below: Figure 3.3   To view the distribution of these values across time, a histogram is shown below: Figure 3.4   In the following box diagram, explore time with the dropdown menus at the top of the visualization screen. The visualization shows the distribution of debt values as a percentage of total income, with the initial time period set to include the entire data set: Figure 3.5   The following SQL query will detail the above visualizations in one table, displaying averaged annual values of each component described above: non-essential credit payments, mortgage credit payments, and consumer credit payments, as well as the Financial Obligation Ratio (FOR), or total debt obligations. Query 3.1 Table 3.1 The above dataset can illuminate a number of features of the American economy and a number of characteristics of the average American consumer. While modern Americans are quick to denounce the zeitgeist of living outside of one's means, the data shows that in fact, the amount of debt carried by the average American is on par with or even lower in some cases than that of his 1980's counterpart. In fact, the only metric which has demonstrated a legitimate increase in value over the last several decades has been the roughly one percent increase in non-essential credit holdings by the average consumer. According to data from the Economic Research Department of the Saint Louis Branch of the Federal Reserve, the 2015 US median household income was $56,516 per year in 2015 USD. This number can be applied to the above table and visualized in ChartLab to create more comprehensive data. Figure 3.6   The above visualization aggregates the values from Table 3.1 based on a time period of the user's selection. Use the dropdown menu at the top of the screen to select the aggregation period. The initial visualization shows the average values for each metric over the entire period of time in 2015 USD by obligation amount per quarter. The following query summons the same data shown above, but further parses it to show annual average monthly payments instead of quarterly values in 2015 USD for a person making the 2015 median United States income of $56,516 a year. Query 4.3 Table 3.2 As it turns out, the idea that your parents paid less for their house than you will is only true in absolute terms. When compared with current numbers and controlled for inflation, the average 2017 consumer will pay roughly the same portion of their income towards a place to hang their hat up as the average 1980 consumer. The Federal Reserve is able to pull certain levers of power from the Eccles Building in Washington, D.C. such as printing more money, or raising and lowering interest rates to cope with inflation. However, all of these are reactionary measures meant to create small changes that have a butterfly effect over time. Ultimately, the machinations of the Board of Governers have always be something opaque and esoteric to the average man, leading to many people denouncing the Federal Reserve System entirely, occasionally opting for a return of the gold standard or leveling accusations of wrong-doing. However, after reviewing the data above, it seems that at least on a consumer level, the average American actually has more today than they would have had thirty years ago, or even just five years ago. Of course, the Federal Reserve isn't completely responsible for the wise consumer choices made in the current decades, but monetary policy enacted by the various branches of the Federal Reserve are responsible for maintaining the economic conditions that Americans, and consumers the world over, have come to expect from the United States economy.", "links": ["https://github.com/", "https://github.com/axibase/atsd-use-cases/pull/86", "http://www.usdebtclock.org/", "https://www.federalreserve.gov/", "https://www.federalreserve.gov/datadownload/Download.aspx?rel=FOR&series=91e0f9a6b8e6a4b1ef334ce2eaf22860&filetype=csv&label=include&layout=seriescolumn&from=01/01/1980&to=12/31/2017", "https://axibase.com/products/axibase-time-series-database/writing-data/csv/", "https://apps.axibase.com/chartlab/842f1dd9/#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql", "https://apps.axibase.com/chartlab/85522dd3/#fullscreen", "https://apps.axibase.com/chartlab/f25de723/#fullscreen", "https://apps.axibase.com/chartlab/f25de723/3/#fullscreen", "https://apps.axibase.com/chartlab/81ea0ea0/#fullscreen", "https://apps.axibase.com/chartlab/9f74c179/#fullscreen", "https://apps.axibase.com/chartlab/20ff0ade/#fullscreen", "https://fred.stlouisfed.org/series/MEHOINUSA646N", "https://research.stlouisfed.org/", "https://www.stlouisfed.org/", "https://apps.axibase.com", "https://apps.axibase.com/chartlab/da132e01/11/#fullscreen", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "18": {"url": "http://tech.marksblogg.com/billion-nyc-taxi-rides-aws-ec2-p2-16xlarge-brytlytdb.html", "title": "", "text": "I have 14 years of hands-on build and consulting experience with clients in the UK, Ireland & Germany. I've done both back- and frontend work for Bank of America Merrill Lynch, Blackberry, Bloomberg, British Telecom, Ford, Google, ITV, News UK, Pizza Hut, Royal Bank of Scotland, Royal Mail, T-Mobile, Vertu (Nokia subsidiary at the time), Williams Formula 1 Team and Xerox. I hold both a Canadian and a British passport. My CV & my LinkedIn profile.         Home           | Benchmarks           | Archives            | Atom Feed  Posted on Fri 28 July 2017 BrytlytDB is an in-GPU-memory database built on top of PostgreSQL. It's operated using many of PostgreSQL's command line utilities, it's wire protocol compatible so third-party PostgreSQL clients can connect to BrytlytDB and queries are even parsed, planned and optimised by PostgreSQL's regular codebase before the execution plan is passed off to GPU-optimised portions of code BrytlytDB offer. Clustering works right out of the box, GPU-powered JOINs are supported, Stored Procedures are fully functional, Deep- and Machine Learning workloads via Torch are supported and BI visualisation software in the form of SpotLyt is included with BrytlytDB as well. The feature list is too long for one blog to cover so in this post I'll just be taking a look at how performant BrytlytDB's OLAP functionality is. In this benchmark I'll see how well 32 Tesla K80 GPUs spread across two EC2 instances perform when querying 1.1 billion taxi trips. I'll be using the same dataset I've used to benchmark Amazon Athena, BigQuery, ClickHouse, Elasticsearch, EMR, kdb+/q, MapD, PostgreSQL, Redshift and Vertica. I have a single-page summary of all these benchmarks for comparison. For this benchmark I'll be using two p2.16xlarge EC2 instances running Ubuntu 16.04.2 LTS in Amazon Web Services' eu-west-1a region in Ireland. Each machine has 8 Nvidia K80  cards which have 2 GPUs each, 64 virtual CPUs and 732 GB of memory. There's also 20 Gbit/s of networking capacity available to each instance. Below are the specifications of the compute capabilities available each one of the EC2 instances. This is the layout of the GPUs available on one of the two EC2 instances. The first machine has an IP address of 52.214.237.134 and has three roles: Global Transaction Manager, Coordinator and host to the first 16 data nodes. The second machine has an IP address of 34.250.232.38 and has two roles: it is also a coordinator and hosts the second set of 16 data nodes. TCP port 5432 is open between the two machines for communicating via PostgreSQL's wire protocol, TCP port 7777 is open for global transaction manager communication and ports 20,000 through to 20,0031 are open for data node communication. In addition to the default 20 GB EBS volumes on each EC2 instance there are six 500 GB General Purpose SSDs attached which offer a baseline of 100 IOPS each and can burst to 3,000 IOPS if need be. These drives are setup in a RAID 0 configuration on each instance. Before setting up the RAID array I'll install a few dependencies. This was run on both EC2 instances. Below are the commands used to setup the RAID array on each instance. Here's what the RAID layout looked like after it was setup. This RAID 0 setup offers a partition with a capacity of 3.2 TB on each instance: I've run the following to download BrytlytDB's install script. BrytlytDB is commercial software so I cannot divulge the URL I pulled this from at this time. The following was run on both EC2 instances. I've then edited the install script on the first instance with the following instance-specific values: And the install script on the second instance was edited with the following instance-specific values: With those changes in place I then ran the install script on both EC2 instances. The install script conducts the following: Once that's done I can form the cluster with the following registration script. The following was run on the first EC2 instance. The second registration script run on the other EC2 instance is identical with the first with the exception of the coordinator line pointing to 52.214.237.134 instead. For this benchmark I've downloaded and decompressed one half the 500 GB of CSV data I created in my Billion Taxi Rides in Redshift blog post onto each EC2 instance. The data sits across 56 files across both machines but for the quickest load time I want to create 32 reasonably equally-sized CSV files and load 16 files on each EC2 instance simultaneously. The 32 files will pair up with the 32 GPUs available across the cluster and should allow for the best utilisation of the GPU resources when running queries. Here is half of the original 500 GB data set on the first EC2 instance: And this is the other half on the second instance: On each instance I'll decompress the GZIP files. Below took about four minutes to complete on each instance. Then I'll concatenate the ~560 million lines of CSV data on each instance and break it up into files of 35 million lines each. The above completed in 8 minutes on each EC2 instance. I'm now left with 16 CSV files on the first EC2 instance: And 16 files on the other EC2 instance: I'll then connect to the coordinator node on each EC2 instance and setup BrytlytDB's gpu_manager_fdw extension and foreign data wrapper. I can then create the trips table. The above sets the maximum row count per GPU device at 38 million and creates an index on the cab_type column. The cab_type column has very low cardinality so the indexing will be less beneficial versus a column with a higher cardinality of values. Some of the data types used in the above table aren't the types I normally use in my benchmarks. The reason for this is that there is still limited data type support available in this early version of BrytlytDB. I've had to substitute DATETIME with DATE which will truncate the timestamps in the dataset to just the date rather than the full date and time. There is no SMALLINT support yet so I've had to use the larger INT type as an replacement for those fields. DECIMAL types aren't yet supported so I'll be using DOUBLE PRECISION as a replacement there. The above replacements could speed up or slow down the queries I benchmark with so I'm hoping to do another benchmark when data type support is widened in the future. With the table created I'll launch 32 simultaneous load jobs across the two EC2 instances to load the data into BrytlytDB's trips table. This is the import script I ran on the first EC2 instance: And this is the load script for the second instance. The first EC2 instance loaded its half of the dataset in 1 hour, 12 minutes and 21 seconds. The second instance loaded its half in 1 hour, 9 minutes and 57 seconds. After the data was loaded in I could see the PostgreSQL data directory filled up with reasonably evenly-sized data folders. Here's what they look like on the first EC2 instance: I was then able to use PostgreSQL's CLI tool to make sure I can see the table and all 1.1 billion records. The times quoted below are the lowest query times seen during a series of runs. As with all my benchmarks, I use the lowest query time as a way of indicating \"top speed\". The following completed in 0.762 seconds. The following completed in 2.472 seconds. The following completed in 4.131 seconds. The following completed in 6.041 seconds. Given all the features of PostgreSQL are still available I'm blown away at how fast BrytlytDB is able to aggregate data. The cluster I used cost around $30 / hour which means this system out performed other Cloud-based and PostgreSQL-based data warehousing solutions both in terms of wall clock time and in terms of cost per hour of running the cluster. That being said I know Nvidia's K80 chips use extremely fast memory and have thousands of compute cores so I expect further optimisations from BrytlytDB to drive down these already fast query times. \u2190 Back to Index Copyright \u00a9 2014 - 2017 Mark Litwintschik. This site's template is based off a template by Giulio Fidente.", "links": ["https://twitter.com/marklit82", "http://tech.marksblogg.com/theme/cv.pdf", "https://uk.linkedin.com/in/marklitwintschik/", "http://tech.marksblogg.com/feeds/all.atom.xml", "http://www.brytlyt.com/", "https://uk.linkedin.com/in/marklitwintschik/", "https://github.com/giulivo/pelican-svbhack"]}, "19": {"url": "https://blog.datazar.com/why-we-focus-on-design-just-as-much-as-functionality-f6ccb3b2c926", "title": "Why We Focus on Design Just as Much as Functionality", "text": "It\u2019s no secret that scientific software isn\u2019t the most beautiful software in terms of design/aesthetics (generally speaking here). It is very effective though; it gets the job done. It does exactly what you expect it to do. Being a huge movie buff, I\u2019ve always wondered why the software we use in the laboratories or classrooms looks nothing like what we see in the movies. Movies do this thing where tech looks exciting and futuristic and super fun to use. And then I go back to reality and it\u2019s nothing like what I just watched. It\u2019s worth mentioning that not everything in the movies looks practical, especially when it comes to software or even more ridiculous: computer hacking. But that\u2019s another subject. The fact there\u2019s some art missing is something I noticed not only in the software we use in the lab, but also the instruments we build and the papers we write. It seems as if the art has been completely taken out of the sciences. Before you say it, we obviously can\u2019t incorporate art into somethings like highly sensitive instruments as unnecessary additions area big no because then you\u2019re interfering. Another thing to note might be the fact that hundreds or even some thousands years ago, the scientists were also artists. Art came naturally to them as they were also discovering mother nature. At that time, science was also only available to the rich which meant a couple of things: they could afford to spend a little bit of time incorporating art into their scientific work as they weren\u2019t rushing to meet grant deadlines, but it also meant they were already educated in the arts from an early age. These days anyone can be whatever they want to be. To the very least, it\u2019s easier now than it was in that era by an order of magnitude. Today we\u2019re taught that art almost has no place in the sciences and we must be efficient and to the point. Anything extra is generally referred to as fluff. If you like to romanticize science, you\u2019re not really a scientist. But I strongly believe that it\u2019s the artistic and I might add\u200a\u2014\u200aphilosophical\u200a\u2014\u200afluff that gets people interested in science. The hard math and core scientific values obviously being there regardless. All of this results in the absence of art in the sciences which makes art seem dry and boring. Science in its purest form is absolutely beautiful if communicated right. That\u2019s why we love to do our part at Datazar. We\u2019re far, far from perfect or even close to what we want to be. But putting just as much thought in design and UX as we put in functionality is a core principle. There\u2019s a misconception that incorporating art means adding more stuff. Incorporating art can also mean doing less, using less and making the most of what you already have in a beautiful and simple way. An example would be removing that obscene amount of jargon from your paper. So I ask everyone designing software for scientists, design for humans not robots. Scientists are humans too. So go ahead add that CSS animation.", "links": ["https://blog.datazar.com?source=logo-lo_bcb68bd759e4---e2c7e6e1c75", "https://twitter.com/DatazarHQ", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.datazar.com%2Fwhy-we-focus-on-design-just-as-much-as-functionality-f6ccb3b2c926", "https://blog.datazar.com", "https://blog.datazar.com/tagged/r-language", "https://blog.datazar.com/tagged/open-data", "https://blog.datazar.com/tagged/how-to", "https://www.r-bloggers.com/", "https://blog.datazar.com/search", "https://blog.datazar.com/@ata_aman?source=post_header_lockup", "https://blog.datazar.com/@ata_aman?source=post_header_lockup", "https://blog.datazar.com/tagged/science?source=post", "https://blog.datazar.com/tagged/art?source=post", "https://blog.datazar.com/tagged/research?source=post", "https://blog.datazar.com/tagged/software-development?source=post", "https://blog.datazar.com/tagged/design?source=post", "https://blog.datazar.com/@ata_aman?source=footer_card", "https://blog.datazar.com/@ata_aman", "http://twitter.com/datazarhq", "https://blog.datazar.com?source=footer_card", "https://blog.datazar.com?source=footer_card", "https://blog.datazar.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "20": {"url": "https://news.ycombinator.com/item?id=14950255", "title": "", "text": "", "links": ["https://news.ycombinator.com", "https://edgylabs.com/machine-learning-to-enhance-smartphone-pictures/", "https://hn.algolia.com/?query=Machine%20Learning%20to%20Enhance%20Smartphone%20Pictures&sort=byDate&dateRange=all&type=story&storyText=false&prefix&page=0", "https://www.google.com/search?q=Machine%20Learning%20to%20Enhance%20Smartphone%20Pictures", "https://github.com/HackerNews/API", "http://www.ycombinator.com/apply/"]}, "21": {"url": "http://starmine.ai/datasets/ds02.html", "title": "", "text": "reddit Slack Twitter Instagram Facebook LinkedIn", "links": ["http://starmine.ai", "http://reddit.com/r/datasets", "https://join.slack.com/t/starmineai/shared_invite/MjIwNjg5OTUyODAzLTE1MDE2MTk1MzQtMmJhYTExNDViMA", "https://twitter.com/starmineAI", "https://www.instagram.com/starmine.ai/", "https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Fstarmine.ai/index.html&t=starmine%20ICO", "https://www.linkedin.com/cws/share?url=http%3A%2F%2Fstarmine.ai/index.html&token=&isFramed=true", "http://research.kraeutli.com/index.php/2013/11/the-tate-collection-on-github", "http://54.174.116.134/recommend/datasets/subscribers/free/supercolumns/supercolumns-elements-CMDB-nasdaq-nyse-otcbb-general-2017/supercolumns-elements-nasdaq-nyse-otcbb-general-UPDATE-2017-03-01.csv", "http://54.174.116.134/recommend/datasets/subscribers/free/supercolumns/supercolumns-elements-CMDB-nasdaq-nyse-otcbb-general-2017/supercolumns-elements-nasdaq-nyse-otcbb-general-UPDATE-2017-03-01.csv", "http://54.174.116.134/recommend/app/ai_connect-finance-historicaltrends_api?ccl1=playstation&ccl2=helium&ccl3=korea&ccl4=shampoo&ccl5=coffee&query=concept_column_labels&themesource=theme_CMDB-nasdaq-nyse-otcbb&month=01&year=all&db=CMDB-nasdaq-nyse-otcbb-general", "https://www0.gsb.columbia.edu/mygsb/faculty/research/pubfiles/1555/contagious.pdf", "https://trends.google.com/trends/explore?q=playstation,helium,korea,shampoo,coffee", "https://trends.google.com/trends/explore?q=playstation,helium,korea,shampoo,coffee"]}, "22": {"url": "https://github.com/maciejkula/spotlight/blob/master/examples/movielens_sequence/readme.rst", "title": "", "text": "Using sequences of user-item interactions as an input for recommender models has a number of attractive properties. Firstly, it recognizes that recommending the next item that a user may want to buy or see is precisely the goal we are trying to achieve. Secondly, it's plausible that the ordering of users' interactions carries additional information over and above just the identities of items they have interacted with. For example, a user is more likely to watch the next episode of a given TV series if they've just finished the previous episode. Finally, when the sequence of past interactions rather than the identity of the user is the input to a model, online systems can incorporate new users (and old users' new actions) in real time. They are fed to the existing model, and do not require a new model to be fit to incorporate new information (unlike factorization models). Recurrent neural networks are the most natural way of modelling such sequence problems. In recommendations, gated recurrent units (GRUs) have been used with success in the Session-based recommendations with recurrent neural networks paper. Spotlight implements a similar model using LSTM units as one of its sequence representations. But recurrent neural networks are not the only way of effectively representing sequences: convolutions can also do the job. In particular, we can use causal convolutions: convolutional filters applied to the sequence in a left-to-right fashion, emitting a representation at each step. They are causal in that the their output at time t is conditional on input up to t-1: this is necessary to ensure that they do not have access to the elements of the sequence we are trying to predict. Like LSTMs, causal convolutions can model sequences with long-term dependencies. This is achieved in two ways: stacking convolutional layers (with padding, every convolutional layer preserves the shape of the input), and dilation: insertion of gaps into the convolutional filters (otherwise known as atrous convolutions). Causal convolutions have been used in several recent high-profile papers: Using convolutional rather than recurrent networks for representing sequences has a couple of advantages, as described in this blog post: Spotlight implements causal convolution models as part of its sequence models package, alongside more traditional recurrent and pooling models. The Spotlight implementation has the following characteristics: The model is trained using one of Spotlight's implicit feedback losses, including pointwise (logistic and hinge) and pairwise (BPR as well as WARP-like adaptive hinge) losses. As with other Spotlight sequence models, the loss is computed for all the time steps of the sequence in one pass: for all timesteps t in the sequence, a prediction using elements up to t-1 is made, and the loss is averaged along both the time and the minibatch axis. This leads to siginficant training speed-ups relative to only computing the loss for the last element in the sequence. To see how causal CNNs compare to more traditional sequence models we can have a look at how they perform at predicting the next rated movie on the Movielens 1M dataset. With 1 million interactions spread among 6000 users and around 4000 movies it should be small enough to run quick experiments, but large enough to yield meaningful results. I chose to split the dataset into 80% train, and 10% test and validation sets. I construct 200-long sequences by splitting each user's item sequence into 200-long chunks; if a chunk is shorter than 200 elements, it's padded with zeros. I use mean reciprocal rank (MRR) as the evaluation metric. To choose hyperparameters, I run a quick, coarse grained hyperparameter search, using random sampling to draw 100 hyperparameter sets. With the data and hyperparameters ready, fitting and evaluating the model is relatively simple: Fitting the models is fairly quick, taking at most two or three minutes on a single K80 GPU. The code for the experiments is available in the experiments folder of the Spotlight repo. The results are as follows: It's difficult to draw clear-cut conclusions about the effect of each hyperparameter, but it looks like: To compare causal convolutions with more traditional sequence models I run similar hyperparameter searches for LSTM-based representations and pooling representations. The pooling representation is a simple averaging of item embedding across the sequence; the LSTM-based model runs an LSTM along a user's interactions, using the hidden state for prediction of the next element at each step. The results are as follows: A single layer LSTM seems to outperform causal convolutions, by an over 10% margin, helped by the adaptive hinge loss. Simple pooling performs quite badly. It looks like causal convolutions need some more work before beating recurrent networks. There are a couple of possible avenues for making them better: I'd love to get some input on these. If you have suggestions, let me know on Twitter or open an issue or PR in Spotlight.", "links": ["https://github.com/", "https://arxiv.org/abs/1511.06939", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.LSTMNet", "https://travis-ci.org/maciejkula/spotlight", "https://arxiv.org/pdf/1609.03499.pdf", "http://papers.nips.cc/paper/6527-conditional-image-generation-with-pixelcnn-decoders", "https://arxiv.org/abs/1610.10099", "https://medium.com/@TalPerry/convolutional-methods-for-text-d5260fd5675f", "https://arxiv.org/pdf/1610.10099.pdf", "https://maciejkula.github.io/spotlight/sequence/sequence.html", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.CNNNet", "https://maciejkula.github.io/spotlight/losses.html", "https://grouplens.org/datasets/movielens/1m/", "https://en.wikipedia.org/wiki/Mean_reciprocal_rank", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.LSTMNet", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.PoolNet", "https://maciejkula.github.io/spotlight/losses.html#spotlight.losses.adaptive_hinge_loss", "https://twitter.com/Maciej_Kula", "https://github.com/maciejkula/spotlight", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "23": {"url": "https://github.com/ynqa/word-embedding", "title": "", "text": "GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together.  Sign up             Use Git or checkout with SVN using the web URL.             This is an implementation of word embedding (also referred to as word representation) models in Golang. Word embedding makes words' meaning, structure, and concept mapping into vector space (and low dimension). For representative instance: Like this example, it could calculate word meaning by arithmetic operations between vectors. Listed models for word embedding, and checked it already implemented. Downloading text8 corpus, and training by Skip-Gram with negative sampling.", "links": ["https://github.com/", "https://github.com/ynqa/word-embedding/blob/master/LICENSE", "https://help.github.com/articles/which-remote-url-should-i-use", "https://travis-ci.org/ynqa/word-embedding", "https://godoc.org/github.com/ynqa/word-embedding", "https://goreportcard.com/report/github.com/ynqa/word-embedding", "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf", "http://nlp.stanford.edu/pubs/glove.pdf", "https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf", "http://mattmahoney.net/dc/textdata", "https://github.com/cjlin1/libsvm", "http://www.aclweb.org/anthology/Q15-1016", "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.648.8023&rep=rep1&type=pdf", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "24": {"url": "https://github.com/brannondorsey/keras_weight_animator", "title": "", "text": "GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together.  Sign up             Use Git or checkout with SVN using the web URL.          Save Keras weight matrices as short animated videos to better understand what and how your neural network models are learning. Below are examples of the first LSTM layer and the final output layer of a six-class RNN model trained over one epoch. Blue represents low values and red represents high values.   In order to render videos from the saved weight images you must also have the following packages installed on your machine: This module is named keras_weight_animator. It exposes a Keras callback function that you can include in any model fit(...) method. The two required parameters to image_saver_callback(...) are the Keras model and an output_directory to periodically save weight images to. By default, keras_weight_animator saves layer weights every 100 batches to output_directory as PNGs in folders named epoch_XXX-layer_NAME-weights_YY. Once training is complete, you can optionally create short animated video clips from the image sequences saved in output_directory using [bin/create_image_sequence.sh](bin/create_image_sequence.sh) path/to/output_directory. This will use parallel, mogrify, and ffmpeg to create a .mp4 from the image sequences located in each folder of output_directory. Video files will be named like epoch_XXX-layer_NAME-weights_YY.mp4. You can run this script automatically from your training script by passing the render_videos=True parameter to image_saver_callback(...). weight_image_sequences(...) takes a variety of optional keyword arguments. I've included an example usage of the module in examples/wisdm.py. This example uses smartphone accelerometer data from WISDM to classify human activity tasks like walking, standing, sitting, walking upstairs, etc... This example uses a one layer LSTM to classify a set of 60 data points (representing three seconds of data sampled at 20hz) as belonging to one of six classes. It outputs image sequences and videos to data/wisdm. Using a bash script to leverage parallel, ImageMagick, and FFMPEG isn't necessarily the most elegant solution, but its the one I had time for. The goal of this here lil' project was to write a quick tool that allows me to better understand how weights change over mini-batch updates in a variety of neural networks. Perhaps in the future I will come back and clean up some of the inelegancies. If you have interest in contributing or maintaining a cleaner version of this lib, please reach out at brannon@brannondorsey.com. This module is \u00a9 Brannon Dorsey 2017, released under an \u2665 MIT License \u2665. You are free to use, modify, distribute, sell, etc... this software under those terms. Example data is from the WIreless Sensor Datamining (WISDM) Actitracker dataset published by Fordham University: This idea is tangentially inspired by much of the work on Visualizing and Understanding Recurrent Networks by Andrej Karpathy, Justin Johnson, and Fei-Fei Li. GNU Parallel is adamant about citation to the point of excess IMHO, but for what its worth, here is their bibtex:", "links": ["https://github.com/", "https://github.com/brannondorsey/keras_weight_animator/blob/master/LICENSE", "https://help.github.com/articles/which-remote-url-should-i-use", "https://github.com/brannondorsey/keras_weight_animator/pull/1", "http://keras.io", "https://www.gnu.org/software/parallel/", "https://www.imagemagick.org/script/index.php", "https://ffmpeg.org/download.html", "https://keras.io/callbacks/", "http://matplotlib.org/users/colormaps.html", "http://www.cis.fordham.edu/wisdm/dataset.php", "https://arxiv.org/abs/1506.02078", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "25": {"url": "https://axibase.github.io/atsd-use-cases/Expatriation_Q2/", "title": "", "text": "Expatriation is the temporary or permanent relocation of a person by choice or by force from their native country to any other country in the world for work, pleasure, or purpose. History has seen expatriation from nearly every country in the world up to and including the current epoch. Famous American expatriates throughout history have included American founding father Benjamin Franklin, who worked as the first United States Minister to France for almost a decade after appointment by the Continental Congress in 1778. The  sixth President of the United States John Quincy Adams, who served as a U.S. Diplomat in four countries before winning the Presidency as a Democratic-Republican in 1825, most notably opening American diplomatic ties as the first United States Minister to the Russian Empire, a position he loved  so much he declined a subsequent offer to serve as an Associate Justice on the Supreme Court of the United States a few years later. American expatriates have come from all walks of life: authors, such as Ernest Hemingway who lived in China, Spain, Cuba, and was even present on the beach codenamed Omaha in Normandy during the allied landing. Musicians, like Jimi Hendrix who rose to international fame in London after leaving his native Seattle, directors like Stanley Kubrick, singers like Tina Turner, and even basketball players like  Allen Iverson, who played an incomplete season for a Turkish basketball team in 2010 after retiring from the NBA. For those who decide to make the change permanent, there comes a time when the inconvenience of living abroad as a resident alien outweighs the novelty and many decide to renounce their citizenship for one reason or another. Unsurprisingly, this is a long and complicated process, one of the main features of which is settling up with the Internal Revenue Service (IRS) via the eponymous Expatriation Tax. The Federal Register is a publishing outlet for the United States Government where the IRS officially releases quarterly information about American citizens who renounce their passport in lieu of a new one and other long-term residents who have decided to repatriate, view the raw data here and use the Axibase Data Crawler designed specifically for collecting, parsing, organzing and inserting historical expatriation data and new data as it becomes available.  Axibase covered last quarter\u2019s expatriation release here and is using this quarter\u2019s release to follow-up. Every election cycle countless celebrities and netizens issue their promise to leave if their candidate fails to get elected and the growing notoriety surrounding the 2016 Presidental Election has proven to  be no different. This type of social theater is almost as old as the country itself as there have been a number of similarly divisive elections throughout American history: Andrew Jackson\u2019s victory in 1828 was seen as a populace uprising against the corrupt political elite and a return to the authority of the common man, Abraham Lincoln\u2019s second term was viewed as the tightening of federal authority to excessive levels that ultimately led to the American Civil War, Franklin Roosevelt was called a dangerous communist whose infamous court-packing plan favorably increased the number of seats on the Supreme Court to his  advantage and was seen as abhorrent and traitorous by many, and of course who can forget recent elections that have featured scandal after scandal and two modern Presidents who served after losing the popular vote, but winning in the electorate. Figure 1   Query 1 Data queried in the SQL Console. Table 1 To give the issue further context, Figure 1 and Table 1 use the most current Department of Homeland Security (DHS) data  available to display current naturalization numbers in the United States. Naturalization is the sister process to expatriation, because as a person expatriates from one country they must be naturalized in another or end up a stateless person. Naturalization numbers in America have remained fairly constant throughout the last decade because there are a number of federal regulations that control the amount of long-term immigrants that America accepts each year and that quota is met without fail. As noted in the our Q1 article, the sister value and subject of this examination, expatriation numbers, has been steadily growing for several consecutive years. Figure 2.1   Query 2 The above query features robust syntax and calculated values. See the following tutorials  to understand more about SQL Console. Table 2 Figure 2.2   Expatriation has been increasing each year by roughly 30% since 2010, which featured abnormally high expatriation rates, most likely attributable to the economic turndown of the Great Recession which began in the United States as a result of the sub-prime mortgage crisis. American citizenship is often sought after for the economic opportunity that comes along with the passport, as the ability to work and do business in the country is heavily restricted or regulated, and with the value of that investment or  opportunity in question, it is unsurprising that the number of investors in the system, that is, new citizens, would fluctuate. It seems appropriate to call naturalization an investment because of the nature of the process, which is long, complicated, and often quite expensive similar to a long-position that will cost more at purchasing time but promises high returns after reaching maturity. The peak, or more appropriately, valley of the global recession occurred in 2009 when the global GDP contracted causing a decline in the median familty income of about five percent. Figure 3   Query 3 Table 3 Reducing the timespan to look at data over the last 5 years captures the repetition of this trend for this period of time. Of the eighteen quarters included in this query, only six of them, roughly 33% showed negative growth for this statistic. Growing expatriation numbers provide interesting insight into the perception of America both domestically and abroad, as these numbers not only include U.S. citizens who renounce their citizenship, but also long-term resident aliens who have returned home. Almost unbelievably, the IRS does not strip characteristic information about expatriates from their publications meaning that first and last names are included in the data. Using Social Security data concerning the most common first names chosen throughout the last century and United States Census data concerning the most common last names to compare, the following queries and visualizations show  the number of expatriates by common first and common last name throughout the entire observed period, ranked by their frequency in the IRS publication. The goal of these queries is purely demonstrative and lighthearted. There is no intention to invade privacy, defame people, or otherwise cause harm. Figure 4  Query 4 Table 4 All of these names appear on the list of most common American first names, linked here Figure 5  Query 5 Table 5 Only the name Smith appears on the list of most common American last names, linked here Use the Axibase Data Crawler to collect the data used in this artcle and examine it yourself in ChartLab.", "links": ["https://github.com/axibase/atsd-use-cases", "https://www.irs.gov/", "https://www.irs.gov/individuals/international-taxpayers/expatriation-tax", "https://www.federalregister.gov/", "https://www.federalregister.gov/documents/2017/05/10/2017-09475/quarterly-publication-of-individuals-who-have-chosen-to-expatriate-as-required-by-section-6039g", "https://github.com/axibase/atsd-data-crawlers/tree/irs-expatriation-data-crawler", "https://axibase.com", "https://apps.axibase.com/chartlab/654b9945#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql", "https://www.uscis.gov/news/fact-sheets/naturalization-fact-sheet", "http://www.unhcr.org/stateless-people.html", "https://apps.axibase.com/chartlab/7fa5b643#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql#examples", "https://apps.axibase.com/chartlab/7fa5b643/5/#fullscreen", "https://web.stanford.edu/group/recessiontrends/cgi-bin/web/sites/all/themes/barron/pdf/IncomeWealthDebt_fact_sheet.pdf", "https://apps.axibase.com/chartlab/7fa5b643/2/#fullscreen", "https://www.ssa.gov/oact/babynames/decades/century.html", "http://www.census.gov/main/www/cen2000.html", "https://www.ssa.gov/oact/babynames/decades/century.html", "http://www.census.gov/main/www/cen2000.html", "https://github.com/axibase/atsd-data-crawlers/tree/irs-expatriation-data-crawler", "https://apps.axibase.com", "https://github.com/axibase/atsd-use-cases", "https://github.com/axibase", "https://pages.github.com"]}, "26": {"url": "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/", "title": "", "text": "Machine learning is eating the world right now. Everyone and their mother are learning about machine learning models, classification, neural networks, and Andrew Ng. You\u2019ve decided you want to be a part of it, but where to start? In this article we\u2019ll cover some important characteristics of Python and why it\u2019s great for machine learning. We\u2019ll also cover some of the most important libraries it has for ML, and if it piques your interest, some places where you can learn more. Python is a great choice for machine learning for several reasons. First and foremost, it\u2019s a simple language\u00a0on the surface; even if you\u2019re not familiar with Python, getting up to speed is very quick if you\u2019ve ever used any other language with C-like syntax (i.e. every language out there). Second, Python has a great community, which results in good documentation and friendly, comprehensive answers in StackOverflow (fundamental!). Third, also stemming from the great community, there are plenty of useful libraries for Python (both as \u201cbatteries included\u201d and third party), which solve basically any problem that you can have (including machine learning). Yeah and it\u2019s true. Python isn\u2019t the fastest language out there: all those handy abstractions come at a cost. But here\u2019s the trick: libraries can and do offload the expensive calculations to the much more performant (but harder to use) C and C++. For instance, there\u2019s NumPy, which is a library for numerical computation. It\u2019s written in C, and it\u2019s fast. Practically every library out there that involves intensive calculations uses it \u2014 almost all the libraries listed next use it in some form. So if you read NumPy, think fast. Therefore, you can make your scripts run basically as fast as straight up writing them in a lower level language. So there\u2019s really nothing to worry about when it comes to speed. Are you starting out in machine learning? Want something that covers everything from feature engineering to training and testing a model? Look no further than scikit-learn! This fantastic piece of free software provides every tool necessary for machine learning and data mining. It\u2019s the de facto standard library for machine learning in Python, recommended for most of the \u2018old\u2019 ML algorithms. This library does both classification and regression, supporting basically every algorithm out there (support vector machines, random forest, naive bayes, and so on). It\u2019s built in such a way that allows easy switching of algorithms, so experimentation is easy. These \u2018older\u2019 algorithms are surprisingly resilient and work very well in a lot of cases. But that\u2019s not all! Scikit-learn also does dimensionality reduction, clustering, you name it. It\u2019s also blazingly fast since it runs on NumPy and SciPy (meaning that all the heavy number crunching is run on C instead of Python). Check out some examples to see everything this library is capable of, and the tutorials if you want to learn how it works. While not a machine learning library per se, NLTK is a must when working with natural language processing (NLP). It comes with a bundle of datasets and other lexical resources (useful for training models) in addition to libraries for working with text \u2014 for functions such as classification, tokenization, stemming, tagging, parsing and more. The usefulness of having all of this stuff neatly packaged can\u2019t be overstated. So if you are interested in NLP, check out some tutorials! Used widely in research and academia, Theano is the grandfather of all deep learning frameworks. Written in Python, it\u2019s tightly integrated with NumPy. Theano allows you to create neural networks, which are represented as mathematical expressions with multi-dimensional arrays. Theano handles this for you so you don\u2019t have to worry about the actual implementation of the math involved. It supports offloading calculations to the much faster GPU, which is a feature that everyone supports today, but back when they introduced it this wasn\u2019t the case. The library is very mature at this point and supports a very wide range of operations, which is a great plus when it comes to comparing it with other similar libraries. The biggest complaint out there is that the API may be unwieldy for some, making the library hard to use for beginners. However, there are wrappers that ease the pain and make working with Theano simple, such as Keras, Blocks and Lasagne. Interested in learning about Theano? Check out this Jupyter Notebook tutorial. The Google Brain team created TensorFlow for internal use in machine learning applications, and open sourced it in late 2015. They wanted something that could replace their older, closed source machine learning framework, DistBelief, which they said wasn\u2019t flexible enough and too tightly coupled to their infrastructure to be shared with other researchers around the world. And so TensorFlow was created. Learning from the mistakes of the past, many consider this library to be an improvement over Theano, claiming more flexibility and a more intuitive API. Not only can it be used for research but also for production environments, supporting huge clusters of GPUs for training. While it doesn\u2019t support as wide a range of operations as Theano, it has better computational graph visualizations. TensorFlow is very popular nowadays. In fact, if you\u2019ve heard about a single library on this list, it\u2019s probably this one: there isn\u2019t a day that goes by without a new blog post or paper mentioning TensorFlow gets published. This popularity translates into a lot of new users and a lot of tutorials, making it very welcoming to beginners. Keras is a fantastic library that provides a high-level API for neural networks and is capable of running on top of either Theano or TensorFlow. It makes harnessing the full power of these complex pieces of software much easier than using them directly. It\u2019s very user-friendly, putting user experience as a top priority. They manage this by using simple APIs and excellent feedback on errors. It\u2019s also modular, meaning that different models (neural layers, cost functions, and so on) can be plugged together with little restrictions. This also makes it very easy to extend, since it\u2019s simple to add new modules and connect them with the existing ones. Some people have called Keras so good that it is effectively cheating in machine learning. So if you\u2019re starting out with deep learning, go through the examples and documentation to get a feel for what you can do with it. And if you want to learn, start out with this tutorial and see where you can go from there. Two similar alternatives are Lasagne and Blocks, but they only run on Theano. So if you tried Keras and are unhappy with it, maybe try out one of these alternatives to see if they work out for you. Another popular deep learning framework is Torch, which is written in Lua. Facebook open-sourced a Python implementation of Torch called PyTorch, which allows you to conveniently use the same low-level libraries that Torch uses, but from Python instead of Lua. PyTorch is much better for debugging since one of the biggest differences between Theano/TensorFlow and PyTorch is that the former use symbolic computation while the latter doesn\u2019t. Symbolic computation means that coding an operation (say, \u2018x + y\u2019), it\u2019s not computed when that line is interpreted. Before getting executed it has to be compiled (translated to CUDA or C). This makes debugging harder in Theano/TensorFlow, since an error is much harder to associate with the line of code that caused it. Of course, doing things this way has its advantages, but debugging isn\u2019t one of them. If you want to start out with PyTorch the official tutorials are very friendly to beginners but get to advanced topics as well. Alright, you\u2019ve presented me with a lot of alternatives for machine learning libraries in Python. What should I choose? How do I compare these things? Where do I start? Our Ape Advice\u2122 for beginners is to try and not get bogged down by details. If you\u2019ve never done anything machine learning related, try out scikit-learn. You\u2019ll get an idea of how the cycle of tagging, training and testing work and how a model is developed. Now, if you want to try out deep learning, start out with Keras \u2014 which is widely agreed to be the easiest framework \u2014 and see where that takes you. After you have more experience, you will start to see what it is that you actually want from the framework: greater speed, a different API, or maybe something else, and you\u2019ll be able to make a more informed decision. And even then, there is an endless supply of articles out there comparing Theano, Torch, and TensorFlow. There\u2019s no real way to tell which one is the good one. It\u2019s important to take into account that all of them have wide support and are improving constantly, making comparisons harder to make. A six month old benchmark may be outdated, and year old claims of framework X doesn\u2019t support operation Y could no longer be valid. Finally, if you\u2019re interested in doing machine learning specifically applied to NLP, why not check out MonkeyLearn! Our platform provides a unique UX that makes it super easy to build, train and improve NLP models. You can either use pre-trained models for common use cases (like sentiment analysis, topic detection or keyword extraction) or train custom algorithms using your particular data. Also, you don\u2019t have to worry about the underlying infrastructure or deploying your models, our scalable cloud does this for you. You can start for free and integrate right away with our beautiful API. There are plenty of online resources out there to learn about machine learning ! Here are a few: So that was a brief intro to machine learning in Python and some of its libraries. The important part is not getting bogged down by details and just trying stuff out. Follow your curiosity, and don\u2019t be afraid to experiment. Know about a python library that was left out? Share it in the comments below!", "links": ["http://www.monkeylearn.com", "http://monkeylearn.com/use-cases/", "http://monkeylearn.com/docs/", "http://www.monkeylearn.com/pricing", "http://www.monkeylearn.com/blog/", "https://app.monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/?utm_source=ml-blog&utm_medium=header&utm_campaign=blog", "https://monkeylearn.com/blog", "http://monkeylearn.com/use-cases/", "http://monkeylearn.com/docs/", "http://www.monkeylearn.com/pricing", "http://www.monkeylearn.com/blog/", "https://app.monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/?utm_source=ml-blog&utm_medium=header&utm_campaign=blog", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/07/Post_2c.png", "http://scikit-learn.org/", "http://scikit-learn.org/stable/auto_examples/index.html#general-examples", "http://scikit-learn.org/stable/tutorial/index.html", "http://www.nltk.org/", "http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk", "http://deeplearning.net/software/theano/", "https://keras.io/", "https://github.com/mila-udem/blocks", "https://github.com/Lasagne/Lasagne", "http://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/Theano%20Tutorial.ipynb", "https://www.tensorflow.org/", "https://www.tensorflow.org/get_started/graph_viz", "https://www.tensorflow.org/tutorials/", "https://keras.io/", "https://news.ycombinator.com/item?id=13872764", "https://github.com/fchollet/keras#getting-started-30-seconds-to-keras", "https://keras.io/getting-started/functional-api-guide/", "https://elitedatascience.com/keras-tutorial-deep-learning-in-python", "https://github.com/Lasagne/Lasagne", "https://github.com/mila-udem/blocks", "http://torch.ch/", "http://pytorch.org/", "http://pytorch.org/tutorials/", "http://scikit-learn.org/stable/tutorial/basic/tutorial.html", "https://elitedatascience.com/keras-tutorial-deep-learning-in-python", "https://www.quora.com/Is-TensorFlow-better-than-other-leading-libraries-such-as-Torch-Theano", "http://www.ccri.com/2016/12/09/torch-vs-tensorflow-vs-theano/", "https://www.reddit.com/r/MachineLearning/comments/5w3q74/d_so_pytorch_vs_tensorflow_whats_the_verdict_on/", "https://github.com/zer0n/deepframeworks/blob/master/README.md", "https://monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/", "http://nbviewer.jupyter.org/github/rhiever/Data-Analysis-and-Machine-Learning-Projects/blob/master/example-data-science-notebook/Example%20Machine%20Learning%20Notebook.ipynb", "https://blog.monkeylearn.com/a-gentle-guide-to-machine-learning/", "https://www.coursera.org/learn/machine-learning", "http://course.fast.ai/", "https://monkeylearn.com/blog/author/bruno/", "https://monkeylearn.com/blog/category/news/", "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/#respond", "https://monkeylearn.com/blog/author/bruno/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/monkeylearn_zapier.png", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/wp-content/uploads/2017/06/Post_1e.png", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/naive-bayes-classifier.png", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/monkey-startup.png", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/category/applications/", "https://monkeylearn.com/blog/category/guides/", "https://monkeylearn.com/blog/category/howto/", "https://monkeylearn.com/blog/category/news/", "https://monkeylearn.com/blog/category/text-classification/", "https://www.monkeylearn.com", "https://www.monkeylearn.com", "http://slack.monkeylearn.com/", "https://twitter.com/monkeylearn", "https://github.com/monkeylearn/"]}, "27": {"url": "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "title": "Machine Learning Translation and the Google Translate Algorithm", "text": "Every day we use different technologies without even knowing how exactly they work. In fact, it\u2019s not very easy to understand engines powered by machine learning. The Statsbot team wants to make machine learning clear by telling data stories in this blog. Today, we\u2019ve decided to explore machine translators and explain how the Google Translate algorithm works. Years ago, it was very time consuming to translate the text from an unknown language. Using simple vocabularies with word-for-word translation was hard for two reasons: 1) the reader had to know the grammar rules and 2) needed to keep in mind all language versions while translating the whole sentence. Now, we don\u2019t need to struggle so much\u2013 we can translate phrases, sentences, and even large texts just by putting them in Google Translate. But most people don\u2019t actually care how the engine of machine learning translation works. This post is for those who do care. If the Google Translate engine tried to kept the translations for even short sentences, it wouldn\u2019t work because of the huge number of possible variations. The best idea can be to teach the computer sets of grammar rules and translate the sentences according to them. If only it were as easy as it sounds. If you have ever tried learning a foreign language, you know that there are always a lot of exceptions to rules. When we try to capture all these rules, exceptions and exceptions to the exceptions in the program, the quality of translation breaks down. Creating your own simple machine translator would be a great project for any data science resume. Let\u2019s try to investigate what hides in the \u201cblack boxes\u201d that we call machine translators. Deep neural networks can achieve excellent results in very complicated tasks (speech/visual object recognition), but despite their flexibility, they can be applied only for tasks where the input and target have fixed dimensionality. Here is where Long Short-Term Memory networks (LSTMs) come into play, helping us to work with sequences whose length we can\u2019t know a priori. LSTMs are a special kind of recurrent neural network (RNN), capable of learning long-term dependencies. All RNNs look like a chain of repeating modules. So the LSTM transmits data from module to module and, for example, for generating Ht we use not only Xt, but all previous input values X. To learn more about structure and mathematical models of LSTM, you can read the great article \u201cUnderstanding LSTM Networks.\u201d Our next step is bidirectional recurrent neural networks (BRNNs). What a BRNN does, is split the neurons of a regular RNN into two directions. One direction is for positive time, or forward states. The other direction is for negative time, or backward states. The output of these two states are not connected to inputs of the opposite direction states. To understand why BRNNs can work better than a simple RNN, imagine that we have a sentence of 9 words and we want to predict the 5th word. We can make it know either only the first 4 words, or the first 4 words and last 4 words. Of course, the quality in the second case would be better. Now we\u2019re ready to move to sequence to sequence models (also called seq2seq). The basic seq2seq model consist of two RNNs: an encoder network that processes the input and a decoder network that generates the output. Finally, we can make our first machine translator! However, let\u2019s think about one trick. Google Translate currently supports 103 languages, so we should have 103x102 different models for each pair of languages. Of course, the quality of these models varies according to the popularity of languages and the amount of documents needed for training this network. The best that we can do is to make one NN to take any language as input and translate into any language. That very idea was realized by Google engineers at the end of 2016. Architecture of NN was build on the seq2seq model, which we have already studied. The only exception is that between the encoder and decoder there are 8 layers of LSTM-RNN that have residual connections between layers with some tweaks for accuracy and speed. If you want to go deeper with that, take a look at the article Google\u2019s Neural Machine Translation System. The system requires a \u201ctoken\u201d at the beginning of the input sentence which specifies the language you\u2019re trying to translate the phrase into. This improves translation quality and enables translations even between two languages which the system hasn\u2019t seen yet, a method termed \u201cZero-Shot Translation.\u201d When we\u2019re talking about improvements and better results from Google Translate algorithms, how can we correctly evaluate that the first candidate for translation is better than the second? It\u2019s not a trivial problem, because for some commonly used sentences we have the sets of reference translations from the professional translators, that have, of course, some differences. There are a lot of approaches that partly solve this problem, but the most popular and effective metric is BLEU (bilingual evaluation understudy). Imagine, we have two candidates from machine translators: Although they have the same meaning they differ in quality and have different structure. Let\u2019s look at two human translations: Obviously, Candidate 1 is better, sharing more words and phrases compared to Candidate 2. This is a key idea of the simple BLEU approach. We can compare n-grams of the candidate with n-grams of the reference translation and count the number of matches (independent from their position). We use only n-gram precisions, because calculating recall is difficult with multiple refs and the result is the geometric average of n-gram scores. Now you can evaluate the complex engine of machine learning translation. Next time when you translate something with Google Translate, imagine how many millions of documents it analyzed before giving you the best language version.", "links": ["https://blog.statsbot.co?source=logo-lo_7cd6f9e56ba5---cfc9f21a543a", "https://twitter.com/statsbotco", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.statsbot.co%2Fmachine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co", "https://blog.statsbot.co/analytics/home", "https://blog.statsbot.co/datascience/home", "https://blog.statsbot.co/design/home", "https://blog.statsbot.co/startups/home", "https://blog.statsbot.co/bots/home", "https://blog.statsbot.co/news/home", "https://blog.statsbot.co/statsbot-digest-b0d7372f842a", "http://statsbot.co?utm_source=blog&utm_medium=navigation&utm_campaign=robot_face", "https://blog.statsbot.co/search", "https://blog.statsbot.co/@daniilkorbut?source=post_header_lockup", "https://blog.statsbot.co/@daniilkorbut?source=post_header_lockup", "https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html", "http://statsbot.co?utm_source=blog&utm_medium=article&utm_campaign=machine_translation", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6?utm_source=blog&utm_medium=article&utm_campaign=machine_translation", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6", "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "https://www.semanticscholar.org/paper/Hybrid-speech-recognition-with-Deep-Bidirectional-Graves-Jaitly/5807664af8e63d5207f59fb263c9e7bd3673be79", "https://research.googleblog.com/2016/09/a-neural-network-for-machine.html", "https://www.newscientist.com/article/2114748-google-translate-ai-invents-its-own-language-to-translate-with/", "https://research.googleblog.com/2016/09/a-neural-network-for-machine.html", "https://arxiv.org/abs/1609.08144", "https://en.wikipedia.org/wiki/BLEU", "https://en.wikipedia.org/wiki/N-gram", "https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3", "https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/tagged/machine-learning?source=post", "https://blog.statsbot.co/tagged/machine-translation?source=post", "https://blog.statsbot.co/tagged/google-translate?source=post", "https://blog.statsbot.co/tagged/data-science?source=post", "https://blog.statsbot.co/tagged/neural-networks?source=post", "https://blog.statsbot.co/@daniilkorbut?source=footer_card", "https://blog.statsbot.co/@daniilkorbut", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "28": {"url": "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "title": "Yanir SeroussiMy 10-step path to becoming a remote data scientist with\u00a0Automattic", "text": "About two years ago, I read the book The Year without Pants, which describes the author\u2019s experience leading a team at Automattic (the company behind WordPress.com, among other products). Automattic is a fully-distributed company, which means that all of its employees work remotely (hence pants are optional). While the book discusses some of the challenges of working remotely, the author\u2019s general experience was very positive. A few months after reading the book, I decided to look for a full-time position after a period of independent work. Ideally, I wanted a well-paid data science-y remote job with an established distributed tech company that offers a good life balance and makes products I care about. Automattic seemed to tick all my boxes, so I decided to apply for a job with them. This post describes my application steps, which ultimately led to me becoming a data scientist with Automattic. Before jumping in, it\u2019s worth noting that this post describes my personal experience. If you apply for a job with Automattic, your experience is likely to be different, as the process varies across teams, and evolves over time. I decided to apply for a data wrangler position with Automattic in October 2015. While data wrangler may sound less sexy than data scientist, reading the job ad led me to believe that the position may involve interesting data science work. This impression was strengthened by some LinkedIn stalking, which included finding current data wranglers and reading through their profiles and websites. I later found out that all the people on the data division start out as data wranglers, and then they may pick their own title. Some data wranglers do data science work, while others are more focused on data engineering, and there are some projects that require a broad range of skills. As the usefulness of the term data scientist is questionable, I\u2019m not too fussed about fancy job titles. It\u2019s more important to do interesting work in a supportive environment. Applying for the job was fairly straightforward. I simply followed the instructions from the ad:  Does this sound interesting? If yes, please send a short email to jobs @ this domain telling us about yourself and attach a resum\u00e9. Let us know what you can contribute to the team. Include the title of the position you\u2019re applying for and your name in the subject. Proofread! Make sure you spell and capitalize WordPress and Automattic correctly. We are lucky to receive hundreds of applications for every position, so try to make your application stand out. If you apply for multiple positions or send multiple emails there will be one reply.  Having been on the receiving side of job applications, I find it surprising that many people don\u2019t bother writing a cover letter, addressing the selection criteria in the ad, or even applying for a job they\u2019re qualified to do. Hence, my cover letter was fairly short, comprising of several bullet points that highlight the similarities between the job requirements and my experience. It was nothing fancy, but simple cover letters have worked well for me in the past. The initial application was followed by a long wait. From my research, this is the typical scenario. This is unsurprising, as Automattic is a fairly small company with a large footprint, which is both distributed and known as a great place to work (e.g., its Glassdoor rating is 4.9). Therefore, it attracts many applicants from all over the world, which take a while to process. In addition, Matt Mullenweg (Automattic\u2019s CEO) reviews job applications before passing them on to the team leads. As I didn\u2019t know that Matt reviewed job applications, I decided to try to shorten the wait by getting introduced to someone in the data division. My first attempt was via a second-degree LinkedIn connection who works for Automattic. He responded quickly when I reached out to him, saying that his experience working with the company is in line with the Glassdoor reviews \u2013 it\u2019s the best job he\u2019s had in his 15-year-long career. However, he couldn\u2019t help me with an intro, because there is no simple way around Automattic\u2019s internal processes. Nonetheless, he reassured me that it is worth waiting patiently, as the strict process means that you end up working with great people. I wasn\u2019t in a huge rush to find a job, but in December 2015 I decided to accept an offer to become the head of data science at Car Next Door. This was a good decision at the time, as I believe in the company\u2019s original vision of reducing the number of cars on the road through car sharing, and it seemed like there would be many interesting projects for me to work on. The position wasn\u2019t completely remote, but as the company was already spread across several cities, I was able to work from home for a day or two every week. In addition, it was a pleasant commute by bike from my Sydney home to the office, so putting the fully-remote job search on hold didn\u2019t seem like a major sacrifice. As I haven\u2019t heard anything from Automattic at that stage, it seemed unwise to reject a good offer, so I started working full-time with Car Next Door in January 2016. I successfully attracted Automattic\u2019s attention with a post I published on the misuse of the word insights by many tech companies, which included an example from WordPress.com. Greg Ichneumon Brown, one of the data wranglers, commented on the post, and invited me to apply to join Automattic and help them address the issues I raised. This happened after I accepted the offer from Car Next Door, and hasn\u2019t resulted in any speed up of the process, so I just gave up on Automattic and carried on with my life. I finally heard back from Automattic in February 2016 (four months after my initial application and a month into my employment with Car Next Door). Martin Remy, who leads the data division, emailed me to enquire if I\u2019m still interested in the position. I informed him that I was no longer looking for a job, but we agreed to have an informal chat, as I\u2019ve been waiting for such a long time. As is often the case with Automattic interviews, the chat with Martin was completely text-based. Working with a distributed team means that voice and video calls can be hard to schedule. Hence, Automattic relies heavily on textual channels, and text-based interviews allow the company to test the written communication skills of candidates. The chat revolved around my past work experience, and Martin also took the time to answer my questions about the company and the data division. At the conclusion of the chat, Martin suggested I contact him directly if I was ever interested in continuing the application process. While I was happy with my position at the time, the chat strengthened my positive impression of Automattic, and I decided that I would reapply if I were to look for a full-time position again. My next job search started earlier than I had anticipated. In October 2016, I decided to leave Car Next Door due to disagreements with the founders over the general direction of the company. In addition, I had more flexibility in choosing where to live, as my personal circumstances had changed. As I\u2019ve always been curious about life outside the capital cities of Australia, I wanted to move away from Sydney. While I could have probably continued working remotely with Car Next Door, I felt that it would be better to find a job with a fully-distributed team. Therefore, I messaged Martin and we scheduled another chat. The second chat with Martin took place in early November. Similarly to the first chat, it was conducted via Skype text messages, and revolved around my work in the time that has passed since the first chat. This time, as I was keen on continuing with the process, I asked more specific questions about what kind of work I\u2019m likely to end up doing and what the next steps would be. The answers were that I\u2019d be joining the data science team, and that the next steps are a pre-trial test, a paid trial, and a final interview with Matt. While this sounds straightforward, it took another six months until I finally became an Automattic employee (but I wasn\u2019t in a rush). The pre-trial test consisted of a data analysis task, where I was given a dataset and a set of questions to answer by Carly Stambaugh, the data science lead. The goal of the test is to evaluate the candidate\u2019s approach to a problem, and assess organisational and communication skills. As such, the focus isn\u2019t on obtaining a specific result, so candidates are given a choice of several potential avenues to explore. The open-ended nature of the task is reminiscent of many real-world data science projects, where you don\u2019t always have a clear idea of what you\u2019re going to discover. While some people may find this kind of uncertainty daunting, I find it interesting, as it is one of the things that makes data science a science. I spent a few days analysing the data and preparing a report, which was submitted as a Jupyter Notebook. After submitting my initial report, there were a few follow-up questions, which I answered by email. The report was reviewed by Carly and Martin, and as they were satisfied with my work, I was invited to proceed to the next stage: A paid trial project. The main part of the application process with Automattic is the paid trial project. The rationale behind doing paid trials was explained a few years ago by Matt in Hire by Auditions, Not Resumes:  Before we hire anyone, they go through a trial process first, on contract. They can do the work at night or over the weekend, so they don\u2019t have to leave their current job in the meantime. We pay a standard rate of $25 per hour, regardless of whether you\u2019re applying to be an engineer or the chief financial officer. During the trials, we give the applicants actual work. If you\u2019re applying to work in customer support, you\u2019ll answer tickets. If you\u2019re an engineer, you\u2019ll work on engineering problems. If you\u2019re a designer, you\u2019ll design. There\u2019s nothing like being in the trenches with someone, working with them day by day. It tells you something you can\u2019t learn from resumes, interviews, or reference checks. At the end of the trial, everyone involved has a great sense of whether they want to work together going forward. And, yes, that means everyone \u2014 it\u2019s a mutual tryout. Some people decide we\u2019re not the right fit for them.  The goal of my trial project was to improve the Elasticsearch language detection algorithm. This took about a month, and ultimately resulted in a pull request that got merged into the language detection plugin. I find this aspect of the process pretty exciting: While the plugin is used to classify millions of documents internally by Automattic, its impact extends beyond the company, as Elasticsearch is used by many other organisations and projects. This stands in contrast to many other technical job interviews, which consist of unpaid work on toy problems under stressful conditions, where the work performed is ultimately thrown away. While the monetary compensation for the trial work is lower than the market rate for data science consulting, I valued the opportunity to work on a real open source project, even if this hadn\u2019t led to me getting hired. There was much more to the trial project than what\u2019s shown in the final pull request. Most of the discussions were held on an internal project thread, primarly under the guidance of Carly (the data science lead), and Greg (the data wrangler who replied to my post a year earlier). The project was kicked off with a general problem statement: There was some evidence that the Elasticsearch language detection plugin doesn\u2019t perform well on short texts, and my mission was to improve it. As the plugin didn\u2019t include any tests for short texts, one of the main contributions of my work was the creation of datasets and tests to measure its accuracy on texts of different lengths. This was followed by some tweaks that improved the plugin\u2019s performance, as summarised in the pull request. Internally, this work consisted of several iterations where I came up with ideas, asked questions, implemented the ideas, shared the results, and discussed further steps. There are still many possible improvements to the work done in the trial. However, as trials generally last around a month, we decided to end it after a few iterations. I enjoyed the trial process, but it is definitely not for everyone. Most notably, there is a strong emphasis on asynchronous text-based communication, which is the main mode by which projects are coordinated at Automattic. People who don\u2019t enjoy written communication may find this aspect challenging, but I have always found that writing helps me organise my thoughts, and that I retain information better when reading than when listening to people speak. That being said, Automatticians do meet in person several times a year, and some teams have video chats for some discussions. While doing the trial, I had a video chat with Carly, which was the first (and last) time in the process that I got to see and hear a live human. However, this was not an essential part of the trial project, as our chat was mostly on the data scientist role and my job expectations. I finished working on the trial project just before Christmas. The feedback I received throughout the trial was positive, but Martin, Carly, and Greg had to go through the work and discuss it among themselves before making a final decision. This took about a month, due to the holiday period, various personal circumstances, and the data science team meetup that was scheduled for January 2017. Eventually, Martin got back to me with positive news: They were satisfied with my trial work, which meant there was only one stage left \u2013 the final interview with Matt Mullenweg, Automattic\u2019s CEO. Like other parts of the process, the interview with Matt is text-based. The way it works is fairly simple: I was instructed to message Matt on Slack and wait for a response, which may take days or weeks. I sent Matt a message on January 25, and was surprised to hear back from him the following morning. However, that day was Australia Day, which is a public holiday here. Therefore, I only got back to him two hours after he messaged me that morning, and by that time he was probably already busy with other things. This was the start of a pretty long wait. I left Car Next Door at the end of January, as I figured that I would be able to line up some other work even if things didn\u2019t work out with Automattic. My plan was to take some time off, and then move up to the Northern Rivers area of New South Wales. I had two Reef Life Survey trips planned, so I wasn\u2019t going to start working again before mid-April. I assumed that I would hear back from Matt before then, which would have allowed me to make an informed decision whether to look for another job or not. After two weeks of waiting, the time for my dive trips was nearing. As I was going to be without mobile reception for a while, I thought it\u2019d be worth letting Matt know my schedule. After discussing the matter with Martin, I messaged Matt. He responded, saying that we might as well do the interview at the beginning of April, as I won\u2019t be starting work before that time anyway. I would have preferred to be done with the interview earlier, but was happy to have some certainty and not worry about missing more chat messages before April. In early April, I returned from my second dive trip (which included a close encounter with Cyclone Debbie), and was hoping to sort out my remote work situation while completing the move up north. Unfortunately, while the move was successful, I was ready to give up on Automattic because I haven\u2019t heard back from Matt at all in April. However, Martin remained optimistic and encouraged me to wait patiently, which I did as I was pretty busy with the move and with some casual freelancing projects. The chat with Matt finally happened on May 2. As is often the case, it took a few hours and covered my background, the trial process, and some other general questions. I asked him about my long wait for the final chat, and he apologised for me being an outlier, as most chats happen within two weeks of a candidate being passed over to him. As the chat was about to conclude, we got to the topic of salary negotiation (which went well), and then the process was finally over! Within a few hours of the chat I was sent an offer letter and an employment contract. As Automattic has an entity in Australia (called Ausomattic), it\u2019s a fairly standard contract. I signed the contract and started work the following week \u2013 over a year and a half after my initial application. Even before I started working, I booked tickets to meet the data division in Montr\u00e9al \u2013 a fairly swift transition from the long wait for the final interview. As noted above, Automatticians get to choose their own job titles, so to become a data scientist with Automattic, I had to set my job title to Data Scientist. This is generally how many people become data scientists these days, even outside Automattic. However, job titles don\u2019t matter as much as job satisfaction. And after 2.5 months with Automattic, I\u2019m very satisfied with my decision to join the company. My first three weeks were spent doing customer support, like all new Automattic employees. Since then, I\u2019ve been involved in projects to make engagement measurement more consistent (harder than it sounds, as counting things is hard), and to improve the data science codebase (e.g., moving away from Legacy Python). Besides that, I also went to Montr\u00e9al for the data division meetup, and have started getting into chatbot work. I\u2019m looking forward to doing more work and sharing my experience here and on data.blog. Very enlightening post! It was very awesome to see that the insights you saw to Elasticsearch went to a PR. I bet that was worth the whole thing!  That\u2019s very exciting, I wanted to ask are you a self learner or do you have a degree,can you please share your background. Thank you  Thanks Mostafa. Yes, I have a BSc in computer science, and a PhD in what you would now call data science. See: https://www.linkedin.com/in/yanirseroussi/  Fill in your details below or click an icon to log in:     Connecting to %s         Enter your email address to follow this blog and receive notifications of new posts by email.", "links": ["https://yanirseroussi.com/", "https://yanirseroussi.com/", "https://yanirseroussi.com/about/", "https://yanirseroussi.com/presentations/", "https://yanirseroussi.com/phd-work/", "https://yanirseroussi.com/2014/04/05/kaggle-competition-summaries/", "http://scottberkun.com/yearwithoutpants/", "https://automattic.com/", "https://yanirseroussi.com/2015/03/22/the-long-road-to-a-lifestyle-business/", "http://web.archive.org/web/20150908140923/https://automattic.com/work-with-us/data-wrangler/", "https://yanirseroussi.com/2016/08/04/is-data-scientist-a-useless-job-title/", "https://automattic.com/about/", "https://www.glassdoor.com.au/Reviews/Automattic-Reviews-E751107.htm", "http://davemart.in/remote-hiring/", "https://www.carnextdoor.com.au/", "https://yanirseroussi.com/2015/12/08/this-holiday-season-give-me-real-insights/", "https://yanirseroussi.com/2015/12/08/this-holiday-season-give-me-real-insights/#comment-957", "http://jupyter.org/", "https://hbr.org/2014/01/hire-by-auditions-not-resumes", "https://www.elastic.co/products/elasticsearch", "https://github.com/jprante/elasticsearch-langdetect/pull/69", "https://github.com/jprante/elasticsearch-langdetect/pull/69", "https://yanirseroussi.com/2016/01/24/the-joys-of-offline-data-collection/", "https://www.whitsundaytimes.com.au/news/boat-caught-in-eye-of-cyclone-cruises-home/3164170/", "https://data.blog/2017/06/29/data-coalesce-automattic-data-division-meets-in-montreal/", "http://daynebatten.com/2016/06/counting-hard-data-science/", "http://powerfulpython.com/blog/magic-word-legacy-python/", "https://data.blog/2017/05/24/may-the-bot-be-with-you-how-algorithms-are-supporting-happiness-at-wordpress-com/", "https://data.blog/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=twitter", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=facebook", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=google-plus-1", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=linkedin", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=reddit", "https://yanirseroussi.com/category/data-science-2/", "https://yanirseroussi.com/tag/automattic/", "https://yanirseroussi.com/tag/career/", "https://yanirseroussi.com/tag/data-science/", "https://yanirseroussi.com/tag/elasticsearch/", "https://yanirseroussi.com/tag/personal/", "https://yanirseroussi.com/tag/wordpress/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "https://yanirseroussi.com/author/yanirseroussi/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comments", "https://yanirseroussi.com/2017/06/03/exploring-and-visualising-reef-life-survey-data/", "https://dotnetmeditations.com/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1698", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1698&_wpnonce=6c197b2684", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1698#respond", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1700", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1700&_wpnonce=350fa40ca9", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1700#respond", "http://yanirseroussi.com", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1705", "https://www.linkedin.com/in/yanirseroussi/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1705&_wpnonce=7f1e8d7061", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1705#respond", "https://gravatar.com/site/signup/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "https://yanirseroussi.com/2017/06/03/exploring-and-visualising-reef-life-survey-data/", "https://yanirseroussi.com/2017/01/08/customer-lifetime-value-and-the-proliferation-of-misinformation-on-the-internet/", "https://yanirseroussi.com/2016/09/19/ask-why-finding-motives-causes-and-purpose-in-data-science/", "https://yanirseroussi.com/2016/08/21/seven-ways-to-be-data-driven-off-a-cliff/", "https://yanirseroussi.com/category/bcrecommender/", "https://yanirseroussi.com/category/data-science-2/", "https://yanirseroussi.com/category/environment/", "https://yanirseroussi.com/category/general/", "https://yanirseroussi.com/category/kaggle-2/", "https://yanirseroussi.com/category/machine-intelligence/", "https://yanirseroussi.com/category/phd-work/", "https://wordpress.com/?ref=footer_blog"]}, "29": {"url": "https://medium.com/airbnb-engineering/using-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d", "title": "Using Machine Learning to Predict Value of Homes On\u00a0Airbnb", "text": "by Robert Chang Data products have always been an instrumental part of Airbnb\u2019s service. However, we have long recognized that it\u2019s costly to make data products. For example, personalized search ranking enables guests to more easily discover homes, and smart pricing allows hosts to set more competitive prices according to supply and demand. However, these projects each required a lot of dedicated data science and engineering time and effort. Recently, advances in Airbnb\u2019s machine learning infrastructure have lowered the cost significantly to deploy new machine learning models to production. For example, our ML Infra team built a general feature repository that allows users to leverage high quality, vetted, reusable features in their models. Data scientists have started to incorporate several AutoML tools into their workflows to speed up model selection and performance benchmarking. Additionally, ML infra created a new framework that will automatically translate Jupyter notebooks into Airflow pipelines. In this post, I will describe how these tools worked together to expedite the modeling process and hence lower the overall development costs for a specific use case of LTV modeling\u200a\u2014\u200apredicting the value of homes on Airbnb. Customer Lifetime Value (LTV), a popular concept among e-commerce and marketplace companies, captures the projected value of a user for a fixed time horizon, often measured in dollar terms. At e-commerce companies like Spotify or Netflix, LTV is often used to make pricing decisions like setting subscription fees. At marketplace companies like Airbnb, knowing users\u2019 LTVs enable us to allocate budget across different marketing channels more efficiently, calculate more precise bidding prices for online marketing based on keywords, and create better listing segments. While one can use past data to calculate the historical value of existing listings, we took one step further to predict LTV of new listings using machine learning. Data scientists are typically accustomed to machine learning related tasks such as feature engineering, prototyping, and model selection. However, taking a model prototype to production often requires an orthogonal set of data engineering skills that data scientists might not be familiar with. Luckily, At Airbnb we have machine learning tools that abstract away the engineering work behind productionizing ML models. In fact, we could not have put our model into production without these amazing tools. The remainder of this post is organized into four topics, along with the tools we used to tackle each task: One of the first steps of any supervised machine learning project is to define relevant features that are correlated with the chosen outcome variable, a process called feature engineering. For example, in predicting LTV, one might compute the percentage of the next 180 calendar dates that a listing is available or a listing\u2019s price relative to comparable listings in the same market. At Airbnb, feature engineering often means writing Hive queries to create features from scratch. However, this work is tedious and time consuming as it requires specific domain knowledge and business logic, which means the feature pipelines are often not easily sharable or even reusable. To make this work more scalable, we developed Zipline\u200a\u2014\u200aa training feature repository that provides features at different levels of granularity, such as at the host, guest, listing, or market level. The crowdsourced nature of this internal tool allows data scientists to use a wide variety of high quality, vetted features that others have prepared for past projects. If a desired feature is not available, a user can create her own feature with a feature configuration file like the following: When multiple features are required for the construction of a training set, Zipline will automatically perform intelligent key joins and backfill the training dataset behind the scenes. For the listing LTV model, we used existing Zipline features and also added a handful of our own. In sum, there were over 150 features in our model, including: With our features and outcome variable defined, we can now train a model to learn from our historical data. As in the example training dataset above, we often need to perform additional data processing before we can fit a model: In this step, we don\u2019t quite know what is the best set of features to use, so writing code that allows us to rapidly iterate is essential. The pipeline construct, commonly available in open-source tools like Scikit-Learn and Spark, is a very convenient tool for prototyping. Pipelines allow data scientists to specify high-level blueprints that describe how features should be transformed, and which models to train. To make it more concrete, below is a code snippet from our LTV model pipeline: At a high level, we use pipelines to specify data transformations for different types of features, depending on whether those features are of type binary, categorical, or numeric. FeatureUnion at the end simply combines the features column-wise to create the final training dataset. The advantage of writing prototypes with pipelines is that it abstracts away tedious data transformations using data transforms. Collectively, these transforms ensure that data will be transformed consistently across training and scoring, which solves a common problem of data transformation inconsistency when translating a prototype into production. Furthermore, pipelines also separates data transformations from model fitting. While not shown in the code above, data scientists can add a final step to specify an estimator for model fitting. By exploring different estimators, data scientists can perform model selection to pick the best model to improve the model\u2019s out of sample error. As mentioned in the previous section, we need to decide which candidate model is the best to put into production. To make such a decision, we need to weigh the tradeoffs between model interpretability and model complexity. For example, a sparse linear model might be very interpretable but not complex enough to generalize well. A tree based model might be flexible enough to capture non-linear patterns but not very interpretable. This is known as the Bias-Variance tradeoff. In applications such as insurance or credit screening, a model needs to be interpretable because it\u2019s important for the model to avoid inadvertently discriminating against certain customers. In applications such as image classification, however, it is much more important to have a performant classifier than an interpretable model. Given that model selection can be quite time consuming, we experimented with using various AutoML tools to speed up the process. By exploring a wide variety of models, we found which types of models tended to perform best. For example, we learned that eXtreme gradient boosted trees (XGBoost) significantly outperformed benchmark models such as mean response models, ridge regression models, and single decision trees. Given that our primary goal was to predict listing values, we felt comfortable productionizing our final model using XGBoost, which favors flexibility over interpretability. As we alluded to earlier, building a production pipeline is quite different from building a prototype on a local laptop. For example, how can we perform periodic re-training? How do we score a large number of examples efficiently? How do we build a pipeline to monitor model performance over time? At Airbnb, we built a framework called ML Automator that automagically translates a Jupyter notebook into an Airflow machine learning pipeline. This framework is designed specifically for data scientists who are already familiar with writing prototypes in Python, and want to take their model to production with limited experience in data engineering. Here is a code snippet demonstrating how the fit and transform functions are defined in our LTV model. The fit function tells the framework that a XGBoost model will be trained, and that data transformations will be carried out according to the pipeline we defined previously. Once the notebook is merged, ML Automator will wrap the trained model inside a Python UDF and create an Airflow pipeline like the one below. Data engineering tasks such as data serialization, scheduling of periodic re-training, and distributed scoring are all encapsulated as a part of this daily batch job. As a result, this framework significantly lowers the cost of model development for data scientists, as if there was a dedicated data engineer working alongside the data scientists to take the model into production! Note: Beyond productionization, there are other topics, such as tracking model performance over time or leveraging elastic compute environment for modeling, which we will not cover in this post. Rest assured, these are all active areas under development. In the past few months, data scientists have partnered very closely with ML Infra, and many great patterns and ideas arose out of this collaboration. In fact, we believe that these tools will unlock a new paradigm for how to develop machine learning models at Airbnb. We are very excited about the future of this framework and the new paradigm it brought along. By bridging the gap between prototyping and productionization, we can truly enable data scientists and engineers to pursue end-to-end machine learning projects and make our product better. Want to use or build these ML tools? We\u2019re always looking for talented people to join our Data Science and Analytics team! Special thanks to members of Data Science & ML Infra team who were involved in this work: Aaron Keys, Brad Hunter, Hamel Husain, Jiaying Shi, Krishna Puttaswamy, Michael Musson, Nick Handel, Varant Zanoyan, Vaughn Quoss et al. Additional thanks to Gary Tang, Jason Goodman, Jeff Feng, Lindsay Pettingill for reviewing this blog post.", "links": ["https://medium.com/", "https://medium.com/airbnb-engineering?source=logo-lo_cd9b3fefd19a---53c7c27702d5", "https://twitter.com/AirbnbEng", "https://medium.com/m/signin?redirect=https%3A%2F%2Fmedium.com%2Fairbnb-engineering%2Fusing-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d", "https://medium.com/airbnb-engineering", "https://medium.com/airbnb-engineering/ai/home", "https://medium.com/airbnb-engineering/airbnb-engineering-backend/home", "https://medium.com/airbnb-engineering/data/home", "https://medium.com/airbnb-engineering/airbnb-engineering-infrastructure/home", "https://medium.com/airbnb-engineering/tagged/mobile", "https://medium.com/airbnb-engineering/web/home", "http://airbnb.io/projects/", "https://medium.com/airbnb-engineering/search", "https://medium.com/@rchang?source=post_header_lockup", "https://medium.com/@rchang?source=post_header_lockup", "https://twitter.com/_rchang", "https://medium.com/swlh/diligence-at-social-capital-part-3-cohorts-and-revenue-ltv-ab65a07464e1", "http://scikit-learn.org/stable/", "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html", "https://www.kaggle.com/general/16927", "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html", "https://spark.apache.org/docs/latest/ml-pipeline.html", "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html", "http://scikit-learn.org/stable/data_transforms.html", "http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html", "https://medium.com/airbnb-engineering/automated-machine-learning-a-paradigm-shift-that-accelerates-data-scientist-productivity-airbnb-f1f8a10d61f8", "http://scott.fortmann-roe.com/docs/BiasVariance.html", "https://medium.com/airbnb-engineering/automated-machine-learning-a-paradigm-shift-that-accelerates-data-scientist-productivity-airbnb-f1f8a10d61f8", "https://github.com/dmlc/xgboost", "https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8", "http://www.florianwilhelm.info/2016/10/python_udf_in_hive/", "https://airflow.incubator.apache.org/", "https://www.airbnb.com/careers/departments/data-science-analytics", "https://www.linkedin.com/in/aaronkeys/", "https://www.linkedin.com/in/brad-hunter-497621a/", "https://www.linkedin.com/in/hamelhusain/", "https://www.linkedin.com/in/jiaying-shi-a2142733/", "https://www.linkedin.com/in/krishnaputtaswamy/", "https://www.linkedin.com/in/michael-m-a37b1932/", "https://www.linkedin.com/in/nicholashandel/", "https://www.linkedin.com/in/vzanoyan/", "https://www.linkedin.com/in/vquoss/", "https://www.linkedin.com/in/thegarytang/", "https://medium.com/@jasonkgoodman", "https://twitter.com/jtfeng", "https://medium.com/@lpettingill", "https://medium.com/@jtfeng?source=post_page", "https://medium.com/@jasonkgoodman?source=post_page", "https://medium.com/@gary.tang_94319?source=post_page", "https://medium.com/@lpettingill?source=post_page", "https://medium.com/@vquoss?source=post_page", "https://medium.com/@eddie.santos.3?source=post_page", "https://medium.com/@NicholasHandel?source=post_page", "https://medium.com/tag/machine-learning?source=post", "https://medium.com/tag/data-science?source=post", "https://medium.com/tag/airbnb?source=post", "https://medium.com/tag/technology?source=post", "https://medium.com/@rchang?source=footer_card", "https://medium.com/@rchang", "http://twitter.com/Airbnb", "http://twitter.com/Twitter", "https://medium.com/airbnb-engineering?source=footer_card", "https://medium.com/airbnb-engineering?source=footer_card", "http://airbnb.io", "https://medium.com/airbnb-engineering", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "30": {"url": "https://www.ayasdi.com/blog/artificial-intelligence/why_ai_is_the_new_bigdata/", "title": "The \u201cBig Data-ization\u201d of Artificial Intelligence", "text": "\u2039 Back to Blog  Artificial Intelligence, Data, Machine Intelligence, Machine Learning It seems like it was only a few years ago that the term \u201cbig data\u201d went from a promising area of research and interest to something so ubiquitous that it lost all meaning, descending ultimately into the butt of jokes.  As everyone piled onto the big data bandwagon, it became impossible to separate truth from fiction. Every executive and entrepreneur that I ran into was doing a \u201cbig data\u201d thing. I recall meeting someone whose company shot videos for enterprise customers and was pitching it as a \u2018Big Data play\u2019 \u2013 because video files, you know, are huge \u2013 they take up lots of space.  Thankfully, the noise associated with \u201cbig data\u201d is abating as sophistication and common sense take hold. In fact, in many circles the term actually exposes the user as someone who doesn\u2019t really understand the space.  Unfortunately, the same malady has now afflicted AI. Everyone I meet is doing an \u2018AI play\u2019 \u2013 even if all they did was to build a simple linear regressor in Excel.  AI is, unfortunately, the new \u201cbig data.\u201d While not good, it is not all bad either.  After all, the data ecosystem benefited from all of the \u201cbig data\u201d attention and investment \u2013 creating some amazing software and producing some exceptional productivity gains.  The same will happen with AI \u2013 with the increased attention comes investment dollars which in turn will drive adoption \u2013 enhancing the ecosystem. Having said that we need to stop calling regression on excel AI \u2013 it\u2019s ridiculous and undermines some incredible work being done in the space.  Regression has been around for 200+ years. Gauss and Legendre didn\u2019t don the AI cloak when they discovered regression. Neither should the recent stats graduate trying to raise money for his/her startup.  I will tell you what I think qualifies as AI in a moment \u2013 but here are some thoughts that influence that discussion. First, the AI definition I use is focused on the narrow, application specific AI, not the more general problem of artificial general intelligence (AGI) where simulating a person using software is the equivalent of intelligence.  Second, the vast, vast majority of the data that exists in the world is unlabeled. It is not practical to label that data manually and doing so would likely create bias anyway. One can argue that the Internet as a whole is simply a mechanism for humans to entertain ourselves while providing label data for machines.  Unlabeled data presents a different challenge, one we will address shortly, but the key point here is that it is everywhere and represents the key to extracting business value (or any value).  Third, we are not producing data scientists at a rate that can keep pace with the growth of data. Even with the moniker as a the \u201csexiest job of the 21st century\u201d the pace at which data scientists are created doesn\u2019t begin to approach the growth rate we are seeing in data.  Fourth, data scientists, for the most part, are not UX designers or product managers or, in many cases even engineers. As a result, the subject matter experts, those that sit in the business, don\u2019t have effective interfaces to the data science outputs. The interfaces that they have \u2013 powerpoint, excel, or PDF reports have limited utility in transforming the behavior of a company. What is required is something to shape behavior is something more \u2013 applications.  So what does qualify as intelligence? In \u2018On Intelligence\u2019 Jeff Hawkins says that all AI boils down to memory and prediction. The argument is very persuasive, but a bit reductive. We have a slightly different take for what an AI should display and it encompasses a framework: Here are characteristics that I think any AI should display. While some of these elements may seem self-evident that is because they are taken as a single item. Intelligence has a broader context. All the elements must work in conjunction with each other to qualify as AI. \u00a0\u00a0\u00a0 The five elements are: Let\u2019s take each of these concepts in turn. Discovery is the ability of an intelligent system to learn from data without upfront human intervention. Often, this needs to be done without being presented with an explicit target. It relies on the use of unsupervised and semi-supervised machine learning techniques (such as segmentation, dimensionality reduction, anomaly detection, etc.), as well as more supervised techniques where there is an outcome or there are several outcomes of interest. Usually, in enterprise software, the term discovery refers to the ability of ETL/MDM solutions to discover the various schemas of tables in large databases and automatically find join keys etc. This is not what we mean by discovery. We use of the term very differently and has this has important implications. In complex datasets, it is nearly impossible to ask the \u201cright\u201d questions. To discover what value lies within the data one must \u00a0understand all the relationships that are inherent and important in the data. That requires a principled approach to hypothesis generation. \u00a0 One technique, topological data analysis (TDA), is exceptional at surfacing hidden relationships that exist in the data and identifying those relationships that are meaningful without having to ask specific questions of the data. The result is an output that is able to represent complex phenomena, and is therefore able to surface weaker signals as well as the stronger signals. \u00a0 This permits the detection of emergent phenomena. \u00a0 As a result, enterprises can now discover answers to questions they didn\u2019t even know to ask and do so with data that is unlabeled. Once the data set is understood through intelligent discovery, supervised approaches are applied to predict what will happen in the future. These types of problems include classification, regression and ranking.  For this pillar, most companies use a standard set of supervised machine learning algorithms including random forests, gradient boosting, linear/sparse learners. It should be noted, however, that the unsupervised work from the previous step is highly useful in many ways. For example, it can generate relevant features for use in prediction tasks or finding local patches of data where supervised algorithms may struggle (systematic errors). The predict phase is an important part of the business value associated with data science, however, generally, in predictive analytics, there exists a notion that this is the sum total of machine learning.  This is not the case by far.  Prediction, while important, is pretty well understood and does not, on its own qualify as \u201cintelligence.\u201d \u00a0It goes back to calling a maxed out Excel table and a linear regressor AI. It is just not the case. Further, Prediction can go wrong along a number of dimensions, particularly if the groups on which you are predicting are racked with some type of bias (algorithmic, sampling etc.)  Again, Prediction is key and provides tremendous business value done correctly, but in and of itself it is not AI. We need to stop calling it as such. Applications need to support interaction with humans in a way which makes outcomes recognizable and believable. For example, when one builds a predictive model, it is important to have an explanation of how the model is doing what it is doing, i.e. what the features in the model are doing in terms that are familiar to the users of the model. \u00a0This level of familiarity is important in generating trust and intuition.  Similarly, in the same way that automobiles have mechanisms not just for detecting the presence of a malfunction, but also for specifying the nature of the malfunction and suggesting a method for correcting it, so one needs to have a \u201cnuts and bolts\u201d understanding of how an application is working in order to \u201crepair\u201d it when it goes awry. \u00a0 Transparency AND Justification. There is a difference. Transparency tells you what algorithms and parameters were used, while, Justification tells you why. For intelligence to be meaningful, it must be able to justify and explain its assertions, as well as to be able to diagnose failures.  No business leader should deploy intelligent and autonomous applications against critical business problems without a thorough understanding of what variables power the model.  Enterprises cannot move to a model of intelligent applications without trust and transparency. \u00a0 AI without UX is of limited utility.  UX is what distributes that intelligence across the organization and pushes it to the edge \u2013 where it can consumed by practitioners and subject matter experts.  Ultimately, the process of operationalizing an intelligent application within the enterprise requires some change in the organization, an acceptance that the application will evolve over time and that will demand downstream changes \u2013 automated or otherwise.  \u00a0For this to happen, intelligent applications need to be \u201clive\u201d in the business process, seeing new data and automatically executing the loop of Discover, Predict, Justify on a frequency that makes sense for that business process. For some processes that may be quarterly, for others daily. That loop can even be measured in seconds. \u00a0 Intelligent systems are designed to detect and react as the data evolves. An intelligent system is one that is always learning, live in the workflow and constantly improving. \u00a0In the modern data world, an application that is not getting more intelligent is getting dumber.  Intelligent applications are designed to detect and react when data distributions evolve. As a result, they need to be \u201con the wire\u201d in order to detect that phenomena before it becomes a problem. Too many solutions provide an answer in a point of time, an intelligent system is one that is always learning through the framework outlined here. This is what defines intelligence \u2013 not a machine learning algorithm kicking out predictions or the results of a data scientist\u2019s work as a PDF. For the industry to continue to grow and evolve we need to start doing a better job of recognizing what is truly AI and what is imposter-ware.  Let\u2019s make it a point of celebrating the real AI work.", "links": ["https://www.ayasdi.com", "https://www.ayasdi.com/platform/", "https://www.ayasdi.com/industry-overview/", "https://www.ayasdi.com/financial-services/", "https://www.ayasdi.com/healthcare/", "https://www.ayasdi.com/public-sector/", "https://www.ayasdi.com/solutions/anti-money-laundering/", "https://www.ayasdi.com/solutions/clinical-variation-management/", "https://www.ayasdi.com/solutions/denials/", "https://www.ayasdi.com/solutions/regulatory-risk/", "https://www.ayasdi.com/solutions/population-health/", "https://www.ayasdi.com/company/", "https://www.ayasdi.com/company/leadership/", "https://www.ayasdi.com/company/careers/", "https://www.ayasdi.com/company/collaborators/", "https://www.ayasdi.com/company/customers/", "https://www.ayasdi.com/company/news-and-events/", "https://www.ayasdi.com/company/contact-us/", "https://www.ayasdi.com/blog/", "https://www.ayasdi.com/request-a-demo/", "https://www.ayasdi.com/blog/author/gurjeet-singh/", "https://www.ayasdi.com/blog/category/artificial-intelligence/", "https://www.ayasdi.com/blog/category/bigdata/", "https://www.ayasdi.com/blog/category/machine-intelligence/", "https://www.ayasdi.com/blog/category/machine-learning/", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/08/02144138/BG2.png", "https://priceonomics.com/the-discovery-of-statistical-regression/", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/05/12132340/Machine_Intelligence_Apps_WP_051617v01.pdf", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/08/02144141/DS1.png", "https://www.wired.com/brandlab/2015/05/jeff-hawkins-firing-silicon-brain/", "http://cdn.ayasdi.com/wp-content/uploads/2015/02/wp-tda-and-machine-learning.pdf", "https://www.ayasdi.com/blog/aml/intelligent-segmentation-as-the-attack-point-for-aml/", "http://feedburner.google.com/fb/a/mailverify?uri=AyasdiBlog", "https://ayasdicommunity.force.com/login", "https://www.ayasdi.com/terms-of-service/", "https://www.ayasdi.com/privacy-policy/", "https://www.ayasdi.com/pilot-terms/", "http://www.youtube.com/user/ayasdi", "http://feeds.feedburner.com/AyasdiBlog", "http://www.linkedin.com/company/ayasdi", "https://www.facebook.com/ayasdi", "https://twitter.com/ayasdi"]}, "31": {"url": "https://www.mapd.com/blog/2017/08/03/whos-got-the-best-mobile-network-in-the-us/", "title": "", "text": "Based on the deluge of commercials and ads in this competitive industry, it seems every major player claims that they do. The competition is apparently so fierce that even their spokesmen are being headhunted. Admittedly, it can be a bit confusing when you find these results in one city:  And these in another:  By the way, how do you even define \u2018best\u2019 network? It\u2019s a loaded question for sure. We\u2019ll get back to that in a minute. It\u2019s a question with relevance to nearly every American, considering the vast majority of us own a cellphone of some kind (over 95% according to Pew). And we don\u2019t leave them alone; we touch them thousands of times each day. In fact, a good mobile network can affect our very quality of life. Ever travelled any significant distance with children and poor mobile data speeds? The same question is also relevant to those very telecommunications providers themselves, of course, and particularly in light of the fact that obtaining new subscribers will increasingly mean winning them away from competitors. Ensuring you are outpacing your rivals with superior network metrics is a key ingredient to both marketing and market share success. Our friends and partners at Tutela have created the world\u2019s largest mobile information platform, collecting billions of crowdsourced data points anonymously. These span device, network, and application information to allow us to get from data to insights. They\u2019ve graciously given us (and now you) a slice of their treasure trove. Our demo contains a few months of US data (~112M rows), although Tutela has data for more metrics, for nearly every country, and for a much greater timeframe. There are obviously many variables to consider when it comes to a great network, and one carrier will almost certainly not take the top spot in each category. But we can identify areas of significance where we hope to find favorable results: Spoiler alert: we\u2019re not actually going to attempt to find a winner. Those are just some of the metrics you can see at a glance on our MapD Immerse demo dashboard:  This is immediately useful. Coverage, speed, signal strength, and latency all matter, so MapD allows you to visualize and correlate them for effective consumption. Not coincidentally, the MapD Immerse visualization tool is purpose-built to leverage the MapD Core backend GPU-powered, in-memory, columnar SQL database at scale. This allows  interactive analysis over multi-billion row data sets, with the ability to drill down to an individual row...in milliseconds. It\u2019s also why Tutela has selected to partner with MapD; no need for indexing, sampling, or pre-computation. This means you can zoom in and out, filter, and more in an ad hoc fashion. Let\u2019s take it for a spin. As a consumer, you might be interested in how well each service provider performs in a certain geographic region (i.e., where you live and work) to aid in making a purchasing decision.  For example, to zoom from country level to the Washington, DC area, just type the city name into the searchbox of the pointmap. MapD Immerse will automatically zoom in when you click enter. It looks like in this region, you might lean toward Verizon:  But it may not be as clear cut if you live in San Francisco:  In either case, Sprint better have some compelling discounts or incentives to entice new subscribers. Further, both Sprint and T-Mobile might leverage this information to prioritize buildout or upgrade of cell towers. If they\u2019re analyzing subscriber loss in these areas, this also may give them insight into why that could be occurring. Another valuable feature of the MapD platform is the ability to easily identify trends, anomalies, or outliers. Visualization can often be an invaluable enabler for these situations. Sometimes new revenue opportunities or cost savings can be had by capitalizing on correlations in data that you didn\u2019t even realize existed! See the spike in data downloads in the line chart? By highlighting the time frame with your mouse, MapD will cross filter all charts using the selected time slice. Doing the same with the line chart for data uploads further reveals that the largest spikes occur on May 28 and 30. Ah, that\u2019s Memorial Day weekend in the United States. So this makes sense.  We can get even more granular and note that the bulk of data transfer occurred between 12-1am local time (4-5am UTC) as the heatmap indicates. I wonder if Uber and Lyft also experienced a spike around that time, as celebrations came to an end?  When data exploration at scale is so easy, what other interesting insights will you find? Explore this demo using Tutela's dataset on MapD\u2019s platform on your own here.  If you\u2019d like to learn more about MapD\u2019s recently announced partnership with Tutela, watch our on-demand webinar, introducing and demonstrating Tutela Explorer, a new mobile data analytics solution that provides real-time, interactive and highly visual insight into the performance of mobile networks and device usage. Tutela Explorer is available now as-a-service with global data coverage from Tutela.", "links": ["http://www.mapd.com/blog", "https://www.nytimes.com/2016/10/15/business/sprint-verizon-hear-me-now-paul-marcarelli.html", "http://www.pewinternet.org/fact-sheet/mobile/", "http://www.networkworld.com/article/3092446/smartphones/we-touch-our-phones-2617-times-a-day-says-study.html", "https://www.tutela.com/explorer/", "https://www.mapd.com/demos/telecom", "https://www.mapd.com/demos/telecom", "https://www.mapd.com/platform/core/", "https://www.mapd.com/demos/telecom", "https://www.brighttalk.com/webcast/14525/267851", "https://www.tutela.com/explorer"]}, "32": {"url": "https://www.oreilly.com/learning/building-a-simple-graphql-server-with-neo4j", "title": "Building a simple GraphQL server with Neo4j", "text": "How to implement a GraphQL API that queries Neo4j for a simple movie app. If you're interested in making the move from REST to GraphQL, check out this OSCON video from the team at Github. GraphQL is a powerful new tool for building APIs that allows clients to ask for only the data they need. Originally designed at Facebook to minimize data sent over the wire and reduce round-trip API requests for rendering views in native mobile apps, GraphQL has since been open sourced to a healthy community that is building developer tools. There are also a number of large companies and startups such as GitHub, Yelp, Coursera, Shopify, and Mattermark building public and internal GraphQL APIs. Despite what the name seems to imply, GraphQL is not a query language for graph databases, it is instead an API query language and runtime for building APIs. The \u201cGraph\u201d component of the name comes from the graph data model that GraphQL uses in the frontend. GraphQL itself is simply a specification, and there are many great tools available for building GraphQL APIs in almost every language. In this post we'll make use of graphql-tools by Apollo to build a simple GraphQL API in JavaScript that queries a Neo4j graph database for movies and movie recommendations. We will follow a recipe approach: first, exploring the problem in more detail, then developing our solution, and finally we discuss our approach. Good resources for learning more about GraphQL are GraphQL.org and the Apollo Dev Blog. GraphQL by design can work with any database or backend system, however in this example we'll be using the Neo4j graph database as our persistence layer. Why use a graph database with GraphQL? The idea of application data as a graph is an underpinning design choice of GraphQL. For example, think of customers who have placed orders that contain products\u2014that's a graph! GraphQL enables developers to translate their backend data into the application data graph on the frontend, but if we use a graph database on the backend we can do away with this impedance mismatch and we have graphs all the way down. We'd like to build a simple GraphQL API for a movie app that can do two things: In a traditional REST-ish API approach we might create two separate endpoints, perhaps /movies/search and /movies/similar. One endpoint to search for movies by substring of the title, and another to return a list of similar movies. As we add additional features we might keep adding endpoints. With GraphQL, our API is served from a single endpoint /graphql that takes one or more GraphQL queries and returns JSON data in a shape that is specified by the GraphQL query - only the data requested by the client is returned. Let's see how we can build this simple API. We'll be building a simple node.js JavaScript web server using Express.js to serve our GraphQL endpoint1. First of all we\u2019ll need a Neo4j database (with data) for our GraphQL server to query. For this example we\u2019ll make use of a Neo4j Sandbox instance. Neo4j Sandbox allows us to quickly spin up a hosted Neo4j instance, optionally with existing datasets focused around specific use cases. We\u2019ll use the Recommendations Neo4j Sandbox which includes data about movies and movie reviews and is designed to be used for generating personalized recommendations (for example, by using collaborative filtering to recommend movies based on similar users\u2019 reviews). We'll be making use of a few dependencies for this GraphQL server. I won't list them all here (you can see the full list in the package.json for this project, but there a few worth noting: We\u2019ll follow the \u201cGraphQL First\u201d development paradigm. In this approach, we start by defining a GraphQL schema. This schema defines the types and queries available in our API and then becomes the specification for the API. If we were building a complete application, the frontend developers could use this schema to build out the frontend while the backend team builds the backend in parallel, speeding development. Once we\u2019ve defined our schema we\u2019ll need to create resolver functions that are responsible for fetching data from Neo4j. This schema will define the types and GraphQL queries that we'll be able to use in our API. You can think of the schema as the API blueprint. schema.js Each GraphQL field is resolved by a resolver function. The resolver function defines how data is fetched for that field. resolvers.js Now that we've defined the GraphQL schema and resolver functions we are ready to serve the GraphQL endpoint, using Express.js. server.js If you've ever used Express this should look familiar, you'll notice that we're creating two endpoints and serving them on localhost:8080 Let's take a look at what we just did: Now that we have our API running, let's use the GraphiQL in-browser IDE for GraphQL to search for movies whose titles contain \"Matrix\" and find similar movies for each matching movie. We can load GraphiQL by opening http://loalhost:8080/graphiql in our browser and querying with this GraphQL query:   Querying our GraphQL movies API using GraphiQL, the in-browser IDE for GraphQL GraphQL is still new to almost everyone with the exception of Facebook, where it has been used since 2012, so many conventions and best practices are still being developed. However, GraphQL First Development is a philosophy the community has adopted which gives structure to the process of building a GraphQL API. By defining a contract for the API, expressed as a GraphQL schema, the frontend and backend developer teams can independently implement their applications, using the schema as a guide. To make GraphQL First Development even easier with Neo4j, the Neo4j team has built neo4j-graphql-cli, a command line tool for easily spinning up a Neo4j backed GraphQL endpoint based on a user defined GraphQL schema that can optionally be annotated with Cypher queries - exposing the power of Cypher from within GraphQL. You can learn more about this here. 1 All code for this project is available on GitHub . This example is also available as an Apollo Launchpad Pad, which you can run live in the browser.\u21a9 William Lyon is an engineer on the Developer Relations team at Neo4j, the open source graph database, where he builds tools for integrating Neo4j with other technologies and helps users be successful with graphs. He also leads Neo4j's Data Journalism Accelerator Program. Prior to Neo4j, he worked as a software engineer for a variety of startups, building mobile apps for the real estate industry, trading tools for quantitative finance, and predictive APIs. William holds a masters degree in Computer Science from the University of Montana. You can... How to package up your own repositories for use via PHP's Composer. Will content-blocking change the Web? Learn how to use Node.js and Express.js to quickly bootstrap a web server, tie in MongoDB, and display it all using dynamic HTML templates Diversity and inclusion in the tech industry has experienced a severe failure. Erica Baker approaches the topic like we would for any other service failure: with a postmortem. \u00a9 2017 O'Reilly Media, Inc. All trademarks and registered trademarks appearing on oreilly.com are the property of their respective owners.", "links": ["https://www.safaribooksonline.com/home/?utm_medium=content&utm_source=oreilly.com&utm_campaign=lgen&utm_content=20170601+nav", "http://www.oreilly.com/conferences/", "http://shop.oreilly.com/", "https://www.safaribooksonline.com/home/?utm_medium=content&utm_source=oreilly.com&utm_campaign=lgen&utm_content=20170601+nav", "http://www.oreilly.com/conferences/", "http://shop.oreilly.com/", "https://pixabay.com/en/louvre-pyramid-mesh-perspective-2189967/", "https://www.safaribooksonline.com/library/view/oscon-2017-/9781491976227/video306664.html?utm_source=oreilly&utm_medium=newsite&utm_campaign=building-a-simple-graphql-server-with-neo4j", "https://github.com/apollographql/graphql-tools", "http://www.apollodata.com/", "http://graphql.org/", "https://dev-blog.apollodata.com/", "https://neo4j.com/sandbox-v2/", "https://github.com/johnymontana/movies-graphql-neo4j-server/blob/master/package.json", "http://dev.apollodata.com/tools/graphql-tools/index.html", "https://expressjs.com/", "https://github.com/neo4j/neo4j-javascript-driver", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://www.graph.cool/docs/faq/graphql-sdl-schema-definition-language-kr84dktnp0/", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "http://loalhost:8080/graphiql", "https://www.npmjs.com/package/neo4j-graphql-cli", "http://neo4j.com/developer/graphql", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://pixabay.com/en/louvre-pyramid-mesh-perspective-2189967/", "https://twitter.com/share", "http://oreilly.com/about/", "http://oreilly.com/work-with-us.html", "http://oreilly.com/careers/", "http://shop.oreilly.com/category/customer-service.do", "http://shop.oreilly.com/category/customer-service.do", "http://fb.co/OReilly", "http://twitter.com/oreillymedia", "https://www.youtube.com/user/OreillyMedia", "https://plus.google.com/+oreillymedia", "https://www.linkedin.com/company/oreilly-media", "http://oreilly.com/terms/", "http://oreilly.com/privacy.html", "http://www.oreilly.com/about/editorial_independence.html"]}, "33": {"url": "http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "title": "", "text": "", "links": ["http://www.getrevue.co/?ref=Revue+Profile", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profileimage&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profiledate&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profilename&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lXPZA?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lXPZA?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blogs.technet.microsoft.com/machinelearning/2017/07/25/lessons-learned-benchmarking-fast-machine-learning-algorithms/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/DM9WJ?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/DM9WJ?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://medium.com/@stathis/design-by-evolution-393e41863f98?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/XYWKo?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/XYWKo?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.datasciencelearner.com/complete-overview-learning-python-data-analysis/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/6nba0?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/6nba0?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://reinforce.io/blog/introduction-to-tensorforce/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xekd3?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xekd3?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.kdnuggets.com/2017/05/top-10-machine-learning-videos-on-youtube-updated.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Ka8JK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Ka8JK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://arxiv.org/abs/1704.01568?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/96KED?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/96KED?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://datascienceplus.com/random-forests-in-r/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Vd1wK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Vd1wK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blog.keras.io/the-future-of-deep-learning.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/eKr5e?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/eKr5e?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://medium.com/airbnb-engineering/using-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/odRNK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/odRNK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://machinelearning.apple.com/2017/07/07/GAN.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xekoa?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xekoa?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://arxiv.org/abs/1707.06642?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/mxRw2?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/mxRw2?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.techleer.com/articles/200-naive-bayes-machine-learning-algorithm-for-classification-problems/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/3K1nx?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/3K1nx?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463?gi=7fe02170b69e&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/OnXK7?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/OnXK7?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://aiukraine.com/en/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/nqRVq?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/nqRVq?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.bonaccorso.eu/2017/07/23/machine-learning-algorithms/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Z4bBj?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Z4bBj?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/jobs", "http://bit.ly/2uqHV4J", "https://flyelephant.net/jobs/Spinbackup-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/jobs/Spinbackup-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/yJRmd?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/yJRmd?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/jobs/Snap-Ukraine-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/jobs/Snap-Ukraine-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/jAR3B?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/jAR3B?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%238%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "https://www.getrevue.co/?utm_source=Data Science Digest&utm_medium=email&utm_content=footerlink&utm_campaign=Issue", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=forwarded&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%238%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "http://www.getrevue.co/?ref=Revue+Profile"]}, "34": {"url": "https://blog.insightdatascience.com/preparing-for-the-transition-to-applied-ai-d41e48403447", "title": "Transitioning from Software Engineering to Artificial Intelligence", "text": "Emmanuel Ameisen and Jeremy Karnowski, Insight AI A significant part of the Software Engineer role requires staying up-to-date with evolving frameworks, standards, and paradigms. Software Engineers strive to constantly learn, in order to always use the best tool for the job. As Machine Learning finds footholds in more applications every day, it has naturally become a topic that many Engineers want to master. Machine Learning, though, is harder to pick up than a new framework. To be an efficient practitioner, you require a solid understanding of the theory of the field, broad knowledge of the current state of the art, and an ability to frame problems in a non deterministic way. Many guides you can find online will simply teach you how to train an out-of-the-box model on a curated data set to achieve good accuracy and call it a day. The truth is that a much more extensive skillset is essential in becoming an effective Machine Learning Engineer. Below is a distillation of the many conversations we\u2019ve had with over 50 top Machine Learning teams all over The Bay Area and New York, who\u2019ve come to Insight to find AI Practitioners poised to tackle their problems and accelerate their expansion into Applied AI. Deploying a Machine Learning solution requires much more than just training an arbitrary model on your data. It requires an understanding of: In other words, in addition to engineering chops, you need to understand the fundamentals of statistics, linear algebra, and optimization theory in order to integrate, deploy, and debug models. Building a custom Machine Learning solution for a problem requires that you consider issues ranging from acquiring, labeling and pre-processing your data to building, updating, and serving an inference model, and everything in between. Finally, building a REST API for a standard web-app is a task that we can deem feasible ahead of time. Machine Learning models, on the other hand are not always guaranteed to converge, or produce usable outputs. The best way to learn how to scope and deliver impactful Machine Learning products, is to understand how their theoretical underpinnings relate to the taxonomy of your data. In order to understand Machine Learning, a solid knowledge of statistics fundamentals is essential. This involves understanding the following: When you are training a neural network, what is actually happening? What makes some tasks doable and others not? A good approach to this might be to first try to understand Machine Learning through graphics and examples, before diving deeper into the theory. Concepts to understand range from how different loss functions work, why back propagation is useful, or what a computational graph is. A deep understanding is crucial both for building a functional model, and to communicate about it efficiently to the rest of the organization. Following are a few resources, starting with high level overviews, and diving deeper. Another fundamental skill is the ability to read, understand and implement research papers. It can seem like a daunting task at first, so a good way to start is to look up a paper that already has code attached to it (on GitXiv for example) and try to understand the implementation in depth. Ask any Data Scientist and they\u2019ll tell you 90% of the work they do is data munging. This is just as important for Applied AI, as the success of your model correlates hugely with the quality (and quantity) of your data. Data work comes in many aspects, and falls within a few categories: The best way to get familiar with data wrangling is to grab a dataset in the wild and try to use it. There are many datasets online and many social media and news outlets sites have great APIs. Following the steps above, a good way to learn is to: Debugging Machine Learning algorithms that fail to converge or to give sensible results involves a very different process from debugging code. In the same vein, finding the right architecture and hyperparameters requires solid theoretical fundamentals, but also good infrastructure work to be able to test different configurations out. Because of the pace at which the fields evolve, the methods to debug models are constantly evolving. Here are a few \u201csanity checks\u201d from our discussions and experience deploying models that mirror in some ways the principles of KISS familiar to many Software Engineers. A lot of those steps can be accelerated significantly by your development skills, which brings us to our last skill. A lot of Applied Machine Learning will allow you to leverage Software Engineering skills, sometimes with a little twist. These skills include: For more details on some of the software skills we recommend acquiring to become a quality Machine Learning Engineer, check out our post dedicated to transitioning to Applied AI from Academia. The resources above will help you approach and tackle actual Machine Learning problems. But the field of Applied AI changes extremely quickly, and the best way to learn, is to get your hands dirty and actually try to build out an end-to-end solution to solve a real problem. Action Items: Find some inspiration, then dive in! Remember that while Machine Learning Engineering is about building products at heart, there is a research aspect to it. You will explore models and paradigms that will prove unsuccessful, and that is perfectly fine, as it will lead you to understand the intricacies of the problem better. AI is an exciting, ever-changing field. The demand for Machine Learning Engineers is strong, and it is easy to get overwhelmed with the amount of news surrounding the topic. We recommend following a few serious sources and newsletters, to be able to separate PR and abstract research from innovations that are immediately relevant to the field. Here are some sources to help out: Want to learn applied Artificial Intelligence from top professionals in Silicon Valley or New York? Learn more about the Artificial Intelligence program.", "links": ["https://blog.insightdatascience.com?source=logo-lo_177e774be072---d02e65779d7b", "https://twitter.com/InsightDataSci", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.insightdatascience.com%2Fpreparing-for-the-transition-to-applied-ai-d41e48403447", "https://blog.insightdatascience.com", "https://blog.insightdatascience.com/tagged/about-insight", "https://blog.insightdatascience.com/tagged/insight-data-science", "https://blog.insightdatascience.com/tagged/insight-data-engineering", "https://blog.insightdatascience.com/tagged/insight-health-data", "https://blog.insightdatascience.com/tagged/insight-ai", "https://blog.insightdatascience.com/search", "https://blog.insightdatascience.com/@emmanuelameisen?source=post_header_lockup", "https://blog.insightdatascience.com/@emmanuelameisen?source=post_header_lockup", "https://www.udacity.com/course/deep-learning--ud730", "http://cs231n.stanford.edu/", "http://cs224d.stanford.edu/", "http://www.deeplearningbook.org/", "http://www.gitxiv.com/", "https://data.sfgov.org/", "https://www.data.gov/", "https://dev.twitter.com/streaming/public", "http://developer.nytimes.com/docs", "http://people.apache.org/~fhanik/kiss.html", "http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf", "https://github.com/JasperSnoek/spearmint", "https://blog.insightdatascience.com/preparing-for-the-transition-to-applied-ai-8eaf53624079", "https://news.ycombinator.com/", "https://jack-clark.net/import-ai/", "http://www.insightdatascience.com/blog/", "https://blog.insightdatascience.com/tagged/insight-ai", "https://blog.insightdatascience.com/using-deep-learning-to-reconstruct-high-resolution-audio-29deee8b7ccd", "https://blog.insightdatascience.com/separating-overlapping-chromosomes-with-deep-learning-based-image-segmentation-22f97afd3283", "http://insightdata.ai/notifications-list", "http:insightdata.ai", "https://medium.com/@jkarnows?source=post_page", "https://medium.com/@InsightData?source=post_page", "https://medium.com/@rossfadely?source=post_page", "https://blog.insightdatascience.com/tagged/machine-learning?source=post", "https://blog.insightdatascience.com/tagged/ai?source=post", "https://blog.insightdatascience.com/tagged/software-engineering?source=post", "https://blog.insightdatascience.com/tagged/insight-ai?source=post", "https://blog.insightdatascience.com/tagged/deep-learning?source=post", "https://blog.insightdatascience.com/@emmanuelameisen?source=footer_card", "https://blog.insightdatascience.com/@emmanuelameisen", "http://twitter.com/EmmanuelAmeisen", "https://blog.insightdatascience.com?source=footer_card", "https://blog.insightdatascience.com?source=footer_card", "https://blog.insightdatascience.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "35": {"url": "http://vooban.com/en/tips-articles-geek-stuff/hyperopt-tutorial-for-optimizing-neural-networks-hyperparameters/", "title": "", "text": "Hyperopt is a way to search through an hyperparameter space. For example, it can use the Tree-structured Parzen Estimator (TPE) algorithm, which explore intelligently the search space while narrowing down to the estimated best parameters. It is hence a good method for meta-optimizing a neural network which is itself an optimisation problem: tuning a neural network uses gradient descent methods, and tuning the hyperparameters needs to be done differently since gradient descent can\u2019t apply. Therefore, Hyperopt can be useful not only for tuning hyperparameters such as the learning rate, but also to tune more fancy parameters in a flexible way, such as changing the number of layers of certain types, or the number of neurons in a layer, or even the type of layer to use at a certain place in the network given an array of choices, each with nested tunable hyperparameters. This is an oriented random search, in contrast with a Grid Search where hyperparameters are pre-established with fixed steps increase. Random Search for Hyper-Parameter Optimization (such as what Hyperopt do) has proven to be an effective search technique. The paper about this technique sits among the most cited deep learning papers. To sum up, it is more efficient to search randomly through values and to intelligently narrow the search space rather than looping on fixed sets of values for the hyperparameters. Note that this blog post is also available as a Notebook on GitHub. It contains code that can be run with Jupyter. A parameter is defined with a certain uniformrange or else a probability distribution, such as: There is also a few quantized versions of those functions, which rounds the generated values at each step of \u201cq\u201d: It is also possible to use a \u201cchoice\u201d which can lead to hyperparameter nesting: Visualisations of the parameters for probability distributions can be found below. Then, more details on choices and parameter nesting will come. Note on the above charts (especially for the loguniform and uniform distributions): the blurred line averaging the values fades out toward the ends of the signal since it is zero-padded. The line ideally would not fade out by using techniques such as mirror-padding. Those are the best distributions for modeling the values a learning rate. That\u2019s because we want to observe changes in the learning rate according to changing it with multiplications rather than additions, e.g.: when adjusting the learning rate, we\u2019ll want to try to divide it or multiply it by 2 rather than adding and substracting a finite value. To proove this, let\u2019s generate a loguniform distribution for a multiplier of the learning rate, centered at 1.0. Dividing 1 by those values should yield the same distribution. Let\u2019s now define a simple search space and solve for f(x) = x^2 - x + 1, where x is an hyperparameter. Let\u2019s solve for minimizing f(x, y) = x^2 + y^2 using a space using a python dict as structure. Later, this will neable us to nest hyperparameters with choices in a clean way. Yet, we have defined spaces as a single parameter. But that is 1D. Normally, spaces contain many parameters. Let\u2019s define a more complex one and with one nested hyperparameter choice for an uniform float: This will require us to import a few more things, and return the results with a dict that has a \u201cstatus\u201d and \u201closs\u201d key at least. Let\u2019s keep in our return dict the evaluated space too as this may come in handy if we save results to disk. Note that the optimization could be parallelized by using MongoDB and storing the trials\u2019 state here. Althought this is a built-in feature of hyperopt, let\u2019s keep things simple for our examples here. Indeed, the TPE algorithm used by the fmin function has state which is stored in the trials and which is useful to narrow the search space dynamically once we have a few trials. It is then interesting to pause and resume a training, and to apply that to a real problem. This is what\u2019s done inside the hyperopt_optimize.py file of the GitHub repository for this project. There, as an example, we optimize a convolutional neural network for solving the CIFAR-100 problem. Star Fork You might as well like this other blog post of mine on how to use Git Large File Storage (Git LFS) to handle the versioning of huge files when working with machine learning projects. 510-1015 av. Wilfrid-Pelletier  Quebec, QC, Canada  G1W 0C4 418 800.0027 TOLL FREE: 1 844 800.0027 info@vooban.com", "links": ["http://vooban.com/en/", "http://vooban.com/en/", "http://vooban.com/en/", "http://vooban.com/en/products/", "http://vooban.com/en/products/#tpti", "http://vooban.com/en/products/#tdti", "http://vooban.com/en/approach/", "http://vooban.com/en/tactical-squad/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/", "http://vooban.com/en/tips-articles-geek-stuff/", "https://pdfs.semanticscholar.org/9f2a/efc3821853e963beda011ed770f740385b77.pdf", "http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a", "https://github.com/terryum/awesome-deep-learning-papers#optimization--training-techniques", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100/blob/master/IntroductionToHyperopt.ipynb", "http://jupyter.org/", "http://vooban.com/wp-content/uploads/2017/08/histogram_1_int.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_2_uniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_3_loguniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_4_normal.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_5_lognormal.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_6_inverse_loguniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_7_inverse_lognormal.png", "http://vooban.com/wp-content/uploads/2017/08/fx2.png", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100/fork", "http://vooban.com/en/tips-articles-geek-stuff/migrating-to-git-lfs-for-developing-deep-learning-applications-with-large-files/", "http://vooban.com/en/tips-articles-geek-stuff/discover-structure-behind-data-with-decision-trees/", "http://vooban.com/en/tips-articles-geek-stuff/", "https://www.google.ca/maps/place/Vooban/@46.7694849,-71.3055195,17z/data=!3m1!4b1!4m5!3m4!1s0x4cb890d721a9234b:0x979022270b4a9fa4!8m2!3d46.7694849!4d-71.3033308", "http://vooban.com/en/products/", "http://vooban.com/en/approach/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/", "https://twitter.com/vooban", "https://www.instagram.com/voobanvibe/", "https://www.facebook.com/vooban", "https://www.linkedin.com/company/vooban", "http://vooban.com/en/", "http://vooban.com/en/products/", "http://vooban.com/en/products/#tpti", "http://vooban.com/en/products/#tdti", "http://vooban.com/en/approach/", "http://vooban.com/en/tactical-squad/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/"]}, "36": {"url": "https://elitedatascience.com/learn-python-for-data-science", "title": "How to Learn Python for Data Science in 2017 (Updated)", "text": "In this guide, we\u2019ll cover how to learn Python for data science, including our favorite curriculum for self-study. You see, data science is about problem solving, exploration, and extracting valuable information from data. To do so effectively, you\u2019ll need to wrangle\u00a0datasets, train machine learning models, visualize results, and much more. Enter Python. This is the best time ever to learn Python. In fact, Forbes named it a top 10 technical skill in terms of job demand growth. Let\u2019s discuss why\u2026 Python is one of the most widespread languages in the world, and it has a passionate community of users: Python Popularity, TIOBE Index It has an even more loyal following within the data science profession. Some people judge the quality of a programming language by the simplicity of its \"hello, world!\" program. Python does pretty well by this standard: For comparison, here's the same output in Java: Great, case closed. See you back here\u00a0after you've mastered Python? Well, in all seriousness, simplicity is one of Python's greatest strengths. Thanks to its precise and efficient syntax, Python can accomplish the same tasks with less code than other languages. This makes implementing\u00a0solutions refreshingly fast. In addition, Python's vibrant data science community means you'll be able to find plenty of tutorials, code snippets, and people to commiserate with\u00a0fixes to common bugs. Stackoverflow will be one of your best friends. Finally,\u00a0Python\u00a0has an\u00a0all-star lineup of libraries (a.k.a. packages) for data analysis and machine learning, which drastically reduce the time it takes to produce results. More on these later. Before we go into what you'll need to learn, let's discuss what you won't need. You won't need a C.S. degree. Most data scientists will never deal with topics such as memory leaks, cryptography, or \"Big O\" notation. You'll be fine as long as you can write clean, logical code in a scripting language such as Python or R. You won't need a complete course on Python. Python and data science are\u00a0not synonymous. You won't need to memorize all the syntax. Instead, focus on grasping the intuition, such as when function is appropriate or how conditional statements work. You'll gradually remember the syntax after Googling, reading documentation, and good ol' fashioned practice. We recommend a top-down approach. We advocate\u00a0a top-down approach with the goal of getting results first and then solidifying concepts over time. In fact, we prefer to cut out \"classroom\" study in favor of real-world practice. This approach will allow you to build mastery over time while having more fun. There are many ways to install Python on your computer, but we recommend the Anaconda bundle, which comes with the libraries you'll need for data science. Effective programming is not about memorizing syntax, but rather mastering a new way of thinking. Therefore, take your time in building a solid foundation of core programming concepts. These will help you translate solutions in your head into instructions for a computer. If you are completely new to programming, we recommend the excellent Automate the Boring Stuff with Python book, which has been released for free online under a creative commons license. The book promises \"practical programming for total beginners,\" and it keeps each lesson down-to-earth. Read up to Chapter 6 - Manipulating Strings and complete the practice questions along the way. If you only need to brush up on Python syntax, then we recommend the following video, aptly named \"Learn Python in One Video:\" Again, the goal of this step is not to learn everything about Python and programming. Instead, focus on the intuition. You should be able to answer questions such as: If you'd like more practice with the core programming concepts, check out the following resources. Next, we're going to focus on the for data science part of \"how to learn Python for data science.\" As we mentioned earlier, Python has an all-star lineup of libraries for data science. Libraries are simply bundles of pre-existing functions and objects that you can import into your script to save time. These are the action steps we recommend for efficiently picking up a new library: We don't recommend diving much deeper into a library right now because you'll likely forget most of what you've learned by the time you jump into projects. Instead, aim to discover what each library is capable of. If you installed Python through the Anaconda bundle as we recommended above, it will also come with Jupyter Notebook. Jupyter Notebook is a lightweight IDE that's a favorite among data scientists. We recommend it for your projects. You can open a new notebook through Anaconda Navigator, which came with Anaconda. Check out this short video for instructions. These are the essential libraries you'll need: NumPy allows easy and efficient numeric computation, and many other data science libraries are built on top of it. Pandas is high-performance library for data structures and exploratory analysis. It's built on top of NumPy. Matplotlib is a flexible plotting and visualization library. It's powerful but somewhat cumbersome. You have the option of skipping Matplotlib for now and using Seaborn to get started (see our Seaborn recommendation below). Scikit-Learn is the premier general-purpose machine learning library in Python. It has many popular algorithms and modules for pre-processing, cross-validation, and much more. Seaborn makes it much easier to plot common data visualizations. It's built on top of Matplotlib and offers a more pleasant high-level wrapper. By now, you'll have a basic understanding of programming and a working knowledge of essential libraries. This actually covers most of the Python you'll need to get started with data science. At this point, some students will feel a bit overwhelmed. That's OK, and it's perfectly normal. If you were to take the slow and traditional bottom-up approach, you might feel less overwhelmed, but it would have taken you 10 times as long to get here. Now the key is to dive in immediately and start gluing everything together. Again, our goal up to here has been to just learn enough to get started. Next, it's time to solidify your knowledge through plenty of practice and projects. You have several options. The first option is to participate on Kaggle, a site that hosts data science competitions. The main advantage of Kaggle is that every project is self-contained. You're given the dataset, a goal, and tutorials to get you started. The major disadvantage of competitions is that they're usually not representative of real-world data science. The \"Getting Started\" competitions are way too basic while the standard competitions (i.e. those with prize pools) are usually too tough for beginners. If you're interested in this path, check out our Beginner's Guide to Kaggle. The next option is to structure your own projects and pick datasets that interest you. The main advantage of this approach is that the projects are more representative of real-world data science. You'll likely need to define your own goals, collect data, clean your dataset, engineer features, and so on. The disadvantage of DIY projects is that you'll need to already be familiar with a proper data science workflow. Without one, you could miss important steps or get stuck without knowing how to proceed. If you go with this path, check out our free 7-day crash course on applied machine learning, which covers the key steps in a data science workflow. We also have another article with several\u00a0DIY project ideas. Finally, there are guided end-to-end projects. Proper guided projects should combine the best of both words - they should be representative of real-world data science and allow you to solidify your skills through a carefully planned learning curve. Many data science bootcamps offer this as a main benefit. Bootcamps usually conclude with a \"capstone project\" that allows you to see all the moving pieces together, from start to finish. We've also crafted our own\u00a0Machine Learning Masterclass to solve this exact need. It will provide you over-the-shoulder mentorship for real-world projects while teaching you all of the key concepts in context. The masterclass also includes a comprehensive Python course that gets you up to speed ASAP. In fact, many successful students have enrolled without any prior programming experience. Learn more about it here. [\u2026] In terms of programming languages \u2013 Python is pretty powerful language with integration to all technologies in the CDH stack. For a basic course check out the link below \u2013 Python For Data Science [\u2026] * Denotes Required Field Copyright \u00a9 2017 \u00b7 EliteDataScience.com \u00b7 All Rights Reserved", "links": ["https://elitedatascience.com/", "https://elitedatascience.com/", "https://elitedatascience.com/blog", "https://elitedatascience.com/machine-learning-masterclass", "https://elitedatascience.com/about", "https://elitedatascience.com/about", "https://elitedatascience.com/guest-post-submissions", "https://elitedatascience.com/author/slice2o", "https://elitedatascience.com/learn-python-for-data-science#comments", "https://www.facebook.com/sharer.php?u=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://plus.google.com/share?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://www.linkedin.com/shareArticle?trk=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://twitter.com/intent/tweet?text=How%20to%20Learn%20Python%20for%20Data%20Science%20%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "https://www.forbes.com/sites/jeffkauflin/2017/01/08/the-10-technical-skills-with-explosive-growth-in-job-demand/#6fa6bf344f5c", "http://www.tiobe.com/tiobe-index/", "https://stackoverflow.com/questions/tagged/python?sort=votes&pageSize=15", "https://www.continuum.io/downloads", "https://automatetheboringstuff.com/", "https://automatetheboringstuff.com/", "https://codefights.com/", "http://www.pythonchallenge.com/index.php", "http://www.practicepython.org/", "http://interactivepython.org/runestone/static/thinkcspy/index.html", "http://jupyter.org/", "https://www.youtube.com/watch?v=-MyjG00la2k", "https://docs.scipy.org/doc/numpy/user/", "https://docs.scipy.org/doc/numpy-dev/user/quickstart.html", "https://pandas.pydata.org/pandas-docs/stable/", "https://pandas.pydata.org/pandas-docs/stable/10min.html", "https://matplotlib.org/contents.html", "https://matplotlib.org/users/pyplot_tutorial.html", "http://scikit-learn.org/stable/documentation.html", "http://elitedatascience.com/python-machine-learning-tutorial-scikit-learn", "https://seaborn.pydata.org/", "http://elitedatascience.com/python-seaborn-tutorial", "http://elitedatascience.com/beginner-kaggle", "http://elitedatascience.com/", "http://elitedatascience.com/machine-learning-projects-for-beginners", "http://elitedatascience.com/machine-learning-masterclass", "http://elitedatascience.com/machine-learning-masterclass", "https://www.facebook.com/sharer.php?u=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://plus.google.com/share?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://www.linkedin.com/shareArticle?trk=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://twitter.com/intent/tweet?text=How%20to%20Learn%20Python%20for%20Data%20Science%20%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "https://datascienceandmore.wordpress.com/2017/03/04/getting-the-basics-down/", "https://elitedatascience.com/learn-machine-learning", "https://elitedatascience.com/learn-python-for-data-science", "https://elitedatascience.com/feature-engineering-best-practices", "https://elitedatascience.com/beginner-kaggle", "https://elitedatascience.com/imbalanced-classes", "https://elitedatascience.com/beginner-mistakes", "https://elitedatascience.com/bias-variance-tradeoff", "https://elitedatascience.com/data-science-resources", "https://elitedatascience.com/guest-post-submissions", "https://elitedatascience.com/", "https://elitedatascience.com/terms-of-service", "https://elitedatascience.com/privacy-policy", "https://elitedatascience.com/about", "https://www.linkedin.com/shareArticle?trk=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science", "http://www.facebook.com/sharer.php?u=https://elitedatascience.com/learn-python-for-data-science", "https://plus.google.com/share?text=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science", "https://twitter.com/intent/tweet?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "http://service.weibo.com/share/share.php?url=https://elitedatascience.com/learn-python-for-data-science&title=How to Learn Python for Data Science (Updated)", "https://getpocket.com/save?title=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science"]}, "37": {"url": "https://stats.stackexchange.com/questions/297380/why-not-just-dump-the-neural-networks-and-deep-learning", "title": "", "text": "Fundamental problem with deep learning and neural networks in general. The solutions that fit training data are infinite. We don't have precise mathematical equation that is satisfied by only a single one and that we can say generalizes best. Simply speaking we don't know which generalizes best. Optimizing weights is not a convex problem, so we never know we end up with a global or a local minimum. So why not just dump the neural networks and instead search for a better ML model? Something that we understand, and something that is consistent with a set of mathematical equations? Linear and SVM do not have this mathematical drawbacks and are fully consistent with a a set of mathematical equations. Why not just think on same lines (need not be linear though) and come up with a new ML model better than Linear and SVM and neural networks and deep learning? Not being able to know what solution generalizes best is an issue, but it shouldn't deter us from otherwise using a good solution. Humans themselves often do not known what generalizes best (consider, for example, competing unifying theories of physics), but that doesn't cause us too many problems. It has been shown that it is extremely rare for training to fail because of local minimums. Most of the local minimums in a deep neural network are close in value to the global minimum, so this is not an issue. source But the broader answer is that you can talk all day about nonconvexity and model selection, and people will still use neural networks simply because they work better than anything else (at least on things like image classification).  Of course there are also people arguing that we shouldn't get too focused on CNNs like the community was focused on SVMs a few decades ago, and instead keep looking for the next big thing. In particular, I think I remember Hinton regretting the effectiveness of CNNs as something which might hinder research. related post As the comments to your question point out, there are a lot of people working on finding something better. I would though like to answer this question by expanding the comment left by @josh All models are wrong but some are useful (Wiki) The above statement is a general truth used to describe the nature of statistical models. Using data that we have available, we can create models that let us do useful things such as approximate a predicted value. Take for example Linear Regression  Using a number of observations, we can fit a model to give us an approximate value for a dependent variable given any value(s) for the independent variable(s).  Burnham, K. P.; Anderson, D. R. (2002), Model Selection and Multimodel > Inference: A Practical Information-Theoretic Approach (2nd ed.): \"A model is a simplification or approximation of reality and hence   will not reflect all of reality. ... Box noted that \u201call models are   wrong, but some are useful.\u201d While a model can never be \u201ctruth,\u201d a   model might be ranked from very useful, to useful, to somewhat useful   to, finally, essentially useless.\" Deviations from our model (as can be seen in the image above) appear random, some observations are below the line and some are above, but our regression line shows a general correlation. Whilst deviations in our model appear random, in realistic scenarios there will be other factors at play which cause this deviation. For example, imagine watching cars as they drove through a junction where they must turn either left or right to continue, the cars turn in no particular pattern. Whilst we could say that the direction the cars turn is completely random, does every driver reach the junction and at that point make a random decision of which way to turn? In reality they are probably heading somewhere specific for a specific reason, and without attempting to stop each car to ask them about their reasoning, we can only describe their actions as random.  Where we are able to fit a model with minimal deviation, how certain can we be that an unknown, unnoticed or immeasurable variable wont at some point throw our model? Does the flap of a butterfly\u2019s wings in Brazil set off a tornado in Texas? The problem with using the Linear and SVN models you mention alone is that we are somewhat required to manually observe our variables and how they each affect each other. We then need to decide what variables are important and write a task-specific algorithm. This can be straight forward if we only have a few variables, but what if we had thousands? What if we wanted to create a generalised image recognition model, could this realistically be achieved with this approach?  Deep Learning and Artificial Neural Networks (ANNs) can help us create useful models for huge data sets containing huge amounts of variables (e.g. image libraries). As you mention, there's an incomprehensible number of solutions which could fit the data using ANNs, but is this number really any different to the amount of solutions we would need to develop ourselves through trial and error? The application of ANNs do much of the work for us, we can specify our inputs and our desired outputs (and tweak them later to make improvements) and leave it up to the ANN to figure out the solution. This is why ANNs are often described as \"black boxes\". From a given input they output an approximation, however (in general terms) these approximations don't include details on how they were approximated. And so it really comes down to what problem you are trying to solve, as the problem will dictate what model approach is more useful. Models are not absolutely accurate and so there is always an element of being 'wrong', however the more accurate your results the more useful they are. Having more detail in the results on how the approximation was made may also be useful, depending on the problem it may even be more useful than increased accuracy. If for example you are calculating a persons credit score, using regression and SVMs provides calculations that can be better explored. Being able to both tweak the model directly and explain to customers the effect separate independent variables have on their overall score is very useful. An ANN may aid in processing larger amounts of variables to achieve a more accurate score, but would this accuracy be more useful? I guess for some problem we care less for the mathematical rigor and simplicity but more for its utility, current status is neural network is better in performing certain task like pattern recognition in image processing.  The global minimum may as well as be useless, so we don't really care if we find it or not. The reason is that, for deep networks, not only the time to find it becomes exponentially longer as the network size increases, but also the global minimum often corresponds to overfitting the training set. Thus the generalization ability of the DNN (which is what we really care about) would suffer. Also, often we prefer flatter minima corresponding to a higher value of the loss function, than sharper minima corresponding to a lower value of the loss function, because the second one will deal very badly with uncertainty in the inputs. This is becoming increasingly clear with the development of Bayesian Deep Learning. Robust Optimization beats Determinist Optimization very often, when applied to real world problems where uncertainty is important. Finally, it's a fact that DNNs just kick the ass of methods such as XGBoost at image classification and NLP. A company which must make a profit out of image classification will correctly select them as modelsto be deployed in production (and invest a significant amount of money on feature engineering, data pipeline, etc. but I digress). This doesn't mean that they dominate all the ML environment: for example, they do worse than XGBoost on structured data (see the last winners of Kaggle competitions) and they seem to not still do as well as particle filters on time series modelling. However, some very recent innovations on RNNs may modify this situation. There are excellent answers, mostly weighing in with the usefulness of DL and ANNs. But I would like to object the OP in a more fundamental way, since the question already takes for granted the mathematical inconsistency of neural networks. First of all, there is a mathematical theory behind (most models of) Neural Networks. You could likewise argue that linear regression does not generalize, unless the underlying model is... well, linear. In neural algorithms, a model is assumed (even if not explicitly) and the fitting error is computed. The fact that algorithms are modified with various heuristics does not void the original mathematical support. BTW, local optimization is also a mathematically consistent, let alone useful, theory. Along this line, if Neural Networks just constitute one class of methods within the whole toolbox of scientists, which is the line that separates Neural Networks from the rest of techniques? In fact, SVMs were once considered a class of NNs and they still appear in the same books. On the other hand, NNs could be regarded as a (nonlinear) regression technique, maybe with some simplification. I agree with the OP that we must search better, well founded, efficient algorithms, regardless you label them as NNs or not. Sign up using Google Sign up using Facebook Sign up using Email and Password   By posting your answer, you agree to the privacy policy and terms of service. asked 3 days ago viewed  7,704 times  active yesterday                                   site design / logo \u00a9 2017 Stack Exchange Inc; user contributions licensed under cc by-sa 3.0                                  with attribution required.                                  rev 2017.8.11.26777", "links": ["https://stats.stackexchange.com", "https://chat.stackexchange.com?tab=site&host=stats.stackexchange.com", "https://stats.stackexchange.com", "https://stats.meta.stackexchange.com", "https://stats.stackexchange.com/users/signup?ssrc=site_switcher&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.stackexchange.com/users/login?ssrc=site_switcher&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stackexchange.com/sites", "https://stackoverflow.blog", "https://stackexchange.com", "https://stats.stackexchange.com/users/signup?ssrc=head&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.stackexchange.com/users/login?ssrc=head&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.meta.stackexchange.com", "https://stackoverflow.com/company/about", "https://www.stackoverflowbusiness.com/?ref=topbar_help", "https://stats.stackexchange.com", "https://arxiv.org/pdf/1406.2572.pdf", "https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/clyohpf/?context=3", "https://arxiv.org/pdf/1406.2572.pdf", "https://arxiv.org/pdf/1412.0233.pdf", "https://en.wikipedia.org/wiki/All_models_are_wrong", "https://en.wikipedia.org/wiki/Butterfly_effect", "https://stats.stackexchange.com/questions/93705/meaning-of-a-neural-network-as-a-black-box", "https://stackexchange.com/legal/privacy-policy", "https://stackexchange.com/legal/terms-of-service", "http://stats.stackexchange.com/election", "http://stats.stackexchange.com/election", "https://stackexchange.com/questions?tab=hot", "https://english.stackexchange.com/questions/405590/does-een-denote-endearment", "https://academia.stackexchange.com/questions/94443/are-course-grade-distributions-supposed-to-be-bell-shaped", "https://mathoverflow.net/questions/278629/a-combinatorial-identity", "https://codereview.stackexchange.com/questions/172849/checking-if-a-number-is-power-of-2-or-not", "https://space.stackexchange.com/questions/22590/how-did-people-know-how-to-build-the-first-space-ship", "https://latin.stackexchange.com/questions/5045/why-is-the-roman-acronym-spqr-and-not-spr", "https://codegolf.stackexchange.com/questions/138790/no-co-prime-neighbors", "https://travel.stackexchange.com/questions/100091/can-us-embassy-help-a-broke-american-citizen-overstayed-in-emirates", "https://codegolf.stackexchange.com/questions/138887/implement-true-string-addition", "https://cseducators.stackexchange.com/questions/3175/learning-fundamental-differences-between-functional-programming-and-object-orien", "https://rpg.stackexchange.com/questions/105287/when-do-you-need-a-free-hand-for-casting-shield", "https://interpersonal.stackexchange.com/questions/1539/is-it-rude-to-ask-tourists-where-they-are-from", "https://mechanics.stackexchange.com/questions/47056/99-ranger-rear-brake-drum-wont-come-off", "https://german.stackexchange.com/questions/38460/why-are-some-words-spelled-with-tz-if-z-already-has-the-ts-sound", "https://worldbuilding.stackexchange.com/questions/88588/in-a-fantasy-world-where-physical-training-has-no-hard-limit-why-isnt-everyone", "https://interpersonal.stackexchange.com/questions/1585/how-do-i-react-when-a-girl-i-like-has-a-new-haircut-that-i-dont-like-very-much", "https://mathoverflow.net/questions/278659/a-sum-involving-euler-totient-function", "https://photo.stackexchange.com/questions/91713/why-is-there-single-in-the-term-dslr-digital-single-lens-reflex", "https://physics.stackexchange.com/questions/351628/why-must-entangled-particles-communicate-their-spin-instantaneously", "https://ell.stackexchange.com/questions/138943/jokes-played-or-cracked", "https://stats.stackexchange.com/questions/297711/what-is-the-difference-between-moment-generating-function-and-probability-genera", "https://interpersonal.stackexchange.com/questions/1737/how-do-i-tell-a-co-worker-that-just-because-something-is-a-hobby-doesnt-mean-i", "https://math.stackexchange.com/questions/2392479/when-is-a-vector-glued-to-the-origin", "https://math.stackexchange.com/questions/2392029/why-does-rudin-say-the-rational-number-system-is-inadequate-as-a-field", "https://chat.stackexchange.com?tab=site&host=stats.stackexchange.com", "https://stats.meta.stackexchange.com", "https://stackoverflow.com/company/about", "https://stackoverflow.com", "https://www.stackoverflowbusiness.com/?utm_source=so-footer&utm_medium=referral&utm_campaign=brand-activation", "https://stackoverflow.com/jobs", "https://stackoverflow.com/company/about", "https://stackoverflow.com/company/press", "https://stackexchange.com/legal", "https://stackexchange.com/legal/privacy-policy", "https://stackexchange.com", "https://stackexchange.com/sites#technology", "https://stackexchange.com/sites#lifearts", "https://stackexchange.com/sites#culturerecreation", "https://stackexchange.com/sites#science", "https://api.stackexchange.com", "https://data.stackexchange.com", "https://stackoverflow.blog?blb=1", "https://www.facebook.com/officialstackoverflow/", "https://twitter.com/stackoverflow", "https://linkedin.com/company/stack-overflow", "https://creativecommons.org/licenses/by-sa/3.0/", "https://stackoverflow.blog/2009/06/25/attribution-required/"]}, "38": {"url": "https://blog.godatadriven.com/practical-airflow-tutorial", "title": "", "text": "Airflow is a scheduler for workflows such as data pipelines, similar to Luigi and Oozie. It's written in Python and we at GoDataDriven have been contributing to it in the last few months. This tutorial is loosely based on the Airflow tutorial in the official documentation. It will walk you through the basics of setting up Airflow and creating an Airflow workflow, and it will give you some practical tips. A (possibly) more up-to-date version of this blog can be found in my git repo. Setting up a basic configuration of Airflow is pretty straightforward. After installing the Python package, we'll need a database to store some data and start the core Airflow services. You can skip this section if Airflow is already set up. Make sure that you can run airflow commands, know where to put your DAGs and have access to the web UI. Airflow is installable with pip via a simple pip install apache-airflow. Either use a separate Python virtual environment or install it in your default python environment. To use the conda virtual environment as defined in environment.yml from my git repo: You should now have an (almost) working Airflow installation. Alternatively, install Airflow yourself by running: Airflow used to be packaged as airflow but is packaged as apache-airflow since version 1.8.1. Make sure that you install any extra packages with the right Python package: e.g. use pip install apache-airflow[dask] if you've installed apache-airflow and do not use pip install airflow[dask]. Leaving out the prefix apache- will install an old version of Airflow next to your current version, leading to a world of hurt. You may run into problems if you don't have the right binaries or Python packages installed for certain backends or operators. When specifying support for e.g. PostgreSQL when installing extra Airflow packages, make sure the database is installed; do a brew install postgresql or apt-get install postgresql before the pip install apache-airflow[postgres]. Similarly, when running into HiveOperator errors, do a pip install apache-airflow[hive] and make sure you can use Hive. Before you can use Airflow you have to initialize its database. The database contains information about historical & running workflows, connections to external data sources,  user management, etc. Once the database is set up, Airflow's UI can be accessed by running a web server and workflows can be started. The default database is a SQLite database, which is fine for this tutorial. In a production setting you'll probably be using something like MySQL or PostgreSQL. You'll probably want to back it up as this database stores the state of everything related to Airflow. Airflow will use the directory set in the environment variable AIRFLOW_HOME to store its configuration and our SQlite database. This directory will be used after your first Airflow command. If you don't set the environment variable AIRFLOW_HOME, Airflow will create the directory ~/airflow/ to put its files in. Set environment variable AIRFLOW_HOME to e.g. your current directory $(pwd): or any other suitable directory. Next, initialize the database: Now start the web server and go to localhost:8080 to check out the UI: It should look something like this:  With the web server running workflows can be started from a new terminal window. Open a new terminal, activate the virtual environment and set the environment variable AIRFLOW_HOME for this terminal as well: Make sure that you're an in the same directory as before when using $(pwd). Run a supplied example: And check in the web UI that it has run by going to Browse -> Task Instances. This concludes all the setting up that you need for this tutorial. We'll create a workflow by specifying actions as a Directed Acyclic Graph (DAG) in Python. The tasks of a workflow make up a Graph; the graph is Directed because the tasks are ordered; and we don't want to get stuck in an eternal loop so the graph also has to be Acyclic. The figure below shows an example of a DAG:  The DAG of this tutorial is a bit easier. It will consist of the following tasks: and we'll plan daily execution of this workflow. Go to the folder that you've designated to be your AIRFLOW_HOME and find the DAGs folder located in subfolder dags/ (if you cannot find, check the setting dags_folder in $AIRFLOW_HOME/airflow.cfg). Create a Python file with the name airflow_tutorial.py that will contain your DAG. Your workflow will automatically be picked up and scheduled to run. First we'll configure settings that are shared by all our tasks. Settings for tasks can be passed as arguments when creating them, but we can also pass a dictionary with default values to the DAG. This allows us to share default arguments for all the tasks in our DAG is the best place to set e.g. the owner and start date of our DAG. Add the following import and dictionary to airflow_tutorial.py to specify the owner, start time, and retry settings that are shared by our tasks: These settings tell Airflow that this workflow is owned by 'me', that the workflow is valid since June 1st of 2017, it should not send emails and it is allowed to retry the workflow once if it fails with a delay of 5 minutes. Other common default arguments are email settings on failure and the end time. We'll now create a DAG object that will contain our tasks. Name it airflow_tutorial_v01 and pass default_args: With schedule_interval='0 * * * *' we've specified a run at every hour 0; the DAG will run each day at 00:00. See crontab.guru for help deciphering cron schedule expressions. Alternatively, you can use strings like '@daily' and '@hourly'. We've used a context manager to create a DAG (new since 1.8). All the tasks for the DAG should be indented to indicate that they are part of this DAG. Without this context manager you'd have to set the dag parameter for each of your tasks. Airflow will generate DAG runs from the start_date with the specified schedule_interval. Once a DAG is active, Airflow continuously checks in the database if all the DAG runs have successfully ran since the start_date. Any missing DAG runs are automatically scheduled. When you initialize on 2016-01-04 a DAG with a start_date at 2016-01-01 and a daily schedule_interval, Airflow will schedule DAG runs for all the days between 2016-01-01 and 2016-01-04. A run starts after the time for the run has passed. The time for which the workflow runs is called the execution_date. The daily workflow for 2016-06-02 runs after 2016-06-02 23:59 and the hourly workflow for 2016-07-03 01:00 starts after 2016-07-03 01:59. From the ETL viewpoint this makes sense: you can only process the daily data for a day after it has passed. This can, however, ask for some juggling with date for other workflows. For Machine Learning models you may want to use all the data up to a given date, you'll have to add the schedule_interval to your execution_date somewhere in the workflow logic. Because Airflow saves all the (scheduled) DAG runs in its database, you should not change the start_date and schedule_interval of a DAG. Instead, up the version number of the DAG (e.g. airflow_tutorial_v02) and avoid running unnecessary tasks by using the web interface or command line tools Timezones and especially daylight savings can mean trouble when scheduling things, so keep your Airflow machine in UTC. You don't want to skip an hour because daylight savings kicks in (or out). Tasks are represented by operators that either perform an action, transfer data, or sense if something has been done. Examples of actions are running a bash script or calling a Python function; of transfers are copying tables between databases or uploading a file; and of sensors are checking if a file exists or data has been added to a database. We'll create a workflow consisting of three tasks: we'll print 'hello', wait for 10 seconds and finally print 'world'.  The first two are done with the BashOperator and the latter with the PythonOperator. Give each operator an unique task ID and something to do: Note how we can pass bash commands in the BashOperator and that the PythonOperator asks for a Python function that can be called. Dependencies in tasks are added by setting other actions as upstream (or downstream).  Link the operations in a chain so that sleep will be run after print_hello and is followed by print_world; print_hello -> sleep -> print_world: After rearranging the code your final DAG should look something like: First check that DAG file contains valid Python code by executing the file with Python: You can manually test a single task for a given execution_date with airflow test: This runs the task locally as if it was for 2017-07-01, ignoring other tasks and without communicating to the database. Now that you're confident that your dag works, turn on the DAG in the web UI and sit back while Airflow starts backfilling the dag runs! You now know the basics of setting up Airflow, creating a DAG and turning it on; time to go deeper!  Address: Wibautstraat 202, 1091 GS Amsterdam, The Netherlands           Phone: +31 (0)35 672 9069           Email: signal@godatadriven.com", "links": ["http://godatadriven.com/contact", "http://godatadriven.com/careers", "http://blog.godatadriven.com", "http://godatadriven.com/business-consultancy", "http://godatadriven.com/big-data-infrastructure", "http://godatadriven.com/predictive-modelling", "http://godatadriven.com/products-overview", "http://godatadriven.com/data-discovery", "http://godatadriven.com/data-science-audit", "http://godatadriven.com/data-science-suite", "http://godatadriven.com/data-science-box", "http://godatadriven.com/divolte", "http://godatadriven.com/customers", "http://godatadriven.com/casestudy-airfranceklm", "http://godatadriven.com/casestudy-bakkersland", "http://godatadriven.com/casestudy-bol", "http://godatadriven.com/casestudy-eneco", "http://godatadriven.com/casestudy-eretail", "http://godatadriven.com/casestudy-ing", "http://godatadriven.com/casestudy-nuon", "http://godatadriven.com/casestudy-npo", "http://godatadriven.com/casestudy-schipholgroup", "http://godatadriven.com/casestudy-transavia", "http://godatadriven.com/technology-overview", "http://godatadriven.com/databricks", "http://godatadriven.com/hadoop", "http://godatadriven.com/python", "http://godatadriven.com/r", "http://godatadriven.com/spark", "http://godatadriven.com/training-overview", "http://godatadriven.com/training-schedule", "http://godatadriven.com/bigdata-training", "http://godatadriven.com/datascience-training", "http://godatadriven.com/data-science-accelerator-program", "http://godatadriven.com/meetups", "http://godatadriven.com/events", "http://blog.godatadriven.com", "http://godatadriven.com/our-story", "http://godatadriven.com/team", "http://godatadriven.com/careers", "http://godatadriven.com/media-resources", "https://blog.godatadriven.com", "http://godatadriven.com/contact", "http://godatadriven.com", "http://blog.godatadriven.com", "https://airflow.incubator.apache.org/", "https://github.com/spotify/luigi", "https://oozie.apache.org/", "https://blog.godatadriven.com/open-source-201702", "https://blog.godatadriven.com/open-source-201703", "https://blog.godatadriven.com/open-source-201704", "https://blog.godatadriven.com/open-source-201705", "https://blog.godatadriven.com/open-source-201707", "https://pythonhosted.org/airflow/tutorial.html", "https://github.com/hgrif/airflow-tutorial", "https://github.com/hgrif/airflow-tutorial", "http://conda.pydata.org/miniconda.html", "http://localhost:8080/", "https://airflow.incubator.apache.org/configuration.html", "https://airflow.incubator.apache.org/security.html", "https://github.com/apache/incubator-airflow/tree/master/scripts", "https://crontab.guru/#0_*_*_*_*", "https://jeffknupp.com/blog/2016/03/07/python-with-context-managers/", "https://airflow.incubator.apache.org/configuration.html#connections", "https://airflow.incubator.apache.org/ui.html#variable-view", "https://airflow.incubator.apache.org/tutorial.html#templating-with-jinja", "https://github.com/apache/incubator-airflow/blob/master/airflow/example_dags/example_bash_operator.py", "https://airflow.incubator.apache.org/code.html#airflow.operators.PythonOperator", "https://pythonhosted.org/airflow/tutorial.html", "https://gtoonstra.github.io/etl-with-airflow/", "https://medium.com/handy-tech/airflow-tips-tricks-and-pitfalls-9ba53fba14eb", "http://godatadriven.com/players/henk-griffioen", "http://blog.godatadriven.com/practical-airflow-tutorial", "http://blog.godatadriven.com/open-source-201707", "http://blog.godatadriven.com/cd-python-eggs-vsts", "http://blog.godatadriven.com/ldappy", "http://blog.godatadriven.com/open-source-201706", "http://blog.godatadriven.com/vendor-free-ds", "https://twitter.com/GoDataDriven", "http://godatadriven.com/business-consultancy", "http://godatadriven.com/big-data-infrastructure", "http://godatadriven.com/predictive-modelling", "http://godatadriven.com/products-overview", "http://godatadriven.com/data-discovery", "http://godatadriven.com/data-science-audit", "http://godatadriven.com/data-science-suite", "http://godatadriven.com/data-science-box", "http://godatadriven.com/divolte", "http://godatadriven.com/customers", "http://godatadriven.com/casestudy-airfranceklm", "http://godatadriven.com/casestudy-bakkersland", "http://godatadriven.com/casestudy-bol", "http://godatadriven.com/casestudy-eneco", "http://godatadriven.com/casestudy-eretail", "http://godatadriven.com/casestudy-ing", "http://godatadriven.com/casestudy-nuon", "http://godatadriven.com/casestudy-npo", "http://godatadriven.com/casestudy-schipholgroup", "http://godatadriven.com/casestudy-transavia", "http://godatadriven.com/technology-overview", "http://godatadriven.com/hadoop", "http://godatadriven.com/python", "http://godatadriven.com/r", "http://godatadriven.com/spark", "http://godatadriven.com/training-overview", "http://godatadriven.com/training-schedule", "http://godatadriven.com/bigdata-training", "http://godatadriven.com/datascience-training", "http://godatadriven.com/data-science-accelerator-program", "http://godatadriven.com/events", "http://godatadriven.com/meetups", "http://blog.godatadriven.com", "http://godatadriven.com/our-story", "http://godatadriven.com/team", "http://godatadriven.com/careers", "http://godatadriven.com/media-resources", "http://godatadriven.com/news", "http://godatadriven.com/contact", "https://cdn.xebia.com/documents/Cookies+Policy+Xebia+20170130.pdf"]}, "39": {"url": "https://concepttoclinic.drivendata.org/", "title": "", "text": "Lung Cancer Early Detection Challenge  We\u2019re calling on a global community of data scientists, engineers, designers, and researchers to build an open source software application that brings advances from machine learning into the clinic. We\u2019re not just optimizing an algorithm for a single metric\u2014we\u2019re collaborating to build tools which put AI in the hands of clinicians. In addition to pushing forward the cutting-edge of open clinical software, top contributors will be eligible for a share of $100,000 in monetary prizes generously provided by the Bonnie J. Addario Lung Cancer Foundation.          Contribute now by grabbing an issue from the project's GitHub repository and submitting a PR!        Lung cancer causes more deaths each year than any other cancer in the US, and early detection makes a big difference. Recent advances in machine learning \u2013 including open source algorithms from the 2017 Data Science Bowl \u2013 help find and interpret early signs of cancer. Using imagery from CT scans, this application will build on cutting-edge algorithms in machine learning to help clinicians identify and interpret lung nodules. Your guide to contributing throughout the challenge. All you need to start contributing is a passion for building great software and a github account.  Contributions of all sizes can earn points as they add value to the project, by building on pre-designated issues, building out new content, and building up the community. $100,000 in cash, as well as other in-kind prizes, will be available for top contributors filling key roles throughout the challenge. A live leaderboard keeps track of points earned throughout the challenge by top contributors filling key roles. Meet the amazing group of of technical, clinical, and domain experts that are helping to guide the end-to-end design of this challenge. The Addario Lung Cancer Foundation (ALCF) has set the audacious goal of making lung cancer a chronically managed disease by 2023.  Ultimately, the goal of this project is to produce a tool that draws on machine intelligence, is useful to clinicians, and makes a lasting difference in the lives of patients. We hope you will find a way to contribute!  Lung Cancer Early Detection Challenge", "links": ["https://www.drivendata.org/termsofuse/", "https://www.drivendata.org/copyrightpolicy/", "https://www.drivendata.org/privacypolicy/", "https://www.drivendata.org/", "http://www.drivendata.co/"]}, "40": {"url": "http://appsilondatascience.com/blog/rstats/2017/08/09/shiny-collections.html", "title": "shiny.collections,  Google Docs-like live collaboration in ShinyAppsilon Data Science Blog", "text": "A few weeks ago, our CTO Marek Rogala gave an engaging talk at the User Conference in Brussels. Despite being one of the last talks, he drew a crowd that filled the room and had significant viewership online. Marek\u2019s talk was entitled shiny.collections: Google Docs-like live collaboration in Shiny. In short, he went over a package we built that allows for persistence and collaboration without losing reactivity in Shiny. Users can enter information and have continuity, knowing that if they exit and return to the app, their inputs will still be there. Let\u2019s go over his talk. Recently, Shiny users have demanded more from their applications. The advent of cloud applications, especially ones like Google Docs, have accustomed users to collaboration. They expect their Shiny apps to be more than just an interactive tool for data visualisation, but rather, a production ready application that works like any other tool. Their data has to be saved automatically. Interactive collaboration is a must in some use cases. And the application must be delivered as fast as possible. You need to use a reactive database such as rethinkDB, Firebase, or mongoDB to achieve this. mongoDB isn\u2019t really a reactive database, but there are ways where it can be used as one. We are going to use rethinkDB in our case. Also, we are going to use rethinker, which is a rethinkDB driver for R. But using rethinker is not the most ideal solution, as callback do not work very well with Shiny and it is quite painful to configure to our needs. To get around this, we created our own package built upon rethinker called shiny.collections.  It lets you easily connect to your shiny app and takes care of all of the trickiness involved. During his talk, Marek showed a live demo that involved creating a chat application. Check out how to do this yourself. I reccomend watching Marek first, as he has some valuable commentary you won\u2019t find in the blog post. This example is just a simple use case of what you can achieve with shiny.collections. We\u2019ve made it a priority to make it convenient and easily integrable with other powerful tools, including DT, leaflet or rhandsontable. Our goals for the future of this package include getting it on cran and diversifying the functionality of the API. Take a look at the our package and contribute. Get in touch with us for all of you other data science needs as well.  Published 09 Aug 2017", "links": ["https://channel9.msdn.com/Events/useR-international-R-User-conferences/useR-International-R-User-2017-Conference/shinycollections-Google-Docs-like-live-collaboration-in-Shiny#comments", "https://cran.r-project.org/web/packages/rethinker/index.html", "https://appsilon.github.io/shiny.collections/", "http://appsilondatascience.com/blog/rstats/2017/07/02/shiny-chat.html", "http://appsilondatascience.com", "http://disqus.com/?ref_noscript"]}, "41": {"url": "https://blog.statsbot.co/introduction-to-imitation-learning-32334c3b1e7a", "title": "Introduction to Imitation Learning", "text": "This post is the first in the series where we will describe what Imitation Learning is. For today\u2019s article, the Statsbot team asked computer scientist Vitaly Kurin to briefly introduce Imitation Learning and outline the basics of Reinforcement Learning. Living organisms are extremely complex. Even the relatively simple ones such as flies or worms. They are able not only to operate successfully in the real world, but are extremely resilient to changes in conditions. And that\u2019s if we\u2019re not even talking about humans. We can plan ahead, we can alter our plans given new information, and we can collaborate with others and execute our plans more effectively. Science and such projects as CERN or a huge Radio Telescope in Arecibo are perfect examples of human collaboration and the art of planning. Yes, we do a lot of stupid stuff, but let\u2019s not talk about that here and instead focus on the good. Yes, we can now beat the best human in chess or Go, we can get a crazy score in Video Pinball for Atari 2600, we can even already make a pure human broke by challenging him in poker. Can we open a bottle of champagne and celebrate the triumph? I\u2019m afraid not. Yes, machine learning has recently made a significant leap forward. The combination of new Deep Learning ideas with old ones has enabled us to advance in many domains, such as computer vision, speech recognition, and text translation. Reinforcement Learning has also benefited greatly from its marriage with Deep Learning. You\u2019ve definitely heard of Deep Reinforcement Learning success such as achieving superhuman score in Atari 2600 games, solving Go, and making robots learn parkour. Though, we must admit, that operating successfully in the real world is much harder than playing Go or Space Invaders. Many of the tasks are much harder than this. Imagine a kid riding a bicycle in the middle of a crowded city center or a man driving a Porsche 911 on an autobahn at 200 miles per hour. Let\u2019s all admit that we are not there yet. The typical machine learning approach is to train a model from scratch. Give it a million images and some time to figure it out. Give it a week and let it play Space Invaders until it reaches some acceptable score. We, as humans, beg to differ. When a typical human starts to play some game he has never seen, he already has a huge amount of prior information. If he sees a door in Montezuma\u2019s Revenge, he realizes that somewhere there should lie a key and he needs to find it. When he finds the key, he remembers that the closed door is back through the two previous rooms and he returns to open it. When he sees a ladder, he realizes that he can climb it because he has done this hundreds of time already. What if we could somehow transfer human knowledge about the world to an agent? How can we extract all this information? How can we create a model out of it? There is such a way. It\u2019s called Imitation Learning. Imitation Learning is not the only name for leveraging human data for good. Some researchers also call it apprenticeship learning, others refer to it as Learning from Demonstration. From our point of view, there is no substantial difference between all of these titles and we will use Imitation Learning from now on. In order to introduce Imitation Learning, we will need to understand the basics of Reinforcement Learning first. Let\u2019s move on. It\u2019s not hard to get the general idea of the Reinforcement Learning setup. There is some agent and we want this agent to learn some task. Let\u2019s say, we have an Atari 2600 console, the agent has access to the joystick and can see what\u2019s happening on the screen. Let\u2019s say, 60 times per second we give our agent a screenshot of the game and ask him what button he wants to press. If our agent does well, he can see that his score is increasing (positive reinforcement), otherwise we can give him a penalty as a negative reward (negative reinforcement). Gradually, by trial and error, the agent starts to understand that it\u2019s better to avoid some of the actions and do those which bring him a reward. Let\u2019s make it more formal and describe the process stated above mathematically. We can describe the RL framework mentioned above (observe -> act -> get the reward and the next state) as a Markov Decision Process (MDP): where: We also need a definition of a policy function for the next section. Policy is a function, that returns an action given the state: And, actually, our final goal when solving an MDP is to learn such a policy in order to maximize the reward for our agent. Let\u2019s take an example of an MDP. The circles represent the states, arrows with green labels are actions, red labels are the rewards for actions, and the square is the terminal state. The green numeric labels are the transition probabilities. Our student starts in a state with the blue circle. He studies, but this is hard and sometimes boring. He decides to open a Facebook app and once he is there, he can either quit or continue scrolling. He then studies more and more, and finally decides to go to the pub. The state is a smaller filled circle, since now there is an element of randomness, based on the amount of knowledge the student forgets after visiting the pub. He can then either study more and pass the exam (+10 in reward), or he can go to sleep and finish the MDP right now. Since we will use DQN and related ideas in the future, let\u2019s briefly understand what is going on here. The whole approach is built upon approximating the so-called Q function and building the agent\u2019s behavior based on it. The idea of the Q function is the following: it returns you the entire expected discounted reward flow for the particular action and the particular state, given that starting from the next state we will be following our policy \ud835\udfb9. It answers the question: \u201cHow good is to press this button in this state?\u201d The Q function obeys the Bellman equation: And, finally, the Bellman principle of optimality is the following: notwithstanding what happened before, we should always take the action with the highest Q to maximize the reward flow: But how do we get such a Q function, you might ask. Let\u2019s look at an example. Imagine, you want to grab a coffee (+20 in reward) and a chocolate in a vending machine (+10 in reward). Your total reward cannot exceed thirty. Moreover, if you have taken the coffee already, it cannot be higher than 10 (the reward for chocolate) from now on. This is the idea: the Q value for the current step and action is equal to the maximum Q value for the next state (since we behave optimally) + the reward we get for the transition. The value of the quadratic objective function becomes: Q-learning itself is not new. Q-learning which uses neural networks as a function approximators is also not new (e.g. neural fitted-q iteration). A DQN paper was the first to use deep convolutional networks to solve this type of problem and introduced a couple of novelties that make the training process much more stable. First of all, experience replay. The vanilla Q-learning point is to make a step, get the reward and the next state, then update the approximation function parameters based on this transition. The DQN idea is to make the transition and save it in a \u201creplay memory\u201d\u200a\u2014\u200aan array that stores the last 10\u2076 (<insert any large number here>) transitions with the information about the reward, states before and after the transition, and if the event is terminal (game over) or not. Having this experience replay we can randomly sample mini-batches from it and learn more effectively. Another thing that makes the algorithm more stable is that DQN uses two neural networks: the first to compute the Q value for the current state and the second to compute the Q value for the next state. You can see that from the equation with the objective: two different Q functions use \ud835\udfb1 and \ud835\udfb1\u2019, respectively. Each 10,000 steps, the parameters \ud835\udfb1\u2019 are copied from the learned parameters \ud835\udfb1 and this helps a lot in increasing the stability. The problem here with using one function is that when we update the weights, both Q(s,a) and Q(s\u2019,a\u2019) increase and this might lead to oscillations or policy divergence. Using two separate networks adds a delay between an update and computation of the target Q value and reduces such cases. If you have further interest in the phenomena, read the Method section in the DQN Nature paper. Okay, everything described above sounds quite simple. If there is still something you do not understand, please, have a look at David Silver\u2019s lecture where he explains everything perfectly! Knowing all of these, can we build an True AI now? I\u2019m sorry, but we can\u2019t. There are several problems that hinder us from building an agent that will beat ByuN at StarCraft II, bring an autonomous car to the market, or give you an opportunity to buy your grandma a robot that will do the dishes for her after lunch. One of these problems is that the rewards our agent gets might be very sparse in time. Let\u2019s say, you play chess. If you lose, how do you know when you made a catastrophic move? Moreover, it\u2019s highly possible that there was not a catastrophic move, but several average ones. Another problem that is closely connected to the previous one, is the sample-efficiency problem. Or, more honestly, sample-inefficiency. Even to master a simple game such as Space Invaders might take a couple of days in-game time. It\u2019s easy to speed up learning in games since we have access to the simulators, but what if we want to learn something in real life? Unfortunately, physics is not there yet and we cannot speed up time. There is an approach that could potentially solve these problems and a bunch of others\u200a\u2014\u200aImitation Learning, as we mentioned at the beginning of this post. As we said, we, humans, rarely learn something without any prior information. Let\u2019s use this data! What should we do? The idea of Imitation Learning is implicitly giving an agent prior information about the world by mimicking human behavior in some sense. Imitation Learning will not only help us solve the sample-inefficiency or computational feasibility problems, it might potentially make the training process safer. We cannot just put an autonomous car in the middle of the street and let it do whatever it wants. We do not want it to kill humans that are around, destroy someone\u2019s property, or the equipment itself. Pretraining it on a human demonstrator\u2019s data might make the training process faster and avoid undesirable situations. Training a model requires some data. Training a Deep Learning model requires even more data. Training a Deep Reinforcement Learning model requires\u2026 Okay, you get the idea. So, this series is only partly describes what we can do with demonstration data. The main point of all of this is to call for human demonstration datasets, because we do not have many, unfortunately, up to this moment. Okay, we should stop here. The key points of this post are: In the next chapter we will write more about Behavior Cloning\u200a\u2014\u200athe simplest approach to an RL problem that can leverage the human demonstration data. Stay tuned!", "links": ["https://blog.statsbot.co?source=logo-lo_7150a91931e0---cfc9f21a543a", "https://twitter.com/statsbotco", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.statsbot.co%2Fintroduction-to-imitation-learning-32334c3b1e7a", "https://blog.statsbot.co", "https://blog.statsbot.co/analytics/home", "https://blog.statsbot.co/datascience/home", "https://blog.statsbot.co/design/home", "https://blog.statsbot.co/startups/home", "https://blog.statsbot.co/bots/home", "https://blog.statsbot.co/news/home", "https://blog.statsbot.co/statsbot-digest-b0d7372f842a", "http://statsbot.co?utm_source=blog&utm_medium=navigation&utm_campaign=robot_face", "https://blog.statsbot.co/search", "https://blog.statsbot.co/@yobibyte?source=post_header_lockup", "https://blog.statsbot.co/@yobibyte?source=post_header_lockup", "https://atlas.cern/discover/about", "http://statsbot.co?utm_source=blog&utm_medium=article&utm_campaign=imitation_learning", "https://en.wikipedia.org/wiki/CERN", "https://en.wikipedia.org/wiki/Arecibo_Observatory", "https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning/", "https://deepmind.com/research/alphago/", "https://www.theverge.com/tldr/2017/7/10/15946542/deepmind-parkour-agent-reinforcement-learning", "http://www.ceva-dsp.com/ourblog/wp-content/uploads/sites/3/2016/04/AlphaGo-Lee-Se-dol.png", "https://simple.wikipedia.org/wiki/File:Rl_agent.png", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf", "https://link.springer.com/article/10.1007/BF00992698", "https://pdfs.semanticscholar.org/2820/01869bd502c7917db8b32b75593addfbbc68.pdf", "https://link.springer.com/article/10.1007/BF00992699", "http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html?foxtrotcallback=true", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "http://wiki.teamliquid.net/starcraft2/ByuN", "http://toyoutheartist.co.uk/technique/imitation/", "https://mitpress.mit.edu/books/reinforcement-learning", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html", "https://arxiv.org/abs/1312.5602", "https://www.nature.com/nature/journal/v518/n7540/full/nature14236.html", "https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner", "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/google-analytics-audit-checklist-and-tools-fca7df2f2e7a", "https://blog.statsbot.co/google-analytics-audit-checklist-and-tools-fca7df2f2e7a", "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co/tagged/imitation-learning?source=post", "https://blog.statsbot.co/tagged/data-science?source=post", "https://blog.statsbot.co/tagged/machine-learning?source=post", "https://blog.statsbot.co/tagged/ai?source=post", "https://blog.statsbot.co/tagged/reinforcement-learning?source=post", "https://blog.statsbot.co/@yobibyte?source=footer_card", "https://blog.statsbot.co/@yobibyte", "https://github.com/yobibyte/yobiblog", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "42": {"url": "https://axibase.github.io/atsd-use-cases/DataShorts/Dollar-EX/", "title": "", "text": "Data Source: Federal Reserve Economic Data (FRED) Visualizations: ChartLab Structured Query Language: SQL Console from Axibase Download the Community Edition of Axibase Time Series Database here Data is aggregated daily in visualizations, and annually in SQL queries. Favorable dollar exhange rates are shown in green while less favorable exchange rates are shown in red, based on the overall 5-year average exchange rate. Each section is accompanied by a brief analysis to give the data context. Using computed value settings thresholds have been calculated using the underlying data, the upper 25th percentile of a given exchange rate is considered favorable while the lower 25th percentile is considered less than favorable. The data is coded with an ID which is provided in the index as well. Open any ChartLab display below to navigate through time, or change the metric to reflect the desired exchange rate. Europe Asia North America As the amount of debt carried by various EU member countries grew to increasingly worrisome heights, the Euro plunged in 2015  losing ground against the dollar. Further, as oil prices slumped the European Central Bank began to compensate by launching a program of government bond purchases. The combination of these factors and the growth of the dollar post-recession have all contributed to the currently favorable exchange rate enjoyed by the dollar. Query 1.1 Table 1.1 Figure 1.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index During the worst of the European Debt Crisis which saw European Union per capita debt grow at unprecedented rates across the continent, EU investors sought refuge in the stability of the Swiss Franc, backed by the world-renowned Swiss banking system. Further, the Swiss National Bank removed the peg to the Euro unexpectably in 2015 causing a huge surge in 2015, clearly visible in the visualization below as a sudden change from a favorable exchange rate for dollar holders to an all-time low for the observed time period. Query 1.2 Table 1.2 Figure 1.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Largely influenced by the divisive Brexit vote, the value of the Pound has remained relatively low since the referendum in late June of 2016, which aligns perfectly with the Pound\u2019s slight depreciation in value against the dollar on the chart below. Query 1.3 Table 1.3 Average Exchange Rate: 1.50 Figure 1.3   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index After purchasing trillions of Yen worth of Japanese national debt from the private sector three years ago, the Central Bank of Japan has been watching the gradual decrease in value of the Yen against the dollar. Most analysts, see this fall as controlled and predict a bright future for the Yen as the Central Bank can no longer afford to collect more debt than they already have. However, the Yen\u2019s rise much also be managed as the sudden appreciation of the currency could result in an increase in loan defaults because industries which have taken loans at current Yen prices would be paying them back at a much higher rate. Query 2.1 Table 2.1 Figure 2.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index To prevent dramatic internal inflation, the People\u2019s Bank of China closely regulates local, or onshore, trading of the Yuan by fixing the price each day. Although independent officially, the international, or offshore, price of the Yuan usually remains fairly close to its onshore value. Additionally, the PBoC carefully controls the outflow of capital and often hedges their own currency by maintaining positions in international currencies. Query 2.2 Table 2.2 Figure 2.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index The Hong Kong Dollar is officially pegged to the value of the United States Dollar so any change in relative  value is planned. The \u201cMiracle of the Orient\u201d continues to be one of the most dominant financial markets in the world. Query 2.3 Table 2.3 Figure 2.3   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index One of the financial centers of the world, Singapore is home to the second-busiest port in the world and one of the largest oil refining industries worldwide as well. With one of the highest per capita GDP values, Singapore has cemented its place of international importance despite inexplicably loose financial policy whereby the Monetary Authority of Singapore does not regulate their currency\u2019s value by adjusting interest rates, as is common with most central banking systems. Query 2.4 Table 2.4 Figure 2.4   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Relative to the USD, the Canadian dollar has been making a bit of a resurgence in 2017 as United States GDP numbers missed its targets in quarter two. Although dependant on the price of oil due to their large natural reserves, the diverse Canadian economy is one of the few globally-active nations with a significant trade surplus, although that too have been reduced in recent years. Query 3.1 Table 3.1 Figure 3.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Boasting the highest per capita income of Latin America, the Mexican economy has continued to demonstrate healthy growth over the last several decades. North America\u2019s number one producer of automobiles also enjoys a $46 billion trade surplus with the United States, and is the 9th ranked owner of United States government debt. This dependence on America affords Mexico the ability to spur dramatic internal growth that has led to the creation of a booming electronics sector, but at the cost of a  certain amount of reliance on the United States as roughly half of the country\u2019s imports and exports originate or end up  on U.S. soil. Query 3.2 Table 3.2 Figure 3.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index", "links": ["https://github.com/axibase/atsd-use-cases", "https://fred.stlouisfed.org/categories/94", "https://apps.axibase.com", "https://github.com/axibase/atsd/tree/master/api/sql", "https://axibase.com", "https://axibase.com/products/axibase-time-series-database/", "https://github.com/axibase/atsd/blob/master/installation/README.md#installation", "https://axibase.com/products/axibase-time-series-database/visualization/widgets/configuring-the-widgets/", "https://apps.axibse.com", "https://apps.axibase.com/chartlab/424eb6b2/5/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/9/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/6/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/4/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/2/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/3/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/8/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/7/#fullscreen", "https://github.com/axibase/atsd-use-cases", "https://github.com/axibase", "https://pages.github.com"]}, "43": {"url": "http://byteacademy.co/blog/overview-NLG", "title": "", "text": "NLG (Natural Language Generation), a subfield of Artificial Intelligence, is a hot topic in the technology news today. We hear a lot about AI that can soon replace writers and journalists beginning the era of machine creativity. But, what\u2019s all this fuss about? In this article, we unveil what NLG really is and show that it can bring a lot of benefits to businesses and consumers. In a nutshell, NLG is a sub-field of NLP (Natural Language Processing) that studies methods of automatic transformation of structured data into a human-readable text. In practice, there are two major types of NLG applications: template-based NLG and advanced NLG. \u00a0 Template-based NLG is the simplest solution that uses templates with canned text and placeholders to insert data into them. Such systems heavily rely on hard-coded rules, which makes them less flexible than advanced NLG. Since template-based NLG tools have a limited number of templates and require special data representations, they can not be easily reused across different projects and business use cases. \u00a0 Advanced NLG tools are more flexible thanks to the use of supervised and unsupervised Machine Learning (ML). Rather than tying down structured data to the Procrustean bed of templates, advanced NLG uses neural networks that learn morphological, lexical, and grammar patterns from large corpora of written language. Soft probabilistic methods used in the advanced NLG algorithms allow predicting the likelihood of one word appearing after another, and correcting language errors, such as misspellings. ML algorithms used in the advanced NLG are also better in dealing with new words and expressions not included in the original training samples. \u00a0 Modern NLG service providers such as Narrative Science and Automated Insights prefer advanced NLG methods because they allow creating rich data-driven models that produce intelligent insights from data. These algorithms are much more skillful in making right word choices and writing narratives that reflect intentions and business needs of the NLG users. As an added bonus, advanced NLG models can preprocess and analyze data which makes them not just translators of structured data into text, but automatic analysts able to provide actionable insights. \u00a0 Despite the fact that NLG methods have been used since the 1970s, they got a powerful momentum only recently and thanks to the AI/ML revolution. Today, many startups offer cloud-based NLG services to businesses. NLG is also gaining traction in mass media and journalism. Major American newspapers are already experimenting with the automatic storytelling. For example, in 2016 the Washington Post unveiled its automatic storytelling AI named Heliograph AI. Heliograph was used in the coverage of Rio Olympics and the US Presidential election in 2016. \u00a0 Leveraging data mining techniques and ML models the machine reporter can convert structured statistical data, diagrams, graphs, weather forecasts and other data-rich content into excellent descriptive reports that sound if though they were written by the professional reporters. But, isn\u2019t this dangerous for journalism as a profession? Proponents of automatic storytellers say that they actually free up much time for reporters to add analysis and real insights to stories rather than spending countless hours publishing news and descriptive reports[i]. NLG tools may be used in other innovative ways as well: Benefits of NLG, however, go beyond journalism. There is a growing demand for NLG services among major companies. For example, Quill, an NLG system developed by Narrative Science, is used by such companies as Deloitte, Groupon, and Credit Suisse[iv]. These companies opt for NLG solutions for a reason. Growing acceptance of NLG among businesses makes it a promising field to study. If you want to learn more about NLG, \u00a0Byte Academy offers a Natural Language course that covers Natural Language Processing and Natural Language Generation. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0  Your Name (required)    Your Email (required)    \u00a0 \u00a0 References: [i] \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 WashPost PR Blog (August 5, 2016). The Washington Post Experiments With Automated Storytelling to Help Power 2016 Rio Olympics Coverage. WashPost PR Blog. Retrieved from https://www.washingtonpost.com/pr/wp/2016/08/05/the-washington-post-experiments-with-automated-storytelling-to-help-power-2016-rio-olympics-coverage/?utm_term=.bf63b03c4aeb [ii] \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dayan, Zohar (2015). Hearst, USA Today Sports, & Viralnova Partner With Wibbitz For Video Strategy. Wibbitz Blog. Retrieved from http://blog.wibbitz.com/wibbitz-partners-hearst-usa-today-sports-group-and-viralnova-to-expand-video-strategy [iii] \u00a0\u00a0\u00a0\u00a0\u00a0 Keohane, Joe (2017). What News-Writing Bots Mean for the Future of Journalism. Wired. Retrieved from https://www.wired.com/2017/02/robots-wrote-this-story/ [iv] \u00a0\u00a0\u00a0\u00a0 Narrative Science. Turn Your Data Into Better Decisions With Quill. Retrieved from https://narrativescience.com/Platform info.in@byteacademy.co Byte Dev Licensed by the State of New York, New York State Education Department \u00a9 2017 Byte Academy LLC.  All rights reserved.", "links": ["http://byteacademy.co", "http://byteacademy.co/program/", "http://byteacademy.co/courses/", "http://byteacademy.co/data-science-mini-courses/", "http://byteacademy.co/upcoming-classes/", "http://byteacademy.co/students/", "http://byteacademy.co/hiring/", "http://byteacademy.co/events/", "http://byteacademy.co/blog/", "http://byteacademy.co/india/", "https://byteacademy.fluidreview.com", "http://feeds.feedburner.com/ByteAcademy", "http://byteacademy.co/category/data-science/", "http://byteacademy.co/category/career/", "http://byteacademy.co/category/fintech/", "http://byteacademy.co/category/medtech/", "http://byteacademy.co/category/programming-tips/", "http://byteacademy.co/category/startup/", "http://byteacademy.co/category/student-stories/", "http://byteacademy.co/category/blockchain/", "http://byteacademy.co/category/current-events/", "http://byteacademy.co/category/diversity/", "http://byteacademy.co/category/events/", "http://byteacademy.co/category/finovate/", "http://byteacademy.co/category/light-reads/", "http://byteacademy.co/category/millennials/", "http://byteacademy.co/category/quant/", "http://byteacademy.co/2017/", "http://byteacademy.co/2016/", "http://byteacademy.co/2015/", "http://byteacademy.co/category/data-science/", "http://byteacademy.co/blog/artificial-intelligence-future", "https://www.washingtonpost.com/pr/wp/2016/08/05/the-washington-post-experiments-with-automated-storytelling-to-help-power-2016-rio-olympics-coverage/?utm_term=.bf63b03c4aeb", "http://www.reddit.com/submit?url=http://byteacademy.co/blog/overview-NLG", "http://www.tumblr.com/share?v=3&u=http://byteacademy.co/blog/overview-NLG&t=Overview of Natural Language Generation (NLG)", "http://byteacademy.co/blog/artificial-intelligence-future", "http://byteacademy.co/blog/artificial-intelligence-future", "http://byteacademy.co/blog/data-science-podcasts", "http://byteacademy.co/blog/data-science-podcasts", "http://byteacademy.co/blog/data-science-fintech/", "http://byteacademy.co/blog/data-science-fintech/", "http://byteacademy.co/financial-aid/", "http://byteacademy.co/upcoming-classes/", "http://byteacademy.co/contact-us/", "http://byteacademy.co/corporate-training/", "http://byteacademy.co/faq/", "http://byteacademy.co/press-page/", "http://byteacademy.co/careers-at-byte/", "http://byteacademy.co/brand-ambassador/", "https://www.meetup.com/Byte-Academy-Finance-and-Technology-community/", "http://www.bytedev.co", "https://www.facebook.com/byteacademy/", "https://twitter.com/ByteAcademyCo", "https://www.linkedin.com/edu/school?id=171001", "https://github.com/ByteAcademyCo", "https://www.instagram.com/byteacademy/", "https://www.meetup.com/Byte-Academy-Finance-and-Technology-community/", "https://www.quora.com/topic/Byte-Academy", "https://www.youtube.com/channel/UCrMcJALnO748TSK27bZQzSg", "http://byteacademy.co/privacy-policy/"]}, "44": {"url": "http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "title": "", "text": "", "links": ["http://www.getrevue.co/?ref=Revue+Profile", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profileimage&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profiledate&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profilename&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Mm6Kn?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Mm6Kn?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.datasciencecentral.com/profiles/blogs/automated-machine-learning-for-professionals?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lXBy8?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lXBy8?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.nature.com/articles/s41467-017-00181-8?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/d0nAm?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/d0nAm?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/4mO6D?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/4mO6D?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/eK78N?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/eK78N?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://sourcedexter.com/amazing-tensorflow-github-projects/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xejEQ?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xejEQ?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://techblog.appnexus.com/a-keras-multithreaded-dataframe-generator-for-millions-of-image-files-84d3027f6f43?gi=79cece23ee37&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/6nA72?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/6nA72?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://blog.revolutionanalytics.com/2017/08/a-modern-database-interface-for-r.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/5kyK3?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/5kyK3?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://ruder.io/deep-learning-nlp-best-practices/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/7yxNV?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/7yxNV?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://hyperparameter.space/blog/when-not-to-use-deep-learning/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/8A1NP?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/8A1NP?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278?gi=8e0386afa61e&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/1bKwn?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/1bKwn?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://adeshpande3.github.io/How-I-Used-Deep-Learning-to-Train-a-Chatbot-to-Talk-Like-Me?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lX0x4?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lX0x4?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.youtube.com/channel/UC0n76gicaarsN_Y9YShWwhw/playlists?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/qYVyK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/qYVyK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.youtube.com/watch?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest&v=0fhUJT21-bs", "http://rev.vu/Nkeov?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Nkeov?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%239%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "https://www.getrevue.co/?utm_source=Data Science Digest&utm_medium=email&utm_content=footerlink&utm_campaign=Issue", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=forwarded&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%239%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "http://www.getrevue.co/?ref=Revue+Profile"]}, "45": {"url": "http://rpsychologist.com/d3/bayes/", "title": "", "text": "Created by Kristoffer Magnusson The visualization shows a Bayesian two-sample t test, for simplicity the variance is assumed to be known. It illustrates both Bayesian estimation via the posterior distribution for the effect, and Bayesian hypothesis testing via Bayes factor. The frequentist p-value is also shown. The null hypothesis, H0 is that the effect \u03b4 = 0, and the alternative H1: \u03b4 \u2260 0, just like a two-tailed t test. You can use the sliders to vary the observed effect (Cohen's d), sample size (n per group) and the prior on \u03b4. The prior on the effect is a scaled unit-information prior. The black, and red circle on the curves represents the likelihood of 0 under the prior and posterior. Their likelihood ratio is the Savage-Dickey density ratio, which I use here as to compute Bayes factor. The p-value is the traditional p-value for a two-sample t test with known variance (i.e. a Z test).        HDI is the posterior highest density interval, which in this case is analogous a credible interval. And CI is the traditional frequentist confidence interval.  Check out Alexander Etz's blog series \"Understanding Bayes\" for a really good introduction to Bayes factor. Fabian Dablander also wrote a really good post, \"Bayesian statistics: why and how\", which introduces Bayesian inference in general. If you're interesting in an easy way to perform a Bayesian t test check out JASP, or BayesFactor if you use R. Interactive visualization of Cohen's d effect size Interactive visualization of statistical power and significance testing Interactive visualization of Confidence Intervals Have any suggestion? Or found any bugs? Send them to me, my contact info can be found here. Designed and built by Kristoffer Magnusson. Built with D3.js, jStat and Bootstrap.", "links": ["http://rpsychologist.com", "http://rpsychologist.com", "https://twitter.com/krstoffr", "https://se.linkedin.com/pub/kristoffer-magnusson/b5/133/1b6", "https://twitter.com/share", "http://alexanderetz.com/understanding-bayes/", "http://blog.efpsa.org/2015/08/03/bayesian-statistics-why-and-how/", "https://jasp-stats.org", "http://bayesfactorpcl.r-forge.r-project.org", "http://rpsychologist.com/d3/cohend/", "http://rpsychologist.com/d3/NHST/", "http://rpsychologist.com/d3/CI/", "http://rpsychologist.com/tag/d3js.html", "http://rpsychologist.com/about", "http://rpsychologist.com", "http://d3js.org/", "http://jstat.org/", "http://getbootstrap.com/", "http://twitter.com/krstoffr", "https://github.com/rpsychologist"]}, "46": {"url": "https://blog.datasyndrome.com/generalists-dominate-data-science-f01882f25347", "title": "Generalists Dominate Data\u00a0Science", "text": "Analytics products and systems are best built by small teams of generalists. Large teams of specialists become dominated by communication overhead, and the effect of \u201cChinese whispers\u201d distorts the flow of tasks and stagnates creativity. Data scientists should develop generalist skills to become more efficient members of a data science team. Building data products takes a team covering a broad and diverse skillset. From the customer representative at one end, to the operations engineer at the other, the spectrum of roles in a product analytics team looks like this: Large companies often fill each role with a pair of shoes, resulting in a twelve person team like the one below. The problem with this setup is that it becomes much more difficult to achieve consensus and to perform any task that spans roles. And in data science, most tasks span roles. To look at a particular example, lets focus on the creation of a chart as part of a data product. To begin, a product manager creates a specification, then an interaction designer mocks up the chart, handing it off to a data scientist to fill with data (and hopefully to explore the data and find a chart worth producing), then a back-end engineer to setup an API to grab that data, a front-end web developer to create a web page using the data that matches the mock, and an experience designer to ensure the entire thing feels right and makes sense. Charts take iteration, so this cycle of communication could happen repeatedly for each chart. You can see how communication overhead starts to predominate. A meeting of six people is a full-blown, formal meeting. It is hard to get things done in formal meetings. In the next figure, we see how a data product team might be composed of four generalists: a data engineer, a data scientist/back-end developer, a designer who can build front ends and a product manager that can write marketing copy and cut deals. This is how a startup team would span the skill spectrum, and you can probably see how this makes them more efficient. Revisiting the chart example, creating a chart becomes a collaboration between the product manager, a designer who codes, and a data scientist. This is the kind of ad hoc meeting of 2\u20133 people where \u201cshit gets done\u201d efficiently. This group will be more efficient than the six man group. Put another way: this small team will kick the large team\u2019s ass. In the big company system, sometimes the only way to get anything done efficiently is to go \u201cguerilla generalist\u201d and work with other generalists to cut people out of the chain. This is bad politically, and is part of what drives effective people from big companies. We\u2019ve shown that small teams generalists outperform large teams of specialists. In fact, generalist skills are something every data scientists should work to develop. That doesn\u2019t mean you can\u2019t specialize, but should combine specialization with generalization in order to develop \u201cT-shaped skills.\u201d The T-shaped employee is one that can lend deep expertise across projects while fulfilling multiple roles in his own. It takes time to develop general skills, and that is why the path to becoming a data scientist is not a six month bootcamp, but a ten year journey. Along this path, remember to try to be T-Shaped! Need help building an analytics product or platform? The Data Syndrome team of data scientists and data engineers is available to build your data products and systems as a service. We also offer training in Agile Data Science for all members of data science teams. Clapping shows how much you appreciated Russell Jurney\u2019s story.", "links": ["https://blog.datasyndrome.com?source=logo-lo_9c8f6deffefd---500653fb51a1", "https://twitter.com/datasyndrome", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.datasyndrome.com%2Fgeneralists-dominate-data-science-f01882f25347", "https://blog.datasyndrome.com/@rjurney?source=post_header_lockup", "https://blog.datasyndrome.com/@rjurney?source=post_header_lockup", "https://personalmba.com/communication-overhead/", "https://en.wikipedia.org/wiki/Chinese_whispers", "https://en.wikipedia.org/wiki/T-shaped_skills", "http://datasyndrome.com", "http://datasyndrome.com/training", "https://blog.datasyndrome.com/tagged/data-science?source=post", "https://blog.datasyndrome.com/tagged/agile?source=post", "https://blog.datasyndrome.com/tagged/generalists?source=post", "https://blog.datasyndrome.com/tagged/analytics?source=post", "https://blog.datasyndrome.com/tagged/software-engineering?source=post", "https://blog.datasyndrome.com/@rjurney?source=footer_card", "https://blog.datasyndrome.com/@rjurney", "https://blog.datasyndrome.com?source=footer_card", "https://blog.datasyndrome.com?source=footer_card", "https://blog.datasyndrome.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "47": {"url": "https://github.com/axibase/atsd-use-cases/tree/master/FED_FORDSR#the-average-american-debt-profile", "title": "", "text": "Debt is a complicated concept. After the sub-prime mortgage crisis of the late 2000s, modern Americans are all too familiar with the problems of irresponsible spending on credit. Student loan recipients who queue up to drop off another application for a job in a field they did not study are quick to point to the trappings of deficit spending as a means of wealth creation. Politicians and voters on both sides of the aisle point to the ever-growing United States Government debt with anxiety for the future. And yet despite all the doom and gloom, the American financial system is one of the most stable and robust in the world, in no small part thanks to ingenious monetary policy and hegemonic economic position organized over the entire course of the country's history, modern American consumers are among the wealthiest on the planet. The United States Federal Reserve is the central banking system of the United States, responsible for monitoring the global financial climate and enacting policy that supports the American economy and American consumers. They maintain a number of statistics about these consumers and their monetary practices to better inform their decisions and practices. Provided by the Federal Reserve, this dataset must be correctly parsed during import. The quarterly date format needs to be converted into a monthly format that ATSD can interprete (Q/q letter is not supported). We also need to discard metadata lines contained in the multi-line header. This can be accompilshed with a schema-based parser that provides granular control over the document's rows and columns using RFC 7111 selectors and Javascript: Script 1.1 For step-by-step instructions on data customization with schema based parsing, see this support tutorial. The Financial Obligation Ratio (FOR) is an estimate of the ratio of required debt payments to disposable income. This is a broad calculation and includes all kinds of debt:  mortgage payments, credit cards, property tax and lease payments. Each of these metrics can be expanded further to include associated costs, such as homeowner's insurance for example. The Federal Reserve releases this number each quarter. Figure 1.1   Use the dropdown menus at the top of the visualization screen to navigate through time, selecting the starttime and endtime values to observe a desired period. The data can also be queried with a structured query language in the SQL Console. The data will be aggregated annually, derived from the average value of each quarter within a given year: Query 1.1 Table 1.1 All values are shown as a percent of one hundred, where the whole is representative of the total income of the average person. The Debt Service Ratio (DSR) is more specific than the Financial Obligation Ratio in that it typically does not include non-essential debt payments. Here, it has been parsed into two categories, mortgage debt and consumer debt. These numbers represent the average percent of a person's earned salary each month which much be used to make the required payments associated with consumer credit and mortgage. Typically the DSR is an initial calculation performed to determine a person's eligibility to receive a mortgage. A DSR value of less than 48% is generally preferred, meaning that with a particular mortgage plus other credit obligations at least 52% of a person's gross monthly earning would still be available to them after making the required payments. Figure 2.1   Query 2.1 Table 2.1 Because the FOR value includes the DSR value plus additional non-essential credit values, and the DSR value is parsed into both consumer and mortgage related debt, these three values can be shown in a new visualization that creates a typical consumer profile of the average American. By using the calculated value setting shown below, additional data not specifically included in the set can be displayed: Script 2.1 Shown below is the debt profile of the average American consumer from 1980 to 2017, navigate through time using the dropdown menus at the top of the screen to select a desired span of time and compare how bearing debt has changed over the course of the last three decades. Figure 3.1   The visualization can also be organized to show the amount of each type of debt as it relates to the others: Figure 3.2   Additionally, these values can be compared on an annual basis as shown in the visualization below: Figure 3.3   To view the distribution of these values across time, a histogram is shown below: Figure 3.4   In the following box diagram, explore time with the dropdown menus at the top of the visualization screen. The visualization shows the distribution of debt values as a percentage of total income, with the initial time period set to include the entire data set: Figure 3.5   The following SQL query will detail the above visualizations in one table, displaying averaged annual values of each component described above: non-essential credit payments, mortgage credit payments, and consumer credit payments, as well as the Financial Obligation Ratio (FOR), or total debt obligations. Query 3.1 Table 3.1 The above dataset can illuminate a number of features of the American economy and a number of characteristics of the average American consumer. While modern Americans are quick to denounce the zeitgeist of living outside of one's means, the data shows that in fact, the amount of debt carried by the average American is on par with or even lower in some cases than that of his 1980's counterpart. In fact, the only metric which has demonstrated a legitimate increase in value over the last several decades has been the roughly one percent increase in non-essential credit holdings by the average consumer. According to data from the Economic Research Department of the Saint Louis Branch of the Federal Reserve, the 2015 US median household income was $56,516 per year in 2015 USD. This number can be applied to the above table and visualized in ChartLab to create more comprehensive data. Figure 3.6   The above visualization aggregates the values from Table 3.1 based on a time period of the user's selection. Use the dropdown menu at the top of the screen to select the aggregation period. The initial visualization shows the average values for each metric over the entire period of time in 2015 USD by obligation amount per quarter. The following query summons the same data shown above, but further parses it to show annual average monthly payments instead of quarterly values in 2015 USD for a person making the 2015 median United States income of $56,516 a year. Query 4.3 Table 3.2 As it turns out, the idea that your parents paid less for their house than you will is only true in absolute terms. When compared with current numbers and controlled for inflation, the average 2017 consumer will pay roughly the same portion of their income towards a place to hang their hat up as the average 1980 consumer. The Federal Reserve is able to pull certain levers of power from the Eccles Building in Washington, D.C. such as printing more money, or raising and lowering interest rates to cope with inflation. However, all of these are reactionary measures meant to create small changes that have a butterfly effect over time. Ultimately, the machinations of the Board of Governers have always be something opaque and esoteric to the average man, leading to many people denouncing the Federal Reserve System entirely, occasionally opting for a return of the gold standard or leveling accusations of wrong-doing. However, after reviewing the data above, it seems that at least on a consumer level, the average American actually has more today than they would have had thirty years ago, or even just five years ago. Of course, the Federal Reserve isn't completely responsible for the wise consumer choices made in the current decades, but monetary policy enacted by the various branches of the Federal Reserve are responsible for maintaining the economic conditions that Americans, and consumers the world over, have come to expect from the United States economy.", "links": ["https://github.com/", "https://github.com/axibase/atsd-use-cases/pull/86", "http://www.usdebtclock.org/", "https://www.federalreserve.gov/", "https://www.federalreserve.gov/datadownload/Download.aspx?rel=FOR&series=91e0f9a6b8e6a4b1ef334ce2eaf22860&filetype=csv&label=include&layout=seriescolumn&from=01/01/1980&to=12/31/2017", "https://axibase.com/products/axibase-time-series-database/writing-data/csv/", "https://apps.axibase.com/chartlab/842f1dd9/#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql", "https://apps.axibase.com/chartlab/85522dd3/#fullscreen", "https://apps.axibase.com/chartlab/f25de723/#fullscreen", "https://apps.axibase.com/chartlab/f25de723/3/#fullscreen", "https://apps.axibase.com/chartlab/81ea0ea0/#fullscreen", "https://apps.axibase.com/chartlab/9f74c179/#fullscreen", "https://apps.axibase.com/chartlab/20ff0ade/#fullscreen", "https://fred.stlouisfed.org/series/MEHOINUSA646N", "https://research.stlouisfed.org/", "https://www.stlouisfed.org/", "https://apps.axibase.com", "https://apps.axibase.com/chartlab/da132e01/11/#fullscreen", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "48": {"url": "http://tech.marksblogg.com/billion-nyc-taxi-rides-aws-ec2-p2-16xlarge-brytlytdb.html", "title": "", "text": "I have 14 years of hands-on build and consulting experience with clients in the UK, Ireland & Germany. I've done both back- and frontend work for Bank of America Merrill Lynch, Blackberry, Bloomberg, British Telecom, Ford, Google, ITV, News UK, Pizza Hut, Royal Bank of Scotland, Royal Mail, T-Mobile, Vertu (Nokia subsidiary at the time), Williams Formula 1 Team and Xerox. I hold both a Canadian and a British passport. My CV & my LinkedIn profile.         Home           | Benchmarks           | Archives            | Atom Feed  Posted on Fri 28 July 2017 BrytlytDB is an in-GPU-memory database built on top of PostgreSQL. It's operated using many of PostgreSQL's command line utilities, it's wire protocol compatible so third-party PostgreSQL clients can connect to BrytlytDB and queries are even parsed, planned and optimised by PostgreSQL's regular codebase before the execution plan is passed off to GPU-optimised portions of code BrytlytDB offer. Clustering works right out of the box, GPU-powered JOINs are supported, Stored Procedures are fully functional, Deep- and Machine Learning workloads via Torch are supported and BI visualisation software in the form of SpotLyt is included with BrytlytDB as well. The feature list is too long for one blog to cover so in this post I'll just be taking a look at how performant BrytlytDB's OLAP functionality is. In this benchmark I'll see how well 32 Tesla K80 GPUs spread across two EC2 instances perform when querying 1.1 billion taxi trips. I'll be using the same dataset I've used to benchmark Amazon Athena, BigQuery, ClickHouse, Elasticsearch, EMR, kdb+/q, MapD, PostgreSQL, Redshift and Vertica. I have a single-page summary of all these benchmarks for comparison. For this benchmark I'll be using two p2.16xlarge EC2 instances running Ubuntu 16.04.2 LTS in Amazon Web Services' eu-west-1a region in Ireland. Each machine has 8 Nvidia K80  cards which have 2 GPUs each, 64 virtual CPUs and 732 GB of memory. There's also 20 Gbit/s of networking capacity available to each instance. Below are the specifications of the compute capabilities available each one of the EC2 instances. This is the layout of the GPUs available on one of the two EC2 instances. The first machine has an IP address of 52.214.237.134 and has three roles: Global Transaction Manager, Coordinator and host to the first 16 data nodes. The second machine has an IP address of 34.250.232.38 and has two roles: it is also a coordinator and hosts the second set of 16 data nodes. TCP port 5432 is open between the two machines for communicating via PostgreSQL's wire protocol, TCP port 7777 is open for global transaction manager communication and ports 20,000 through to 20,0031 are open for data node communication. In addition to the default 20 GB EBS volumes on each EC2 instance there are six 500 GB General Purpose SSDs attached which offer a baseline of 100 IOPS each and can burst to 3,000 IOPS if need be. These drives are setup in a RAID 0 configuration on each instance. Before setting up the RAID array I'll install a few dependencies. This was run on both EC2 instances. Below are the commands used to setup the RAID array on each instance. Here's what the RAID layout looked like after it was setup. This RAID 0 setup offers a partition with a capacity of 3.2 TB on each instance: I've run the following to download BrytlytDB's install script. BrytlytDB is commercial software so I cannot divulge the URL I pulled this from at this time. The following was run on both EC2 instances. I've then edited the install script on the first instance with the following instance-specific values: And the install script on the second instance was edited with the following instance-specific values: With those changes in place I then ran the install script on both EC2 instances. The install script conducts the following: Once that's done I can form the cluster with the following registration script. The following was run on the first EC2 instance. The second registration script run on the other EC2 instance is identical with the first with the exception of the coordinator line pointing to 52.214.237.134 instead. For this benchmark I've downloaded and decompressed one half the 500 GB of CSV data I created in my Billion Taxi Rides in Redshift blog post onto each EC2 instance. The data sits across 56 files across both machines but for the quickest load time I want to create 32 reasonably equally-sized CSV files and load 16 files on each EC2 instance simultaneously. The 32 files will pair up with the 32 GPUs available across the cluster and should allow for the best utilisation of the GPU resources when running queries. Here is half of the original 500 GB data set on the first EC2 instance: And this is the other half on the second instance: On each instance I'll decompress the GZIP files. Below took about four minutes to complete on each instance. Then I'll concatenate the ~560 million lines of CSV data on each instance and break it up into files of 35 million lines each. The above completed in 8 minutes on each EC2 instance. I'm now left with 16 CSV files on the first EC2 instance: And 16 files on the other EC2 instance: I'll then connect to the coordinator node on each EC2 instance and setup BrytlytDB's gpu_manager_fdw extension and foreign data wrapper. I can then create the trips table. The above sets the maximum row count per GPU device at 38 million and creates an index on the cab_type column. The cab_type column has very low cardinality so the indexing will be less beneficial versus a column with a higher cardinality of values. Some of the data types used in the above table aren't the types I normally use in my benchmarks. The reason for this is that there is still limited data type support available in this early version of BrytlytDB. I've had to substitute DATETIME with DATE which will truncate the timestamps in the dataset to just the date rather than the full date and time. There is no SMALLINT support yet so I've had to use the larger INT type as an replacement for those fields. DECIMAL types aren't yet supported so I'll be using DOUBLE PRECISION as a replacement there. The above replacements could speed up or slow down the queries I benchmark with so I'm hoping to do another benchmark when data type support is widened in the future. With the table created I'll launch 32 simultaneous load jobs across the two EC2 instances to load the data into BrytlytDB's trips table. This is the import script I ran on the first EC2 instance: And this is the load script for the second instance. The first EC2 instance loaded its half of the dataset in 1 hour, 12 minutes and 21 seconds. The second instance loaded its half in 1 hour, 9 minutes and 57 seconds. After the data was loaded in I could see the PostgreSQL data directory filled up with reasonably evenly-sized data folders. Here's what they look like on the first EC2 instance: I was then able to use PostgreSQL's CLI tool to make sure I can see the table and all 1.1 billion records. The times quoted below are the lowest query times seen during a series of runs. As with all my benchmarks, I use the lowest query time as a way of indicating \"top speed\". The following completed in 0.762 seconds. The following completed in 2.472 seconds. The following completed in 4.131 seconds. The following completed in 6.041 seconds. Given all the features of PostgreSQL are still available I'm blown away at how fast BrytlytDB is able to aggregate data. The cluster I used cost around $30 / hour which means this system out performed other Cloud-based and PostgreSQL-based data warehousing solutions both in terms of wall clock time and in terms of cost per hour of running the cluster. That being said I know Nvidia's K80 chips use extremely fast memory and have thousands of compute cores so I expect further optimisations from BrytlytDB to drive down these already fast query times. \u2190 Back to Index Copyright \u00a9 2014 - 2017 Mark Litwintschik. This site's template is based off a template by Giulio Fidente.", "links": ["https://twitter.com/marklit82", "http://tech.marksblogg.com/theme/cv.pdf", "https://uk.linkedin.com/in/marklitwintschik/", "http://tech.marksblogg.com/feeds/all.atom.xml", "http://www.brytlyt.com/", "https://uk.linkedin.com/in/marklitwintschik/", "https://github.com/giulivo/pelican-svbhack"]}, "49": {"url": "https://blog.datazar.com/why-we-focus-on-design-just-as-much-as-functionality-f6ccb3b2c926", "title": "Why We Focus on Design Just as Much as Functionality", "text": "It\u2019s no secret that scientific software isn\u2019t the most beautiful software in terms of design/aesthetics (generally speaking here). It is very effective though; it gets the job done. It does exactly what you expect it to do. Being a huge movie buff, I\u2019ve always wondered why the software we use in the laboratories or classrooms looks nothing like what we see in the movies. Movies do this thing where tech looks exciting and futuristic and super fun to use. And then I go back to reality and it\u2019s nothing like what I just watched. It\u2019s worth mentioning that not everything in the movies looks practical, especially when it comes to software or even more ridiculous: computer hacking. But that\u2019s another subject. The fact there\u2019s some art missing is something I noticed not only in the software we use in the lab, but also the instruments we build and the papers we write. It seems as if the art has been completely taken out of the sciences. Before you say it, we obviously can\u2019t incorporate art into somethings like highly sensitive instruments as unnecessary additions area big no because then you\u2019re interfering. Another thing to note might be the fact that hundreds or even some thousands years ago, the scientists were also artists. Art came naturally to them as they were also discovering mother nature. At that time, science was also only available to the rich which meant a couple of things: they could afford to spend a little bit of time incorporating art into their scientific work as they weren\u2019t rushing to meet grant deadlines, but it also meant they were already educated in the arts from an early age. These days anyone can be whatever they want to be. To the very least, it\u2019s easier now than it was in that era by an order of magnitude. Today we\u2019re taught that art almost has no place in the sciences and we must be efficient and to the point. Anything extra is generally referred to as fluff. If you like to romanticize science, you\u2019re not really a scientist. But I strongly believe that it\u2019s the artistic and I might add\u200a\u2014\u200aphilosophical\u200a\u2014\u200afluff that gets people interested in science. The hard math and core scientific values obviously being there regardless. All of this results in the absence of art in the sciences which makes art seem dry and boring. Science in its purest form is absolutely beautiful if communicated right. That\u2019s why we love to do our part at Datazar. We\u2019re far, far from perfect or even close to what we want to be. But putting just as much thought in design and UX as we put in functionality is a core principle. There\u2019s a misconception that incorporating art means adding more stuff. Incorporating art can also mean doing less, using less and making the most of what you already have in a beautiful and simple way. An example would be removing that obscene amount of jargon from your paper. So I ask everyone designing software for scientists, design for humans not robots. Scientists are humans too. So go ahead add that CSS animation.", "links": ["https://blog.datazar.com?source=logo-lo_bddaa0a4e6d---e2c7e6e1c75", "https://twitter.com/DatazarHQ", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.datazar.com%2Fwhy-we-focus-on-design-just-as-much-as-functionality-f6ccb3b2c926", "https://blog.datazar.com", "https://blog.datazar.com/tagged/r-language", "https://blog.datazar.com/tagged/open-data", "https://blog.datazar.com/tagged/how-to", "https://www.r-bloggers.com/", "https://blog.datazar.com/search", "https://blog.datazar.com/@ata_aman?source=post_header_lockup", "https://blog.datazar.com/@ata_aman?source=post_header_lockup", "https://blog.datazar.com/tagged/science?source=post", "https://blog.datazar.com/tagged/art?source=post", "https://blog.datazar.com/tagged/research?source=post", "https://blog.datazar.com/tagged/software-development?source=post", "https://blog.datazar.com/tagged/design?source=post", "https://blog.datazar.com/@ata_aman?source=footer_card", "https://blog.datazar.com/@ata_aman", "http://twitter.com/datazarhq", "https://blog.datazar.com?source=footer_card", "https://blog.datazar.com?source=footer_card", "https://blog.datazar.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "50": {"url": "https://news.ycombinator.com/item?id=14950255", "title": "", "text": "", "links": ["https://news.ycombinator.com", "https://edgylabs.com/machine-learning-to-enhance-smartphone-pictures/", "https://hn.algolia.com/?query=Machine%20Learning%20to%20Enhance%20Smartphone%20Pictures&sort=byDate&dateRange=all&type=story&storyText=false&prefix&page=0", "https://www.google.com/search?q=Machine%20Learning%20to%20Enhance%20Smartphone%20Pictures", "https://github.com/HackerNews/API", "http://www.ycombinator.com/apply/"]}, "51": {"url": "http://starmine.ai/datasets/ds02.html", "title": "", "text": "reddit Slack Twitter Instagram Facebook LinkedIn", "links": ["http://starmine.ai", "http://reddit.com/r/datasets", "https://join.slack.com/t/starmineai/shared_invite/MjIwNjg5OTUyODAzLTE1MDE2MTk1MzQtMmJhYTExNDViMA", "https://twitter.com/starmineAI", "https://www.instagram.com/starmine.ai/", "https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Fstarmine.ai/index.html&t=starmine%20ICO", "https://www.linkedin.com/cws/share?url=http%3A%2F%2Fstarmine.ai/index.html&token=&isFramed=true", "http://research.kraeutli.com/index.php/2013/11/the-tate-collection-on-github", "http://54.174.116.134/recommend/datasets/subscribers/free/supercolumns/supercolumns-elements-CMDB-nasdaq-nyse-otcbb-general-2017/supercolumns-elements-nasdaq-nyse-otcbb-general-UPDATE-2017-03-01.csv", "http://54.174.116.134/recommend/datasets/subscribers/free/supercolumns/supercolumns-elements-CMDB-nasdaq-nyse-otcbb-general-2017/supercolumns-elements-nasdaq-nyse-otcbb-general-UPDATE-2017-03-01.csv", "http://54.174.116.134/recommend/app/ai_connect-finance-historicaltrends_api?ccl1=playstation&ccl2=helium&ccl3=korea&ccl4=shampoo&ccl5=coffee&query=concept_column_labels&themesource=theme_CMDB-nasdaq-nyse-otcbb&month=01&year=all&db=CMDB-nasdaq-nyse-otcbb-general", "https://www0.gsb.columbia.edu/mygsb/faculty/research/pubfiles/1555/contagious.pdf", "https://trends.google.com/trends/explore?q=playstation,helium,korea,shampoo,coffee", "https://trends.google.com/trends/explore?q=playstation,helium,korea,shampoo,coffee"]}, "52": {"url": "https://github.com/maciejkula/spotlight/blob/master/examples/movielens_sequence/readme.rst", "title": "", "text": "Using sequences of user-item interactions as an input for recommender models has a number of attractive properties. Firstly, it recognizes that recommending the next item that a user may want to buy or see is precisely the goal we are trying to achieve. Secondly, it's plausible that the ordering of users' interactions carries additional information over and above just the identities of items they have interacted with. For example, a user is more likely to watch the next episode of a given TV series if they've just finished the previous episode. Finally, when the sequence of past interactions rather than the identity of the user is the input to a model, online systems can incorporate new users (and old users' new actions) in real time. They are fed to the existing model, and do not require a new model to be fit to incorporate new information (unlike factorization models). Recurrent neural networks are the most natural way of modelling such sequence problems. In recommendations, gated recurrent units (GRUs) have been used with success in the Session-based recommendations with recurrent neural networks paper. Spotlight implements a similar model using LSTM units as one of its sequence representations. But recurrent neural networks are not the only way of effectively representing sequences: convolutions can also do the job. In particular, we can use causal convolutions: convolutional filters applied to the sequence in a left-to-right fashion, emitting a representation at each step. They are causal in that the their output at time t is conditional on input up to t-1: this is necessary to ensure that they do not have access to the elements of the sequence we are trying to predict. Like LSTMs, causal convolutions can model sequences with long-term dependencies. This is achieved in two ways: stacking convolutional layers (with padding, every convolutional layer preserves the shape of the input), and dilation: insertion of gaps into the convolutional filters (otherwise known as atrous convolutions). Causal convolutions have been used in several recent high-profile papers: Using convolutional rather than recurrent networks for representing sequences has a couple of advantages, as described in this blog post: Spotlight implements causal convolution models as part of its sequence models package, alongside more traditional recurrent and pooling models. The Spotlight implementation has the following characteristics: The model is trained using one of Spotlight's implicit feedback losses, including pointwise (logistic and hinge) and pairwise (BPR as well as WARP-like adaptive hinge) losses. As with other Spotlight sequence models, the loss is computed for all the time steps of the sequence in one pass: for all timesteps t in the sequence, a prediction using elements up to t-1 is made, and the loss is averaged along both the time and the minibatch axis. This leads to siginficant training speed-ups relative to only computing the loss for the last element in the sequence. To see how causal CNNs compare to more traditional sequence models we can have a look at how they perform at predicting the next rated movie on the Movielens 1M dataset. With 1 million interactions spread among 6000 users and around 4000 movies it should be small enough to run quick experiments, but large enough to yield meaningful results. I chose to split the dataset into 80% train, and 10% test and validation sets. I construct 200-long sequences by splitting each user's item sequence into 200-long chunks; if a chunk is shorter than 200 elements, it's padded with zeros. I use mean reciprocal rank (MRR) as the evaluation metric. To choose hyperparameters, I run a quick, coarse grained hyperparameter search, using random sampling to draw 100 hyperparameter sets. With the data and hyperparameters ready, fitting and evaluating the model is relatively simple: Fitting the models is fairly quick, taking at most two or three minutes on a single K80 GPU. The code for the experiments is available in the experiments folder of the Spotlight repo. The results are as follows: It's difficult to draw clear-cut conclusions about the effect of each hyperparameter, but it looks like: To compare causal convolutions with more traditional sequence models I run similar hyperparameter searches for LSTM-based representations and pooling representations. The pooling representation is a simple averaging of item embedding across the sequence; the LSTM-based model runs an LSTM along a user's interactions, using the hidden state for prediction of the next element at each step. The results are as follows: A single layer LSTM seems to outperform causal convolutions, by an over 10% margin, helped by the adaptive hinge loss. Simple pooling performs quite badly. It looks like causal convolutions need some more work before beating recurrent networks. There are a couple of possible avenues for making them better: I'd love to get some input on these. If you have suggestions, let me know on Twitter or open an issue or PR in Spotlight.", "links": ["https://github.com/", "https://arxiv.org/abs/1511.06939", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.LSTMNet", "https://travis-ci.org/maciejkula/spotlight", "https://arxiv.org/pdf/1609.03499.pdf", "http://papers.nips.cc/paper/6527-conditional-image-generation-with-pixelcnn-decoders", "https://arxiv.org/abs/1610.10099", "https://medium.com/@TalPerry/convolutional-methods-for-text-d5260fd5675f", "https://arxiv.org/pdf/1610.10099.pdf", "https://maciejkula.github.io/spotlight/sequence/sequence.html", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.CNNNet", "https://maciejkula.github.io/spotlight/losses.html", "https://grouplens.org/datasets/movielens/1m/", "https://en.wikipedia.org/wiki/Mean_reciprocal_rank", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.LSTMNet", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.PoolNet", "https://maciejkula.github.io/spotlight/losses.html#spotlight.losses.adaptive_hinge_loss", "https://twitter.com/Maciej_Kula", "https://github.com/maciejkula/spotlight", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "53": {"url": "https://github.com/ynqa/word-embedding", "title": "", "text": "GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together.  Sign up             Use Git or checkout with SVN using the web URL.             This is an implementation of word embedding (also referred to as word representation) models in Golang. Word embedding makes words' meaning, structure, and concept mapping into vector space (and low dimension). For representative instance: Like this example, it could calculate word meaning by arithmetic operations between vectors. Listed models for word embedding, and checked it already implemented. Downloading text8 corpus, and training by Skip-Gram with negative sampling.", "links": ["https://github.com/", "https://github.com/ynqa/word-embedding/blob/master/LICENSE", "https://help.github.com/articles/which-remote-url-should-i-use", "https://travis-ci.org/ynqa/word-embedding", "https://godoc.org/github.com/ynqa/word-embedding", "https://goreportcard.com/report/github.com/ynqa/word-embedding", "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf", "http://nlp.stanford.edu/pubs/glove.pdf", "https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf", "http://mattmahoney.net/dc/textdata", "https://github.com/cjlin1/libsvm", "http://www.aclweb.org/anthology/Q15-1016", "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.648.8023&rep=rep1&type=pdf", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "54": {"url": "https://github.com/brannondorsey/keras_weight_animator", "title": "", "text": "GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together.  Sign up             Use Git or checkout with SVN using the web URL.          Save Keras weight matrices as short animated videos to better understand what and how your neural network models are learning. Below are examples of the first LSTM layer and the final output layer of a six-class RNN model trained over one epoch. Blue represents low values and red represents high values.   In order to render videos from the saved weight images you must also have the following packages installed on your machine: This module is named keras_weight_animator. It exposes a Keras callback function that you can include in any model fit(...) method. The two required parameters to image_saver_callback(...) are the Keras model and an output_directory to periodically save weight images to. By default, keras_weight_animator saves layer weights every 100 batches to output_directory as PNGs in folders named epoch_XXX-layer_NAME-weights_YY. Once training is complete, you can optionally create short animated video clips from the image sequences saved in output_directory using [bin/create_image_sequence.sh](bin/create_image_sequence.sh) path/to/output_directory. This will use parallel, mogrify, and ffmpeg to create a .mp4 from the image sequences located in each folder of output_directory. Video files will be named like epoch_XXX-layer_NAME-weights_YY.mp4. You can run this script automatically from your training script by passing the render_videos=True parameter to image_saver_callback(...). weight_image_sequences(...) takes a variety of optional keyword arguments. I've included an example usage of the module in examples/wisdm.py. This example uses smartphone accelerometer data from WISDM to classify human activity tasks like walking, standing, sitting, walking upstairs, etc... This example uses a one layer LSTM to classify a set of 60 data points (representing three seconds of data sampled at 20hz) as belonging to one of six classes. It outputs image sequences and videos to data/wisdm. Using a bash script to leverage parallel, ImageMagick, and FFMPEG isn't necessarily the most elegant solution, but its the one I had time for. The goal of this here lil' project was to write a quick tool that allows me to better understand how weights change over mini-batch updates in a variety of neural networks. Perhaps in the future I will come back and clean up some of the inelegancies. If you have interest in contributing or maintaining a cleaner version of this lib, please reach out at brannon@brannondorsey.com. This module is \u00a9 Brannon Dorsey 2017, released under an \u2665 MIT License \u2665. You are free to use, modify, distribute, sell, etc... this software under those terms. Example data is from the WIreless Sensor Datamining (WISDM) Actitracker dataset published by Fordham University: This idea is tangentially inspired by much of the work on Visualizing and Understanding Recurrent Networks by Andrej Karpathy, Justin Johnson, and Fei-Fei Li. GNU Parallel is adamant about citation to the point of excess IMHO, but for what its worth, here is their bibtex:", "links": ["https://github.com/", "https://github.com/brannondorsey/keras_weight_animator/blob/master/LICENSE", "https://help.github.com/articles/which-remote-url-should-i-use", "https://github.com/brannondorsey/keras_weight_animator/pull/1", "http://keras.io", "https://www.gnu.org/software/parallel/", "https://www.imagemagick.org/script/index.php", "https://ffmpeg.org/download.html", "https://keras.io/callbacks/", "http://matplotlib.org/users/colormaps.html", "http://www.cis.fordham.edu/wisdm/dataset.php", "https://arxiv.org/abs/1506.02078", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "55": {"url": "https://axibase.github.io/atsd-use-cases/Expatriation_Q2/", "title": "", "text": "Expatriation is the temporary or permanent relocation of a person by choice or by force from their native country to any other country in the world for work, pleasure, or purpose. History has seen expatriation from nearly every country in the world up to and including the current epoch. Famous American expatriates throughout history have included American founding father Benjamin Franklin, who worked as the first United States Minister to France for almost a decade after appointment by the Continental Congress in 1778. The  sixth President of the United States John Quincy Adams, who served as a U.S. Diplomat in four countries before winning the Presidency as a Democratic-Republican in 1825, most notably opening American diplomatic ties as the first United States Minister to the Russian Empire, a position he loved  so much he declined a subsequent offer to serve as an Associate Justice on the Supreme Court of the United States a few years later. American expatriates have come from all walks of life: authors, such as Ernest Hemingway who lived in China, Spain, Cuba, and was even present on the beach codenamed Omaha in Normandy during the allied landing. Musicians, like Jimi Hendrix who rose to international fame in London after leaving his native Seattle, directors like Stanley Kubrick, singers like Tina Turner, and even basketball players like  Allen Iverson, who played an incomplete season for a Turkish basketball team in 2010 after retiring from the NBA. For those who decide to make the change permanent, there comes a time when the inconvenience of living abroad as a resident alien outweighs the novelty and many decide to renounce their citizenship for one reason or another. Unsurprisingly, this is a long and complicated process, one of the main features of which is settling up with the Internal Revenue Service (IRS) via the eponymous Expatriation Tax. The Federal Register is a publishing outlet for the United States Government where the IRS officially releases quarterly information about American citizens who renounce their passport in lieu of a new one and other long-term residents who have decided to repatriate, view the raw data here and use the Axibase Data Crawler designed specifically for collecting, parsing, organzing and inserting historical expatriation data and new data as it becomes available.  Axibase covered last quarter\u2019s expatriation release here and is using this quarter\u2019s release to follow-up. Every election cycle countless celebrities and netizens issue their promise to leave if their candidate fails to get elected and the growing notoriety surrounding the 2016 Presidental Election has proven to  be no different. This type of social theater is almost as old as the country itself as there have been a number of similarly divisive elections throughout American history: Andrew Jackson\u2019s victory in 1828 was seen as a populace uprising against the corrupt political elite and a return to the authority of the common man, Abraham Lincoln\u2019s second term was viewed as the tightening of federal authority to excessive levels that ultimately led to the American Civil War, Franklin Roosevelt was called a dangerous communist whose infamous court-packing plan favorably increased the number of seats on the Supreme Court to his  advantage and was seen as abhorrent and traitorous by many, and of course who can forget recent elections that have featured scandal after scandal and two modern Presidents who served after losing the popular vote, but winning in the electorate. Figure 1   Query 1 Data queried in the SQL Console. Table 1 To give the issue further context, Figure 1 and Table 1 use the most current Department of Homeland Security (DHS) data  available to display current naturalization numbers in the United States. Naturalization is the sister process to expatriation, because as a person expatriates from one country they must be naturalized in another or end up a stateless person. Naturalization numbers in America have remained fairly constant throughout the last decade because there are a number of federal regulations that control the amount of long-term immigrants that America accepts each year and that quota is met without fail. As noted in the our Q1 article, the sister value and subject of this examination, expatriation numbers, has been steadily growing for several consecutive years. Figure 2.1   Query 2 The above query features robust syntax and calculated values. See the following tutorials  to understand more about SQL Console. Table 2 Figure 2.2   Expatriation has been increasing each year by roughly 30% since 2010, which featured abnormally high expatriation rates, most likely attributable to the economic turndown of the Great Recession which began in the United States as a result of the sub-prime mortgage crisis. American citizenship is often sought after for the economic opportunity that comes along with the passport, as the ability to work and do business in the country is heavily restricted or regulated, and with the value of that investment or  opportunity in question, it is unsurprising that the number of investors in the system, that is, new citizens, would fluctuate. It seems appropriate to call naturalization an investment because of the nature of the process, which is long, complicated, and often quite expensive similar to a long-position that will cost more at purchasing time but promises high returns after reaching maturity. The peak, or more appropriately, valley of the global recession occurred in 2009 when the global GDP contracted causing a decline in the median familty income of about five percent. Figure 3   Query 3 Table 3 Reducing the timespan to look at data over the last 5 years captures the repetition of this trend for this period of time. Of the eighteen quarters included in this query, only six of them, roughly 33% showed negative growth for this statistic. Growing expatriation numbers provide interesting insight into the perception of America both domestically and abroad, as these numbers not only include U.S. citizens who renounce their citizenship, but also long-term resident aliens who have returned home. Almost unbelievably, the IRS does not strip characteristic information about expatriates from their publications meaning that first and last names are included in the data. Using Social Security data concerning the most common first names chosen throughout the last century and United States Census data concerning the most common last names to compare, the following queries and visualizations show  the number of expatriates by common first and common last name throughout the entire observed period, ranked by their frequency in the IRS publication. The goal of these queries is purely demonstrative and lighthearted. There is no intention to invade privacy, defame people, or otherwise cause harm. Figure 4  Query 4 Table 4 All of these names appear on the list of most common American first names, linked here Figure 5  Query 5 Table 5 Only the name Smith appears on the list of most common American last names, linked here Use the Axibase Data Crawler to collect the data used in this artcle and examine it yourself in ChartLab.", "links": ["https://github.com/axibase/atsd-use-cases", "https://www.irs.gov/", "https://www.irs.gov/individuals/international-taxpayers/expatriation-tax", "https://www.federalregister.gov/", "https://www.federalregister.gov/documents/2017/05/10/2017-09475/quarterly-publication-of-individuals-who-have-chosen-to-expatriate-as-required-by-section-6039g", "https://github.com/axibase/atsd-data-crawlers/tree/irs-expatriation-data-crawler", "https://axibase.com", "https://apps.axibase.com/chartlab/654b9945#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql", "https://www.uscis.gov/news/fact-sheets/naturalization-fact-sheet", "http://www.unhcr.org/stateless-people.html", "https://apps.axibase.com/chartlab/7fa5b643#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql#examples", "https://apps.axibase.com/chartlab/7fa5b643/5/#fullscreen", "https://web.stanford.edu/group/recessiontrends/cgi-bin/web/sites/all/themes/barron/pdf/IncomeWealthDebt_fact_sheet.pdf", "https://apps.axibase.com/chartlab/7fa5b643/2/#fullscreen", "https://www.ssa.gov/oact/babynames/decades/century.html", "http://www.census.gov/main/www/cen2000.html", "https://www.ssa.gov/oact/babynames/decades/century.html", "http://www.census.gov/main/www/cen2000.html", "https://github.com/axibase/atsd-data-crawlers/tree/irs-expatriation-data-crawler", "https://apps.axibase.com", "https://github.com/axibase/atsd-use-cases", "https://github.com/axibase", "https://pages.github.com"]}, "56": {"url": "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/", "title": "", "text": "Machine learning is eating the world right now. Everyone and their mother are learning about machine learning models, classification, neural networks, and Andrew Ng. You\u2019ve decided you want to be a part of it, but where to start? In this article we\u2019ll cover some important characteristics of Python and why it\u2019s great for machine learning. We\u2019ll also cover some of the most important libraries it has for ML, and if it piques your interest, some places where you can learn more. Python is a great choice for machine learning for several reasons. First and foremost, it\u2019s a simple language\u00a0on the surface; even if you\u2019re not familiar with Python, getting up to speed is very quick if you\u2019ve ever used any other language with C-like syntax (i.e. every language out there). Second, Python has a great community, which results in good documentation and friendly, comprehensive answers in StackOverflow (fundamental!). Third, also stemming from the great community, there are plenty of useful libraries for Python (both as \u201cbatteries included\u201d and third party), which solve basically any problem that you can have (including machine learning). Yeah and it\u2019s true. Python isn\u2019t the fastest language out there: all those handy abstractions come at a cost. But here\u2019s the trick: libraries can and do offload the expensive calculations to the much more performant (but harder to use) C and C++. For instance, there\u2019s NumPy, which is a library for numerical computation. It\u2019s written in C, and it\u2019s fast. Practically every library out there that involves intensive calculations uses it \u2014 almost all the libraries listed next use it in some form. So if you read NumPy, think fast. Therefore, you can make your scripts run basically as fast as straight up writing them in a lower level language. So there\u2019s really nothing to worry about when it comes to speed. Are you starting out in machine learning? Want something that covers everything from feature engineering to training and testing a model? Look no further than scikit-learn! This fantastic piece of free software provides every tool necessary for machine learning and data mining. It\u2019s the de facto standard library for machine learning in Python, recommended for most of the \u2018old\u2019 ML algorithms. This library does both classification and regression, supporting basically every algorithm out there (support vector machines, random forest, naive bayes, and so on). It\u2019s built in such a way that allows easy switching of algorithms, so experimentation is easy. These \u2018older\u2019 algorithms are surprisingly resilient and work very well in a lot of cases. But that\u2019s not all! Scikit-learn also does dimensionality reduction, clustering, you name it. It\u2019s also blazingly fast since it runs on NumPy and SciPy (meaning that all the heavy number crunching is run on C instead of Python). Check out some examples to see everything this library is capable of, and the tutorials if you want to learn how it works. While not a machine learning library per se, NLTK is a must when working with natural language processing (NLP). It comes with a bundle of datasets and other lexical resources (useful for training models) in addition to libraries for working with text \u2014 for functions such as classification, tokenization, stemming, tagging, parsing and more. The usefulness of having all of this stuff neatly packaged can\u2019t be overstated. So if you are interested in NLP, check out some tutorials! Used widely in research and academia, Theano is the grandfather of all deep learning frameworks. Written in Python, it\u2019s tightly integrated with NumPy. Theano allows you to create neural networks, which are represented as mathematical expressions with multi-dimensional arrays. Theano handles this for you so you don\u2019t have to worry about the actual implementation of the math involved. It supports offloading calculations to the much faster GPU, which is a feature that everyone supports today, but back when they introduced it this wasn\u2019t the case. The library is very mature at this point and supports a very wide range of operations, which is a great plus when it comes to comparing it with other similar libraries. The biggest complaint out there is that the API may be unwieldy for some, making the library hard to use for beginners. However, there are wrappers that ease the pain and make working with Theano simple, such as Keras, Blocks and Lasagne. Interested in learning about Theano? Check out this Jupyter Notebook tutorial. The Google Brain team created TensorFlow for internal use in machine learning applications, and open sourced it in late 2015. They wanted something that could replace their older, closed source machine learning framework, DistBelief, which they said wasn\u2019t flexible enough and too tightly coupled to their infrastructure to be shared with other researchers around the world. And so TensorFlow was created. Learning from the mistakes of the past, many consider this library to be an improvement over Theano, claiming more flexibility and a more intuitive API. Not only can it be used for research but also for production environments, supporting huge clusters of GPUs for training. While it doesn\u2019t support as wide a range of operations as Theano, it has better computational graph visualizations. TensorFlow is very popular nowadays. In fact, if you\u2019ve heard about a single library on this list, it\u2019s probably this one: there isn\u2019t a day that goes by without a new blog post or paper mentioning TensorFlow gets published. This popularity translates into a lot of new users and a lot of tutorials, making it very welcoming to beginners. Keras is a fantastic library that provides a high-level API for neural networks and is capable of running on top of either Theano or TensorFlow. It makes harnessing the full power of these complex pieces of software much easier than using them directly. It\u2019s very user-friendly, putting user experience as a top priority. They manage this by using simple APIs and excellent feedback on errors. It\u2019s also modular, meaning that different models (neural layers, cost functions, and so on) can be plugged together with little restrictions. This also makes it very easy to extend, since it\u2019s simple to add new modules and connect them with the existing ones. Some people have called Keras so good that it is effectively cheating in machine learning. So if you\u2019re starting out with deep learning, go through the examples and documentation to get a feel for what you can do with it. And if you want to learn, start out with this tutorial and see where you can go from there. Two similar alternatives are Lasagne and Blocks, but they only run on Theano. So if you tried Keras and are unhappy with it, maybe try out one of these alternatives to see if they work out for you. Another popular deep learning framework is Torch, which is written in Lua. Facebook open-sourced a Python implementation of Torch called PyTorch, which allows you to conveniently use the same low-level libraries that Torch uses, but from Python instead of Lua. PyTorch is much better for debugging since one of the biggest differences between Theano/TensorFlow and PyTorch is that the former use symbolic computation while the latter doesn\u2019t. Symbolic computation means that coding an operation (say, \u2018x + y\u2019), it\u2019s not computed when that line is interpreted. Before getting executed it has to be compiled (translated to CUDA or C). This makes debugging harder in Theano/TensorFlow, since an error is much harder to associate with the line of code that caused it. Of course, doing things this way has its advantages, but debugging isn\u2019t one of them. If you want to start out with PyTorch the official tutorials are very friendly to beginners but get to advanced topics as well. Alright, you\u2019ve presented me with a lot of alternatives for machine learning libraries in Python. What should I choose? How do I compare these things? Where do I start? Our Ape Advice\u2122 for beginners is to try and not get bogged down by details. If you\u2019ve never done anything machine learning related, try out scikit-learn. You\u2019ll get an idea of how the cycle of tagging, training and testing work and how a model is developed. Now, if you want to try out deep learning, start out with Keras \u2014 which is widely agreed to be the easiest framework \u2014 and see where that takes you. After you have more experience, you will start to see what it is that you actually want from the framework: greater speed, a different API, or maybe something else, and you\u2019ll be able to make a more informed decision. And even then, there is an endless supply of articles out there comparing Theano, Torch, and TensorFlow. There\u2019s no real way to tell which one is the good one. It\u2019s important to take into account that all of them have wide support and are improving constantly, making comparisons harder to make. A six month old benchmark may be outdated, and year old claims of framework X doesn\u2019t support operation Y could no longer be valid. Finally, if you\u2019re interested in doing machine learning specifically applied to NLP, why not check out MonkeyLearn! Our platform provides a unique UX that makes it super easy to build, train and improve NLP models. You can either use pre-trained models for common use cases (like sentiment analysis, topic detection or keyword extraction) or train custom algorithms using your particular data. Also, you don\u2019t have to worry about the underlying infrastructure or deploying your models, our scalable cloud does this for you. You can start for free and integrate right away with our beautiful API. There are plenty of online resources out there to learn about machine learning ! Here are a few: So that was a brief intro to machine learning in Python and some of its libraries. The important part is not getting bogged down by details and just trying stuff out. Follow your curiosity, and don\u2019t be afraid to experiment. Know about a python library that was left out? Share it in the comments below!", "links": ["http://www.monkeylearn.com", "http://monkeylearn.com/use-cases/", "http://monkeylearn.com/docs/", "http://www.monkeylearn.com/pricing", "http://www.monkeylearn.com/blog/", "https://app.monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/?utm_source=ml-blog&utm_medium=header&utm_campaign=blog", "https://monkeylearn.com/blog", "http://monkeylearn.com/use-cases/", "http://monkeylearn.com/docs/", "http://www.monkeylearn.com/pricing", "http://www.monkeylearn.com/blog/", "https://app.monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/?utm_source=ml-blog&utm_medium=header&utm_campaign=blog", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/07/Post_2c.png", "http://scikit-learn.org/", "http://scikit-learn.org/stable/auto_examples/index.html#general-examples", "http://scikit-learn.org/stable/tutorial/index.html", "http://www.nltk.org/", "http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk", "http://deeplearning.net/software/theano/", "https://keras.io/", "https://github.com/mila-udem/blocks", "https://github.com/Lasagne/Lasagne", "http://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/Theano%20Tutorial.ipynb", "https://www.tensorflow.org/", "https://www.tensorflow.org/get_started/graph_viz", "https://www.tensorflow.org/tutorials/", "https://keras.io/", "https://news.ycombinator.com/item?id=13872764", "https://github.com/fchollet/keras#getting-started-30-seconds-to-keras", "https://keras.io/getting-started/functional-api-guide/", "https://elitedatascience.com/keras-tutorial-deep-learning-in-python", "https://github.com/Lasagne/Lasagne", "https://github.com/mila-udem/blocks", "http://torch.ch/", "http://pytorch.org/", "http://pytorch.org/tutorials/", "http://scikit-learn.org/stable/tutorial/basic/tutorial.html", "https://elitedatascience.com/keras-tutorial-deep-learning-in-python", "https://www.quora.com/Is-TensorFlow-better-than-other-leading-libraries-such-as-Torch-Theano", "http://www.ccri.com/2016/12/09/torch-vs-tensorflow-vs-theano/", "https://www.reddit.com/r/MachineLearning/comments/5w3q74/d_so_pytorch_vs_tensorflow_whats_the_verdict_on/", "https://github.com/zer0n/deepframeworks/blob/master/README.md", "https://monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/", "http://nbviewer.jupyter.org/github/rhiever/Data-Analysis-and-Machine-Learning-Projects/blob/master/example-data-science-notebook/Example%20Machine%20Learning%20Notebook.ipynb", "https://blog.monkeylearn.com/a-gentle-guide-to-machine-learning/", "https://www.coursera.org/learn/machine-learning", "http://course.fast.ai/", "https://monkeylearn.com/blog/author/bruno/", "https://monkeylearn.com/blog/category/news/", "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/#respond", "https://monkeylearn.com/blog/author/bruno/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/monkeylearn_zapier.png", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/wp-content/uploads/2017/06/Post_1e.png", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/naive-bayes-classifier.png", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/monkey-startup.png", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/category/applications/", "https://monkeylearn.com/blog/category/guides/", "https://monkeylearn.com/blog/category/howto/", "https://monkeylearn.com/blog/category/news/", "https://monkeylearn.com/blog/category/text-classification/", "https://www.monkeylearn.com", "https://www.monkeylearn.com", "http://slack.monkeylearn.com/", "https://twitter.com/monkeylearn", "https://github.com/monkeylearn/"]}, "57": {"url": "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "title": "Machine Learning Translation and the Google Translate Algorithm", "text": "Every day we use different technologies without even knowing how exactly they work. In fact, it\u2019s not very easy to understand engines powered by machine learning. The Statsbot team wants to make machine learning clear by telling data stories in this blog. Today, we\u2019ve decided to explore machine translators and explain how the Google Translate algorithm works. Years ago, it was very time consuming to translate the text from an unknown language. Using simple vocabularies with word-for-word translation was hard for two reasons: 1) the reader had to know the grammar rules and 2) needed to keep in mind all language versions while translating the whole sentence. Now, we don\u2019t need to struggle so much\u2013 we can translate phrases, sentences, and even large texts just by putting them in Google Translate. But most people don\u2019t actually care how the engine of machine learning translation works. This post is for those who do care. If the Google Translate engine tried to kept the translations for even short sentences, it wouldn\u2019t work because of the huge number of possible variations. The best idea can be to teach the computer sets of grammar rules and translate the sentences according to them. If only it were as easy as it sounds. If you have ever tried learning a foreign language, you know that there are always a lot of exceptions to rules. When we try to capture all these rules, exceptions and exceptions to the exceptions in the program, the quality of translation breaks down. Creating your own simple machine translator would be a great project for any data science resume. Let\u2019s try to investigate what hides in the \u201cblack boxes\u201d that we call machine translators. Deep neural networks can achieve excellent results in very complicated tasks (speech/visual object recognition), but despite their flexibility, they can be applied only for tasks where the input and target have fixed dimensionality. Here is where Long Short-Term Memory networks (LSTMs) come into play, helping us to work with sequences whose length we can\u2019t know a priori. LSTMs are a special kind of recurrent neural network (RNN), capable of learning long-term dependencies. All RNNs look like a chain of repeating modules. So the LSTM transmits data from module to module and, for example, for generating Ht we use not only Xt, but all previous input values X. To learn more about structure and mathematical models of LSTM, you can read the great article \u201cUnderstanding LSTM Networks.\u201d Our next step is bidirectional recurrent neural networks (BRNNs). What a BRNN does, is split the neurons of a regular RNN into two directions. One direction is for positive time, or forward states. The other direction is for negative time, or backward states. The output of these two states are not connected to inputs of the opposite direction states. To understand why BRNNs can work better than a simple RNN, imagine that we have a sentence of 9 words and we want to predict the 5th word. We can make it know either only the first 4 words, or the first 4 words and last 4 words. Of course, the quality in the second case would be better. Now we\u2019re ready to move to sequence to sequence models (also called seq2seq). The basic seq2seq model consist of two RNNs: an encoder network that processes the input and a decoder network that generates the output. Finally, we can make our first machine translator! However, let\u2019s think about one trick. Google Translate currently supports 103 languages, so we should have 103x102 different models for each pair of languages. Of course, the quality of these models varies according to the popularity of languages and the amount of documents needed for training this network. The best that we can do is to make one NN to take any language as input and translate into any language. That very idea was realized by Google engineers at the end of 2016. Architecture of NN was build on the seq2seq model, which we have already studied. The only exception is that between the encoder and decoder there are 8 layers of LSTM-RNN that have residual connections between layers with some tweaks for accuracy and speed. If you want to go deeper with that, take a look at the article Google\u2019s Neural Machine Translation System. The system requires a \u201ctoken\u201d at the beginning of the input sentence which specifies the language you\u2019re trying to translate the phrase into. This improves translation quality and enables translations even between two languages which the system hasn\u2019t seen yet, a method termed \u201cZero-Shot Translation.\u201d When we\u2019re talking about improvements and better results from Google Translate algorithms, how can we correctly evaluate that the first candidate for translation is better than the second? It\u2019s not a trivial problem, because for some commonly used sentences we have the sets of reference translations from the professional translators, that have, of course, some differences. There are a lot of approaches that partly solve this problem, but the most popular and effective metric is BLEU (bilingual evaluation understudy). Imagine, we have two candidates from machine translators: Although they have the same meaning they differ in quality and have different structure. Let\u2019s look at two human translations: Obviously, Candidate 1 is better, sharing more words and phrases compared to Candidate 2. This is a key idea of the simple BLEU approach. We can compare n-grams of the candidate with n-grams of the reference translation and count the number of matches (independent from their position). We use only n-gram precisions, because calculating recall is difficult with multiple refs and the result is the geometric average of n-gram scores. Now you can evaluate the complex engine of machine learning translation. Next time when you translate something with Google Translate, imagine how many millions of documents it analyzed before giving you the best language version.", "links": ["https://blog.statsbot.co?source=logo-lo_ef1ef7b67a46---cfc9f21a543a", "https://twitter.com/statsbotco", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.statsbot.co%2Fmachine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co", "https://blog.statsbot.co/analytics/home", "https://blog.statsbot.co/datascience/home", "https://blog.statsbot.co/design/home", "https://blog.statsbot.co/startups/home", "https://blog.statsbot.co/bots/home", "https://blog.statsbot.co/news/home", "https://blog.statsbot.co/statsbot-digest-b0d7372f842a", "http://statsbot.co?utm_source=blog&utm_medium=navigation&utm_campaign=robot_face", "https://blog.statsbot.co/search", "https://blog.statsbot.co/@daniilkorbut?source=post_header_lockup", "https://blog.statsbot.co/@daniilkorbut?source=post_header_lockup", "https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html", "http://statsbot.co?utm_source=blog&utm_medium=article&utm_campaign=machine_translation", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6?utm_source=blog&utm_medium=article&utm_campaign=machine_translation", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6", "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "https://www.semanticscholar.org/paper/Hybrid-speech-recognition-with-Deep-Bidirectional-Graves-Jaitly/5807664af8e63d5207f59fb263c9e7bd3673be79", "https://research.googleblog.com/2016/09/a-neural-network-for-machine.html", "https://www.newscientist.com/article/2114748-google-translate-ai-invents-its-own-language-to-translate-with/", "https://research.googleblog.com/2016/09/a-neural-network-for-machine.html", "https://arxiv.org/abs/1609.08144", "https://en.wikipedia.org/wiki/BLEU", "https://en.wikipedia.org/wiki/N-gram", "https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3", "https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/tagged/machine-learning?source=post", "https://blog.statsbot.co/tagged/machine-translation?source=post", "https://blog.statsbot.co/tagged/google-translate?source=post", "https://blog.statsbot.co/tagged/data-science?source=post", "https://blog.statsbot.co/tagged/neural-networks?source=post", "https://blog.statsbot.co/@daniilkorbut?source=footer_card", "https://blog.statsbot.co/@daniilkorbut", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "58": {"url": "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "title": "Yanir SeroussiMy 10-step path to becoming a remote data scientist with\u00a0Automattic", "text": "About two years ago, I read the book The Year without Pants, which describes the author\u2019s experience leading a team at Automattic (the company behind WordPress.com, among other products). Automattic is a fully-distributed company, which means that all of its employees work remotely (hence pants are optional). While the book discusses some of the challenges of working remotely, the author\u2019s general experience was very positive. A few months after reading the book, I decided to look for a full-time position after a period of independent work. Ideally, I wanted a well-paid data science-y remote job with an established distributed tech company that offers a good life balance and makes products I care about. Automattic seemed to tick all my boxes, so I decided to apply for a job with them. This post describes my application steps, which ultimately led to me becoming a data scientist with Automattic. Before jumping in, it\u2019s worth noting that this post describes my personal experience. If you apply for a job with Automattic, your experience is likely to be different, as the process varies across teams, and evolves over time. I decided to apply for a data wrangler position with Automattic in October 2015. While data wrangler may sound less sexy than data scientist, reading the job ad led me to believe that the position may involve interesting data science work. This impression was strengthened by some LinkedIn stalking, which included finding current data wranglers and reading through their profiles and websites. I later found out that all the people on the data division start out as data wranglers, and then they may pick their own title. Some data wranglers do data science work, while others are more focused on data engineering, and there are some projects that require a broad range of skills. As the usefulness of the term data scientist is questionable, I\u2019m not too fussed about fancy job titles. It\u2019s more important to do interesting work in a supportive environment. Applying for the job was fairly straightforward. I simply followed the instructions from the ad:  Does this sound interesting? If yes, please send a short email to jobs @ this domain telling us about yourself and attach a resum\u00e9. Let us know what you can contribute to the team. Include the title of the position you\u2019re applying for and your name in the subject. Proofread! Make sure you spell and capitalize WordPress and Automattic correctly. We are lucky to receive hundreds of applications for every position, so try to make your application stand out. If you apply for multiple positions or send multiple emails there will be one reply.  Having been on the receiving side of job applications, I find it surprising that many people don\u2019t bother writing a cover letter, addressing the selection criteria in the ad, or even applying for a job they\u2019re qualified to do. Hence, my cover letter was fairly short, comprising of several bullet points that highlight the similarities between the job requirements and my experience. It was nothing fancy, but simple cover letters have worked well for me in the past. The initial application was followed by a long wait. From my research, this is the typical scenario. This is unsurprising, as Automattic is a fairly small company with a large footprint, which is both distributed and known as a great place to work (e.g., its Glassdoor rating is 4.9). Therefore, it attracts many applicants from all over the world, which take a while to process. In addition, Matt Mullenweg (Automattic\u2019s CEO) reviews job applications before passing them on to the team leads. As I didn\u2019t know that Matt reviewed job applications, I decided to try to shorten the wait by getting introduced to someone in the data division. My first attempt was via a second-degree LinkedIn connection who works for Automattic. He responded quickly when I reached out to him, saying that his experience working with the company is in line with the Glassdoor reviews \u2013 it\u2019s the best job he\u2019s had in his 15-year-long career. However, he couldn\u2019t help me with an intro, because there is no simple way around Automattic\u2019s internal processes. Nonetheless, he reassured me that it is worth waiting patiently, as the strict process means that you end up working with great people. I wasn\u2019t in a huge rush to find a job, but in December 2015 I decided to accept an offer to become the head of data science at Car Next Door. This was a good decision at the time, as I believe in the company\u2019s original vision of reducing the number of cars on the road through car sharing, and it seemed like there would be many interesting projects for me to work on. The position wasn\u2019t completely remote, but as the company was already spread across several cities, I was able to work from home for a day or two every week. In addition, it was a pleasant commute by bike from my Sydney home to the office, so putting the fully-remote job search on hold didn\u2019t seem like a major sacrifice. As I haven\u2019t heard anything from Automattic at that stage, it seemed unwise to reject a good offer, so I started working full-time with Car Next Door in January 2016. I successfully attracted Automattic\u2019s attention with a post I published on the misuse of the word insights by many tech companies, which included an example from WordPress.com. Greg Ichneumon Brown, one of the data wranglers, commented on the post, and invited me to apply to join Automattic and help them address the issues I raised. This happened after I accepted the offer from Car Next Door, and hasn\u2019t resulted in any speed up of the process, so I just gave up on Automattic and carried on with my life. I finally heard back from Automattic in February 2016 (four months after my initial application and a month into my employment with Car Next Door). Martin Remy, who leads the data division, emailed me to enquire if I\u2019m still interested in the position. I informed him that I was no longer looking for a job, but we agreed to have an informal chat, as I\u2019ve been waiting for such a long time. As is often the case with Automattic interviews, the chat with Martin was completely text-based. Working with a distributed team means that voice and video calls can be hard to schedule. Hence, Automattic relies heavily on textual channels, and text-based interviews allow the company to test the written communication skills of candidates. The chat revolved around my past work experience, and Martin also took the time to answer my questions about the company and the data division. At the conclusion of the chat, Martin suggested I contact him directly if I was ever interested in continuing the application process. While I was happy with my position at the time, the chat strengthened my positive impression of Automattic, and I decided that I would reapply if I were to look for a full-time position again. My next job search started earlier than I had anticipated. In October 2016, I decided to leave Car Next Door due to disagreements with the founders over the general direction of the company. In addition, I had more flexibility in choosing where to live, as my personal circumstances had changed. As I\u2019ve always been curious about life outside the capital cities of Australia, I wanted to move away from Sydney. While I could have probably continued working remotely with Car Next Door, I felt that it would be better to find a job with a fully-distributed team. Therefore, I messaged Martin and we scheduled another chat. The second chat with Martin took place in early November. Similarly to the first chat, it was conducted via Skype text messages, and revolved around my work in the time that has passed since the first chat. This time, as I was keen on continuing with the process, I asked more specific questions about what kind of work I\u2019m likely to end up doing and what the next steps would be. The answers were that I\u2019d be joining the data science team, and that the next steps are a pre-trial test, a paid trial, and a final interview with Matt. While this sounds straightforward, it took another six months until I finally became an Automattic employee (but I wasn\u2019t in a rush). The pre-trial test consisted of a data analysis task, where I was given a dataset and a set of questions to answer by Carly Stambaugh, the data science lead. The goal of the test is to evaluate the candidate\u2019s approach to a problem, and assess organisational and communication skills. As such, the focus isn\u2019t on obtaining a specific result, so candidates are given a choice of several potential avenues to explore. The open-ended nature of the task is reminiscent of many real-world data science projects, where you don\u2019t always have a clear idea of what you\u2019re going to discover. While some people may find this kind of uncertainty daunting, I find it interesting, as it is one of the things that makes data science a science. I spent a few days analysing the data and preparing a report, which was submitted as a Jupyter Notebook. After submitting my initial report, there were a few follow-up questions, which I answered by email. The report was reviewed by Carly and Martin, and as they were satisfied with my work, I was invited to proceed to the next stage: A paid trial project. The main part of the application process with Automattic is the paid trial project. The rationale behind doing paid trials was explained a few years ago by Matt in Hire by Auditions, Not Resumes:  Before we hire anyone, they go through a trial process first, on contract. They can do the work at night or over the weekend, so they don\u2019t have to leave their current job in the meantime. We pay a standard rate of $25 per hour, regardless of whether you\u2019re applying to be an engineer or the chief financial officer. During the trials, we give the applicants actual work. If you\u2019re applying to work in customer support, you\u2019ll answer tickets. If you\u2019re an engineer, you\u2019ll work on engineering problems. If you\u2019re a designer, you\u2019ll design. There\u2019s nothing like being in the trenches with someone, working with them day by day. It tells you something you can\u2019t learn from resumes, interviews, or reference checks. At the end of the trial, everyone involved has a great sense of whether they want to work together going forward. And, yes, that means everyone \u2014 it\u2019s a mutual tryout. Some people decide we\u2019re not the right fit for them.  The goal of my trial project was to improve the Elasticsearch language detection algorithm. This took about a month, and ultimately resulted in a pull request that got merged into the language detection plugin. I find this aspect of the process pretty exciting: While the plugin is used to classify millions of documents internally by Automattic, its impact extends beyond the company, as Elasticsearch is used by many other organisations and projects. This stands in contrast to many other technical job interviews, which consist of unpaid work on toy problems under stressful conditions, where the work performed is ultimately thrown away. While the monetary compensation for the trial work is lower than the market rate for data science consulting, I valued the opportunity to work on a real open source project, even if this hadn\u2019t led to me getting hired. There was much more to the trial project than what\u2019s shown in the final pull request. Most of the discussions were held on an internal project thread, primarly under the guidance of Carly (the data science lead), and Greg (the data wrangler who replied to my post a year earlier). The project was kicked off with a general problem statement: There was some evidence that the Elasticsearch language detection plugin doesn\u2019t perform well on short texts, and my mission was to improve it. As the plugin didn\u2019t include any tests for short texts, one of the main contributions of my work was the creation of datasets and tests to measure its accuracy on texts of different lengths. This was followed by some tweaks that improved the plugin\u2019s performance, as summarised in the pull request. Internally, this work consisted of several iterations where I came up with ideas, asked questions, implemented the ideas, shared the results, and discussed further steps. There are still many possible improvements to the work done in the trial. However, as trials generally last around a month, we decided to end it after a few iterations. I enjoyed the trial process, but it is definitely not for everyone. Most notably, there is a strong emphasis on asynchronous text-based communication, which is the main mode by which projects are coordinated at Automattic. People who don\u2019t enjoy written communication may find this aspect challenging, but I have always found that writing helps me organise my thoughts, and that I retain information better when reading than when listening to people speak. That being said, Automatticians do meet in person several times a year, and some teams have video chats for some discussions. While doing the trial, I had a video chat with Carly, which was the first (and last) time in the process that I got to see and hear a live human. However, this was not an essential part of the trial project, as our chat was mostly on the data scientist role and my job expectations. I finished working on the trial project just before Christmas. The feedback I received throughout the trial was positive, but Martin, Carly, and Greg had to go through the work and discuss it among themselves before making a final decision. This took about a month, due to the holiday period, various personal circumstances, and the data science team meetup that was scheduled for January 2017. Eventually, Martin got back to me with positive news: They were satisfied with my trial work, which meant there was only one stage left \u2013 the final interview with Matt Mullenweg, Automattic\u2019s CEO. Like other parts of the process, the interview with Matt is text-based. The way it works is fairly simple: I was instructed to message Matt on Slack and wait for a response, which may take days or weeks. I sent Matt a message on January 25, and was surprised to hear back from him the following morning. However, that day was Australia Day, which is a public holiday here. Therefore, I only got back to him two hours after he messaged me that morning, and by that time he was probably already busy with other things. This was the start of a pretty long wait. I left Car Next Door at the end of January, as I figured that I would be able to line up some other work even if things didn\u2019t work out with Automattic. My plan was to take some time off, and then move up to the Northern Rivers area of New South Wales. I had two Reef Life Survey trips planned, so I wasn\u2019t going to start working again before mid-April. I assumed that I would hear back from Matt before then, which would have allowed me to make an informed decision whether to look for another job or not. After two weeks of waiting, the time for my dive trips was nearing. As I was going to be without mobile reception for a while, I thought it\u2019d be worth letting Matt know my schedule. After discussing the matter with Martin, I messaged Matt. He responded, saying that we might as well do the interview at the beginning of April, as I won\u2019t be starting work before that time anyway. I would have preferred to be done with the interview earlier, but was happy to have some certainty and not worry about missing more chat messages before April. In early April, I returned from my second dive trip (which included a close encounter with Cyclone Debbie), and was hoping to sort out my remote work situation while completing the move up north. Unfortunately, while the move was successful, I was ready to give up on Automattic because I haven\u2019t heard back from Matt at all in April. However, Martin remained optimistic and encouraged me to wait patiently, which I did as I was pretty busy with the move and with some casual freelancing projects. The chat with Matt finally happened on May 2. As is often the case, it took a few hours and covered my background, the trial process, and some other general questions. I asked him about my long wait for the final chat, and he apologised for me being an outlier, as most chats happen within two weeks of a candidate being passed over to him. As the chat was about to conclude, we got to the topic of salary negotiation (which went well), and then the process was finally over! Within a few hours of the chat I was sent an offer letter and an employment contract. As Automattic has an entity in Australia (called Ausomattic), it\u2019s a fairly standard contract. I signed the contract and started work the following week \u2013 over a year and a half after my initial application. Even before I started working, I booked tickets to meet the data division in Montr\u00e9al \u2013 a fairly swift transition from the long wait for the final interview. As noted above, Automatticians get to choose their own job titles, so to become a data scientist with Automattic, I had to set my job title to Data Scientist. This is generally how many people become data scientists these days, even outside Automattic. However, job titles don\u2019t matter as much as job satisfaction. And after 2.5 months with Automattic, I\u2019m very satisfied with my decision to join the company. My first three weeks were spent doing customer support, like all new Automattic employees. Since then, I\u2019ve been involved in projects to make engagement measurement more consistent (harder than it sounds, as counting things is hard), and to improve the data science codebase (e.g., moving away from Legacy Python). Besides that, I also went to Montr\u00e9al for the data division meetup, and have started getting into chatbot work. I\u2019m looking forward to doing more work and sharing my experience here and on data.blog. Very enlightening post! It was very awesome to see that the insights you saw to Elasticsearch went to a PR. I bet that was worth the whole thing!  That\u2019s very exciting, I wanted to ask are you a self learner or do you have a degree,can you please share your background. Thank you  Thanks Mostafa. Yes, I have a BSc in computer science, and a PhD in what you would now call data science. See: https://www.linkedin.com/in/yanirseroussi/  Fill in your details below or click an icon to log in:     Connecting to %s         Enter your email address to follow this blog and receive notifications of new posts by email.", "links": ["https://yanirseroussi.com/", "https://yanirseroussi.com/", "https://yanirseroussi.com/about/", "https://yanirseroussi.com/presentations/", "https://yanirseroussi.com/phd-work/", "https://yanirseroussi.com/2014/04/05/kaggle-competition-summaries/", "http://scottberkun.com/yearwithoutpants/", "https://automattic.com/", "https://yanirseroussi.com/2015/03/22/the-long-road-to-a-lifestyle-business/", "http://web.archive.org/web/20150908140923/https://automattic.com/work-with-us/data-wrangler/", "https://yanirseroussi.com/2016/08/04/is-data-scientist-a-useless-job-title/", "https://automattic.com/about/", "https://www.glassdoor.com.au/Reviews/Automattic-Reviews-E751107.htm", "http://davemart.in/remote-hiring/", "https://www.carnextdoor.com.au/", "https://yanirseroussi.com/2015/12/08/this-holiday-season-give-me-real-insights/", "https://yanirseroussi.com/2015/12/08/this-holiday-season-give-me-real-insights/#comment-957", "http://jupyter.org/", "https://hbr.org/2014/01/hire-by-auditions-not-resumes", "https://www.elastic.co/products/elasticsearch", "https://github.com/jprante/elasticsearch-langdetect/pull/69", "https://github.com/jprante/elasticsearch-langdetect/pull/69", "https://yanirseroussi.com/2016/01/24/the-joys-of-offline-data-collection/", "https://www.whitsundaytimes.com.au/news/boat-caught-in-eye-of-cyclone-cruises-home/3164170/", "https://data.blog/2017/06/29/data-coalesce-automattic-data-division-meets-in-montreal/", "http://daynebatten.com/2016/06/counting-hard-data-science/", "http://powerfulpython.com/blog/magic-word-legacy-python/", "https://data.blog/2017/05/24/may-the-bot-be-with-you-how-algorithms-are-supporting-happiness-at-wordpress-com/", "https://data.blog/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=twitter", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=facebook", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=google-plus-1", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=linkedin", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=reddit", "https://yanirseroussi.com/category/data-science-2/", "https://yanirseroussi.com/tag/automattic/", "https://yanirseroussi.com/tag/career/", "https://yanirseroussi.com/tag/data-science/", "https://yanirseroussi.com/tag/elasticsearch/", "https://yanirseroussi.com/tag/personal/", "https://yanirseroussi.com/tag/wordpress/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "https://yanirseroussi.com/author/yanirseroussi/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comments", "https://yanirseroussi.com/2017/06/03/exploring-and-visualising-reef-life-survey-data/", "https://dotnetmeditations.com/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1698", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1698&_wpnonce=6c197b2684", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1698#respond", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1700", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1700&_wpnonce=350fa40ca9", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1700#respond", "http://yanirseroussi.com", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1705", "https://www.linkedin.com/in/yanirseroussi/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1705&_wpnonce=7f1e8d7061", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1705#respond", "https://gravatar.com/site/signup/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "https://yanirseroussi.com/2017/06/03/exploring-and-visualising-reef-life-survey-data/", "https://yanirseroussi.com/2017/01/08/customer-lifetime-value-and-the-proliferation-of-misinformation-on-the-internet/", "https://yanirseroussi.com/2016/09/19/ask-why-finding-motives-causes-and-purpose-in-data-science/", "https://yanirseroussi.com/2016/08/21/seven-ways-to-be-data-driven-off-a-cliff/", "https://yanirseroussi.com/category/bcrecommender/", "https://yanirseroussi.com/category/data-science-2/", "https://yanirseroussi.com/category/environment/", "https://yanirseroussi.com/category/general/", "https://yanirseroussi.com/category/kaggle-2/", "https://yanirseroussi.com/category/machine-intelligence/", "https://yanirseroussi.com/category/phd-work/", "https://wordpress.com/?ref=footer_blog"]}, "59": {"url": "https://medium.com/airbnb-engineering/using-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d", "title": "Using Machine Learning to Predict Value of Homes On\u00a0Airbnb", "text": "by Robert Chang Data products have always been an instrumental part of Airbnb\u2019s service. However, we have long recognized that it\u2019s costly to make data products. For example, personalized search ranking enables guests to more easily discover homes, and smart pricing allows hosts to set more competitive prices according to supply and demand. However, these projects each required a lot of dedicated data science and engineering time and effort. Recently, advances in Airbnb\u2019s machine learning infrastructure have lowered the cost significantly to deploy new machine learning models to production. For example, our ML Infra team built a general feature repository that allows users to leverage high quality, vetted, reusable features in their models. Data scientists have started to incorporate several AutoML tools into their workflows to speed up model selection and performance benchmarking. Additionally, ML infra created a new framework that will automatically translate Jupyter notebooks into Airflow pipelines. In this post, I will describe how these tools worked together to expedite the modeling process and hence lower the overall development costs for a specific use case of LTV modeling\u200a\u2014\u200apredicting the value of homes on Airbnb. Customer Lifetime Value (LTV), a popular concept among e-commerce and marketplace companies, captures the projected value of a user for a fixed time horizon, often measured in dollar terms. At e-commerce companies like Spotify or Netflix, LTV is often used to make pricing decisions like setting subscription fees. At marketplace companies like Airbnb, knowing users\u2019 LTVs enable us to allocate budget across different marketing channels more efficiently, calculate more precise bidding prices for online marketing based on keywords, and create better listing segments. While one can use past data to calculate the historical value of existing listings, we took one step further to predict LTV of new listings using machine learning. Data scientists are typically accustomed to machine learning related tasks such as feature engineering, prototyping, and model selection. However, taking a model prototype to production often requires an orthogonal set of data engineering skills that data scientists might not be familiar with. Luckily, At Airbnb we have machine learning tools that abstract away the engineering work behind productionizing ML models. In fact, we could not have put our model into production without these amazing tools. The remainder of this post is organized into four topics, along with the tools we used to tackle each task: One of the first steps of any supervised machine learning project is to define relevant features that are correlated with the chosen outcome variable, a process called feature engineering. For example, in predicting LTV, one might compute the percentage of the next 180 calendar dates that a listing is available or a listing\u2019s price relative to comparable listings in the same market. At Airbnb, feature engineering often means writing Hive queries to create features from scratch. However, this work is tedious and time consuming as it requires specific domain knowledge and business logic, which means the feature pipelines are often not easily sharable or even reusable. To make this work more scalable, we developed Zipline\u200a\u2014\u200aa training feature repository that provides features at different levels of granularity, such as at the host, guest, listing, or market level. The crowdsourced nature of this internal tool allows data scientists to use a wide variety of high quality, vetted features that others have prepared for past projects. If a desired feature is not available, a user can create her own feature with a feature configuration file like the following: When multiple features are required for the construction of a training set, Zipline will automatically perform intelligent key joins and backfill the training dataset behind the scenes. For the listing LTV model, we used existing Zipline features and also added a handful of our own. In sum, there were over 150 features in our model, including: With our features and outcome variable defined, we can now train a model to learn from our historical data. As in the example training dataset above, we often need to perform additional data processing before we can fit a model: In this step, we don\u2019t quite know what is the best set of features to use, so writing code that allows us to rapidly iterate is essential. The pipeline construct, commonly available in open-source tools like Scikit-Learn and Spark, is a very convenient tool for prototyping. Pipelines allow data scientists to specify high-level blueprints that describe how features should be transformed, and which models to train. To make it more concrete, below is a code snippet from our LTV model pipeline: At a high level, we use pipelines to specify data transformations for different types of features, depending on whether those features are of type binary, categorical, or numeric. FeatureUnion at the end simply combines the features column-wise to create the final training dataset. The advantage of writing prototypes with pipelines is that it abstracts away tedious data transformations using data transforms. Collectively, these transforms ensure that data will be transformed consistently across training and scoring, which solves a common problem of data transformation inconsistency when translating a prototype into production. Furthermore, pipelines also separates data transformations from model fitting. While not shown in the code above, data scientists can add a final step to specify an estimator for model fitting. By exploring different estimators, data scientists can perform model selection to pick the best model to improve the model\u2019s out of sample error. As mentioned in the previous section, we need to decide which candidate model is the best to put into production. To make such a decision, we need to weigh the tradeoffs between model interpretability and model complexity. For example, a sparse linear model might be very interpretable but not complex enough to generalize well. A tree based model might be flexible enough to capture non-linear patterns but not very interpretable. This is known as the Bias-Variance tradeoff. In applications such as insurance or credit screening, a model needs to be interpretable because it\u2019s important for the model to avoid inadvertently discriminating against certain customers. In applications such as image classification, however, it is much more important to have a performant classifier than an interpretable model. Given that model selection can be quite time consuming, we experimented with using various AutoML tools to speed up the process. By exploring a wide variety of models, we found which types of models tended to perform best. For example, we learned that eXtreme gradient boosted trees (XGBoost) significantly outperformed benchmark models such as mean response models, ridge regression models, and single decision trees. Given that our primary goal was to predict listing values, we felt comfortable productionizing our final model using XGBoost, which favors flexibility over interpretability. As we alluded to earlier, building a production pipeline is quite different from building a prototype on a local laptop. For example, how can we perform periodic re-training? How do we score a large number of examples efficiently? How do we build a pipeline to monitor model performance over time? At Airbnb, we built a framework called ML Automator that automagically translates a Jupyter notebook into an Airflow machine learning pipeline. This framework is designed specifically for data scientists who are already familiar with writing prototypes in Python, and want to take their model to production with limited experience in data engineering. Here is a code snippet demonstrating how the fit and transform functions are defined in our LTV model. The fit function tells the framework that a XGBoost model will be trained, and that data transformations will be carried out according to the pipeline we defined previously. Once the notebook is merged, ML Automator will wrap the trained model inside a Python UDF and create an Airflow pipeline like the one below. Data engineering tasks such as data serialization, scheduling of periodic re-training, and distributed scoring are all encapsulated as a part of this daily batch job. As a result, this framework significantly lowers the cost of model development for data scientists, as if there was a dedicated data engineer working alongside the data scientists to take the model into production! Note: Beyond productionization, there are other topics, such as tracking model performance over time or leveraging elastic compute environment for modeling, which we will not cover in this post. Rest assured, these are all active areas under development. In the past few months, data scientists have partnered very closely with ML Infra, and many great patterns and ideas arose out of this collaboration. In fact, we believe that these tools will unlock a new paradigm for how to develop machine learning models at Airbnb. We are very excited about the future of this framework and the new paradigm it brought along. By bridging the gap between prototyping and productionization, we can truly enable data scientists and engineers to pursue end-to-end machine learning projects and make our product better. Want to use or build these ML tools? We\u2019re always looking for talented people to join our Data Science and Analytics team! Special thanks to members of Data Science & ML Infra team who were involved in this work: Aaron Keys, Brad Hunter, Hamel Husain, Jiaying Shi, Krishna Puttaswamy, Michael Musson, Nick Handel, Varant Zanoyan, Vaughn Quoss et al. Additional thanks to Gary Tang, Jason Goodman, Jeff Feng, Lindsay Pettingill for reviewing this blog post.", "links": ["https://medium.com/", "https://medium.com/airbnb-engineering?source=logo-lo_c031a43706d3---53c7c27702d5", "https://twitter.com/AirbnbEng", "https://medium.com/m/signin?redirect=https%3A%2F%2Fmedium.com%2Fairbnb-engineering%2Fusing-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d", "https://medium.com/airbnb-engineering", "https://medium.com/airbnb-engineering/ai/home", "https://medium.com/airbnb-engineering/airbnb-engineering-backend/home", "https://medium.com/airbnb-engineering/data/home", "https://medium.com/airbnb-engineering/airbnb-engineering-infrastructure/home", "https://medium.com/airbnb-engineering/tagged/mobile", "https://medium.com/airbnb-engineering/web/home", "http://airbnb.io/projects/", "https://medium.com/airbnb-engineering/search", "https://medium.com/@rchang?source=post_header_lockup", "https://medium.com/@rchang?source=post_header_lockup", "https://twitter.com/_rchang", "https://medium.com/swlh/diligence-at-social-capital-part-3-cohorts-and-revenue-ltv-ab65a07464e1", "http://scikit-learn.org/stable/", "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html", "https://www.kaggle.com/general/16927", "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html", "https://spark.apache.org/docs/latest/ml-pipeline.html", "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html", "http://scikit-learn.org/stable/data_transforms.html", "http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html", "https://medium.com/airbnb-engineering/automated-machine-learning-a-paradigm-shift-that-accelerates-data-scientist-productivity-airbnb-f1f8a10d61f8", "http://scott.fortmann-roe.com/docs/BiasVariance.html", "https://medium.com/airbnb-engineering/automated-machine-learning-a-paradigm-shift-that-accelerates-data-scientist-productivity-airbnb-f1f8a10d61f8", "https://github.com/dmlc/xgboost", "https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8", "http://www.florianwilhelm.info/2016/10/python_udf_in_hive/", "https://airflow.incubator.apache.org/", "https://www.airbnb.com/careers/departments/data-science-analytics", "https://www.linkedin.com/in/aaronkeys/", "https://www.linkedin.com/in/brad-hunter-497621a/", "https://www.linkedin.com/in/hamelhusain/", "https://www.linkedin.com/in/jiaying-shi-a2142733/", "https://www.linkedin.com/in/krishnaputtaswamy/", "https://www.linkedin.com/in/michael-m-a37b1932/", "https://www.linkedin.com/in/nicholashandel/", "https://www.linkedin.com/in/vzanoyan/", "https://www.linkedin.com/in/vquoss/", "https://www.linkedin.com/in/thegarytang/", "https://medium.com/@jasonkgoodman", "https://twitter.com/jtfeng", "https://medium.com/@lpettingill", "https://medium.com/@jtfeng?source=post_page", "https://medium.com/@jasonkgoodman?source=post_page", "https://medium.com/@gary.tang_94319?source=post_page", "https://medium.com/@lpettingill?source=post_page", "https://medium.com/@vquoss?source=post_page", "https://medium.com/@eddie.santos.3?source=post_page", "https://medium.com/@NicholasHandel?source=post_page", "https://medium.com/tag/machine-learning?source=post", "https://medium.com/tag/data-science?source=post", "https://medium.com/tag/airbnb?source=post", "https://medium.com/tag/technology?source=post", "https://medium.com/@rchang?source=footer_card", "https://medium.com/@rchang", "http://twitter.com/Airbnb", "http://twitter.com/Twitter", "https://medium.com/airbnb-engineering?source=footer_card", "https://medium.com/airbnb-engineering?source=footer_card", "http://airbnb.io", "https://medium.com/airbnb-engineering", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "60": {"url": "https://www.ayasdi.com/blog/artificial-intelligence/why_ai_is_the_new_bigdata/", "title": "The \u201cBig Data-ization\u201d of Artificial Intelligence", "text": "\u2039 Back to Blog  Artificial Intelligence, Data, Machine Intelligence, Machine Learning It seems like it was only a few years ago that the term \u201cbig data\u201d went from a promising area of research and interest to something so ubiquitous that it lost all meaning, descending ultimately into the butt of jokes.  As everyone piled onto the big data bandwagon, it became impossible to separate truth from fiction. Every executive and entrepreneur that I ran into was doing a \u201cbig data\u201d thing. I recall meeting someone whose company shot videos for enterprise customers and was pitching it as a \u2018Big Data play\u2019 \u2013 because video files, you know, are huge \u2013 they take up lots of space.  Thankfully, the noise associated with \u201cbig data\u201d is abating as sophistication and common sense take hold. In fact, in many circles the term actually exposes the user as someone who doesn\u2019t really understand the space.  Unfortunately, the same malady has now afflicted AI. Everyone I meet is doing an \u2018AI play\u2019 \u2013 even if all they did was to build a simple linear regressor in Excel.  AI is, unfortunately, the new \u201cbig data.\u201d While not good, it is not all bad either.  After all, the data ecosystem benefited from all of the \u201cbig data\u201d attention and investment \u2013 creating some amazing software and producing some exceptional productivity gains.  The same will happen with AI \u2013 with the increased attention comes investment dollars which in turn will drive adoption \u2013 enhancing the ecosystem. Having said that we need to stop calling regression on excel AI \u2013 it\u2019s ridiculous and undermines some incredible work being done in the space.  Regression has been around for 200+ years. Gauss and Legendre didn\u2019t don the AI cloak when they discovered regression. Neither should the recent stats graduate trying to raise money for his/her startup.  I will tell you what I think qualifies as AI in a moment \u2013 but here are some thoughts that influence that discussion. First, the AI definition I use is focused on the narrow, application specific AI, not the more general problem of artificial general intelligence (AGI) where simulating a person using software is the equivalent of intelligence.  Second, the vast, vast majority of the data that exists in the world is unlabeled. It is not practical to label that data manually and doing so would likely create bias anyway. One can argue that the Internet as a whole is simply a mechanism for humans to entertain ourselves while providing label data for machines.  Unlabeled data presents a different challenge, one we will address shortly, but the key point here is that it is everywhere and represents the key to extracting business value (or any value).  Third, we are not producing data scientists at a rate that can keep pace with the growth of data. Even with the moniker as a the \u201csexiest job of the 21st century\u201d the pace at which data scientists are created doesn\u2019t begin to approach the growth rate we are seeing in data.  Fourth, data scientists, for the most part, are not UX designers or product managers or, in many cases even engineers. As a result, the subject matter experts, those that sit in the business, don\u2019t have effective interfaces to the data science outputs. The interfaces that they have \u2013 powerpoint, excel, or PDF reports have limited utility in transforming the behavior of a company. What is required is something to shape behavior is something more \u2013 applications.  So what does qualify as intelligence? In \u2018On Intelligence\u2019 Jeff Hawkins says that all AI boils down to memory and prediction. The argument is very persuasive, but a bit reductive. We have a slightly different take for what an AI should display and it encompasses a framework: Here are characteristics that I think any AI should display. While some of these elements may seem self-evident that is because they are taken as a single item. Intelligence has a broader context. All the elements must work in conjunction with each other to qualify as AI. \u00a0\u00a0\u00a0 The five elements are: Let\u2019s take each of these concepts in turn. Discovery is the ability of an intelligent system to learn from data without upfront human intervention. Often, this needs to be done without being presented with an explicit target. It relies on the use of unsupervised and semi-supervised machine learning techniques (such as segmentation, dimensionality reduction, anomaly detection, etc.), as well as more supervised techniques where there is an outcome or there are several outcomes of interest. Usually, in enterprise software, the term discovery refers to the ability of ETL/MDM solutions to discover the various schemas of tables in large databases and automatically find join keys etc. This is not what we mean by discovery. We use of the term very differently and has this has important implications. In complex datasets, it is nearly impossible to ask the \u201cright\u201d questions. To discover what value lies within the data one must \u00a0understand all the relationships that are inherent and important in the data. That requires a principled approach to hypothesis generation. \u00a0 One technique, topological data analysis (TDA), is exceptional at surfacing hidden relationships that exist in the data and identifying those relationships that are meaningful without having to ask specific questions of the data. The result is an output that is able to represent complex phenomena, and is therefore able to surface weaker signals as well as the stronger signals. \u00a0 This permits the detection of emergent phenomena. \u00a0 As a result, enterprises can now discover answers to questions they didn\u2019t even know to ask and do so with data that is unlabeled. Once the data set is understood through intelligent discovery, supervised approaches are applied to predict what will happen in the future. These types of problems include classification, regression and ranking.  For this pillar, most companies use a standard set of supervised machine learning algorithms including random forests, gradient boosting, linear/sparse learners. It should be noted, however, that the unsupervised work from the previous step is highly useful in many ways. For example, it can generate relevant features for use in prediction tasks or finding local patches of data where supervised algorithms may struggle (systematic errors). The predict phase is an important part of the business value associated with data science, however, generally, in predictive analytics, there exists a notion that this is the sum total of machine learning.  This is not the case by far.  Prediction, while important, is pretty well understood and does not, on its own qualify as \u201cintelligence.\u201d \u00a0It goes back to calling a maxed out Excel table and a linear regressor AI. It is just not the case. Further, Prediction can go wrong along a number of dimensions, particularly if the groups on which you are predicting are racked with some type of bias (algorithmic, sampling etc.)  Again, Prediction is key and provides tremendous business value done correctly, but in and of itself it is not AI. We need to stop calling it as such. Applications need to support interaction with humans in a way which makes outcomes recognizable and believable. For example, when one builds a predictive model, it is important to have an explanation of how the model is doing what it is doing, i.e. what the features in the model are doing in terms that are familiar to the users of the model. \u00a0This level of familiarity is important in generating trust and intuition.  Similarly, in the same way that automobiles have mechanisms not just for detecting the presence of a malfunction, but also for specifying the nature of the malfunction and suggesting a method for correcting it, so one needs to have a \u201cnuts and bolts\u201d understanding of how an application is working in order to \u201crepair\u201d it when it goes awry. \u00a0 Transparency AND Justification. There is a difference. Transparency tells you what algorithms and parameters were used, while, Justification tells you why. For intelligence to be meaningful, it must be able to justify and explain its assertions, as well as to be able to diagnose failures.  No business leader should deploy intelligent and autonomous applications against critical business problems without a thorough understanding of what variables power the model.  Enterprises cannot move to a model of intelligent applications without trust and transparency. \u00a0 AI without UX is of limited utility.  UX is what distributes that intelligence across the organization and pushes it to the edge \u2013 where it can consumed by practitioners and subject matter experts.  Ultimately, the process of operationalizing an intelligent application within the enterprise requires some change in the organization, an acceptance that the application will evolve over time and that will demand downstream changes \u2013 automated or otherwise.  \u00a0For this to happen, intelligent applications need to be \u201clive\u201d in the business process, seeing new data and automatically executing the loop of Discover, Predict, Justify on a frequency that makes sense for that business process. For some processes that may be quarterly, for others daily. That loop can even be measured in seconds. \u00a0 Intelligent systems are designed to detect and react as the data evolves. An intelligent system is one that is always learning, live in the workflow and constantly improving. \u00a0In the modern data world, an application that is not getting more intelligent is getting dumber.  Intelligent applications are designed to detect and react when data distributions evolve. As a result, they need to be \u201con the wire\u201d in order to detect that phenomena before it becomes a problem. Too many solutions provide an answer in a point of time, an intelligent system is one that is always learning through the framework outlined here. This is what defines intelligence \u2013 not a machine learning algorithm kicking out predictions or the results of a data scientist\u2019s work as a PDF. For the industry to continue to grow and evolve we need to start doing a better job of recognizing what is truly AI and what is imposter-ware.  Let\u2019s make it a point of celebrating the real AI work.", "links": ["https://www.ayasdi.com", "https://www.ayasdi.com/platform/", "https://www.ayasdi.com/industry-overview/", "https://www.ayasdi.com/financial-services/", "https://www.ayasdi.com/healthcare/", "https://www.ayasdi.com/public-sector/", "https://www.ayasdi.com/solutions/anti-money-laundering/", "https://www.ayasdi.com/solutions/clinical-variation-management/", "https://www.ayasdi.com/solutions/denials/", "https://www.ayasdi.com/solutions/regulatory-risk/", "https://www.ayasdi.com/solutions/population-health/", "https://www.ayasdi.com/company/", "https://www.ayasdi.com/company/leadership/", "https://www.ayasdi.com/company/careers/", "https://www.ayasdi.com/company/collaborators/", "https://www.ayasdi.com/company/customers/", "https://www.ayasdi.com/company/news-and-events/", "https://www.ayasdi.com/company/contact-us/", "https://www.ayasdi.com/blog/", "https://www.ayasdi.com/request-a-demo/", "https://www.ayasdi.com/blog/author/gurjeet-singh/", "https://www.ayasdi.com/blog/category/artificial-intelligence/", "https://www.ayasdi.com/blog/category/bigdata/", "https://www.ayasdi.com/blog/category/machine-intelligence/", "https://www.ayasdi.com/blog/category/machine-learning/", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/08/02144138/BG2.png", "https://priceonomics.com/the-discovery-of-statistical-regression/", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/05/12132340/Machine_Intelligence_Apps_WP_051617v01.pdf", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/08/02144141/DS1.png", "https://www.wired.com/brandlab/2015/05/jeff-hawkins-firing-silicon-brain/", "http://cdn.ayasdi.com/wp-content/uploads/2015/02/wp-tda-and-machine-learning.pdf", "https://www.ayasdi.com/blog/aml/intelligent-segmentation-as-the-attack-point-for-aml/", "http://feedburner.google.com/fb/a/mailverify?uri=AyasdiBlog", "https://ayasdicommunity.force.com/login", "https://www.ayasdi.com/terms-of-service/", "https://www.ayasdi.com/privacy-policy/", "https://www.ayasdi.com/pilot-terms/", "http://www.youtube.com/user/ayasdi", "http://feeds.feedburner.com/AyasdiBlog", "http://www.linkedin.com/company/ayasdi", "https://www.facebook.com/ayasdi", "https://twitter.com/ayasdi"]}, "61": {"url": "https://www.mapd.com/blog/2017/08/03/whos-got-the-best-mobile-network-in-the-us/", "title": "", "text": "Based on the deluge of commercials and ads in this competitive industry, it seems every major player claims that they do. The competition is apparently so fierce that even their spokesmen are being headhunted. Admittedly, it can be a bit confusing when you find these results in one city:  And these in another:  By the way, how do you even define \u2018best\u2019 network? It\u2019s a loaded question for sure. We\u2019ll get back to that in a minute. It\u2019s a question with relevance to nearly every American, considering the vast majority of us own a cellphone of some kind (over 95% according to Pew). And we don\u2019t leave them alone; we touch them thousands of times each day. In fact, a good mobile network can affect our very quality of life. Ever travelled any significant distance with children and poor mobile data speeds? The same question is also relevant to those very telecommunications providers themselves, of course, and particularly in light of the fact that obtaining new subscribers will increasingly mean winning them away from competitors. Ensuring you are outpacing your rivals with superior network metrics is a key ingredient to both marketing and market share success. Our friends and partners at Tutela have created the world\u2019s largest mobile information platform, collecting billions of crowdsourced data points anonymously. These span device, network, and application information to allow us to get from data to insights. They\u2019ve graciously given us (and now you) a slice of their treasure trove. Our demo contains a few months of US data (~112M rows), although Tutela has data for more metrics, for nearly every country, and for a much greater timeframe. There are obviously many variables to consider when it comes to a great network, and one carrier will almost certainly not take the top spot in each category. But we can identify areas of significance where we hope to find favorable results: Spoiler alert: we\u2019re not actually going to attempt to find a winner. Those are just some of the metrics you can see at a glance on our MapD Immerse demo dashboard:  This is immediately useful. Coverage, speed, signal strength, and latency all matter, so MapD allows you to visualize and correlate them for effective consumption. Not coincidentally, the MapD Immerse visualization tool is purpose-built to leverage the MapD Core backend GPU-powered, in-memory, columnar SQL database at scale. This allows  interactive analysis over multi-billion row data sets, with the ability to drill down to an individual row...in milliseconds. It\u2019s also why Tutela has selected to partner with MapD; no need for indexing, sampling, or pre-computation. This means you can zoom in and out, filter, and more in an ad hoc fashion. Let\u2019s take it for a spin. As a consumer, you might be interested in how well each service provider performs in a certain geographic region (i.e., where you live and work) to aid in making a purchasing decision.  For example, to zoom from country level to the Washington, DC area, just type the city name into the searchbox of the pointmap. MapD Immerse will automatically zoom in when you click enter. It looks like in this region, you might lean toward Verizon:  But it may not be as clear cut if you live in San Francisco:  In either case, Sprint better have some compelling discounts or incentives to entice new subscribers. Further, both Sprint and T-Mobile might leverage this information to prioritize buildout or upgrade of cell towers. If they\u2019re analyzing subscriber loss in these areas, this also may give them insight into why that could be occurring. Another valuable feature of the MapD platform is the ability to easily identify trends, anomalies, or outliers. Visualization can often be an invaluable enabler for these situations. Sometimes new revenue opportunities or cost savings can be had by capitalizing on correlations in data that you didn\u2019t even realize existed! See the spike in data downloads in the line chart? By highlighting the time frame with your mouse, MapD will cross filter all charts using the selected time slice. Doing the same with the line chart for data uploads further reveals that the largest spikes occur on May 28 and 30. Ah, that\u2019s Memorial Day weekend in the United States. So this makes sense.  We can get even more granular and note that the bulk of data transfer occurred between 12-1am local time (4-5am UTC) as the heatmap indicates. I wonder if Uber and Lyft also experienced a spike around that time, as celebrations came to an end?  When data exploration at scale is so easy, what other interesting insights will you find? Explore this demo using Tutela's dataset on MapD\u2019s platform on your own here.  If you\u2019d like to learn more about MapD\u2019s recently announced partnership with Tutela, watch our on-demand webinar, introducing and demonstrating Tutela Explorer, a new mobile data analytics solution that provides real-time, interactive and highly visual insight into the performance of mobile networks and device usage. Tutela Explorer is available now as-a-service with global data coverage from Tutela.", "links": ["http://www.mapd.com/blog", "https://www.nytimes.com/2016/10/15/business/sprint-verizon-hear-me-now-paul-marcarelli.html", "http://www.pewinternet.org/fact-sheet/mobile/", "http://www.networkworld.com/article/3092446/smartphones/we-touch-our-phones-2617-times-a-day-says-study.html", "https://www.tutela.com/explorer/", "https://www.mapd.com/demos/telecom", "https://www.mapd.com/demos/telecom", "https://www.mapd.com/platform/core/", "https://www.mapd.com/demos/telecom", "https://www.brighttalk.com/webcast/14525/267851", "https://www.tutela.com/explorer"]}, "62": {"url": "https://www.oreilly.com/learning/building-a-simple-graphql-server-with-neo4j", "title": "Building a simple GraphQL server with Neo4j", "text": "How to implement a GraphQL API that queries Neo4j for a simple movie app. If you're interested in making the move from REST to GraphQL, check out this OSCON video from the team at Github. GraphQL is a powerful new tool for building APIs that allows clients to ask for only the data they need. Originally designed at Facebook to minimize data sent over the wire and reduce round-trip API requests for rendering views in native mobile apps, GraphQL has since been open sourced to a healthy community that is building developer tools. There are also a number of large companies and startups such as GitHub, Yelp, Coursera, Shopify, and Mattermark building public and internal GraphQL APIs. Despite what the name seems to imply, GraphQL is not a query language for graph databases, it is instead an API query language and runtime for building APIs. The \u201cGraph\u201d component of the name comes from the graph data model that GraphQL uses in the frontend. GraphQL itself is simply a specification, and there are many great tools available for building GraphQL APIs in almost every language. In this post we'll make use of graphql-tools by Apollo to build a simple GraphQL API in JavaScript that queries a Neo4j graph database for movies and movie recommendations. We will follow a recipe approach: first, exploring the problem in more detail, then developing our solution, and finally we discuss our approach. Good resources for learning more about GraphQL are GraphQL.org and the Apollo Dev Blog. GraphQL by design can work with any database or backend system, however in this example we'll be using the Neo4j graph database as our persistence layer. Why use a graph database with GraphQL? The idea of application data as a graph is an underpinning design choice of GraphQL. For example, think of customers who have placed orders that contain products\u2014that's a graph! GraphQL enables developers to translate their backend data into the application data graph on the frontend, but if we use a graph database on the backend we can do away with this impedance mismatch and we have graphs all the way down. We'd like to build a simple GraphQL API for a movie app that can do two things: In a traditional REST-ish API approach we might create two separate endpoints, perhaps /movies/search and /movies/similar. One endpoint to search for movies by substring of the title, and another to return a list of similar movies. As we add additional features we might keep adding endpoints. With GraphQL, our API is served from a single endpoint /graphql that takes one or more GraphQL queries and returns JSON data in a shape that is specified by the GraphQL query - only the data requested by the client is returned. Let's see how we can build this simple API. We'll be building a simple node.js JavaScript web server using Express.js to serve our GraphQL endpoint1. First of all we\u2019ll need a Neo4j database (with data) for our GraphQL server to query. For this example we\u2019ll make use of a Neo4j Sandbox instance. Neo4j Sandbox allows us to quickly spin up a hosted Neo4j instance, optionally with existing datasets focused around specific use cases. We\u2019ll use the Recommendations Neo4j Sandbox which includes data about movies and movie reviews and is designed to be used for generating personalized recommendations (for example, by using collaborative filtering to recommend movies based on similar users\u2019 reviews). We'll be making use of a few dependencies for this GraphQL server. I won't list them all here (you can see the full list in the package.json for this project, but there a few worth noting: We\u2019ll follow the \u201cGraphQL First\u201d development paradigm. In this approach, we start by defining a GraphQL schema. This schema defines the types and queries available in our API and then becomes the specification for the API. If we were building a complete application, the frontend developers could use this schema to build out the frontend while the backend team builds the backend in parallel, speeding development. Once we\u2019ve defined our schema we\u2019ll need to create resolver functions that are responsible for fetching data from Neo4j. This schema will define the types and GraphQL queries that we'll be able to use in our API. You can think of the schema as the API blueprint. schema.js Each GraphQL field is resolved by a resolver function. The resolver function defines how data is fetched for that field. resolvers.js Now that we've defined the GraphQL schema and resolver functions we are ready to serve the GraphQL endpoint, using Express.js. server.js If you've ever used Express this should look familiar, you'll notice that we're creating two endpoints and serving them on localhost:8080 Let's take a look at what we just did: Now that we have our API running, let's use the GraphiQL in-browser IDE for GraphQL to search for movies whose titles contain \"Matrix\" and find similar movies for each matching movie. We can load GraphiQL by opening http://loalhost:8080/graphiql in our browser and querying with this GraphQL query:   Querying our GraphQL movies API using GraphiQL, the in-browser IDE for GraphQL GraphQL is still new to almost everyone with the exception of Facebook, where it has been used since 2012, so many conventions and best practices are still being developed. However, GraphQL First Development is a philosophy the community has adopted which gives structure to the process of building a GraphQL API. By defining a contract for the API, expressed as a GraphQL schema, the frontend and backend developer teams can independently implement their applications, using the schema as a guide. To make GraphQL First Development even easier with Neo4j, the Neo4j team has built neo4j-graphql-cli, a command line tool for easily spinning up a Neo4j backed GraphQL endpoint based on a user defined GraphQL schema that can optionally be annotated with Cypher queries - exposing the power of Cypher from within GraphQL. You can learn more about this here. 1 All code for this project is available on GitHub . This example is also available as an Apollo Launchpad Pad, which you can run live in the browser.\u21a9 William Lyon is an engineer on the Developer Relations team at Neo4j, the open source graph database, where he builds tools for integrating Neo4j with other technologies and helps users be successful with graphs. He also leads Neo4j's Data Journalism Accelerator Program. Prior to Neo4j, he worked as a software engineer for a variety of startups, building mobile apps for the real estate industry, trading tools for quantitative finance, and predictive APIs. William holds a masters degree in Computer Science from the University of Montana. You can... How to package up your own repositories for use via PHP's Composer. Will content-blocking change the Web? Learn how to use Node.js and Express.js to quickly bootstrap a web server, tie in MongoDB, and display it all using dynamic HTML templates Diversity and inclusion in the tech industry has experienced a severe failure. Erica Baker approaches the topic like we would for any other service failure: with a postmortem. \u00a9 2017 O'Reilly Media, Inc. All trademarks and registered trademarks appearing on oreilly.com are the property of their respective owners.", "links": ["https://www.safaribooksonline.com/home/?utm_medium=content&utm_source=oreilly.com&utm_campaign=lgen&utm_content=20170601+nav", "http://www.oreilly.com/conferences/", "http://shop.oreilly.com/", "https://www.safaribooksonline.com/home/?utm_medium=content&utm_source=oreilly.com&utm_campaign=lgen&utm_content=20170601+nav", "http://www.oreilly.com/conferences/", "http://shop.oreilly.com/", "https://pixabay.com/en/louvre-pyramid-mesh-perspective-2189967/", "https://www.safaribooksonline.com/library/view/oscon-2017-/9781491976227/video306664.html?utm_source=oreilly&utm_medium=newsite&utm_campaign=building-a-simple-graphql-server-with-neo4j", "https://github.com/apollographql/graphql-tools", "http://www.apollodata.com/", "http://graphql.org/", "https://dev-blog.apollodata.com/", "https://neo4j.com/sandbox-v2/", "https://github.com/johnymontana/movies-graphql-neo4j-server/blob/master/package.json", "http://dev.apollodata.com/tools/graphql-tools/index.html", "https://expressjs.com/", "https://github.com/neo4j/neo4j-javascript-driver", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://www.graph.cool/docs/faq/graphql-sdl-schema-definition-language-kr84dktnp0/", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "http://loalhost:8080/graphiql", "https://www.npmjs.com/package/neo4j-graphql-cli", "http://neo4j.com/developer/graphql", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://pixabay.com/en/louvre-pyramid-mesh-perspective-2189967/", "https://twitter.com/share", "http://oreilly.com/about/", "http://oreilly.com/work-with-us.html", "http://oreilly.com/careers/", "http://shop.oreilly.com/category/customer-service.do", "http://shop.oreilly.com/category/customer-service.do", "http://fb.co/OReilly", "http://twitter.com/oreillymedia", "https://www.youtube.com/user/OreillyMedia", "https://plus.google.com/+oreillymedia", "https://www.linkedin.com/company/oreilly-media", "http://oreilly.com/terms/", "http://oreilly.com/privacy.html", "http://www.oreilly.com/about/editorial_independence.html"]}, "63": {"url": "http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "title": "", "text": "", "links": ["http://www.getrevue.co/?ref=Revue+Profile", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profileimage&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profiledate&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profilename&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lXPZA?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lXPZA?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blogs.technet.microsoft.com/machinelearning/2017/07/25/lessons-learned-benchmarking-fast-machine-learning-algorithms/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/DM9WJ?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/DM9WJ?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://medium.com/@stathis/design-by-evolution-393e41863f98?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/XYWKo?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/XYWKo?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.datasciencelearner.com/complete-overview-learning-python-data-analysis/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/6nba0?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/6nba0?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://reinforce.io/blog/introduction-to-tensorforce/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xekd3?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xekd3?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.kdnuggets.com/2017/05/top-10-machine-learning-videos-on-youtube-updated.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Ka8JK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Ka8JK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://arxiv.org/abs/1704.01568?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/96KED?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/96KED?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://datascienceplus.com/random-forests-in-r/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Vd1wK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Vd1wK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blog.keras.io/the-future-of-deep-learning.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/eKr5e?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/eKr5e?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://medium.com/airbnb-engineering/using-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/odRNK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/odRNK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://machinelearning.apple.com/2017/07/07/GAN.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xekoa?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xekoa?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://arxiv.org/abs/1707.06642?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/mxRw2?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/mxRw2?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.techleer.com/articles/200-naive-bayes-machine-learning-algorithm-for-classification-problems/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/3K1nx?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/3K1nx?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463?gi=7fe02170b69e&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/OnXK7?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/OnXK7?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://aiukraine.com/en/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/nqRVq?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/nqRVq?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.bonaccorso.eu/2017/07/23/machine-learning-algorithms/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Z4bBj?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Z4bBj?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/jobs", "http://bit.ly/2uqHV4J", "https://flyelephant.net/jobs/Spinbackup-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/jobs/Spinbackup-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/yJRmd?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/yJRmd?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/jobs/Snap-Ukraine-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/jobs/Snap-Ukraine-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/jAR3B?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/jAR3B?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%238%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "https://www.getrevue.co/?utm_source=Data Science Digest&utm_medium=email&utm_content=footerlink&utm_campaign=Issue", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=forwarded&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%238%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "http://www.getrevue.co/?ref=Revue+Profile"]}, "64": {"url": "https://blog.insightdatascience.com/preparing-for-the-transition-to-applied-ai-d41e48403447", "title": "Transitioning from Software Engineering to Artificial Intelligence", "text": "Emmanuel Ameisen and Jeremy Karnowski, Insight AI A significant part of the Software Engineer role requires staying up-to-date with evolving frameworks, standards, and paradigms. Software Engineers strive to constantly learn, in order to always use the best tool for the job. As Machine Learning finds footholds in more applications every day, it has naturally become a topic that many Engineers want to master. Machine Learning, though, is harder to pick up than a new framework. To be an efficient practitioner, you require a solid understanding of the theory of the field, broad knowledge of the current state of the art, and an ability to frame problems in a non deterministic way. Many guides you can find online will simply teach you how to train an out-of-the-box model on a curated data set to achieve good accuracy and call it a day. The truth is that a much more extensive skillset is essential in becoming an effective Machine Learning Engineer. Below is a distillation of the many conversations we\u2019ve had with over 50 top Machine Learning teams all over The Bay Area and New York, who\u2019ve come to Insight to find AI Practitioners poised to tackle their problems and accelerate their expansion into Applied AI. Deploying a Machine Learning solution requires much more than just training an arbitrary model on your data. It requires an understanding of: In other words, in addition to engineering chops, you need to understand the fundamentals of statistics, linear algebra, and optimization theory in order to integrate, deploy, and debug models. Building a custom Machine Learning solution for a problem requires that you consider issues ranging from acquiring, labeling and pre-processing your data to building, updating, and serving an inference model, and everything in between. Finally, building a REST API for a standard web-app is a task that we can deem feasible ahead of time. Machine Learning models, on the other hand are not always guaranteed to converge, or produce usable outputs. The best way to learn how to scope and deliver impactful Machine Learning products, is to understand how their theoretical underpinnings relate to the taxonomy of your data. In order to understand Machine Learning, a solid knowledge of statistics fundamentals is essential. This involves understanding the following: When you are training a neural network, what is actually happening? What makes some tasks doable and others not? A good approach to this might be to first try to understand Machine Learning through graphics and examples, before diving deeper into the theory. Concepts to understand range from how different loss functions work, why back propagation is useful, or what a computational graph is. A deep understanding is crucial both for building a functional model, and to communicate about it efficiently to the rest of the organization. Following are a few resources, starting with high level overviews, and diving deeper. Another fundamental skill is the ability to read, understand and implement research papers. It can seem like a daunting task at first, so a good way to start is to look up a paper that already has code attached to it (on GitXiv for example) and try to understand the implementation in depth. Ask any Data Scientist and they\u2019ll tell you 90% of the work they do is data munging. This is just as important for Applied AI, as the success of your model correlates hugely with the quality (and quantity) of your data. Data work comes in many aspects, and falls within a few categories: The best way to get familiar with data wrangling is to grab a dataset in the wild and try to use it. There are many datasets online and many social media and news outlets sites have great APIs. Following the steps above, a good way to learn is to: Debugging Machine Learning algorithms that fail to converge or to give sensible results involves a very different process from debugging code. In the same vein, finding the right architecture and hyperparameters requires solid theoretical fundamentals, but also good infrastructure work to be able to test different configurations out. Because of the pace at which the fields evolve, the methods to debug models are constantly evolving. Here are a few \u201csanity checks\u201d from our discussions and experience deploying models that mirror in some ways the principles of KISS familiar to many Software Engineers. A lot of those steps can be accelerated significantly by your development skills, which brings us to our last skill. A lot of Applied Machine Learning will allow you to leverage Software Engineering skills, sometimes with a little twist. These skills include: For more details on some of the software skills we recommend acquiring to become a quality Machine Learning Engineer, check out our post dedicated to transitioning to Applied AI from Academia. The resources above will help you approach and tackle actual Machine Learning problems. But the field of Applied AI changes extremely quickly, and the best way to learn, is to get your hands dirty and actually try to build out an end-to-end solution to solve a real problem. Action Items: Find some inspiration, then dive in! Remember that while Machine Learning Engineering is about building products at heart, there is a research aspect to it. You will explore models and paradigms that will prove unsuccessful, and that is perfectly fine, as it will lead you to understand the intricacies of the problem better. AI is an exciting, ever-changing field. The demand for Machine Learning Engineers is strong, and it is easy to get overwhelmed with the amount of news surrounding the topic. We recommend following a few serious sources and newsletters, to be able to separate PR and abstract research from innovations that are immediately relevant to the field. Here are some sources to help out: Want to learn applied Artificial Intelligence from top professionals in Silicon Valley or New York? Learn more about the Artificial Intelligence program.", "links": ["https://blog.insightdatascience.com?source=logo-lo_1a89036ecf7f---d02e65779d7b", "https://twitter.com/InsightDataSci", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.insightdatascience.com%2Fpreparing-for-the-transition-to-applied-ai-d41e48403447", "https://blog.insightdatascience.com", "https://blog.insightdatascience.com/tagged/about-insight", "https://blog.insightdatascience.com/tagged/insight-data-science", "https://blog.insightdatascience.com/tagged/insight-data-engineering", "https://blog.insightdatascience.com/tagged/insight-health-data", "https://blog.insightdatascience.com/tagged/insight-ai", "https://blog.insightdatascience.com/search", "https://blog.insightdatascience.com/@emmanuelameisen?source=post_header_lockup", "https://blog.insightdatascience.com/@emmanuelameisen?source=post_header_lockup", "https://www.udacity.com/course/deep-learning--ud730", "http://cs231n.stanford.edu/", "http://cs224d.stanford.edu/", "http://www.deeplearningbook.org/", "http://www.gitxiv.com/", "https://data.sfgov.org/", "https://www.data.gov/", "https://dev.twitter.com/streaming/public", "http://developer.nytimes.com/docs", "http://people.apache.org/~fhanik/kiss.html", "http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf", "https://github.com/JasperSnoek/spearmint", "https://blog.insightdatascience.com/preparing-for-the-transition-to-applied-ai-8eaf53624079", "https://news.ycombinator.com/", "https://jack-clark.net/import-ai/", "http://www.insightdatascience.com/blog/", "https://blog.insightdatascience.com/tagged/insight-ai", "https://blog.insightdatascience.com/using-deep-learning-to-reconstruct-high-resolution-audio-29deee8b7ccd", "https://blog.insightdatascience.com/separating-overlapping-chromosomes-with-deep-learning-based-image-segmentation-22f97afd3283", "http://insightdata.ai/notifications-list", "http:insightdata.ai", "https://medium.com/@jkarnows?source=post_page", "https://medium.com/@InsightData?source=post_page", "https://medium.com/@rossfadely?source=post_page", "https://blog.insightdatascience.com/tagged/machine-learning?source=post", "https://blog.insightdatascience.com/tagged/ai?source=post", "https://blog.insightdatascience.com/tagged/software-engineering?source=post", "https://blog.insightdatascience.com/tagged/insight-ai?source=post", "https://blog.insightdatascience.com/tagged/deep-learning?source=post", "https://blog.insightdatascience.com/@emmanuelameisen?source=footer_card", "https://blog.insightdatascience.com/@emmanuelameisen", "http://twitter.com/EmmanuelAmeisen", "https://blog.insightdatascience.com?source=footer_card", "https://blog.insightdatascience.com?source=footer_card", "https://blog.insightdatascience.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "65": {"url": "http://vooban.com/en/tips-articles-geek-stuff/hyperopt-tutorial-for-optimizing-neural-networks-hyperparameters/", "title": "", "text": "Hyperopt is a way to search through an hyperparameter space. For example, it can use the Tree-structured Parzen Estimator (TPE) algorithm, which explore intelligently the search space while narrowing down to the estimated best parameters. It is hence a good method for meta-optimizing a neural network which is itself an optimisation problem: tuning a neural network uses gradient descent methods, and tuning the hyperparameters needs to be done differently since gradient descent can\u2019t apply. Therefore, Hyperopt can be useful not only for tuning hyperparameters such as the learning rate, but also to tune more fancy parameters in a flexible way, such as changing the number of layers of certain types, or the number of neurons in a layer, or even the type of layer to use at a certain place in the network given an array of choices, each with nested tunable hyperparameters. This is an oriented random search, in contrast with a Grid Search where hyperparameters are pre-established with fixed steps increase. Random Search for Hyper-Parameter Optimization (such as what Hyperopt do) has proven to be an effective search technique. The paper about this technique sits among the most cited deep learning papers. To sum up, it is more efficient to search randomly through values and to intelligently narrow the search space rather than looping on fixed sets of values for the hyperparameters. Note that this blog post is also available as a Notebook on GitHub. It contains code that can be run with Jupyter. A parameter is defined with a certain uniformrange or else a probability distribution, such as: There is also a few quantized versions of those functions, which rounds the generated values at each step of \u201cq\u201d: It is also possible to use a \u201cchoice\u201d which can lead to hyperparameter nesting: Visualisations of the parameters for probability distributions can be found below. Then, more details on choices and parameter nesting will come. Note on the above charts (especially for the loguniform and uniform distributions): the blurred line averaging the values fades out toward the ends of the signal since it is zero-padded. The line ideally would not fade out by using techniques such as mirror-padding. Those are the best distributions for modeling the values a learning rate. That\u2019s because we want to observe changes in the learning rate according to changing it with multiplications rather than additions, e.g.: when adjusting the learning rate, we\u2019ll want to try to divide it or multiply it by 2 rather than adding and substracting a finite value. To proove this, let\u2019s generate a loguniform distribution for a multiplier of the learning rate, centered at 1.0. Dividing 1 by those values should yield the same distribution. Let\u2019s now define a simple search space and solve for f(x) = x^2 - x + 1, where x is an hyperparameter. Let\u2019s solve for minimizing f(x, y) = x^2 + y^2 using a space using a python dict as structure. Later, this will neable us to nest hyperparameters with choices in a clean way. Yet, we have defined spaces as a single parameter. But that is 1D. Normally, spaces contain many parameters. Let\u2019s define a more complex one and with one nested hyperparameter choice for an uniform float: This will require us to import a few more things, and return the results with a dict that has a \u201cstatus\u201d and \u201closs\u201d key at least. Let\u2019s keep in our return dict the evaluated space too as this may come in handy if we save results to disk. Note that the optimization could be parallelized by using MongoDB and storing the trials\u2019 state here. Althought this is a built-in feature of hyperopt, let\u2019s keep things simple for our examples here. Indeed, the TPE algorithm used by the fmin function has state which is stored in the trials and which is useful to narrow the search space dynamically once we have a few trials. It is then interesting to pause and resume a training, and to apply that to a real problem. This is what\u2019s done inside the hyperopt_optimize.py file of the GitHub repository for this project. There, as an example, we optimize a convolutional neural network for solving the CIFAR-100 problem. Star Fork You might as well like this other blog post of mine on how to use Git Large File Storage (Git LFS) to handle the versioning of huge files when working with machine learning projects. 510-1015 av. Wilfrid-Pelletier  Quebec, QC, Canada  G1W 0C4 418 800.0027 TOLL FREE: 1 844 800.0027 info@vooban.com", "links": ["http://vooban.com/en/", "http://vooban.com/en/", "http://vooban.com/en/", "http://vooban.com/en/products/", "http://vooban.com/en/products/#tpti", "http://vooban.com/en/products/#tdti", "http://vooban.com/en/approach/", "http://vooban.com/en/tactical-squad/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/", "http://vooban.com/en/tips-articles-geek-stuff/", "https://pdfs.semanticscholar.org/9f2a/efc3821853e963beda011ed770f740385b77.pdf", "http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a", "https://github.com/terryum/awesome-deep-learning-papers#optimization--training-techniques", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100/blob/master/IntroductionToHyperopt.ipynb", "http://jupyter.org/", "http://vooban.com/wp-content/uploads/2017/08/histogram_1_int.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_2_uniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_3_loguniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_4_normal.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_5_lognormal.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_6_inverse_loguniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_7_inverse_lognormal.png", "http://vooban.com/wp-content/uploads/2017/08/fx2.png", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100/fork", "http://vooban.com/en/tips-articles-geek-stuff/migrating-to-git-lfs-for-developing-deep-learning-applications-with-large-files/", "http://vooban.com/en/tips-articles-geek-stuff/discover-structure-behind-data-with-decision-trees/", "http://vooban.com/en/tips-articles-geek-stuff/", "https://www.google.ca/maps/place/Vooban/@46.7694849,-71.3055195,17z/data=!3m1!4b1!4m5!3m4!1s0x4cb890d721a9234b:0x979022270b4a9fa4!8m2!3d46.7694849!4d-71.3033308", "http://vooban.com/en/products/", "http://vooban.com/en/approach/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/", "https://twitter.com/vooban", "https://www.instagram.com/voobanvibe/", "https://www.facebook.com/vooban", "https://www.linkedin.com/company/vooban", "http://vooban.com/en/", "http://vooban.com/en/products/", "http://vooban.com/en/products/#tpti", "http://vooban.com/en/products/#tdti", "http://vooban.com/en/approach/", "http://vooban.com/en/tactical-squad/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/"]}, "66": {"url": "https://elitedatascience.com/learn-python-for-data-science", "title": "How to Learn Python for Data Science in 2017 (Updated)", "text": "In this guide, we\u2019ll cover how to learn Python for data science, including our favorite curriculum for self-study. You see, data science is about problem solving, exploration, and extracting valuable information from data. To do so effectively, you\u2019ll need to wrangle\u00a0datasets, train machine learning models, visualize results, and much more. Enter Python. This is the best time ever to learn Python. In fact, Forbes named it a top 10 technical skill in terms of job demand growth. Let\u2019s discuss why\u2026 Python is one of the most widespread languages in the world, and it has a passionate community of users: Python Popularity, TIOBE Index It has an even more loyal following within the data science profession. Some people judge the quality of a programming language by the simplicity of its \"hello, world!\" program. Python does pretty well by this standard: For comparison, here's the same output in Java: Great, case closed. See you back here\u00a0after you've mastered Python? Well, in all seriousness, simplicity is one of Python's greatest strengths. Thanks to its precise and efficient syntax, Python can accomplish the same tasks with less code than other languages. This makes implementing\u00a0solutions refreshingly fast. In addition, Python's vibrant data science community means you'll be able to find plenty of tutorials, code snippets, and people to commiserate with\u00a0fixes to common bugs. Stackoverflow will be one of your best friends. Finally,\u00a0Python\u00a0has an\u00a0all-star lineup of libraries (a.k.a. packages) for data analysis and machine learning, which drastically reduce the time it takes to produce results. More on these later. Before we go into what you'll need to learn, let's discuss what you won't need. You won't need a C.S. degree. Most data scientists will never deal with topics such as memory leaks, cryptography, or \"Big O\" notation. You'll be fine as long as you can write clean, logical code in a scripting language such as Python or R. You won't need a complete course on Python. Python and data science are\u00a0not synonymous. You won't need to memorize all the syntax. Instead, focus on grasping the intuition, such as when function is appropriate or how conditional statements work. You'll gradually remember the syntax after Googling, reading documentation, and good ol' fashioned practice. We recommend a top-down approach. We advocate\u00a0a top-down approach with the goal of getting results first and then solidifying concepts over time. In fact, we prefer to cut out \"classroom\" study in favor of real-world practice. This approach will allow you to build mastery over time while having more fun. There are many ways to install Python on your computer, but we recommend the Anaconda bundle, which comes with the libraries you'll need for data science. Effective programming is not about memorizing syntax, but rather mastering a new way of thinking. Therefore, take your time in building a solid foundation of core programming concepts. These will help you translate solutions in your head into instructions for a computer. If you are completely new to programming, we recommend the excellent Automate the Boring Stuff with Python book, which has been released for free online under a creative commons license. The book promises \"practical programming for total beginners,\" and it keeps each lesson down-to-earth. Read up to Chapter 6 - Manipulating Strings and complete the practice questions along the way. If you only need to brush up on Python syntax, then we recommend the following video, aptly named \"Learn Python in One Video:\" Again, the goal of this step is not to learn everything about Python and programming. Instead, focus on the intuition. You should be able to answer questions such as: If you'd like more practice with the core programming concepts, check out the following resources. Next, we're going to focus on the for data science part of \"how to learn Python for data science.\" As we mentioned earlier, Python has an all-star lineup of libraries for data science. Libraries are simply bundles of pre-existing functions and objects that you can import into your script to save time. These are the action steps we recommend for efficiently picking up a new library: We don't recommend diving much deeper into a library right now because you'll likely forget most of what you've learned by the time you jump into projects. Instead, aim to discover what each library is capable of. If you installed Python through the Anaconda bundle as we recommended above, it will also come with Jupyter Notebook. Jupyter Notebook is a lightweight IDE that's a favorite among data scientists. We recommend it for your projects. You can open a new notebook through Anaconda Navigator, which came with Anaconda. Check out this short video for instructions. These are the essential libraries you'll need: NumPy allows easy and efficient numeric computation, and many other data science libraries are built on top of it. Pandas is high-performance library for data structures and exploratory analysis. It's built on top of NumPy. Matplotlib is a flexible plotting and visualization library. It's powerful but somewhat cumbersome. You have the option of skipping Matplotlib for now and using Seaborn to get started (see our Seaborn recommendation below). Scikit-Learn is the premier general-purpose machine learning library in Python. It has many popular algorithms and modules for pre-processing, cross-validation, and much more. Seaborn makes it much easier to plot common data visualizations. It's built on top of Matplotlib and offers a more pleasant high-level wrapper. By now, you'll have a basic understanding of programming and a working knowledge of essential libraries. This actually covers most of the Python you'll need to get started with data science. At this point, some students will feel a bit overwhelmed. That's OK, and it's perfectly normal. If you were to take the slow and traditional bottom-up approach, you might feel less overwhelmed, but it would have taken you 10 times as long to get here. Now the key is to dive in immediately and start gluing everything together. Again, our goal up to here has been to just learn enough to get started. Next, it's time to solidify your knowledge through plenty of practice and projects. You have several options. The first option is to participate on Kaggle, a site that hosts data science competitions. The main advantage of Kaggle is that every project is self-contained. You're given the dataset, a goal, and tutorials to get you started. The major disadvantage of competitions is that they're usually not representative of real-world data science. The \"Getting Started\" competitions are way too basic while the standard competitions (i.e. those with prize pools) are usually too tough for beginners. If you're interested in this path, check out our Beginner's Guide to Kaggle. The next option is to structure your own projects and pick datasets that interest you. The main advantage of this approach is that the projects are more representative of real-world data science. You'll likely need to define your own goals, collect data, clean your dataset, engineer features, and so on. The disadvantage of DIY projects is that you'll need to already be familiar with a proper data science workflow. Without one, you could miss important steps or get stuck without knowing how to proceed. If you go with this path, check out our free 7-day crash course on applied machine learning, which covers the key steps in a data science workflow. We also have another article with several\u00a0DIY project ideas. Finally, there are guided end-to-end projects. Proper guided projects should combine the best of both words - they should be representative of real-world data science and allow you to solidify your skills through a carefully planned learning curve. Many data science bootcamps offer this as a main benefit. Bootcamps usually conclude with a \"capstone project\" that allows you to see all the moving pieces together, from start to finish. We've also crafted our own\u00a0Machine Learning Masterclass to solve this exact need. It will provide you over-the-shoulder mentorship for real-world projects while teaching you all of the key concepts in context. The masterclass also includes a comprehensive Python course that gets you up to speed ASAP. In fact, many successful students have enrolled without any prior programming experience. Learn more about it here. [\u2026] In terms of programming languages \u2013 Python is pretty powerful language with integration to all technologies in the CDH stack. For a basic course check out the link below \u2013 Python For Data Science [\u2026] * Denotes Required Field Copyright \u00a9 2017 \u00b7 EliteDataScience.com \u00b7 All Rights Reserved", "links": ["https://elitedatascience.com/", "https://elitedatascience.com/", "https://elitedatascience.com/blog", "https://elitedatascience.com/machine-learning-masterclass", "https://elitedatascience.com/about", "https://elitedatascience.com/about", "https://elitedatascience.com/guest-post-submissions", "https://elitedatascience.com/author/slice2o", "https://elitedatascience.com/learn-python-for-data-science#comments", "https://www.facebook.com/sharer.php?u=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://plus.google.com/share?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://www.linkedin.com/shareArticle?trk=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://twitter.com/intent/tweet?text=How%20to%20Learn%20Python%20for%20Data%20Science%20%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "https://www.forbes.com/sites/jeffkauflin/2017/01/08/the-10-technical-skills-with-explosive-growth-in-job-demand/#6fa6bf344f5c", "http://www.tiobe.com/tiobe-index/", "https://stackoverflow.com/questions/tagged/python?sort=votes&pageSize=15", "https://www.continuum.io/downloads", "https://automatetheboringstuff.com/", "https://automatetheboringstuff.com/", "https://codefights.com/", "http://www.pythonchallenge.com/index.php", "http://www.practicepython.org/", "http://interactivepython.org/runestone/static/thinkcspy/index.html", "http://jupyter.org/", "https://www.youtube.com/watch?v=-MyjG00la2k", "https://docs.scipy.org/doc/numpy/user/", "https://docs.scipy.org/doc/numpy-dev/user/quickstart.html", "https://pandas.pydata.org/pandas-docs/stable/", "https://pandas.pydata.org/pandas-docs/stable/10min.html", "https://matplotlib.org/contents.html", "https://matplotlib.org/users/pyplot_tutorial.html", "http://scikit-learn.org/stable/documentation.html", "http://elitedatascience.com/python-machine-learning-tutorial-scikit-learn", "https://seaborn.pydata.org/", "http://elitedatascience.com/python-seaborn-tutorial", "http://elitedatascience.com/beginner-kaggle", "http://elitedatascience.com/", "http://elitedatascience.com/machine-learning-projects-for-beginners", "http://elitedatascience.com/machine-learning-masterclass", "http://elitedatascience.com/machine-learning-masterclass", "https://www.facebook.com/sharer.php?u=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://plus.google.com/share?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://www.linkedin.com/shareArticle?trk=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://twitter.com/intent/tweet?text=How%20to%20Learn%20Python%20for%20Data%20Science%20%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "https://datascienceandmore.wordpress.com/2017/03/04/getting-the-basics-down/", "https://elitedatascience.com/learn-machine-learning", "https://elitedatascience.com/learn-python-for-data-science", "https://elitedatascience.com/feature-engineering-best-practices", "https://elitedatascience.com/beginner-kaggle", "https://elitedatascience.com/imbalanced-classes", "https://elitedatascience.com/beginner-mistakes", "https://elitedatascience.com/bias-variance-tradeoff", "https://elitedatascience.com/data-science-resources", "https://elitedatascience.com/guest-post-submissions", "https://elitedatascience.com/", "https://elitedatascience.com/terms-of-service", "https://elitedatascience.com/privacy-policy", "https://elitedatascience.com/about", "https://www.linkedin.com/shareArticle?trk=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science", "http://www.facebook.com/sharer.php?u=https://elitedatascience.com/learn-python-for-data-science", "https://plus.google.com/share?text=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science", "https://twitter.com/intent/tweet?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "http://service.weibo.com/share/share.php?url=https://elitedatascience.com/learn-python-for-data-science&title=How to Learn Python for Data Science (Updated)", "https://getpocket.com/save?title=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science"]}, "67": {"url": "https://stats.stackexchange.com/questions/297380/why-not-just-dump-the-neural-networks-and-deep-learning", "title": "", "text": "Fundamental problem with deep learning and neural networks in general. The solutions that fit training data are infinite. We don't have precise mathematical equation that is satisfied by only a single one and that we can say generalizes best. Simply speaking we don't know which generalizes best. Optimizing weights is not a convex problem, so we never know we end up with a global or a local minimum. So why not just dump the neural networks and instead search for a better ML model? Something that we understand, and something that is consistent with a set of mathematical equations? Linear and SVM do not have this mathematical drawbacks and are fully consistent with a a set of mathematical equations. Why not just think on same lines (need not be linear though) and come up with a new ML model better than Linear and SVM and neural networks and deep learning? Not being able to know what solution generalizes best is an issue, but it shouldn't deter us from otherwise using a good solution. Humans themselves often do not known what generalizes best (consider, for example, competing unifying theories of physics), but that doesn't cause us too many problems. It has been shown that it is extremely rare for training to fail because of local minimums. Most of the local minimums in a deep neural network are close in value to the global minimum, so this is not an issue. source But the broader answer is that you can talk all day about nonconvexity and model selection, and people will still use neural networks simply because they work better than anything else (at least on things like image classification).  Of course there are also people arguing that we shouldn't get too focused on CNNs like the community was focused on SVMs a few decades ago, and instead keep looking for the next big thing. In particular, I think I remember Hinton regretting the effectiveness of CNNs as something which might hinder research. related post As the comments to your question point out, there are a lot of people working on finding something better. I would though like to answer this question by expanding the comment left by @josh All models are wrong but some are useful (Wiki) The above statement is a general truth used to describe the nature of statistical models. Using data that we have available, we can create models that let us do useful things such as approximate a predicted value. Take for example Linear Regression  Using a number of observations, we can fit a model to give us an approximate value for a dependent variable given any value(s) for the independent variable(s).  Burnham, K. P.; Anderson, D. R. (2002), Model Selection and Multimodel > Inference: A Practical Information-Theoretic Approach (2nd ed.): \"A model is a simplification or approximation of reality and hence   will not reflect all of reality. ... Box noted that \u201call models are   wrong, but some are useful.\u201d While a model can never be \u201ctruth,\u201d a   model might be ranked from very useful, to useful, to somewhat useful   to, finally, essentially useless.\" Deviations from our model (as can be seen in the image above) appear random, some observations are below the line and some are above, but our regression line shows a general correlation. Whilst deviations in our model appear random, in realistic scenarios there will be other factors at play which cause this deviation. For example, imagine watching cars as they drove through a junction where they must turn either left or right to continue, the cars turn in no particular pattern. Whilst we could say that the direction the cars turn is completely random, does every driver reach the junction and at that point make a random decision of which way to turn? In reality they are probably heading somewhere specific for a specific reason, and without attempting to stop each car to ask them about their reasoning, we can only describe their actions as random.  Where we are able to fit a model with minimal deviation, how certain can we be that an unknown, unnoticed or immeasurable variable wont at some point throw our model? Does the flap of a butterfly\u2019s wings in Brazil set off a tornado in Texas? The problem with using the Linear and SVN models you mention alone is that we are somewhat required to manually observe our variables and how they each affect each other. We then need to decide what variables are important and write a task-specific algorithm. This can be straight forward if we only have a few variables, but what if we had thousands? What if we wanted to create a generalised image recognition model, could this realistically be achieved with this approach?  Deep Learning and Artificial Neural Networks (ANNs) can help us create useful models for huge data sets containing huge amounts of variables (e.g. image libraries). As you mention, there's an incomprehensible number of solutions which could fit the data using ANNs, but is this number really any different to the amount of solutions we would need to develop ourselves through trial and error? The application of ANNs do much of the work for us, we can specify our inputs and our desired outputs (and tweak them later to make improvements) and leave it up to the ANN to figure out the solution. This is why ANNs are often described as \"black boxes\". From a given input they output an approximation, however (in general terms) these approximations don't include details on how they were approximated. And so it really comes down to what problem you are trying to solve, as the problem will dictate what model approach is more useful. Models are not absolutely accurate and so there is always an element of being 'wrong', however the more accurate your results the more useful they are. Having more detail in the results on how the approximation was made may also be useful, depending on the problem it may even be more useful than increased accuracy. If for example you are calculating a persons credit score, using regression and SVMs provides calculations that can be better explored. Being able to both tweak the model directly and explain to customers the effect separate independent variables have on their overall score is very useful. An ANN may aid in processing larger amounts of variables to achieve a more accurate score, but would this accuracy be more useful? I guess for some problem we care less for the mathematical rigor and simplicity but more for its utility, current status is neural network is better in performing certain task like pattern recognition in image processing.  The global minimum may as well as be useless, so we don't really care if we find it or not. The reason is that, for deep networks, not only the time to find it becomes exponentially longer as the network size increases, but also the global minimum often corresponds to overfitting the training set. Thus the generalization ability of the DNN (which is what we really care about) would suffer. Also, often we prefer flatter minima corresponding to a higher value of the loss function, than sharper minima corresponding to a lower value of the loss function, because the second one will deal very badly with uncertainty in the inputs. This is becoming increasingly clear with the development of Bayesian Deep Learning. Robust Optimization beats Determinist Optimization very often, when applied to real world problems where uncertainty is important. Finally, it's a fact that DNNs just kick the ass of methods such as XGBoost at image classification and NLP. A company which must make a profit out of image classification will correctly select them as modelsto be deployed in production (and invest a significant amount of money on feature engineering, data pipeline, etc. but I digress). This doesn't mean that they dominate all the ML environment: for example, they do worse than XGBoost on structured data (see the last winners of Kaggle competitions) and they seem to not still do as well as particle filters on time series modelling. However, some very recent innovations on RNNs may modify this situation. There are excellent answers, mostly weighing in with the usefulness of DL and ANNs. But I would like to object the OP in a more fundamental way, since the question already takes for granted the mathematical inconsistency of neural networks. First of all, there is a mathematical theory behind (most models of) Neural Networks. You could likewise argue that linear regression does not generalize, unless the underlying model is... well, linear. In neural algorithms, a model is assumed (even if not explicitly) and the fitting error is computed. The fact that algorithms are modified with various heuristics does not void the original mathematical support. BTW, local optimization is also a mathematically consistent, let alone useful, theory. Along this line, if Neural Networks just constitute one class of methods within the whole toolbox of scientists, which is the line that separates Neural Networks from the rest of techniques? In fact, SVMs were once considered a class of NNs and they still appear in the same books. On the other hand, NNs could be regarded as a (nonlinear) regression technique, maybe with some simplification. I agree with the OP that we must search better, well founded, efficient algorithms, regardless you label them as NNs or not. Sign up using Google Sign up using Facebook Sign up using Email and Password   By posting your answer, you agree to the privacy policy and terms of service. asked 3 days ago viewed  7,704 times  active yesterday                                   site design / logo \u00a9 2017 Stack Exchange Inc; user contributions licensed under cc by-sa 3.0                                  with attribution required.                                  rev 2017.8.11.26777", "links": ["https://stats.stackexchange.com", "https://chat.stackexchange.com?tab=site&host=stats.stackexchange.com", "https://stats.stackexchange.com", "https://stats.meta.stackexchange.com", "https://stats.stackexchange.com/users/signup?ssrc=site_switcher&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.stackexchange.com/users/login?ssrc=site_switcher&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stackexchange.com/sites", "https://stackoverflow.blog", "https://stackexchange.com", "https://stats.stackexchange.com/users/signup?ssrc=head&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.stackexchange.com/users/login?ssrc=head&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.meta.stackexchange.com", "https://stackoverflow.com/company/about", "https://www.stackoverflowbusiness.com/?ref=topbar_help", "https://stats.stackexchange.com", "https://arxiv.org/pdf/1406.2572.pdf", "https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/clyohpf/?context=3", "https://arxiv.org/pdf/1406.2572.pdf", "https://arxiv.org/pdf/1412.0233.pdf", "https://en.wikipedia.org/wiki/All_models_are_wrong", "https://en.wikipedia.org/wiki/Butterfly_effect", "https://stats.stackexchange.com/questions/93705/meaning-of-a-neural-network-as-a-black-box", "https://stackexchange.com/legal/privacy-policy", "https://stackexchange.com/legal/terms-of-service", "http://stats.stackexchange.com/election", "http://stats.stackexchange.com/election", "https://stackexchange.com/questions?tab=hot", "https://german.stackexchange.com/questions/38460/why-are-some-words-spelled-with-tz-if-z-already-has-the-ts-sound", "https://cseducators.stackexchange.com/questions/3175/learning-fundamental-differences-between-functional-programming-and-object-orien", "https://gaming.stackexchange.com/questions/316141/can-i-learn-lockpickin-after-i-leave-the-farm", "https://softwareengineering.stackexchange.com/questions/355580/how-many-cores-should-i-utilize-for-calculations-cores-or-cores-1", "https://math.stackexchange.com/questions/2392577/why-is-the-determinant-of-the-all-one-matrix-minus-the-identity-matrix-n-1", "https://math.stackexchange.com/questions/2392309/closed-circle-as-a-metric-space", "https://rpg.stackexchange.com/questions/105275/can-blessings-of-knowledge-be-applied-twice-to-different-skills", "https://rpg.stackexchange.com/questions/105254/meaning-of-honor-quarter-for-cavaliers-of-the-order-of-the-blue-rose", "https://interpersonal.stackexchange.com/questions/1556/how-to-ask-a-vegan-to-stop-telling-me-about-veganism-because-i-am-not-interested", "https://stackoverflow.com/questions/45657517/why-isnt-this-c-program-looping-from-a-negative-origin", "https://english.stackexchange.com/questions/405054/is-there-an-english-equivalent-for-the-persian-proverb-to-play-with-tail-of-lio", "https://codegolf.stackexchange.com/questions/138887/implement-true-string-addition", "https://tex.stackexchange.com/questions/386263/suppress-globally-set-before-end-axis-code-in-pgfplots", "https://physics.stackexchange.com/questions/351628/why-must-entangled-particles-communicate-their-spin-instantaneously", "https://math.stackexchange.com/questions/2392440/is-this-an-issue-with-the-law-of-the-excluded-middle-or-an-issue-with-the-proof", "https://travel.stackexchange.com/questions/99713/in-germany-what-are-you-supposed-to-do-if-your-train-station-doesnt-have-any-w", "https://politics.stackexchange.com/questions/23595/why-does-the-united-states-call-japan-an-ally", "https://codegolf.stackexchange.com/questions/138827/script-that-outputs-a-script-that-prints-a-given-input", "https://aviation.stackexchange.com/questions/42927/why-does-atc-ask-a-crew-who-has-declared-an-emergency-if-their-aircraft-will-be", "https://codereview.stackexchange.com/questions/172849/checking-if-a-number-is-power-of-2-or-not", "https://retrocomputing.stackexchange.com/questions/4481/what-is-the-relative-code-density-of-8-bit-microprocessors", "https://mathoverflow.net/questions/278629/a-combinatorial-identity", "https://money.stackexchange.com/questions/83759/are-my-parents-ripping-me-off-with-this-deal-that-doesnt-allow-me-to-build-my-e", "https://english.stackexchange.com/questions/405560/what-does-presidential-president-mean", "https://chat.stackexchange.com?tab=site&host=stats.stackexchange.com", "https://stats.meta.stackexchange.com", "https://stackoverflow.com/company/about", "https://stackoverflow.com", "https://www.stackoverflowbusiness.com/?utm_source=so-footer&utm_medium=referral&utm_campaign=brand-activation", "https://stackoverflow.com/jobs", "https://stackoverflow.com/company/about", "https://stackoverflow.com/company/press", "https://stackexchange.com/legal", "https://stackexchange.com/legal/privacy-policy", "https://stackexchange.com", "https://stackexchange.com/sites#technology", "https://stackexchange.com/sites#lifearts", "https://stackexchange.com/sites#culturerecreation", "https://stackexchange.com/sites#science", "https://api.stackexchange.com", "https://data.stackexchange.com", "https://stackoverflow.blog?blb=1", "https://www.facebook.com/officialstackoverflow/", "https://twitter.com/stackoverflow", "https://linkedin.com/company/stack-overflow", "https://creativecommons.org/licenses/by-sa/3.0/", "https://stackoverflow.blog/2009/06/25/attribution-required/"]}, "68": {"url": "https://blog.godatadriven.com/practical-airflow-tutorial", "title": "", "text": "Airflow is a scheduler for workflows such as data pipelines, similar to Luigi and Oozie. It's written in Python and we at GoDataDriven have been contributing to it in the last few months. This tutorial is loosely based on the Airflow tutorial in the official documentation. It will walk you through the basics of setting up Airflow and creating an Airflow workflow, and it will give you some practical tips. A (possibly) more up-to-date version of this blog can be found in my git repo. Setting up a basic configuration of Airflow is pretty straightforward. After installing the Python package, we'll need a database to store some data and start the core Airflow services. You can skip this section if Airflow is already set up. Make sure that you can run airflow commands, know where to put your DAGs and have access to the web UI. Airflow is installable with pip via a simple pip install apache-airflow. Either use a separate Python virtual environment or install it in your default python environment. To use the conda virtual environment as defined in environment.yml from my git repo: You should now have an (almost) working Airflow installation. Alternatively, install Airflow yourself by running: Airflow used to be packaged as airflow but is packaged as apache-airflow since version 1.8.1. Make sure that you install any extra packages with the right Python package: e.g. use pip install apache-airflow[dask] if you've installed apache-airflow and do not use pip install airflow[dask]. Leaving out the prefix apache- will install an old version of Airflow next to your current version, leading to a world of hurt. You may run into problems if you don't have the right binaries or Python packages installed for certain backends or operators. When specifying support for e.g. PostgreSQL when installing extra Airflow packages, make sure the database is installed; do a brew install postgresql or apt-get install postgresql before the pip install apache-airflow[postgres]. Similarly, when running into HiveOperator errors, do a pip install apache-airflow[hive] and make sure you can use Hive. Before you can use Airflow you have to initialize its database. The database contains information about historical & running workflows, connections to external data sources,  user management, etc. Once the database is set up, Airflow's UI can be accessed by running a web server and workflows can be started. The default database is a SQLite database, which is fine for this tutorial. In a production setting you'll probably be using something like MySQL or PostgreSQL. You'll probably want to back it up as this database stores the state of everything related to Airflow. Airflow will use the directory set in the environment variable AIRFLOW_HOME to store its configuration and our SQlite database. This directory will be used after your first Airflow command. If you don't set the environment variable AIRFLOW_HOME, Airflow will create the directory ~/airflow/ to put its files in. Set environment variable AIRFLOW_HOME to e.g. your current directory $(pwd): or any other suitable directory. Next, initialize the database: Now start the web server and go to localhost:8080 to check out the UI: It should look something like this:  With the web server running workflows can be started from a new terminal window. Open a new terminal, activate the virtual environment and set the environment variable AIRFLOW_HOME for this terminal as well: Make sure that you're an in the same directory as before when using $(pwd). Run a supplied example: And check in the web UI that it has run by going to Browse -> Task Instances. This concludes all the setting up that you need for this tutorial. We'll create a workflow by specifying actions as a Directed Acyclic Graph (DAG) in Python. The tasks of a workflow make up a Graph; the graph is Directed because the tasks are ordered; and we don't want to get stuck in an eternal loop so the graph also has to be Acyclic. The figure below shows an example of a DAG:  The DAG of this tutorial is a bit easier. It will consist of the following tasks: and we'll plan daily execution of this workflow. Go to the folder that you've designated to be your AIRFLOW_HOME and find the DAGs folder located in subfolder dags/ (if you cannot find, check the setting dags_folder in $AIRFLOW_HOME/airflow.cfg). Create a Python file with the name airflow_tutorial.py that will contain your DAG. Your workflow will automatically be picked up and scheduled to run. First we'll configure settings that are shared by all our tasks. Settings for tasks can be passed as arguments when creating them, but we can also pass a dictionary with default values to the DAG. This allows us to share default arguments for all the tasks in our DAG is the best place to set e.g. the owner and start date of our DAG. Add the following import and dictionary to airflow_tutorial.py to specify the owner, start time, and retry settings that are shared by our tasks: These settings tell Airflow that this workflow is owned by 'me', that the workflow is valid since June 1st of 2017, it should not send emails and it is allowed to retry the workflow once if it fails with a delay of 5 minutes. Other common default arguments are email settings on failure and the end time. We'll now create a DAG object that will contain our tasks. Name it airflow_tutorial_v01 and pass default_args: With schedule_interval='0 * * * *' we've specified a run at every hour 0; the DAG will run each day at 00:00. See crontab.guru for help deciphering cron schedule expressions. Alternatively, you can use strings like '@daily' and '@hourly'. We've used a context manager to create a DAG (new since 1.8). All the tasks for the DAG should be indented to indicate that they are part of this DAG. Without this context manager you'd have to set the dag parameter for each of your tasks. Airflow will generate DAG runs from the start_date with the specified schedule_interval. Once a DAG is active, Airflow continuously checks in the database if all the DAG runs have successfully ran since the start_date. Any missing DAG runs are automatically scheduled. When you initialize on 2016-01-04 a DAG with a start_date at 2016-01-01 and a daily schedule_interval, Airflow will schedule DAG runs for all the days between 2016-01-01 and 2016-01-04. A run starts after the time for the run has passed. The time for which the workflow runs is called the execution_date. The daily workflow for 2016-06-02 runs after 2016-06-02 23:59 and the hourly workflow for 2016-07-03 01:00 starts after 2016-07-03 01:59. From the ETL viewpoint this makes sense: you can only process the daily data for a day after it has passed. This can, however, ask for some juggling with date for other workflows. For Machine Learning models you may want to use all the data up to a given date, you'll have to add the schedule_interval to your execution_date somewhere in the workflow logic. Because Airflow saves all the (scheduled) DAG runs in its database, you should not change the start_date and schedule_interval of a DAG. Instead, up the version number of the DAG (e.g. airflow_tutorial_v02) and avoid running unnecessary tasks by using the web interface or command line tools Timezones and especially daylight savings can mean trouble when scheduling things, so keep your Airflow machine in UTC. You don't want to skip an hour because daylight savings kicks in (or out). Tasks are represented by operators that either perform an action, transfer data, or sense if something has been done. Examples of actions are running a bash script or calling a Python function; of transfers are copying tables between databases or uploading a file; and of sensors are checking if a file exists or data has been added to a database. We'll create a workflow consisting of three tasks: we'll print 'hello', wait for 10 seconds and finally print 'world'.  The first two are done with the BashOperator and the latter with the PythonOperator. Give each operator an unique task ID and something to do: Note how we can pass bash commands in the BashOperator and that the PythonOperator asks for a Python function that can be called. Dependencies in tasks are added by setting other actions as upstream (or downstream).  Link the operations in a chain so that sleep will be run after print_hello and is followed by print_world; print_hello -> sleep -> print_world: After rearranging the code your final DAG should look something like: First check that DAG file contains valid Python code by executing the file with Python: You can manually test a single task for a given execution_date with airflow test: This runs the task locally as if it was for 2017-07-01, ignoring other tasks and without communicating to the database. Now that you're confident that your dag works, turn on the DAG in the web UI and sit back while Airflow starts backfilling the dag runs! You now know the basics of setting up Airflow, creating a DAG and turning it on; time to go deeper!  Address: Wibautstraat 202, 1091 GS Amsterdam, The Netherlands           Phone: +31 (0)35 672 9069           Email: signal@godatadriven.com", "links": ["http://godatadriven.com/contact", "http://godatadriven.com/careers", "http://blog.godatadriven.com", "http://godatadriven.com/business-consultancy", "http://godatadriven.com/big-data-infrastructure", "http://godatadriven.com/predictive-modelling", "http://godatadriven.com/products-overview", "http://godatadriven.com/data-discovery", "http://godatadriven.com/data-science-audit", "http://godatadriven.com/data-science-suite", "http://godatadriven.com/data-science-box", "http://godatadriven.com/divolte", "http://godatadriven.com/customers", "http://godatadriven.com/casestudy-airfranceklm", "http://godatadriven.com/casestudy-bakkersland", "http://godatadriven.com/casestudy-bol", "http://godatadriven.com/casestudy-eneco", "http://godatadriven.com/casestudy-eretail", "http://godatadriven.com/casestudy-ing", "http://godatadriven.com/casestudy-nuon", "http://godatadriven.com/casestudy-npo", "http://godatadriven.com/casestudy-schipholgroup", "http://godatadriven.com/casestudy-transavia", "http://godatadriven.com/technology-overview", "http://godatadriven.com/databricks", "http://godatadriven.com/hadoop", "http://godatadriven.com/python", "http://godatadriven.com/r", "http://godatadriven.com/spark", "http://godatadriven.com/training-overview", "http://godatadriven.com/training-schedule", "http://godatadriven.com/bigdata-training", "http://godatadriven.com/datascience-training", "http://godatadriven.com/data-science-accelerator-program", "http://godatadriven.com/meetups", "http://godatadriven.com/events", "http://blog.godatadriven.com", "http://godatadriven.com/our-story", "http://godatadriven.com/team", "http://godatadriven.com/careers", "http://godatadriven.com/media-resources", "https://blog.godatadriven.com", "http://godatadriven.com/contact", "http://godatadriven.com", "http://blog.godatadriven.com", "https://airflow.incubator.apache.org/", "https://github.com/spotify/luigi", "https://oozie.apache.org/", "https://blog.godatadriven.com/open-source-201702", "https://blog.godatadriven.com/open-source-201703", "https://blog.godatadriven.com/open-source-201704", "https://blog.godatadriven.com/open-source-201705", "https://blog.godatadriven.com/open-source-201707", "https://pythonhosted.org/airflow/tutorial.html", "https://github.com/hgrif/airflow-tutorial", "https://github.com/hgrif/airflow-tutorial", "http://conda.pydata.org/miniconda.html", "http://localhost:8080/", "https://airflow.incubator.apache.org/configuration.html", "https://airflow.incubator.apache.org/security.html", "https://github.com/apache/incubator-airflow/tree/master/scripts", "https://crontab.guru/#0_*_*_*_*", "https://jeffknupp.com/blog/2016/03/07/python-with-context-managers/", "https://airflow.incubator.apache.org/configuration.html#connections", "https://airflow.incubator.apache.org/ui.html#variable-view", "https://airflow.incubator.apache.org/tutorial.html#templating-with-jinja", "https://github.com/apache/incubator-airflow/blob/master/airflow/example_dags/example_bash_operator.py", "https://airflow.incubator.apache.org/code.html#airflow.operators.PythonOperator", "https://pythonhosted.org/airflow/tutorial.html", "https://gtoonstra.github.io/etl-with-airflow/", "https://medium.com/handy-tech/airflow-tips-tricks-and-pitfalls-9ba53fba14eb", "http://godatadriven.com/players/henk-griffioen", "http://blog.godatadriven.com/practical-airflow-tutorial", "http://blog.godatadriven.com/open-source-201707", "http://blog.godatadriven.com/cd-python-eggs-vsts", "http://blog.godatadriven.com/ldappy", "http://blog.godatadriven.com/open-source-201706", "http://blog.godatadriven.com/vendor-free-ds", "https://twitter.com/GoDataDriven", "http://godatadriven.com/business-consultancy", "http://godatadriven.com/big-data-infrastructure", "http://godatadriven.com/predictive-modelling", "http://godatadriven.com/products-overview", "http://godatadriven.com/data-discovery", "http://godatadriven.com/data-science-audit", "http://godatadriven.com/data-science-suite", "http://godatadriven.com/data-science-box", "http://godatadriven.com/divolte", "http://godatadriven.com/customers", "http://godatadriven.com/casestudy-airfranceklm", "http://godatadriven.com/casestudy-bakkersland", "http://godatadriven.com/casestudy-bol", "http://godatadriven.com/casestudy-eneco", "http://godatadriven.com/casestudy-eretail", "http://godatadriven.com/casestudy-ing", "http://godatadriven.com/casestudy-nuon", "http://godatadriven.com/casestudy-npo", "http://godatadriven.com/casestudy-schipholgroup", "http://godatadriven.com/casestudy-transavia", "http://godatadriven.com/technology-overview", "http://godatadriven.com/hadoop", "http://godatadriven.com/python", "http://godatadriven.com/r", "http://godatadriven.com/spark", "http://godatadriven.com/training-overview", "http://godatadriven.com/training-schedule", "http://godatadriven.com/bigdata-training", "http://godatadriven.com/datascience-training", "http://godatadriven.com/data-science-accelerator-program", "http://godatadriven.com/events", "http://godatadriven.com/meetups", "http://blog.godatadriven.com", "http://godatadriven.com/our-story", "http://godatadriven.com/team", "http://godatadriven.com/careers", "http://godatadriven.com/media-resources", "http://godatadriven.com/news", "http://godatadriven.com/contact", "https://cdn.xebia.com/documents/Cookies+Policy+Xebia+20170130.pdf"]}, "69": {"url": "https://concepttoclinic.drivendata.org/", "title": "", "text": "Lung Cancer Early Detection Challenge  We\u2019re calling on a global community of data scientists, engineers, designers, and researchers to build an open source software application that brings advances from machine learning into the clinic. We\u2019re not just optimizing an algorithm for a single metric\u2014we\u2019re collaborating to build tools which put AI in the hands of clinicians. In addition to pushing forward the cutting-edge of open clinical software, top contributors will be eligible for a share of $100,000 in monetary prizes generously provided by the Bonnie J. Addario Lung Cancer Foundation.          Contribute now by grabbing an issue from the project's GitHub repository and submitting a PR!        Lung cancer causes more deaths each year than any other cancer in the US, and early detection makes a big difference. Recent advances in machine learning \u2013 including open source algorithms from the 2017 Data Science Bowl \u2013 help find and interpret early signs of cancer. Using imagery from CT scans, this application will build on cutting-edge algorithms in machine learning to help clinicians identify and interpret lung nodules. Your guide to contributing throughout the challenge. All you need to start contributing is a passion for building great software and a github account.  Contributions of all sizes can earn points as they add value to the project, by building on pre-designated issues, building out new content, and building up the community. $100,000 in cash, as well as other in-kind prizes, will be available for top contributors filling key roles throughout the challenge. A live leaderboard keeps track of points earned throughout the challenge by top contributors filling key roles. Meet the amazing group of of technical, clinical, and domain experts that are helping to guide the end-to-end design of this challenge. The Addario Lung Cancer Foundation (ALCF) has set the audacious goal of making lung cancer a chronically managed disease by 2023.  Ultimately, the goal of this project is to produce a tool that draws on machine intelligence, is useful to clinicians, and makes a lasting difference in the lives of patients. We hope you will find a way to contribute!  Lung Cancer Early Detection Challenge", "links": ["https://www.drivendata.org/termsofuse/", "https://www.drivendata.org/copyrightpolicy/", "https://www.drivendata.org/privacypolicy/", "https://www.drivendata.org/", "http://www.drivendata.co/"]}, "70": {"url": "http://appsilondatascience.com/blog/rstats/2017/08/09/shiny-collections.html", "title": "shiny.collections,  Google Docs-like live collaboration in ShinyAppsilon Data Science Blog", "text": "A few weeks ago, our CTO Marek Rogala gave an engaging talk at the User Conference in Brussels. Despite being one of the last talks, he drew a crowd that filled the room and had significant viewership online. Marek\u2019s talk was entitled shiny.collections: Google Docs-like live collaboration in Shiny. In short, he went over a package we built that allows for persistence and collaboration without losing reactivity in Shiny. Users can enter information and have continuity, knowing that if they exit and return to the app, their inputs will still be there. Let\u2019s go over his talk. Recently, Shiny users have demanded more from their applications. The advent of cloud applications, especially ones like Google Docs, have accustomed users to collaboration. They expect their Shiny apps to be more than just an interactive tool for data visualisation, but rather, a production ready application that works like any other tool. Their data has to be saved automatically. Interactive collaboration is a must in some use cases. And the application must be delivered as fast as possible. You need to use a reactive database such as rethinkDB, Firebase, or mongoDB to achieve this. mongoDB isn\u2019t really a reactive database, but there are ways where it can be used as one. We are going to use rethinkDB in our case. Also, we are going to use rethinker, which is a rethinkDB driver for R. But using rethinker is not the most ideal solution, as callback do not work very well with Shiny and it is quite painful to configure to our needs. To get around this, we created our own package built upon rethinker called shiny.collections.  It lets you easily connect to your shiny app and takes care of all of the trickiness involved. During his talk, Marek showed a live demo that involved creating a chat application. Check out how to do this yourself. I reccomend watching Marek first, as he has some valuable commentary you won\u2019t find in the blog post. This example is just a simple use case of what you can achieve with shiny.collections. We\u2019ve made it a priority to make it convenient and easily integrable with other powerful tools, including DT, leaflet or rhandsontable. Our goals for the future of this package include getting it on cran and diversifying the functionality of the API. Take a look at the our package and contribute. Get in touch with us for all of you other data science needs as well.  Published 09 Aug 2017", "links": ["https://channel9.msdn.com/Events/useR-international-R-User-conferences/useR-International-R-User-2017-Conference/shinycollections-Google-Docs-like-live-collaboration-in-Shiny#comments", "https://cran.r-project.org/web/packages/rethinker/index.html", "https://appsilon.github.io/shiny.collections/", "http://appsilondatascience.com/blog/rstats/2017/07/02/shiny-chat.html", "http://appsilondatascience.com", "http://disqus.com/?ref_noscript"]}, "71": {"url": "https://blog.statsbot.co/introduction-to-imitation-learning-32334c3b1e7a", "title": "Introduction to Imitation Learning", "text": "This post is the first in the series where we will describe what Imitation Learning is. For today\u2019s article, the Statsbot team asked computer scientist Vitaly Kurin to briefly introduce Imitation Learning and outline the basics of Reinforcement Learning. Living organisms are extremely complex. Even the relatively simple ones such as flies or worms. They are able not only to operate successfully in the real world, but are extremely resilient to changes in conditions. And that\u2019s if we\u2019re not even talking about humans. We can plan ahead, we can alter our plans given new information, and we can collaborate with others and execute our plans more effectively. Science and such projects as CERN or a huge Radio Telescope in Arecibo are perfect examples of human collaboration and the art of planning. Yes, we do a lot of stupid stuff, but let\u2019s not talk about that here and instead focus on the good. Yes, we can now beat the best human in chess or Go, we can get a crazy score in Video Pinball for Atari 2600, we can even already make a pure human broke by challenging him in poker. Can we open a bottle of champagne and celebrate the triumph? I\u2019m afraid not. Yes, machine learning has recently made a significant leap forward. The combination of new Deep Learning ideas with old ones has enabled us to advance in many domains, such as computer vision, speech recognition, and text translation. Reinforcement Learning has also benefited greatly from its marriage with Deep Learning. You\u2019ve definitely heard of Deep Reinforcement Learning success such as achieving superhuman score in Atari 2600 games, solving Go, and making robots learn parkour. Though, we must admit, that operating successfully in the real world is much harder than playing Go or Space Invaders. Many of the tasks are much harder than this. Imagine a kid riding a bicycle in the middle of a crowded city center or a man driving a Porsche 911 on an autobahn at 200 miles per hour. Let\u2019s all admit that we are not there yet. The typical machine learning approach is to train a model from scratch. Give it a million images and some time to figure it out. Give it a week and let it play Space Invaders until it reaches some acceptable score. We, as humans, beg to differ. When a typical human starts to play some game he has never seen, he already has a huge amount of prior information. If he sees a door in Montezuma\u2019s Revenge, he realizes that somewhere there should lie a key and he needs to find it. When he finds the key, he remembers that the closed door is back through the two previous rooms and he returns to open it. When he sees a ladder, he realizes that he can climb it because he has done this hundreds of time already. What if we could somehow transfer human knowledge about the world to an agent? How can we extract all this information? How can we create a model out of it? There is such a way. It\u2019s called Imitation Learning. Imitation Learning is not the only name for leveraging human data for good. Some researchers also call it apprenticeship learning, others refer to it as Learning from Demonstration. From our point of view, there is no substantial difference between all of these titles and we will use Imitation Learning from now on. In order to introduce Imitation Learning, we will need to understand the basics of Reinforcement Learning first. Let\u2019s move on. It\u2019s not hard to get the general idea of the Reinforcement Learning setup. There is some agent and we want this agent to learn some task. Let\u2019s say, we have an Atari 2600 console, the agent has access to the joystick and can see what\u2019s happening on the screen. Let\u2019s say, 60 times per second we give our agent a screenshot of the game and ask him what button he wants to press. If our agent does well, he can see that his score is increasing (positive reinforcement), otherwise we can give him a penalty as a negative reward (negative reinforcement). Gradually, by trial and error, the agent starts to understand that it\u2019s better to avoid some of the actions and do those which bring him a reward. Let\u2019s make it more formal and describe the process stated above mathematically. We can describe the RL framework mentioned above (observe -> act -> get the reward and the next state) as a Markov Decision Process (MDP): where: We also need a definition of a policy function for the next section. Policy is a function, that returns an action given the state: And, actually, our final goal when solving an MDP is to learn such a policy in order to maximize the reward for our agent. Let\u2019s take an example of an MDP. The circles represent the states, arrows with green labels are actions, red labels are the rewards for actions, and the square is the terminal state. The green numeric labels are the transition probabilities. Our student starts in a state with the blue circle. He studies, but this is hard and sometimes boring. He decides to open a Facebook app and once he is there, he can either quit or continue scrolling. He then studies more and more, and finally decides to go to the pub. The state is a smaller filled circle, since now there is an element of randomness, based on the amount of knowledge the student forgets after visiting the pub. He can then either study more and pass the exam (+10 in reward), or he can go to sleep and finish the MDP right now. Since we will use DQN and related ideas in the future, let\u2019s briefly understand what is going on here. The whole approach is built upon approximating the so-called Q function and building the agent\u2019s behavior based on it. The idea of the Q function is the following: it returns you the entire expected discounted reward flow for the particular action and the particular state, given that starting from the next state we will be following our policy \ud835\udfb9. It answers the question: \u201cHow good is to press this button in this state?\u201d The Q function obeys the Bellman equation: And, finally, the Bellman principle of optimality is the following: notwithstanding what happened before, we should always take the action with the highest Q to maximize the reward flow: But how do we get such a Q function, you might ask. Let\u2019s look at an example. Imagine, you want to grab a coffee (+20 in reward) and a chocolate in a vending machine (+10 in reward). Your total reward cannot exceed thirty. Moreover, if you have taken the coffee already, it cannot be higher than 10 (the reward for chocolate) from now on. This is the idea: the Q value for the current step and action is equal to the maximum Q value for the next state (since we behave optimally) + the reward we get for the transition. The value of the quadratic objective function becomes: Q-learning itself is not new. Q-learning which uses neural networks as a function approximators is also not new (e.g. neural fitted-q iteration). A DQN paper was the first to use deep convolutional networks to solve this type of problem and introduced a couple of novelties that make the training process much more stable. First of all, experience replay. The vanilla Q-learning point is to make a step, get the reward and the next state, then update the approximation function parameters based on this transition. The DQN idea is to make the transition and save it in a \u201creplay memory\u201d\u200a\u2014\u200aan array that stores the last 10\u2076 (<insert any large number here>) transitions with the information about the reward, states before and after the transition, and if the event is terminal (game over) or not. Having this experience replay we can randomly sample mini-batches from it and learn more effectively. Another thing that makes the algorithm more stable is that DQN uses two neural networks: the first to compute the Q value for the current state and the second to compute the Q value for the next state. You can see that from the equation with the objective: two different Q functions use \ud835\udfb1 and \ud835\udfb1\u2019, respectively. Each 10,000 steps, the parameters \ud835\udfb1\u2019 are copied from the learned parameters \ud835\udfb1 and this helps a lot in increasing the stability. The problem here with using one function is that when we update the weights, both Q(s,a) and Q(s\u2019,a\u2019) increase and this might lead to oscillations or policy divergence. Using two separate networks adds a delay between an update and computation of the target Q value and reduces such cases. If you have further interest in the phenomena, read the Method section in the DQN Nature paper. Okay, everything described above sounds quite simple. If there is still something you do not understand, please, have a look at David Silver\u2019s lecture where he explains everything perfectly! Knowing all of these, can we build an True AI now? I\u2019m sorry, but we can\u2019t. There are several problems that hinder us from building an agent that will beat ByuN at StarCraft II, bring an autonomous car to the market, or give you an opportunity to buy your grandma a robot that will do the dishes for her after lunch. One of these problems is that the rewards our agent gets might be very sparse in time. Let\u2019s say, you play chess. If you lose, how do you know when you made a catastrophic move? Moreover, it\u2019s highly possible that there was not a catastrophic move, but several average ones. Another problem that is closely connected to the previous one, is the sample-efficiency problem. Or, more honestly, sample-inefficiency. Even to master a simple game such as Space Invaders might take a couple of days in-game time. It\u2019s easy to speed up learning in games since we have access to the simulators, but what if we want to learn something in real life? Unfortunately, physics is not there yet and we cannot speed up time. There is an approach that could potentially solve these problems and a bunch of others\u200a\u2014\u200aImitation Learning, as we mentioned at the beginning of this post. As we said, we, humans, rarely learn something without any prior information. Let\u2019s use this data! What should we do? The idea of Imitation Learning is implicitly giving an agent prior information about the world by mimicking human behavior in some sense. Imitation Learning will not only help us solve the sample-inefficiency or computational feasibility problems, it might potentially make the training process safer. We cannot just put an autonomous car in the middle of the street and let it do whatever it wants. We do not want it to kill humans that are around, destroy someone\u2019s property, or the equipment itself. Pretraining it on a human demonstrator\u2019s data might make the training process faster and avoid undesirable situations. Training a model requires some data. Training a Deep Learning model requires even more data. Training a Deep Reinforcement Learning model requires\u2026 Okay, you get the idea. So, this series is only partly describes what we can do with demonstration data. The main point of all of this is to call for human demonstration datasets, because we do not have many, unfortunately, up to this moment. Okay, we should stop here. The key points of this post are: In the next chapter we will write more about Behavior Cloning\u200a\u2014\u200athe simplest approach to an RL problem that can leverage the human demonstration data. Stay tuned!", "links": ["https://blog.statsbot.co?source=logo-lo_2089ac78e050---cfc9f21a543a", "https://twitter.com/statsbotco", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.statsbot.co%2Fintroduction-to-imitation-learning-32334c3b1e7a", "https://blog.statsbot.co", "https://blog.statsbot.co/analytics/home", "https://blog.statsbot.co/datascience/home", "https://blog.statsbot.co/design/home", "https://blog.statsbot.co/startups/home", "https://blog.statsbot.co/bots/home", "https://blog.statsbot.co/news/home", "https://blog.statsbot.co/statsbot-digest-b0d7372f842a", "http://statsbot.co?utm_source=blog&utm_medium=navigation&utm_campaign=robot_face", "https://blog.statsbot.co/search", "https://blog.statsbot.co/@yobibyte?source=post_header_lockup", "https://blog.statsbot.co/@yobibyte?source=post_header_lockup", "https://atlas.cern/discover/about", "http://statsbot.co?utm_source=blog&utm_medium=article&utm_campaign=imitation_learning", "https://en.wikipedia.org/wiki/CERN", "https://en.wikipedia.org/wiki/Arecibo_Observatory", "https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning/", "https://deepmind.com/research/alphago/", "https://www.theverge.com/tldr/2017/7/10/15946542/deepmind-parkour-agent-reinforcement-learning", "http://www.ceva-dsp.com/ourblog/wp-content/uploads/sites/3/2016/04/AlphaGo-Lee-Se-dol.png", "https://simple.wikipedia.org/wiki/File:Rl_agent.png", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf", "https://link.springer.com/article/10.1007/BF00992698", "https://pdfs.semanticscholar.org/2820/01869bd502c7917db8b32b75593addfbbc68.pdf", "https://link.springer.com/article/10.1007/BF00992699", "http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html?foxtrotcallback=true", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "http://wiki.teamliquid.net/starcraft2/ByuN", "http://toyoutheartist.co.uk/technique/imitation/", "https://mitpress.mit.edu/books/reinforcement-learning", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html", "https://arxiv.org/abs/1312.5602", "https://www.nature.com/nature/journal/v518/n7540/full/nature14236.html", "https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner", "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/google-analytics-audit-checklist-and-tools-fca7df2f2e7a", "https://blog.statsbot.co/google-analytics-audit-checklist-and-tools-fca7df2f2e7a", "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co/tagged/imitation-learning?source=post", "https://blog.statsbot.co/tagged/data-science?source=post", "https://blog.statsbot.co/tagged/machine-learning?source=post", "https://blog.statsbot.co/tagged/ai?source=post", "https://blog.statsbot.co/tagged/reinforcement-learning?source=post", "https://blog.statsbot.co/@yobibyte?source=footer_card", "https://blog.statsbot.co/@yobibyte", "https://github.com/yobibyte/yobiblog", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "72": {"url": "https://axibase.github.io/atsd-use-cases/DataShorts/Dollar-EX/", "title": "", "text": "Data Source: Federal Reserve Economic Data (FRED) Visualizations: ChartLab Structured Query Language: SQL Console from Axibase Download the Community Edition of Axibase Time Series Database here Data is aggregated daily in visualizations, and annually in SQL queries. Favorable dollar exhange rates are shown in green while less favorable exchange rates are shown in red, based on the overall 5-year average exchange rate. Each section is accompanied by a brief analysis to give the data context. Using computed value settings thresholds have been calculated using the underlying data, the upper 25th percentile of a given exchange rate is considered favorable while the lower 25th percentile is considered less than favorable. The data is coded with an ID which is provided in the index as well. Open any ChartLab display below to navigate through time, or change the metric to reflect the desired exchange rate. Europe Asia North America As the amount of debt carried by various EU member countries grew to increasingly worrisome heights, the Euro plunged in 2015  losing ground against the dollar. Further, as oil prices slumped the European Central Bank began to compensate by launching a program of government bond purchases. The combination of these factors and the growth of the dollar post-recession have all contributed to the currently favorable exchange rate enjoyed by the dollar. Query 1.1 Table 1.1 Figure 1.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index During the worst of the European Debt Crisis which saw European Union per capita debt grow at unprecedented rates across the continent, EU investors sought refuge in the stability of the Swiss Franc, backed by the world-renowned Swiss banking system. Further, the Swiss National Bank removed the peg to the Euro unexpectably in 2015 causing a huge surge in 2015, clearly visible in the visualization below as a sudden change from a favorable exchange rate for dollar holders to an all-time low for the observed time period. Query 1.2 Table 1.2 Figure 1.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Largely influenced by the divisive Brexit vote, the value of the Pound has remained relatively low since the referendum in late June of 2016, which aligns perfectly with the Pound\u2019s slight depreciation in value against the dollar on the chart below. Query 1.3 Table 1.3 Average Exchange Rate: 1.50 Figure 1.3   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index After purchasing trillions of Yen worth of Japanese national debt from the private sector three years ago, the Central Bank of Japan has been watching the gradual decrease in value of the Yen against the dollar. Most analysts, see this fall as controlled and predict a bright future for the Yen as the Central Bank can no longer afford to collect more debt than they already have. However, the Yen\u2019s rise much also be managed as the sudden appreciation of the currency could result in an increase in loan defaults because industries which have taken loans at current Yen prices would be paying them back at a much higher rate. Query 2.1 Table 2.1 Figure 2.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index To prevent dramatic internal inflation, the People\u2019s Bank of China closely regulates local, or onshore, trading of the Yuan by fixing the price each day. Although independent officially, the international, or offshore, price of the Yuan usually remains fairly close to its onshore value. Additionally, the PBoC carefully controls the outflow of capital and often hedges their own currency by maintaining positions in international currencies. Query 2.2 Table 2.2 Figure 2.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index The Hong Kong Dollar is officially pegged to the value of the United States Dollar so any change in relative  value is planned. The \u201cMiracle of the Orient\u201d continues to be one of the most dominant financial markets in the world. Query 2.3 Table 2.3 Figure 2.3   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index One of the financial centers of the world, Singapore is home to the second-busiest port in the world and one of the largest oil refining industries worldwide as well. With one of the highest per capita GDP values, Singapore has cemented its place of international importance despite inexplicably loose financial policy whereby the Monetary Authority of Singapore does not regulate their currency\u2019s value by adjusting interest rates, as is common with most central banking systems. Query 2.4 Table 2.4 Figure 2.4   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Relative to the USD, the Canadian dollar has been making a bit of a resurgence in 2017 as United States GDP numbers missed its targets in quarter two. Although dependant on the price of oil due to their large natural reserves, the diverse Canadian economy is one of the few globally-active nations with a significant trade surplus, although that too have been reduced in recent years. Query 3.1 Table 3.1 Figure 3.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Boasting the highest per capita income of Latin America, the Mexican economy has continued to demonstrate healthy growth over the last several decades. North America\u2019s number one producer of automobiles also enjoys a $46 billion trade surplus with the United States, and is the 9th ranked owner of United States government debt. This dependence on America affords Mexico the ability to spur dramatic internal growth that has led to the creation of a booming electronics sector, but at the cost of a  certain amount of reliance on the United States as roughly half of the country\u2019s imports and exports originate or end up  on U.S. soil. Query 3.2 Table 3.2 Figure 3.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index", "links": ["https://github.com/axibase/atsd-use-cases", "https://fred.stlouisfed.org/categories/94", "https://apps.axibase.com", "https://github.com/axibase/atsd/tree/master/api/sql", "https://axibase.com", "https://axibase.com/products/axibase-time-series-database/", "https://github.com/axibase/atsd/blob/master/installation/README.md#installation", "https://axibase.com/products/axibase-time-series-database/visualization/widgets/configuring-the-widgets/", "https://apps.axibse.com", "https://apps.axibase.com/chartlab/424eb6b2/5/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/9/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/6/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/4/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/2/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/3/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/8/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/7/#fullscreen", "https://github.com/axibase/atsd-use-cases", "https://github.com/axibase", "https://pages.github.com"]}, "73": {"url": "http://byteacademy.co/blog/overview-NLG", "title": "", "text": "NLG (Natural Language Generation), a subfield of Artificial Intelligence, is a hot topic in the technology news today. We hear a lot about AI that can soon replace writers and journalists beginning the era of machine creativity. But, what\u2019s all this fuss about? In this article, we unveil what NLG really is and show that it can bring a lot of benefits to businesses and consumers. In a nutshell, NLG is a sub-field of NLP (Natural Language Processing) that studies methods of automatic transformation of structured data into a human-readable text. In practice, there are two major types of NLG applications: template-based NLG and advanced NLG. \u00a0 Template-based NLG is the simplest solution that uses templates with canned text and placeholders to insert data into them. Such systems heavily rely on hard-coded rules, which makes them less flexible than advanced NLG. Since template-based NLG tools have a limited number of templates and require special data representations, they can not be easily reused across different projects and business use cases. \u00a0 Advanced NLG tools are more flexible thanks to the use of supervised and unsupervised Machine Learning (ML). Rather than tying down structured data to the Procrustean bed of templates, advanced NLG uses neural networks that learn morphological, lexical, and grammar patterns from large corpora of written language. Soft probabilistic methods used in the advanced NLG algorithms allow predicting the likelihood of one word appearing after another, and correcting language errors, such as misspellings. ML algorithms used in the advanced NLG are also better in dealing with new words and expressions not included in the original training samples. \u00a0 Modern NLG service providers such as Narrative Science and Automated Insights prefer advanced NLG methods because they allow creating rich data-driven models that produce intelligent insights from data. These algorithms are much more skillful in making right word choices and writing narratives that reflect intentions and business needs of the NLG users. As an added bonus, advanced NLG models can preprocess and analyze data which makes them not just translators of structured data into text, but automatic analysts able to provide actionable insights. \u00a0 Despite the fact that NLG methods have been used since the 1970s, they got a powerful momentum only recently and thanks to the AI/ML revolution. Today, many startups offer cloud-based NLG services to businesses. NLG is also gaining traction in mass media and journalism. Major American newspapers are already experimenting with the automatic storytelling. For example, in 2016 the Washington Post unveiled its automatic storytelling AI named Heliograph AI. Heliograph was used in the coverage of Rio Olympics and the US Presidential election in 2016. \u00a0 Leveraging data mining techniques and ML models the machine reporter can convert structured statistical data, diagrams, graphs, weather forecasts and other data-rich content into excellent descriptive reports that sound if though they were written by the professional reporters. But, isn\u2019t this dangerous for journalism as a profession? Proponents of automatic storytellers say that they actually free up much time for reporters to add analysis and real insights to stories rather than spending countless hours publishing news and descriptive reports[i]. NLG tools may be used in other innovative ways as well: Benefits of NLG, however, go beyond journalism. There is a growing demand for NLG services among major companies. For example, Quill, an NLG system developed by Narrative Science, is used by such companies as Deloitte, Groupon, and Credit Suisse[iv]. These companies opt for NLG solutions for a reason. Growing acceptance of NLG among businesses makes it a promising field to study. If you want to learn more about NLG, \u00a0Byte Academy offers a Natural Language course that covers Natural Language Processing and Natural Language Generation. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0  Your Name (required)    Your Email (required)    \u00a0 \u00a0 References: [i] \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 WashPost PR Blog (August 5, 2016). The Washington Post Experiments With Automated Storytelling to Help Power 2016 Rio Olympics Coverage. WashPost PR Blog. Retrieved from https://www.washingtonpost.com/pr/wp/2016/08/05/the-washington-post-experiments-with-automated-storytelling-to-help-power-2016-rio-olympics-coverage/?utm_term=.bf63b03c4aeb [ii] \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dayan, Zohar (2015). Hearst, USA Today Sports, & Viralnova Partner With Wibbitz For Video Strategy. Wibbitz Blog. Retrieved from http://blog.wibbitz.com/wibbitz-partners-hearst-usa-today-sports-group-and-viralnova-to-expand-video-strategy [iii] \u00a0\u00a0\u00a0\u00a0\u00a0 Keohane, Joe (2017). What News-Writing Bots Mean for the Future of Journalism. Wired. Retrieved from https://www.wired.com/2017/02/robots-wrote-this-story/ [iv] \u00a0\u00a0\u00a0\u00a0 Narrative Science. Turn Your Data Into Better Decisions With Quill. Retrieved from https://narrativescience.com/Platform info.in@byteacademy.co Byte Dev Licensed by the State of New York, New York State Education Department \u00a9 2017 Byte Academy LLC.  All rights reserved.", "links": ["http://byteacademy.co", "http://byteacademy.co/program/", "http://byteacademy.co/courses/", "http://byteacademy.co/data-science-mini-courses/", "http://byteacademy.co/upcoming-classes/", "http://byteacademy.co/students/", "http://byteacademy.co/hiring/", "http://byteacademy.co/events/", "http://byteacademy.co/blog/", "http://byteacademy.co/india/", "https://byteacademy.fluidreview.com", "http://feeds.feedburner.com/ByteAcademy", "http://byteacademy.co/category/data-science/", "http://byteacademy.co/category/career/", "http://byteacademy.co/category/fintech/", "http://byteacademy.co/category/medtech/", "http://byteacademy.co/category/programming-tips/", "http://byteacademy.co/category/startup/", "http://byteacademy.co/category/student-stories/", "http://byteacademy.co/category/blockchain/", "http://byteacademy.co/category/current-events/", "http://byteacademy.co/category/diversity/", "http://byteacademy.co/category/events/", "http://byteacademy.co/category/finovate/", "http://byteacademy.co/category/light-reads/", "http://byteacademy.co/category/millennials/", "http://byteacademy.co/category/quant/", "http://byteacademy.co/2017/", "http://byteacademy.co/2016/", "http://byteacademy.co/2015/", "http://byteacademy.co/category/data-science/", "http://byteacademy.co/blog/artificial-intelligence-future", "https://www.washingtonpost.com/pr/wp/2016/08/05/the-washington-post-experiments-with-automated-storytelling-to-help-power-2016-rio-olympics-coverage/?utm_term=.bf63b03c4aeb", "http://www.reddit.com/submit?url=http://byteacademy.co/blog/overview-NLG", "http://www.tumblr.com/share?v=3&u=http://byteacademy.co/blog/overview-NLG&t=Overview of Natural Language Generation (NLG)", "http://byteacademy.co/blog/artificial-intelligence-future", "http://byteacademy.co/blog/artificial-intelligence-future", "http://byteacademy.co/blog/data-science-podcasts", "http://byteacademy.co/blog/data-science-podcasts", "http://byteacademy.co/blog/data-science-fintech/", "http://byteacademy.co/blog/data-science-fintech/", "http://byteacademy.co/financial-aid/", "http://byteacademy.co/upcoming-classes/", "http://byteacademy.co/contact-us/", "http://byteacademy.co/corporate-training/", "http://byteacademy.co/faq/", "http://byteacademy.co/press-page/", "http://byteacademy.co/careers-at-byte/", "http://byteacademy.co/brand-ambassador/", "https://www.meetup.com/Byte-Academy-Finance-and-Technology-community/", "http://www.bytedev.co", "https://www.facebook.com/byteacademy/", "https://twitter.com/ByteAcademyCo", "https://www.linkedin.com/edu/school?id=171001", "https://github.com/ByteAcademyCo", "https://www.instagram.com/byteacademy/", "https://www.meetup.com/Byte-Academy-Finance-and-Technology-community/", "https://www.quora.com/topic/Byte-Academy", "https://www.youtube.com/channel/UCrMcJALnO748TSK27bZQzSg", "http://byteacademy.co/privacy-policy/"]}, "74": {"url": "http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "title": "", "text": "", "links": ["http://www.getrevue.co/?ref=Revue+Profile", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profileimage&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profiledate&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profilename&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Mm6Kn?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Mm6Kn?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.datasciencecentral.com/profiles/blogs/automated-machine-learning-for-professionals?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lXBy8?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lXBy8?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.nature.com/articles/s41467-017-00181-8?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/d0nAm?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/d0nAm?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/4mO6D?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/4mO6D?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/eK78N?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/eK78N?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://sourcedexter.com/amazing-tensorflow-github-projects/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xejEQ?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xejEQ?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://techblog.appnexus.com/a-keras-multithreaded-dataframe-generator-for-millions-of-image-files-84d3027f6f43?gi=79cece23ee37&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/6nA72?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/6nA72?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://blog.revolutionanalytics.com/2017/08/a-modern-database-interface-for-r.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/5kyK3?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/5kyK3?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://ruder.io/deep-learning-nlp-best-practices/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/7yxNV?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/7yxNV?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://hyperparameter.space/blog/when-not-to-use-deep-learning/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/8A1NP?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/8A1NP?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278?gi=8e0386afa61e&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/1bKwn?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/1bKwn?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://adeshpande3.github.io/How-I-Used-Deep-Learning-to-Train-a-Chatbot-to-Talk-Like-Me?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lX0x4?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lX0x4?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.youtube.com/channel/UC0n76gicaarsN_Y9YShWwhw/playlists?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/qYVyK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/qYVyK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.youtube.com/watch?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest&v=0fhUJT21-bs", "http://rev.vu/Nkeov?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Nkeov?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%239%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "https://www.getrevue.co/?utm_source=Data Science Digest&utm_medium=email&utm_content=footerlink&utm_campaign=Issue", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=forwarded&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%239%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "http://www.getrevue.co/?ref=Revue+Profile"]}, "75": {"url": "http://rpsychologist.com/d3/bayes/", "title": "", "text": "Created by Kristoffer Magnusson The visualization shows a Bayesian two-sample t test, for simplicity the variance is assumed to be known. It illustrates both Bayesian estimation via the posterior distribution for the effect, and Bayesian hypothesis testing via Bayes factor. The frequentist p-value is also shown. The null hypothesis, H0 is that the effect \u03b4 = 0, and the alternative H1: \u03b4 \u2260 0, just like a two-tailed t test. You can use the sliders to vary the observed effect (Cohen's d), sample size (n per group) and the prior on \u03b4. The prior on the effect is a scaled unit-information prior. The black, and red circle on the curves represents the likelihood of 0 under the prior and posterior. Their likelihood ratio is the Savage-Dickey density ratio, which I use here as to compute Bayes factor. The p-value is the traditional p-value for a two-sample t test with known variance (i.e. a Z test).        HDI is the posterior highest density interval, which in this case is analogous a credible interval. And CI is the traditional frequentist confidence interval.  Check out Alexander Etz's blog series \"Understanding Bayes\" for a really good introduction to Bayes factor. Fabian Dablander also wrote a really good post, \"Bayesian statistics: why and how\", which introduces Bayesian inference in general. If you're interesting in an easy way to perform a Bayesian t test check out JASP, or BayesFactor if you use R. Interactive visualization of Cohen's d effect size Interactive visualization of statistical power and significance testing Interactive visualization of Confidence Intervals Have any suggestion? Or found any bugs? Send them to me, my contact info can be found here. Designed and built by Kristoffer Magnusson. Built with D3.js, jStat and Bootstrap.", "links": ["http://rpsychologist.com", "http://rpsychologist.com", "https://twitter.com/krstoffr", "https://se.linkedin.com/pub/kristoffer-magnusson/b5/133/1b6", "https://twitter.com/share", "http://alexanderetz.com/understanding-bayes/", "http://blog.efpsa.org/2015/08/03/bayesian-statistics-why-and-how/", "https://jasp-stats.org", "http://bayesfactorpcl.r-forge.r-project.org", "http://rpsychologist.com/d3/cohend/", "http://rpsychologist.com/d3/NHST/", "http://rpsychologist.com/d3/CI/", "http://rpsychologist.com/tag/d3js.html", "http://rpsychologist.com/about", "http://rpsychologist.com", "http://d3js.org/", "http://jstat.org/", "http://getbootstrap.com/", "http://twitter.com/krstoffr", "https://github.com/rpsychologist"]}, "76": {"url": "https://blog.datasyndrome.com/generalists-dominate-data-science-f01882f25347", "title": "Generalists Dominate Data\u00a0Science", "text": "Analytics products and systems are best built by small teams of generalists. Large teams of specialists become dominated by communication overhead, and the effect of \u201cChinese whispers\u201d distorts the flow of tasks and stagnates creativity. Data scientists should develop generalist skills to become more efficient members of a data science team. Building data products takes a team covering a broad and diverse skillset. From the customer representative at one end, to the operations engineer at the other, the spectrum of roles in a product analytics team looks like this: Large companies often fill each role with a pair of shoes, resulting in a twelve person team like the one below. The problem with this setup is that it becomes much more difficult to achieve consensus and to perform any task that spans roles. And in data science, most tasks span roles. To look at a particular example, lets focus on the creation of a chart as part of a data product. To begin, a product manager creates a specification, then an interaction designer mocks up the chart, handing it off to a data scientist to fill with data (and hopefully to explore the data and find a chart worth producing), then a back-end engineer to setup an API to grab that data, a front-end web developer to create a web page using the data that matches the mock, and an experience designer to ensure the entire thing feels right and makes sense. Charts take iteration, so this cycle of communication could happen repeatedly for each chart. You can see how communication overhead starts to predominate. A meeting of six people is a full-blown, formal meeting. It is hard to get things done in formal meetings. In the next figure, we see how a data product team might be composed of four generalists: a data engineer, a data scientist/back-end developer, a designer who can build front ends and a product manager that can write marketing copy and cut deals. This is how a startup team would span the skill spectrum, and you can probably see how this makes them more efficient. Revisiting the chart example, creating a chart becomes a collaboration between the product manager, a designer who codes, and a data scientist. This is the kind of ad hoc meeting of 2\u20133 people where \u201cshit gets done\u201d efficiently. This group will be more efficient than the six man group. Put another way: this small team will kick the large team\u2019s ass. In the big company system, sometimes the only way to get anything done efficiently is to go \u201cguerilla generalist\u201d and work with other generalists to cut people out of the chain. This is bad politically, and is part of what drives effective people from big companies. We\u2019ve shown that small teams generalists outperform large teams of specialists. In fact, generalist skills are something every data scientists should work to develop. That doesn\u2019t mean you can\u2019t specialize, but should combine specialization with generalization in order to develop \u201cT-shaped skills.\u201d The T-shaped employee is one that can lend deep expertise across projects while fulfilling multiple roles in his own. It takes time to develop general skills, and that is why the path to becoming a data scientist is not a six month bootcamp, but a ten year journey. Along this path, remember to try to be T-Shaped! Need help building an analytics product or platform? The Data Syndrome team of data scientists and data engineers is available to build your data products and systems as a service. We also offer training in Agile Data Science for all members of data science teams.", "links": ["https://blog.datasyndrome.com?source=logo-lo_b376d128337a---500653fb51a1", "https://twitter.com/datasyndrome", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.datasyndrome.com%2Fgeneralists-dominate-data-science-f01882f25347", "https://blog.datasyndrome.com/@rjurney?source=post_header_lockup", "https://blog.datasyndrome.com/@rjurney?source=post_header_lockup", "https://personalmba.com/communication-overhead/", "https://en.wikipedia.org/wiki/Chinese_whispers", "https://en.wikipedia.org/wiki/T-shaped_skills", "http://datasyndrome.com", "http://datasyndrome.com/training", "https://blog.datasyndrome.com/tagged/data-science?source=post", "https://blog.datasyndrome.com/tagged/agile?source=post", "https://blog.datasyndrome.com/tagged/generalists?source=post", "https://blog.datasyndrome.com/tagged/analytics?source=post", "https://blog.datasyndrome.com/tagged/software-engineering?source=post", "https://blog.datasyndrome.com/@rjurney?source=footer_card", "https://blog.datasyndrome.com/@rjurney", "https://blog.datasyndrome.com?source=footer_card", "https://blog.datasyndrome.com?source=footer_card", "https://blog.datasyndrome.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "77": {"url": "https://github.com/axibase/atsd-use-cases/tree/master/FED_FORDSR#the-average-american-debt-profile", "title": "", "text": "Debt is a complicated concept. After the sub-prime mortgage crisis of the late 2000s, modern Americans are all too familiar with the problems of irresponsible spending on credit. Student loan recipients who queue up to drop off another application for a job in a field they did not study are quick to point to the trappings of deficit spending as a means of wealth creation. Politicians and voters on both sides of the aisle point to the ever-growing United States Government debt with anxiety for the future. And yet despite all the doom and gloom, the American financial system is one of the most stable and robust in the world, in no small part thanks to ingenious monetary policy and hegemonic economic position organized over the entire course of the country's history, modern American consumers are among the wealthiest on the planet. The United States Federal Reserve is the central banking system of the United States, responsible for monitoring the global financial climate and enacting policy that supports the American economy and American consumers. They maintain a number of statistics about these consumers and their monetary practices to better inform their decisions and practices. Provided by the Federal Reserve, this dataset must be correctly parsed during import. The quarterly date format needs to be converted into a monthly format that ATSD can interprete (Q/q letter is not supported). We also need to discard metadata lines contained in the multi-line header. This can be accompilshed with a schema-based parser that provides granular control over the document's rows and columns using RFC 7111 selectors and Javascript: Script 1.1 For step-by-step instructions on data customization with schema based parsing, see this support tutorial. The Financial Obligation Ratio (FOR) is an estimate of the ratio of required debt payments to disposable income. This is a broad calculation and includes all kinds of debt:  mortgage payments, credit cards, property tax and lease payments. Each of these metrics can be expanded further to include associated costs, such as homeowner's insurance for example. The Federal Reserve releases this number each quarter. Figure 1.1   Use the dropdown menus at the top of the visualization screen to navigate through time, selecting the starttime and endtime values to observe a desired period. The data can also be queried with a structured query language in the SQL Console. The data will be aggregated annually, derived from the average value of each quarter within a given year: Query 1.1 Table 1.1 All values are shown as a percent of one hundred, where the whole is representative of the total income of the average person. The Debt Service Ratio (DSR) is more specific than the Financial Obligation Ratio in that it typically does not include non-essential debt payments. Here, it has been parsed into two categories, mortgage debt and consumer debt. These numbers represent the average percent of a person's earned salary each month which much be used to make the required payments associated with consumer credit and mortgage. Typically the DSR is an initial calculation performed to determine a person's eligibility to receive a mortgage. A DSR value of less than 48% is generally preferred, meaning that with a particular mortgage plus other credit obligations at least 52% of a person's gross monthly earning would still be available to them after making the required payments. Figure 2.1   Query 2.1 Table 2.1 Because the FOR value includes the DSR value plus additional non-essential credit values, and the DSR value is parsed into both consumer and mortgage related debt, these three values can be shown in a new visualization that creates a typical consumer profile of the average American. By using the calculated value setting shown below, additional data not specifically included in the set can be displayed: Script 2.1 Shown below is the debt profile of the average American consumer from 1980 to 2017, navigate through time using the dropdown menus at the top of the screen to select a desired span of time and compare how bearing debt has changed over the course of the last three decades. Figure 3.1   The visualization can also be organized to show the amount of each type of debt as it relates to the others: Figure 3.2   Additionally, these values can be compared on an annual basis as shown in the visualization below: Figure 3.3   To view the distribution of these values across time, a histogram is shown below: Figure 3.4   In the following box diagram, explore time with the dropdown menus at the top of the visualization screen. The visualization shows the distribution of debt values as a percentage of total income, with the initial time period set to include the entire data set: Figure 3.5   The following SQL query will detail the above visualizations in one table, displaying averaged annual values of each component described above: non-essential credit payments, mortgage credit payments, and consumer credit payments, as well as the Financial Obligation Ratio (FOR), or total debt obligations. Query 3.1 Table 3.1 The above dataset can illuminate a number of features of the American economy and a number of characteristics of the average American consumer. While modern Americans are quick to denounce the zeitgeist of living outside of one's means, the data shows that in fact, the amount of debt carried by the average American is on par with or even lower in some cases than that of his 1980's counterpart. In fact, the only metric which has demonstrated a legitimate increase in value over the last several decades has been the roughly one percent increase in non-essential credit holdings by the average consumer. According to data from the Economic Research Department of the Saint Louis Branch of the Federal Reserve, the 2015 US median household income was $56,516 per year in 2015 USD. This number can be applied to the above table and visualized in ChartLab to create more comprehensive data. Figure 3.6   The above visualization aggregates the values from Table 3.1 based on a time period of the user's selection. Use the dropdown menu at the top of the screen to select the aggregation period. The initial visualization shows the average values for each metric over the entire period of time in 2015 USD by obligation amount per quarter. The following query summons the same data shown above, but further parses it to show annual average monthly payments instead of quarterly values in 2015 USD for a person making the 2015 median United States income of $56,516 a year. Query 4.3 Table 3.2 As it turns out, the idea that your parents paid less for their house than you will is only true in absolute terms. When compared with current numbers and controlled for inflation, the average 2017 consumer will pay roughly the same portion of their income towards a place to hang their hat up as the average 1980 consumer. The Federal Reserve is able to pull certain levers of power from the Eccles Building in Washington, D.C. such as printing more money, or raising and lowering interest rates to cope with inflation. However, all of these are reactionary measures meant to create small changes that have a butterfly effect over time. Ultimately, the machinations of the Board of Governers have always be something opaque and esoteric to the average man, leading to many people denouncing the Federal Reserve System entirely, occasionally opting for a return of the gold standard or leveling accusations of wrong-doing. However, after reviewing the data above, it seems that at least on a consumer level, the average American actually has more today than they would have had thirty years ago, or even just five years ago. Of course, the Federal Reserve isn't completely responsible for the wise consumer choices made in the current decades, but monetary policy enacted by the various branches of the Federal Reserve are responsible for maintaining the economic conditions that Americans, and consumers the world over, have come to expect from the United States economy.", "links": ["https://github.com/", "https://github.com/axibase/atsd-use-cases/pull/86", "http://www.usdebtclock.org/", "https://www.federalreserve.gov/", "https://www.federalreserve.gov/datadownload/Download.aspx?rel=FOR&series=91e0f9a6b8e6a4b1ef334ce2eaf22860&filetype=csv&label=include&layout=seriescolumn&from=01/01/1980&to=12/31/2017", "https://axibase.com/products/axibase-time-series-database/writing-data/csv/", "https://apps.axibase.com/chartlab/842f1dd9/#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql", "https://apps.axibase.com/chartlab/85522dd3/#fullscreen", "https://apps.axibase.com/chartlab/f25de723/#fullscreen", "https://apps.axibase.com/chartlab/f25de723/3/#fullscreen", "https://apps.axibase.com/chartlab/81ea0ea0/#fullscreen", "https://apps.axibase.com/chartlab/9f74c179/#fullscreen", "https://apps.axibase.com/chartlab/20ff0ade/#fullscreen", "https://fred.stlouisfed.org/series/MEHOINUSA646N", "https://research.stlouisfed.org/", "https://www.stlouisfed.org/", "https://apps.axibase.com", "https://apps.axibase.com/chartlab/da132e01/11/#fullscreen", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "78": {"url": "http://tech.marksblogg.com/billion-nyc-taxi-rides-aws-ec2-p2-16xlarge-brytlytdb.html", "title": "", "text": "I have 14 years of hands-on build and consulting experience with clients in the UK, Ireland & Germany. I've done both back- and frontend work for Bank of America Merrill Lynch, Blackberry, Bloomberg, British Telecom, Ford, Google, ITV, News UK, Pizza Hut, Royal Bank of Scotland, Royal Mail, T-Mobile, Vertu (Nokia subsidiary at the time), Williams Formula 1 Team and Xerox. I hold both a Canadian and a British passport. My CV & my LinkedIn profile.         Home           | Benchmarks           | Archives            | Atom Feed  Posted on Fri 28 July 2017 BrytlytDB is an in-GPU-memory database built on top of PostgreSQL. It's operated using many of PostgreSQL's command line utilities, it's wire protocol compatible so third-party PostgreSQL clients can connect to BrytlytDB and queries are even parsed, planned and optimised by PostgreSQL's regular codebase before the execution plan is passed off to GPU-optimised portions of code BrytlytDB offer. Clustering works right out of the box, GPU-powered JOINs are supported, Stored Procedures are fully functional, Deep- and Machine Learning workloads via Torch are supported and BI visualisation software in the form of SpotLyt is included with BrytlytDB as well. The feature list is too long for one blog to cover so in this post I'll just be taking a look at how performant BrytlytDB's OLAP functionality is. In this benchmark I'll see how well 32 Tesla K80 GPUs spread across two EC2 instances perform when querying 1.1 billion taxi trips. I'll be using the same dataset I've used to benchmark Amazon Athena, BigQuery, ClickHouse, Elasticsearch, EMR, kdb+/q, MapD, PostgreSQL, Redshift and Vertica. I have a single-page summary of all these benchmarks for comparison. For this benchmark I'll be using two p2.16xlarge EC2 instances running Ubuntu 16.04.2 LTS in Amazon Web Services' eu-west-1a region in Ireland. Each machine has 8 Nvidia K80  cards which have 2 GPUs each, 64 virtual CPUs and 732 GB of memory. There's also 20 Gbit/s of networking capacity available to each instance. Below are the specifications of the compute capabilities available each one of the EC2 instances. This is the layout of the GPUs available on one of the two EC2 instances. The first machine has an IP address of 52.214.237.134 and has three roles: Global Transaction Manager, Coordinator and host to the first 16 data nodes. The second machine has an IP address of 34.250.232.38 and has two roles: it is also a coordinator and hosts the second set of 16 data nodes. TCP port 5432 is open between the two machines for communicating via PostgreSQL's wire protocol, TCP port 7777 is open for global transaction manager communication and ports 20,000 through to 20,0031 are open for data node communication. In addition to the default 20 GB EBS volumes on each EC2 instance there are six 500 GB General Purpose SSDs attached which offer a baseline of 100 IOPS each and can burst to 3,000 IOPS if need be. These drives are setup in a RAID 0 configuration on each instance. Before setting up the RAID array I'll install a few dependencies. This was run on both EC2 instances. Below are the commands used to setup the RAID array on each instance. Here's what the RAID layout looked like after it was setup. This RAID 0 setup offers a partition with a capacity of 3.2 TB on each instance: I've run the following to download BrytlytDB's install script. BrytlytDB is commercial software so I cannot divulge the URL I pulled this from at this time. The following was run on both EC2 instances. I've then edited the install script on the first instance with the following instance-specific values: And the install script on the second instance was edited with the following instance-specific values: With those changes in place I then ran the install script on both EC2 instances. The install script conducts the following: Once that's done I can form the cluster with the following registration script. The following was run on the first EC2 instance. The second registration script run on the other EC2 instance is identical with the first with the exception of the coordinator line pointing to 52.214.237.134 instead. For this benchmark I've downloaded and decompressed one half the 500 GB of CSV data I created in my Billion Taxi Rides in Redshift blog post onto each EC2 instance. The data sits across 56 files across both machines but for the quickest load time I want to create 32 reasonably equally-sized CSV files and load 16 files on each EC2 instance simultaneously. The 32 files will pair up with the 32 GPUs available across the cluster and should allow for the best utilisation of the GPU resources when running queries. Here is half of the original 500 GB data set on the first EC2 instance: And this is the other half on the second instance: On each instance I'll decompress the GZIP files. Below took about four minutes to complete on each instance. Then I'll concatenate the ~560 million lines of CSV data on each instance and break it up into files of 35 million lines each. The above completed in 8 minutes on each EC2 instance. I'm now left with 16 CSV files on the first EC2 instance: And 16 files on the other EC2 instance: I'll then connect to the coordinator node on each EC2 instance and setup BrytlytDB's gpu_manager_fdw extension and foreign data wrapper. I can then create the trips table. The above sets the maximum row count per GPU device at 38 million and creates an index on the cab_type column. The cab_type column has very low cardinality so the indexing will be less beneficial versus a column with a higher cardinality of values. Some of the data types used in the above table aren't the types I normally use in my benchmarks. The reason for this is that there is still limited data type support available in this early version of BrytlytDB. I've had to substitute DATETIME with DATE which will truncate the timestamps in the dataset to just the date rather than the full date and time. There is no SMALLINT support yet so I've had to use the larger INT type as an replacement for those fields. DECIMAL types aren't yet supported so I'll be using DOUBLE PRECISION as a replacement there. The above replacements could speed up or slow down the queries I benchmark with so I'm hoping to do another benchmark when data type support is widened in the future. With the table created I'll launch 32 simultaneous load jobs across the two EC2 instances to load the data into BrytlytDB's trips table. This is the import script I ran on the first EC2 instance: And this is the load script for the second instance. The first EC2 instance loaded its half of the dataset in 1 hour, 12 minutes and 21 seconds. The second instance loaded its half in 1 hour, 9 minutes and 57 seconds. After the data was loaded in I could see the PostgreSQL data directory filled up with reasonably evenly-sized data folders. Here's what they look like on the first EC2 instance: I was then able to use PostgreSQL's CLI tool to make sure I can see the table and all 1.1 billion records. The times quoted below are the lowest query times seen during a series of runs. As with all my benchmarks, I use the lowest query time as a way of indicating \"top speed\". The following completed in 0.762 seconds. The following completed in 2.472 seconds. The following completed in 4.131 seconds. The following completed in 6.041 seconds. Given all the features of PostgreSQL are still available I'm blown away at how fast BrytlytDB is able to aggregate data. The cluster I used cost around $30 / hour which means this system out performed other Cloud-based and PostgreSQL-based data warehousing solutions both in terms of wall clock time and in terms of cost per hour of running the cluster. That being said I know Nvidia's K80 chips use extremely fast memory and have thousands of compute cores so I expect further optimisations from BrytlytDB to drive down these already fast query times. \u2190 Back to Index Copyright \u00a9 2014 - 2017 Mark Litwintschik. This site's template is based off a template by Giulio Fidente.", "links": ["https://twitter.com/marklit82", "http://tech.marksblogg.com/theme/cv.pdf", "https://uk.linkedin.com/in/marklitwintschik/", "http://tech.marksblogg.com/feeds/all.atom.xml", "http://www.brytlyt.com/", "https://uk.linkedin.com/in/marklitwintschik/", "https://github.com/giulivo/pelican-svbhack"]}, "79": {"url": "https://blog.datazar.com/why-we-focus-on-design-just-as-much-as-functionality-f6ccb3b2c926", "title": "Why We Focus on Design Just as Much as Functionality", "text": "It\u2019s no secret that scientific software isn\u2019t the most beautiful software in terms of design/aesthetics (generally speaking here). It is very effective though; it gets the job done. It does exactly what you expect it to do. Being a huge movie buff, I\u2019ve always wondered why the software we use in the laboratories or classrooms looks nothing like what we see in the movies. Movies do this thing where tech looks exciting and futuristic and super fun to use. And then I go back to reality and it\u2019s nothing like what I just watched. It\u2019s worth mentioning that not everything in the movies looks practical, especially when it comes to software or even more ridiculous: computer hacking. But that\u2019s another subject. The fact there\u2019s some art missing is something I noticed not only in the software we use in the lab, but also the instruments we build and the papers we write. It seems as if the art has been completely taken out of the sciences. Before you say it, we obviously can\u2019t incorporate art into somethings like highly sensitive instruments as unnecessary additions area big no because then you\u2019re interfering. Another thing to note might be the fact that hundreds or even some thousands years ago, the scientists were also artists. Art came naturally to them as they were also discovering mother nature. At that time, science was also only available to the rich which meant a couple of things: they could afford to spend a little bit of time incorporating art into their scientific work as they weren\u2019t rushing to meet grant deadlines, but it also meant they were already educated in the arts from an early age. These days anyone can be whatever they want to be. To the very least, it\u2019s easier now than it was in that era by an order of magnitude. Today we\u2019re taught that art almost has no place in the sciences and we must be efficient and to the point. Anything extra is generally referred to as fluff. If you like to romanticize science, you\u2019re not really a scientist. But I strongly believe that it\u2019s the artistic and I might add\u200a\u2014\u200aphilosophical\u200a\u2014\u200afluff that gets people interested in science. The hard math and core scientific values obviously being there regardless. All of this results in the absence of art in the sciences which makes art seem dry and boring. Science in its purest form is absolutely beautiful if communicated right. That\u2019s why we love to do our part at Datazar. We\u2019re far, far from perfect or even close to what we want to be. But putting just as much thought in design and UX as we put in functionality is a core principle. There\u2019s a misconception that incorporating art means adding more stuff. Incorporating art can also mean doing less, using less and making the most of what you already have in a beautiful and simple way. An example would be removing that obscene amount of jargon from your paper. So I ask everyone designing software for scientists, design for humans not robots. Scientists are humans too. So go ahead add that CSS animation.", "links": ["https://blog.datazar.com?source=logo-lo_4d588bad5343---e2c7e6e1c75", "https://twitter.com/DatazarHQ", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.datazar.com%2Fwhy-we-focus-on-design-just-as-much-as-functionality-f6ccb3b2c926", "https://blog.datazar.com", "https://blog.datazar.com/tagged/r-language", "https://blog.datazar.com/tagged/open-data", "https://blog.datazar.com/tagged/how-to", "https://www.r-bloggers.com/", "https://blog.datazar.com/search", "https://blog.datazar.com/@ata_aman?source=post_header_lockup", "https://blog.datazar.com/@ata_aman?source=post_header_lockup", "https://blog.datazar.com/tagged/science?source=post", "https://blog.datazar.com/tagged/art?source=post", "https://blog.datazar.com/tagged/research?source=post", "https://blog.datazar.com/tagged/software-development?source=post", "https://blog.datazar.com/tagged/design?source=post", "https://blog.datazar.com/@ata_aman?source=footer_card", "https://blog.datazar.com/@ata_aman", "http://twitter.com/datazarhq", "https://blog.datazar.com?source=footer_card", "https://blog.datazar.com?source=footer_card", "https://blog.datazar.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "80": {"url": "https://news.ycombinator.com/item?id=14950255", "title": "", "text": "", "links": ["https://news.ycombinator.com", "https://edgylabs.com/machine-learning-to-enhance-smartphone-pictures/", "https://hn.algolia.com/?query=Machine%20Learning%20to%20Enhance%20Smartphone%20Pictures&sort=byDate&dateRange=all&type=story&storyText=false&prefix&page=0", "https://www.google.com/search?q=Machine%20Learning%20to%20Enhance%20Smartphone%20Pictures", "https://github.com/HackerNews/API", "http://www.ycombinator.com/apply/"]}, "81": {"url": "http://starmine.ai/datasets/ds02.html", "title": "", "text": "reddit Slack Twitter Instagram Facebook LinkedIn", "links": ["http://starmine.ai", "http://reddit.com/r/datasets", "https://join.slack.com/t/starmineai/shared_invite/MjIwNjg5OTUyODAzLTE1MDE2MTk1MzQtMmJhYTExNDViMA", "https://twitter.com/starmineAI", "https://www.instagram.com/starmine.ai/", "https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Fstarmine.ai/index.html&t=starmine%20ICO", "https://www.linkedin.com/cws/share?url=http%3A%2F%2Fstarmine.ai/index.html&token=&isFramed=true", "http://research.kraeutli.com/index.php/2013/11/the-tate-collection-on-github", "http://54.174.116.134/recommend/datasets/subscribers/free/supercolumns/supercolumns-elements-CMDB-nasdaq-nyse-otcbb-general-2017/supercolumns-elements-nasdaq-nyse-otcbb-general-UPDATE-2017-03-01.csv", "http://54.174.116.134/recommend/datasets/subscribers/free/supercolumns/supercolumns-elements-CMDB-nasdaq-nyse-otcbb-general-2017/supercolumns-elements-nasdaq-nyse-otcbb-general-UPDATE-2017-03-01.csv", "http://54.174.116.134/recommend/app/ai_connect-finance-historicaltrends_api?ccl1=playstation&ccl2=helium&ccl3=korea&ccl4=shampoo&ccl5=coffee&query=concept_column_labels&themesource=theme_CMDB-nasdaq-nyse-otcbb&month=01&year=all&db=CMDB-nasdaq-nyse-otcbb-general", "https://www0.gsb.columbia.edu/mygsb/faculty/research/pubfiles/1555/contagious.pdf", "https://trends.google.com/trends/explore?q=playstation,helium,korea,shampoo,coffee", "https://trends.google.com/trends/explore?q=playstation,helium,korea,shampoo,coffee"]}, "82": {"url": "https://github.com/maciejkula/spotlight/blob/master/examples/movielens_sequence/readme.rst", "title": "", "text": "Using sequences of user-item interactions as an input for recommender models has a number of attractive properties. Firstly, it recognizes that recommending the next item that a user may want to buy or see is precisely the goal we are trying to achieve. Secondly, it's plausible that the ordering of users' interactions carries additional information over and above just the identities of items they have interacted with. For example, a user is more likely to watch the next episode of a given TV series if they've just finished the previous episode. Finally, when the sequence of past interactions rather than the identity of the user is the input to a model, online systems can incorporate new users (and old users' new actions) in real time. They are fed to the existing model, and do not require a new model to be fit to incorporate new information (unlike factorization models). Recurrent neural networks are the most natural way of modelling such sequence problems. In recommendations, gated recurrent units (GRUs) have been used with success in the Session-based recommendations with recurrent neural networks paper. Spotlight implements a similar model using LSTM units as one of its sequence representations. But recurrent neural networks are not the only way of effectively representing sequences: convolutions can also do the job. In particular, we can use causal convolutions: convolutional filters applied to the sequence in a left-to-right fashion, emitting a representation at each step. They are causal in that the their output at time t is conditional on input up to t-1: this is necessary to ensure that they do not have access to the elements of the sequence we are trying to predict. Like LSTMs, causal convolutions can model sequences with long-term dependencies. This is achieved in two ways: stacking convolutional layers (with padding, every convolutional layer preserves the shape of the input), and dilation: insertion of gaps into the convolutional filters (otherwise known as atrous convolutions). Causal convolutions have been used in several recent high-profile papers: Using convolutional rather than recurrent networks for representing sequences has a couple of advantages, as described in this blog post: Spotlight implements causal convolution models as part of its sequence models package, alongside more traditional recurrent and pooling models. The Spotlight implementation has the following characteristics: The model is trained using one of Spotlight's implicit feedback losses, including pointwise (logistic and hinge) and pairwise (BPR as well as WARP-like adaptive hinge) losses. As with other Spotlight sequence models, the loss is computed for all the time steps of the sequence in one pass: for all timesteps t in the sequence, a prediction using elements up to t-1 is made, and the loss is averaged along both the time and the minibatch axis. This leads to siginficant training speed-ups relative to only computing the loss for the last element in the sequence. To see how causal CNNs compare to more traditional sequence models we can have a look at how they perform at predicting the next rated movie on the Movielens 1M dataset. With 1 million interactions spread among 6000 users and around 4000 movies it should be small enough to run quick experiments, but large enough to yield meaningful results. I chose to split the dataset into 80% train, and 10% test and validation sets. I construct 200-long sequences by splitting each user's item sequence into 200-long chunks; if a chunk is shorter than 200 elements, it's padded with zeros. I use mean reciprocal rank (MRR) as the evaluation metric. To choose hyperparameters, I run a quick, coarse grained hyperparameter search, using random sampling to draw 100 hyperparameter sets. With the data and hyperparameters ready, fitting and evaluating the model is relatively simple: Fitting the models is fairly quick, taking at most two or three minutes on a single K80 GPU. The code for the experiments is available in the experiments folder of the Spotlight repo. The results are as follows: It's difficult to draw clear-cut conclusions about the effect of each hyperparameter, but it looks like: To compare causal convolutions with more traditional sequence models I run similar hyperparameter searches for LSTM-based representations and pooling representations. The pooling representation is a simple averaging of item embedding across the sequence; the LSTM-based model runs an LSTM along a user's interactions, using the hidden state for prediction of the next element at each step. The results are as follows: A single layer LSTM seems to outperform causal convolutions, by an over 10% margin, helped by the adaptive hinge loss. Simple pooling performs quite badly. It looks like causal convolutions need some more work before beating recurrent networks. There are a couple of possible avenues for making them better: I'd love to get some input on these. If you have suggestions, let me know on Twitter or open an issue or PR in Spotlight.", "links": ["https://github.com/", "https://arxiv.org/abs/1511.06939", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.LSTMNet", "https://travis-ci.org/maciejkula/spotlight", "https://arxiv.org/pdf/1609.03499.pdf", "http://papers.nips.cc/paper/6527-conditional-image-generation-with-pixelcnn-decoders", "https://arxiv.org/abs/1610.10099", "https://medium.com/@TalPerry/convolutional-methods-for-text-d5260fd5675f", "https://arxiv.org/pdf/1610.10099.pdf", "https://maciejkula.github.io/spotlight/sequence/sequence.html", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.CNNNet", "https://maciejkula.github.io/spotlight/losses.html", "https://grouplens.org/datasets/movielens/1m/", "https://en.wikipedia.org/wiki/Mean_reciprocal_rank", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.LSTMNet", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.PoolNet", "https://maciejkula.github.io/spotlight/losses.html#spotlight.losses.adaptive_hinge_loss", "https://twitter.com/Maciej_Kula", "https://github.com/maciejkula/spotlight", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "83": {"url": "https://github.com/ynqa/word-embedding", "title": "", "text": "GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together.  Sign up             Use Git or checkout with SVN using the web URL.             This is an implementation of word embedding (also referred to as word representation) models in Golang. Word embedding makes words' meaning, structure, and concept mapping into vector space (and low dimension). For representative instance: Like this example, it could calculate word meaning by arithmetic operations between vectors. Listed models for word embedding, and checked it already implemented. Downloading text8 corpus, and training by Skip-Gram with negative sampling.", "links": ["https://github.com/", "https://github.com/ynqa/word-embedding/blob/master/LICENSE", "https://help.github.com/articles/which-remote-url-should-i-use", "https://travis-ci.org/ynqa/word-embedding", "https://godoc.org/github.com/ynqa/word-embedding", "https://goreportcard.com/report/github.com/ynqa/word-embedding", "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf", "http://nlp.stanford.edu/pubs/glove.pdf", "https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf", "http://mattmahoney.net/dc/textdata", "https://github.com/cjlin1/libsvm", "http://www.aclweb.org/anthology/Q15-1016", "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.648.8023&rep=rep1&type=pdf", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "84": {"url": "https://github.com/brannondorsey/keras_weight_animator", "title": "", "text": "GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together.  Sign up             Use Git or checkout with SVN using the web URL.          Save Keras weight matrices as short animated videos to better understand what and how your neural network models are learning. Below are examples of the first LSTM layer and the final output layer of a six-class RNN model trained over one epoch. Blue represents low values and red represents high values.   In order to render videos from the saved weight images you must also have the following packages installed on your machine: This module is named keras_weight_animator. It exposes a Keras callback function that you can include in any model fit(...) method. The two required parameters to image_saver_callback(...) are the Keras model and an output_directory to periodically save weight images to. By default, keras_weight_animator saves layer weights every 100 batches to output_directory as PNGs in folders named epoch_XXX-layer_NAME-weights_YY. Once training is complete, you can optionally create short animated video clips from the image sequences saved in output_directory using [bin/create_image_sequence.sh](bin/create_image_sequence.sh) path/to/output_directory. This will use parallel, mogrify, and ffmpeg to create a .mp4 from the image sequences located in each folder of output_directory. Video files will be named like epoch_XXX-layer_NAME-weights_YY.mp4. You can run this script automatically from your training script by passing the render_videos=True parameter to image_saver_callback(...). weight_image_sequences(...) takes a variety of optional keyword arguments. I've included an example usage of the module in examples/wisdm.py. This example uses smartphone accelerometer data from WISDM to classify human activity tasks like walking, standing, sitting, walking upstairs, etc... This example uses a one layer LSTM to classify a set of 60 data points (representing three seconds of data sampled at 20hz) as belonging to one of six classes. It outputs image sequences and videos to data/wisdm. Using a bash script to leverage parallel, ImageMagick, and FFMPEG isn't necessarily the most elegant solution, but its the one I had time for. The goal of this here lil' project was to write a quick tool that allows me to better understand how weights change over mini-batch updates in a variety of neural networks. Perhaps in the future I will come back and clean up some of the inelegancies. If you have interest in contributing or maintaining a cleaner version of this lib, please reach out at brannon@brannondorsey.com. This module is \u00a9 Brannon Dorsey 2017, released under an \u2665 MIT License \u2665. You are free to use, modify, distribute, sell, etc... this software under those terms. Example data is from the WIreless Sensor Datamining (WISDM) Actitracker dataset published by Fordham University: This idea is tangentially inspired by much of the work on Visualizing and Understanding Recurrent Networks by Andrej Karpathy, Justin Johnson, and Fei-Fei Li. GNU Parallel is adamant about citation to the point of excess IMHO, but for what its worth, here is their bibtex:", "links": ["https://github.com/", "https://github.com/brannondorsey/keras_weight_animator/blob/master/LICENSE", "https://help.github.com/articles/which-remote-url-should-i-use", "https://github.com/brannondorsey/keras_weight_animator/pull/1", "http://keras.io", "https://www.gnu.org/software/parallel/", "https://www.imagemagick.org/script/index.php", "https://ffmpeg.org/download.html", "https://keras.io/callbacks/", "http://matplotlib.org/users/colormaps.html", "http://www.cis.fordham.edu/wisdm/dataset.php", "https://arxiv.org/abs/1506.02078", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "85": {"url": "https://axibase.github.io/atsd-use-cases/Expatriation_Q2/", "title": "", "text": "Expatriation is the temporary or permanent relocation of a person by choice or by force from their native country to any other country in the world for work, pleasure, or purpose. History has seen expatriation from nearly every country in the world up to and including the current epoch. Famous American expatriates throughout history have included American founding father Benjamin Franklin, who worked as the first United States Minister to France for almost a decade after appointment by the Continental Congress in 1778. The  sixth President of the United States John Quincy Adams, who served as a U.S. Diplomat in four countries before winning the Presidency as a Democratic-Republican in 1825, most notably opening American diplomatic ties as the first United States Minister to the Russian Empire, a position he loved  so much he declined a subsequent offer to serve as an Associate Justice on the Supreme Court of the United States a few years later. American expatriates have come from all walks of life: authors, such as Ernest Hemingway who lived in China, Spain, Cuba, and was even present on the beach codenamed Omaha in Normandy during the allied landing. Musicians, like Jimi Hendrix who rose to international fame in London after leaving his native Seattle, directors like Stanley Kubrick, singers like Tina Turner, and even basketball players like  Allen Iverson, who played an incomplete season for a Turkish basketball team in 2010 after retiring from the NBA. For those who decide to make the change permanent, there comes a time when the inconvenience of living abroad as a resident alien outweighs the novelty and many decide to renounce their citizenship for one reason or another. Unsurprisingly, this is a long and complicated process, one of the main features of which is settling up with the Internal Revenue Service (IRS) via the eponymous Expatriation Tax. The Federal Register is a publishing outlet for the United States Government where the IRS officially releases quarterly information about American citizens who renounce their passport in lieu of a new one and other long-term residents who have decided to repatriate, view the raw data here and use the Axibase Data Crawler designed specifically for collecting, parsing, organzing and inserting historical expatriation data and new data as it becomes available.  Axibase covered last quarter\u2019s expatriation release here and is using this quarter\u2019s release to follow-up. Every election cycle countless celebrities and netizens issue their promise to leave if their candidate fails to get elected and the growing notoriety surrounding the 2016 Presidental Election has proven to  be no different. This type of social theater is almost as old as the country itself as there have been a number of similarly divisive elections throughout American history: Andrew Jackson\u2019s victory in 1828 was seen as a populace uprising against the corrupt political elite and a return to the authority of the common man, Abraham Lincoln\u2019s second term was viewed as the tightening of federal authority to excessive levels that ultimately led to the American Civil War, Franklin Roosevelt was called a dangerous communist whose infamous court-packing plan favorably increased the number of seats on the Supreme Court to his  advantage and was seen as abhorrent and traitorous by many, and of course who can forget recent elections that have featured scandal after scandal and two modern Presidents who served after losing the popular vote, but winning in the electorate. Figure 1   Query 1 Data queried in the SQL Console. Table 1 To give the issue further context, Figure 1 and Table 1 use the most current Department of Homeland Security (DHS) data  available to display current naturalization numbers in the United States. Naturalization is the sister process to expatriation, because as a person expatriates from one country they must be naturalized in another or end up a stateless person. Naturalization numbers in America have remained fairly constant throughout the last decade because there are a number of federal regulations that control the amount of long-term immigrants that America accepts each year and that quota is met without fail. As noted in the our Q1 article, the sister value and subject of this examination, expatriation numbers, has been steadily growing for several consecutive years. Figure 2.1   Query 2 The above query features robust syntax and calculated values. See the following tutorials  to understand more about SQL Console. Table 2 Figure 2.2   Expatriation has been increasing each year by roughly 30% since 2010, which featured abnormally high expatriation rates, most likely attributable to the economic turndown of the Great Recession which began in the United States as a result of the sub-prime mortgage crisis. American citizenship is often sought after for the economic opportunity that comes along with the passport, as the ability to work and do business in the country is heavily restricted or regulated, and with the value of that investment or  opportunity in question, it is unsurprising that the number of investors in the system, that is, new citizens, would fluctuate. It seems appropriate to call naturalization an investment because of the nature of the process, which is long, complicated, and often quite expensive similar to a long-position that will cost more at purchasing time but promises high returns after reaching maturity. The peak, or more appropriately, valley of the global recession occurred in 2009 when the global GDP contracted causing a decline in the median familty income of about five percent. Figure 3   Query 3 Table 3 Reducing the timespan to look at data over the last 5 years captures the repetition of this trend for this period of time. Of the eighteen quarters included in this query, only six of them, roughly 33% showed negative growth for this statistic. Growing expatriation numbers provide interesting insight into the perception of America both domestically and abroad, as these numbers not only include U.S. citizens who renounce their citizenship, but also long-term resident aliens who have returned home. Almost unbelievably, the IRS does not strip characteristic information about expatriates from their publications meaning that first and last names are included in the data. Using Social Security data concerning the most common first names chosen throughout the last century and United States Census data concerning the most common last names to compare, the following queries and visualizations show  the number of expatriates by common first and common last name throughout the entire observed period, ranked by their frequency in the IRS publication. The goal of these queries is purely demonstrative and lighthearted. There is no intention to invade privacy, defame people, or otherwise cause harm. Figure 4  Query 4 Table 4 All of these names appear on the list of most common American first names, linked here Figure 5  Query 5 Table 5 Only the name Smith appears on the list of most common American last names, linked here Use the Axibase Data Crawler to collect the data used in this artcle and examine it yourself in ChartLab.", "links": ["https://github.com/axibase/atsd-use-cases", "https://www.irs.gov/", "https://www.irs.gov/individuals/international-taxpayers/expatriation-tax", "https://www.federalregister.gov/", "https://www.federalregister.gov/documents/2017/05/10/2017-09475/quarterly-publication-of-individuals-who-have-chosen-to-expatriate-as-required-by-section-6039g", "https://github.com/axibase/atsd-data-crawlers/tree/irs-expatriation-data-crawler", "https://axibase.com", "https://apps.axibase.com/chartlab/654b9945#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql", "https://www.uscis.gov/news/fact-sheets/naturalization-fact-sheet", "http://www.unhcr.org/stateless-people.html", "https://apps.axibase.com/chartlab/7fa5b643#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql#examples", "https://apps.axibase.com/chartlab/7fa5b643/5/#fullscreen", "https://web.stanford.edu/group/recessiontrends/cgi-bin/web/sites/all/themes/barron/pdf/IncomeWealthDebt_fact_sheet.pdf", "https://apps.axibase.com/chartlab/7fa5b643/2/#fullscreen", "https://www.ssa.gov/oact/babynames/decades/century.html", "http://www.census.gov/main/www/cen2000.html", "https://www.ssa.gov/oact/babynames/decades/century.html", "http://www.census.gov/main/www/cen2000.html", "https://github.com/axibase/atsd-data-crawlers/tree/irs-expatriation-data-crawler", "https://apps.axibase.com", "https://github.com/axibase/atsd-use-cases", "https://github.com/axibase", "https://pages.github.com"]}, "86": {"url": "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/", "title": "", "text": "Machine learning is eating the world right now. Everyone and their mother are learning about machine learning models, classification, neural networks, and Andrew Ng. You\u2019ve decided you want to be a part of it, but where to start? In this article we\u2019ll cover some important characteristics of Python and why it\u2019s great for machine learning. We\u2019ll also cover some of the most important libraries it has for ML, and if it piques your interest, some places where you can learn more. Python is a great choice for machine learning for several reasons. First and foremost, it\u2019s a simple language\u00a0on the surface; even if you\u2019re not familiar with Python, getting up to speed is very quick if you\u2019ve ever used any other language with C-like syntax (i.e. every language out there). Second, Python has a great community, which results in good documentation and friendly, comprehensive answers in StackOverflow (fundamental!). Third, also stemming from the great community, there are plenty of useful libraries for Python (both as \u201cbatteries included\u201d and third party), which solve basically any problem that you can have (including machine learning). Yeah and it\u2019s true. Python isn\u2019t the fastest language out there: all those handy abstractions come at a cost. But here\u2019s the trick: libraries can and do offload the expensive calculations to the much more performant (but harder to use) C and C++. For instance, there\u2019s NumPy, which is a library for numerical computation. It\u2019s written in C, and it\u2019s fast. Practically every library out there that involves intensive calculations uses it \u2014 almost all the libraries listed next use it in some form. So if you read NumPy, think fast. Therefore, you can make your scripts run basically as fast as straight up writing them in a lower level language. So there\u2019s really nothing to worry about when it comes to speed. Are you starting out in machine learning? Want something that covers everything from feature engineering to training and testing a model? Look no further than scikit-learn! This fantastic piece of free software provides every tool necessary for machine learning and data mining. It\u2019s the de facto standard library for machine learning in Python, recommended for most of the \u2018old\u2019 ML algorithms. This library does both classification and regression, supporting basically every algorithm out there (support vector machines, random forest, naive bayes, and so on). It\u2019s built in such a way that allows easy switching of algorithms, so experimentation is easy. These \u2018older\u2019 algorithms are surprisingly resilient and work very well in a lot of cases. But that\u2019s not all! Scikit-learn also does dimensionality reduction, clustering, you name it. It\u2019s also blazingly fast since it runs on NumPy and SciPy (meaning that all the heavy number crunching is run on C instead of Python). Check out some examples to see everything this library is capable of, and the tutorials if you want to learn how it works. While not a machine learning library per se, NLTK is a must when working with natural language processing (NLP). It comes with a bundle of datasets and other lexical resources (useful for training models) in addition to libraries for working with text \u2014 for functions such as classification, tokenization, stemming, tagging, parsing and more. The usefulness of having all of this stuff neatly packaged can\u2019t be overstated. So if you are interested in NLP, check out some tutorials! Used widely in research and academia, Theano is the grandfather of all deep learning frameworks. Written in Python, it\u2019s tightly integrated with NumPy. Theano allows you to create neural networks, which are represented as mathematical expressions with multi-dimensional arrays. Theano handles this for you so you don\u2019t have to worry about the actual implementation of the math involved. It supports offloading calculations to the much faster GPU, which is a feature that everyone supports today, but back when they introduced it this wasn\u2019t the case. The library is very mature at this point and supports a very wide range of operations, which is a great plus when it comes to comparing it with other similar libraries. The biggest complaint out there is that the API may be unwieldy for some, making the library hard to use for beginners. However, there are wrappers that ease the pain and make working with Theano simple, such as Keras, Blocks and Lasagne. Interested in learning about Theano? Check out this Jupyter Notebook tutorial. The Google Brain team created TensorFlow for internal use in machine learning applications, and open sourced it in late 2015. They wanted something that could replace their older, closed source machine learning framework, DistBelief, which they said wasn\u2019t flexible enough and too tightly coupled to their infrastructure to be shared with other researchers around the world. And so TensorFlow was created. Learning from the mistakes of the past, many consider this library to be an improvement over Theano, claiming more flexibility and a more intuitive API. Not only can it be used for research but also for production environments, supporting huge clusters of GPUs for training. While it doesn\u2019t support as wide a range of operations as Theano, it has better computational graph visualizations. TensorFlow is very popular nowadays. In fact, if you\u2019ve heard about a single library on this list, it\u2019s probably this one: there isn\u2019t a day that goes by without a new blog post or paper mentioning TensorFlow gets published. This popularity translates into a lot of new users and a lot of tutorials, making it very welcoming to beginners. Keras is a fantastic library that provides a high-level API for neural networks and is capable of running on top of either Theano or TensorFlow. It makes harnessing the full power of these complex pieces of software much easier than using them directly. It\u2019s very user-friendly, putting user experience as a top priority. They manage this by using simple APIs and excellent feedback on errors. It\u2019s also modular, meaning that different models (neural layers, cost functions, and so on) can be plugged together with little restrictions. This also makes it very easy to extend, since it\u2019s simple to add new modules and connect them with the existing ones. Some people have called Keras so good that it is effectively cheating in machine learning. So if you\u2019re starting out with deep learning, go through the examples and documentation to get a feel for what you can do with it. And if you want to learn, start out with this tutorial and see where you can go from there. Two similar alternatives are Lasagne and Blocks, but they only run on Theano. So if you tried Keras and are unhappy with it, maybe try out one of these alternatives to see if they work out for you. Another popular deep learning framework is Torch, which is written in Lua. Facebook open-sourced a Python implementation of Torch called PyTorch, which allows you to conveniently use the same low-level libraries that Torch uses, but from Python instead of Lua. PyTorch is much better for debugging since one of the biggest differences between Theano/TensorFlow and PyTorch is that the former use symbolic computation while the latter doesn\u2019t. Symbolic computation means that coding an operation (say, \u2018x + y\u2019), it\u2019s not computed when that line is interpreted. Before getting executed it has to be compiled (translated to CUDA or C). This makes debugging harder in Theano/TensorFlow, since an error is much harder to associate with the line of code that caused it. Of course, doing things this way has its advantages, but debugging isn\u2019t one of them. If you want to start out with PyTorch the official tutorials are very friendly to beginners but get to advanced topics as well. Alright, you\u2019ve presented me with a lot of alternatives for machine learning libraries in Python. What should I choose? How do I compare these things? Where do I start? Our Ape Advice\u2122 for beginners is to try and not get bogged down by details. If you\u2019ve never done anything machine learning related, try out scikit-learn. You\u2019ll get an idea of how the cycle of tagging, training and testing work and how a model is developed. Now, if you want to try out deep learning, start out with Keras \u2014 which is widely agreed to be the easiest framework \u2014 and see where that takes you. After you have more experience, you will start to see what it is that you actually want from the framework: greater speed, a different API, or maybe something else, and you\u2019ll be able to make a more informed decision. And even then, there is an endless supply of articles out there comparing Theano, Torch, and TensorFlow. There\u2019s no real way to tell which one is the good one. It\u2019s important to take into account that all of them have wide support and are improving constantly, making comparisons harder to make. A six month old benchmark may be outdated, and year old claims of framework X doesn\u2019t support operation Y could no longer be valid. Finally, if you\u2019re interested in doing machine learning specifically applied to NLP, why not check out MonkeyLearn! Our platform provides a unique UX that makes it super easy to build, train and improve NLP models. You can either use pre-trained models for common use cases (like sentiment analysis, topic detection or keyword extraction) or train custom algorithms using your particular data. Also, you don\u2019t have to worry about the underlying infrastructure or deploying your models, our scalable cloud does this for you. You can start for free and integrate right away with our beautiful API. There are plenty of online resources out there to learn about machine learning ! Here are a few: So that was a brief intro to machine learning in Python and some of its libraries. The important part is not getting bogged down by details and just trying stuff out. Follow your curiosity, and don\u2019t be afraid to experiment. Know about a python library that was left out? Share it in the comments below!", "links": ["http://www.monkeylearn.com", "http://monkeylearn.com/use-cases/", "http://monkeylearn.com/docs/", "http://www.monkeylearn.com/pricing", "http://www.monkeylearn.com/blog/", "https://app.monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/?utm_source=ml-blog&utm_medium=header&utm_campaign=blog", "https://monkeylearn.com/blog", "http://monkeylearn.com/use-cases/", "http://monkeylearn.com/docs/", "http://www.monkeylearn.com/pricing", "http://www.monkeylearn.com/blog/", "https://app.monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/?utm_source=ml-blog&utm_medium=header&utm_campaign=blog", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/07/Post_2c.png", "http://scikit-learn.org/", "http://scikit-learn.org/stable/auto_examples/index.html#general-examples", "http://scikit-learn.org/stable/tutorial/index.html", "http://www.nltk.org/", "http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk", "http://deeplearning.net/software/theano/", "https://keras.io/", "https://github.com/mila-udem/blocks", "https://github.com/Lasagne/Lasagne", "http://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/Theano%20Tutorial.ipynb", "https://www.tensorflow.org/", "https://www.tensorflow.org/get_started/graph_viz", "https://www.tensorflow.org/tutorials/", "https://keras.io/", "https://news.ycombinator.com/item?id=13872764", "https://github.com/fchollet/keras#getting-started-30-seconds-to-keras", "https://keras.io/getting-started/functional-api-guide/", "https://elitedatascience.com/keras-tutorial-deep-learning-in-python", "https://github.com/Lasagne/Lasagne", "https://github.com/mila-udem/blocks", "http://torch.ch/", "http://pytorch.org/", "http://pytorch.org/tutorials/", "http://scikit-learn.org/stable/tutorial/basic/tutorial.html", "https://elitedatascience.com/keras-tutorial-deep-learning-in-python", "https://www.quora.com/Is-TensorFlow-better-than-other-leading-libraries-such-as-Torch-Theano", "http://www.ccri.com/2016/12/09/torch-vs-tensorflow-vs-theano/", "https://www.reddit.com/r/MachineLearning/comments/5w3q74/d_so_pytorch_vs_tensorflow_whats_the_verdict_on/", "https://github.com/zer0n/deepframeworks/blob/master/README.md", "https://monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/", "http://nbviewer.jupyter.org/github/rhiever/Data-Analysis-and-Machine-Learning-Projects/blob/master/example-data-science-notebook/Example%20Machine%20Learning%20Notebook.ipynb", "https://blog.monkeylearn.com/a-gentle-guide-to-machine-learning/", "https://www.coursera.org/learn/machine-learning", "http://course.fast.ai/", "https://monkeylearn.com/blog/author/bruno/", "https://monkeylearn.com/blog/category/news/", "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/#respond", "https://monkeylearn.com/blog/author/bruno/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/monkeylearn_zapier.png", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/wp-content/uploads/2017/06/Post_1e.png", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/naive-bayes-classifier.png", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/monkey-startup.png", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/category/applications/", "https://monkeylearn.com/blog/category/guides/", "https://monkeylearn.com/blog/category/howto/", "https://monkeylearn.com/blog/category/news/", "https://monkeylearn.com/blog/category/text-classification/", "https://www.monkeylearn.com", "https://www.monkeylearn.com", "http://slack.monkeylearn.com/", "https://twitter.com/monkeylearn", "https://github.com/monkeylearn/"]}, "87": {"url": "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "title": "Machine Learning Translation and the Google Translate Algorithm", "text": "Every day we use different technologies without even knowing how exactly they work. In fact, it\u2019s not very easy to understand engines powered by machine learning. The Statsbot team wants to make machine learning clear by telling data stories in this blog. Today, we\u2019ve decided to explore machine translators and explain how the Google Translate algorithm works. Years ago, it was very time consuming to translate the text from an unknown language. Using simple vocabularies with word-for-word translation was hard for two reasons: 1) the reader had to know the grammar rules and 2) needed to keep in mind all language versions while translating the whole sentence. Now, we don\u2019t need to struggle so much\u2013 we can translate phrases, sentences, and even large texts just by putting them in Google Translate. But most people don\u2019t actually care how the engine of machine learning translation works. This post is for those who do care. If the Google Translate engine tried to kept the translations for even short sentences, it wouldn\u2019t work because of the huge number of possible variations. The best idea can be to teach the computer sets of grammar rules and translate the sentences according to them. If only it were as easy as it sounds. If you have ever tried learning a foreign language, you know that there are always a lot of exceptions to rules. When we try to capture all these rules, exceptions and exceptions to the exceptions in the program, the quality of translation breaks down. Creating your own simple machine translator would be a great project for any data science resume. Let\u2019s try to investigate what hides in the \u201cblack boxes\u201d that we call machine translators. Deep neural networks can achieve excellent results in very complicated tasks (speech/visual object recognition), but despite their flexibility, they can be applied only for tasks where the input and target have fixed dimensionality. Here is where Long Short-Term Memory networks (LSTMs) come into play, helping us to work with sequences whose length we can\u2019t know a priori. LSTMs are a special kind of recurrent neural network (RNN), capable of learning long-term dependencies. All RNNs look like a chain of repeating modules. So the LSTM transmits data from module to module and, for example, for generating Ht we use not only Xt, but all previous input values X. To learn more about structure and mathematical models of LSTM, you can read the great article \u201cUnderstanding LSTM Networks.\u201d Our next step is bidirectional recurrent neural networks (BRNNs). What a BRNN does, is split the neurons of a regular RNN into two directions. One direction is for positive time, or forward states. The other direction is for negative time, or backward states. The output of these two states are not connected to inputs of the opposite direction states. To understand why BRNNs can work better than a simple RNN, imagine that we have a sentence of 9 words and we want to predict the 5th word. We can make it know either only the first 4 words, or the first 4 words and last 4 words. Of course, the quality in the second case would be better. Now we\u2019re ready to move to sequence to sequence models (also called seq2seq). The basic seq2seq model consist of two RNNs: an encoder network that processes the input and a decoder network that generates the output. Finally, we can make our first machine translator! However, let\u2019s think about one trick. Google Translate currently supports 103 languages, so we should have 103x102 different models for each pair of languages. Of course, the quality of these models varies according to the popularity of languages and the amount of documents needed for training this network. The best that we can do is to make one NN to take any language as input and translate into any language. That very idea was realized by Google engineers at the end of 2016. Architecture of NN was build on the seq2seq model, which we have already studied. The only exception is that between the encoder and decoder there are 8 layers of LSTM-RNN that have residual connections between layers with some tweaks for accuracy and speed. If you want to go deeper with that, take a look at the article Google\u2019s Neural Machine Translation System. The system requires a \u201ctoken\u201d at the beginning of the input sentence which specifies the language you\u2019re trying to translate the phrase into. This improves translation quality and enables translations even between two languages which the system hasn\u2019t seen yet, a method termed \u201cZero-Shot Translation.\u201d When we\u2019re talking about improvements and better results from Google Translate algorithms, how can we correctly evaluate that the first candidate for translation is better than the second? It\u2019s not a trivial problem, because for some commonly used sentences we have the sets of reference translations from the professional translators, that have, of course, some differences. There are a lot of approaches that partly solve this problem, but the most popular and effective metric is BLEU (bilingual evaluation understudy). Imagine, we have two candidates from machine translators: Although they have the same meaning they differ in quality and have different structure. Let\u2019s look at two human translations: Obviously, Candidate 1 is better, sharing more words and phrases compared to Candidate 2. This is a key idea of the simple BLEU approach. We can compare n-grams of the candidate with n-grams of the reference translation and count the number of matches (independent from their position). We use only n-gram precisions, because calculating recall is difficult with multiple refs and the result is the geometric average of n-gram scores. Now you can evaluate the complex engine of machine learning translation. Next time when you translate something with Google Translate, imagine how many millions of documents it analyzed before giving you the best language version.", "links": ["https://blog.statsbot.co?source=logo-lo_738fa0f28111---cfc9f21a543a", "https://twitter.com/statsbotco", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.statsbot.co%2Fmachine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co", "https://blog.statsbot.co/analytics/home", "https://blog.statsbot.co/datascience/home", "https://blog.statsbot.co/design/home", "https://blog.statsbot.co/startups/home", "https://blog.statsbot.co/bots/home", "https://blog.statsbot.co/news/home", "https://blog.statsbot.co/statsbot-digest-b0d7372f842a", "http://statsbot.co?utm_source=blog&utm_medium=navigation&utm_campaign=robot_face", "https://blog.statsbot.co/search", "https://blog.statsbot.co/@daniilkorbut?source=post_header_lockup", "https://blog.statsbot.co/@daniilkorbut?source=post_header_lockup", "https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html", "http://statsbot.co?utm_source=blog&utm_medium=article&utm_campaign=machine_translation", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6?utm_source=blog&utm_medium=article&utm_campaign=machine_translation", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6", "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "https://www.semanticscholar.org/paper/Hybrid-speech-recognition-with-Deep-Bidirectional-Graves-Jaitly/5807664af8e63d5207f59fb263c9e7bd3673be79", "https://research.googleblog.com/2016/09/a-neural-network-for-machine.html", "https://www.newscientist.com/article/2114748-google-translate-ai-invents-its-own-language-to-translate-with/", "https://research.googleblog.com/2016/09/a-neural-network-for-machine.html", "https://arxiv.org/abs/1609.08144", "https://en.wikipedia.org/wiki/BLEU", "https://en.wikipedia.org/wiki/N-gram", "https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3", "https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/tagged/machine-learning?source=post", "https://blog.statsbot.co/tagged/machine-translation?source=post", "https://blog.statsbot.co/tagged/google-translate?source=post", "https://blog.statsbot.co/tagged/data-science?source=post", "https://blog.statsbot.co/tagged/neural-networks?source=post", "https://blog.statsbot.co/@daniilkorbut?source=footer_card", "https://blog.statsbot.co/@daniilkorbut", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "88": {"url": "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "title": "Yanir SeroussiMy 10-step path to becoming a remote data scientist with\u00a0Automattic", "text": "About two years ago, I read the book The Year without Pants, which describes the author\u2019s experience leading a team at Automattic (the company behind WordPress.com, among other products). Automattic is a fully-distributed company, which means that all of its employees work remotely (hence pants are optional). While the book discusses some of the challenges of working remotely, the author\u2019s general experience was very positive. A few months after reading the book, I decided to look for a full-time position after a period of independent work. Ideally, I wanted a well-paid data science-y remote job with an established distributed tech company that offers a good life balance and makes products I care about. Automattic seemed to tick all my boxes, so I decided to apply for a job with them. This post describes my application steps, which ultimately led to me becoming a data scientist with Automattic. Before jumping in, it\u2019s worth noting that this post describes my personal experience. If you apply for a job with Automattic, your experience is likely to be different, as the process varies across teams, and evolves over time. I decided to apply for a data wrangler position with Automattic in October 2015. While data wrangler may sound less sexy than data scientist, reading the job ad led me to believe that the position may involve interesting data science work. This impression was strengthened by some LinkedIn stalking, which included finding current data wranglers and reading through their profiles and websites. I later found out that all the people on the data division start out as data wranglers, and then they may pick their own title. Some data wranglers do data science work, while others are more focused on data engineering, and there are some projects that require a broad range of skills. As the usefulness of the term data scientist is questionable, I\u2019m not too fussed about fancy job titles. It\u2019s more important to do interesting work in a supportive environment. Applying for the job was fairly straightforward. I simply followed the instructions from the ad:  Does this sound interesting? If yes, please send a short email to jobs @ this domain telling us about yourself and attach a resum\u00e9. Let us know what you can contribute to the team. Include the title of the position you\u2019re applying for and your name in the subject. Proofread! Make sure you spell and capitalize WordPress and Automattic correctly. We are lucky to receive hundreds of applications for every position, so try to make your application stand out. If you apply for multiple positions or send multiple emails there will be one reply.  Having been on the receiving side of job applications, I find it surprising that many people don\u2019t bother writing a cover letter, addressing the selection criteria in the ad, or even applying for a job they\u2019re qualified to do. Hence, my cover letter was fairly short, comprising of several bullet points that highlight the similarities between the job requirements and my experience. It was nothing fancy, but simple cover letters have worked well for me in the past. The initial application was followed by a long wait. From my research, this is the typical scenario. This is unsurprising, as Automattic is a fairly small company with a large footprint, which is both distributed and known as a great place to work (e.g., its Glassdoor rating is 4.9). Therefore, it attracts many applicants from all over the world, which take a while to process. In addition, Matt Mullenweg (Automattic\u2019s CEO) reviews job applications before passing them on to the team leads. As I didn\u2019t know that Matt reviewed job applications, I decided to try to shorten the wait by getting introduced to someone in the data division. My first attempt was via a second-degree LinkedIn connection who works for Automattic. He responded quickly when I reached out to him, saying that his experience working with the company is in line with the Glassdoor reviews \u2013 it\u2019s the best job he\u2019s had in his 15-year-long career. However, he couldn\u2019t help me with an intro, because there is no simple way around Automattic\u2019s internal processes. Nonetheless, he reassured me that it is worth waiting patiently, as the strict process means that you end up working with great people. I wasn\u2019t in a huge rush to find a job, but in December 2015 I decided to accept an offer to become the head of data science at Car Next Door. This was a good decision at the time, as I believe in the company\u2019s original vision of reducing the number of cars on the road through car sharing, and it seemed like there would be many interesting projects for me to work on. The position wasn\u2019t completely remote, but as the company was already spread across several cities, I was able to work from home for a day or two every week. In addition, it was a pleasant commute by bike from my Sydney home to the office, so putting the fully-remote job search on hold didn\u2019t seem like a major sacrifice. As I haven\u2019t heard anything from Automattic at that stage, it seemed unwise to reject a good offer, so I started working full-time with Car Next Door in January 2016. I successfully attracted Automattic\u2019s attention with a post I published on the misuse of the word insights by many tech companies, which included an example from WordPress.com. Greg Ichneumon Brown, one of the data wranglers, commented on the post, and invited me to apply to join Automattic and help them address the issues I raised. This happened after I accepted the offer from Car Next Door, and hasn\u2019t resulted in any speed up of the process, so I just gave up on Automattic and carried on with my life. I finally heard back from Automattic in February 2016 (four months after my initial application and a month into my employment with Car Next Door). Martin Remy, who leads the data division, emailed me to enquire if I\u2019m still interested in the position. I informed him that I was no longer looking for a job, but we agreed to have an informal chat, as I\u2019ve been waiting for such a long time. As is often the case with Automattic interviews, the chat with Martin was completely text-based. Working with a distributed team means that voice and video calls can be hard to schedule. Hence, Automattic relies heavily on textual channels, and text-based interviews allow the company to test the written communication skills of candidates. The chat revolved around my past work experience, and Martin also took the time to answer my questions about the company and the data division. At the conclusion of the chat, Martin suggested I contact him directly if I was ever interested in continuing the application process. While I was happy with my position at the time, the chat strengthened my positive impression of Automattic, and I decided that I would reapply if I were to look for a full-time position again. My next job search started earlier than I had anticipated. In October 2016, I decided to leave Car Next Door due to disagreements with the founders over the general direction of the company. In addition, I had more flexibility in choosing where to live, as my personal circumstances had changed. As I\u2019ve always been curious about life outside the capital cities of Australia, I wanted to move away from Sydney. While I could have probably continued working remotely with Car Next Door, I felt that it would be better to find a job with a fully-distributed team. Therefore, I messaged Martin and we scheduled another chat. The second chat with Martin took place in early November. Similarly to the first chat, it was conducted via Skype text messages, and revolved around my work in the time that has passed since the first chat. This time, as I was keen on continuing with the process, I asked more specific questions about what kind of work I\u2019m likely to end up doing and what the next steps would be. The answers were that I\u2019d be joining the data science team, and that the next steps are a pre-trial test, a paid trial, and a final interview with Matt. While this sounds straightforward, it took another six months until I finally became an Automattic employee (but I wasn\u2019t in a rush). The pre-trial test consisted of a data analysis task, where I was given a dataset and a set of questions to answer by Carly Stambaugh, the data science lead. The goal of the test is to evaluate the candidate\u2019s approach to a problem, and assess organisational and communication skills. As such, the focus isn\u2019t on obtaining a specific result, so candidates are given a choice of several potential avenues to explore. The open-ended nature of the task is reminiscent of many real-world data science projects, where you don\u2019t always have a clear idea of what you\u2019re going to discover. While some people may find this kind of uncertainty daunting, I find it interesting, as it is one of the things that makes data science a science. I spent a few days analysing the data and preparing a report, which was submitted as a Jupyter Notebook. After submitting my initial report, there were a few follow-up questions, which I answered by email. The report was reviewed by Carly and Martin, and as they were satisfied with my work, I was invited to proceed to the next stage: A paid trial project. The main part of the application process with Automattic is the paid trial project. The rationale behind doing paid trials was explained a few years ago by Matt in Hire by Auditions, Not Resumes:  Before we hire anyone, they go through a trial process first, on contract. They can do the work at night or over the weekend, so they don\u2019t have to leave their current job in the meantime. We pay a standard rate of $25 per hour, regardless of whether you\u2019re applying to be an engineer or the chief financial officer. During the trials, we give the applicants actual work. If you\u2019re applying to work in customer support, you\u2019ll answer tickets. If you\u2019re an engineer, you\u2019ll work on engineering problems. If you\u2019re a designer, you\u2019ll design. There\u2019s nothing like being in the trenches with someone, working with them day by day. It tells you something you can\u2019t learn from resumes, interviews, or reference checks. At the end of the trial, everyone involved has a great sense of whether they want to work together going forward. And, yes, that means everyone \u2014 it\u2019s a mutual tryout. Some people decide we\u2019re not the right fit for them.  The goal of my trial project was to improve the Elasticsearch language detection algorithm. This took about a month, and ultimately resulted in a pull request that got merged into the language detection plugin. I find this aspect of the process pretty exciting: While the plugin is used to classify millions of documents internally by Automattic, its impact extends beyond the company, as Elasticsearch is used by many other organisations and projects. This stands in contrast to many other technical job interviews, which consist of unpaid work on toy problems under stressful conditions, where the work performed is ultimately thrown away. While the monetary compensation for the trial work is lower than the market rate for data science consulting, I valued the opportunity to work on a real open source project, even if this hadn\u2019t led to me getting hired. There was much more to the trial project than what\u2019s shown in the final pull request. Most of the discussions were held on an internal project thread, primarly under the guidance of Carly (the data science lead), and Greg (the data wrangler who replied to my post a year earlier). The project was kicked off with a general problem statement: There was some evidence that the Elasticsearch language detection plugin doesn\u2019t perform well on short texts, and my mission was to improve it. As the plugin didn\u2019t include any tests for short texts, one of the main contributions of my work was the creation of datasets and tests to measure its accuracy on texts of different lengths. This was followed by some tweaks that improved the plugin\u2019s performance, as summarised in the pull request. Internally, this work consisted of several iterations where I came up with ideas, asked questions, implemented the ideas, shared the results, and discussed further steps. There are still many possible improvements to the work done in the trial. However, as trials generally last around a month, we decided to end it after a few iterations. I enjoyed the trial process, but it is definitely not for everyone. Most notably, there is a strong emphasis on asynchronous text-based communication, which is the main mode by which projects are coordinated at Automattic. People who don\u2019t enjoy written communication may find this aspect challenging, but I have always found that writing helps me organise my thoughts, and that I retain information better when reading than when listening to people speak. That being said, Automatticians do meet in person several times a year, and some teams have video chats for some discussions. While doing the trial, I had a video chat with Carly, which was the first (and last) time in the process that I got to see and hear a live human. However, this was not an essential part of the trial project, as our chat was mostly on the data scientist role and my job expectations. I finished working on the trial project just before Christmas. The feedback I received throughout the trial was positive, but Martin, Carly, and Greg had to go through the work and discuss it among themselves before making a final decision. This took about a month, due to the holiday period, various personal circumstances, and the data science team meetup that was scheduled for January 2017. Eventually, Martin got back to me with positive news: They were satisfied with my trial work, which meant there was only one stage left \u2013 the final interview with Matt Mullenweg, Automattic\u2019s CEO. Like other parts of the process, the interview with Matt is text-based. The way it works is fairly simple: I was instructed to message Matt on Slack and wait for a response, which may take days or weeks. I sent Matt a message on January 25, and was surprised to hear back from him the following morning. However, that day was Australia Day, which is a public holiday here. Therefore, I only got back to him two hours after he messaged me that morning, and by that time he was probably already busy with other things. This was the start of a pretty long wait. I left Car Next Door at the end of January, as I figured that I would be able to line up some other work even if things didn\u2019t work out with Automattic. My plan was to take some time off, and then move up to the Northern Rivers area of New South Wales. I had two Reef Life Survey trips planned, so I wasn\u2019t going to start working again before mid-April. I assumed that I would hear back from Matt before then, which would have allowed me to make an informed decision whether to look for another job or not. After two weeks of waiting, the time for my dive trips was nearing. As I was going to be without mobile reception for a while, I thought it\u2019d be worth letting Matt know my schedule. After discussing the matter with Martin, I messaged Matt. He responded, saying that we might as well do the interview at the beginning of April, as I won\u2019t be starting work before that time anyway. I would have preferred to be done with the interview earlier, but was happy to have some certainty and not worry about missing more chat messages before April. In early April, I returned from my second dive trip (which included a close encounter with Cyclone Debbie), and was hoping to sort out my remote work situation while completing the move up north. Unfortunately, while the move was successful, I was ready to give up on Automattic because I haven\u2019t heard back from Matt at all in April. However, Martin remained optimistic and encouraged me to wait patiently, which I did as I was pretty busy with the move and with some casual freelancing projects. The chat with Matt finally happened on May 2. As is often the case, it took a few hours and covered my background, the trial process, and some other general questions. I asked him about my long wait for the final chat, and he apologised for me being an outlier, as most chats happen within two weeks of a candidate being passed over to him. As the chat was about to conclude, we got to the topic of salary negotiation (which went well), and then the process was finally over! Within a few hours of the chat I was sent an offer letter and an employment contract. As Automattic has an entity in Australia (called Ausomattic), it\u2019s a fairly standard contract. I signed the contract and started work the following week \u2013 over a year and a half after my initial application. Even before I started working, I booked tickets to meet the data division in Montr\u00e9al \u2013 a fairly swift transition from the long wait for the final interview. As noted above, Automatticians get to choose their own job titles, so to become a data scientist with Automattic, I had to set my job title to Data Scientist. This is generally how many people become data scientists these days, even outside Automattic. However, job titles don\u2019t matter as much as job satisfaction. And after 2.5 months with Automattic, I\u2019m very satisfied with my decision to join the company. My first three weeks were spent doing customer support, like all new Automattic employees. Since then, I\u2019ve been involved in projects to make engagement measurement more consistent (harder than it sounds, as counting things is hard), and to improve the data science codebase (e.g., moving away from Legacy Python). Besides that, I also went to Montr\u00e9al for the data division meetup, and have started getting into chatbot work. I\u2019m looking forward to doing more work and sharing my experience here and on data.blog. Very enlightening post! It was very awesome to see that the insights you saw to Elasticsearch went to a PR. I bet that was worth the whole thing!  That\u2019s very exciting, I wanted to ask are you a self learner or do you have a degree,can you please share your background. Thank you  Thanks Mostafa. Yes, I have a BSc in computer science, and a PhD in what you would now call data science. See: https://www.linkedin.com/in/yanirseroussi/  Fill in your details below or click an icon to log in:     Connecting to %s         Enter your email address to follow this blog and receive notifications of new posts by email.", "links": ["https://yanirseroussi.com/", "https://yanirseroussi.com/", "https://yanirseroussi.com/about/", "https://yanirseroussi.com/presentations/", "https://yanirseroussi.com/phd-work/", "https://yanirseroussi.com/2014/04/05/kaggle-competition-summaries/", "http://scottberkun.com/yearwithoutpants/", "https://automattic.com/", "https://yanirseroussi.com/2015/03/22/the-long-road-to-a-lifestyle-business/", "http://web.archive.org/web/20150908140923/https://automattic.com/work-with-us/data-wrangler/", "https://yanirseroussi.com/2016/08/04/is-data-scientist-a-useless-job-title/", "https://automattic.com/about/", "https://www.glassdoor.com.au/Reviews/Automattic-Reviews-E751107.htm", "http://davemart.in/remote-hiring/", "https://www.carnextdoor.com.au/", "https://yanirseroussi.com/2015/12/08/this-holiday-season-give-me-real-insights/", "https://yanirseroussi.com/2015/12/08/this-holiday-season-give-me-real-insights/#comment-957", "http://jupyter.org/", "https://hbr.org/2014/01/hire-by-auditions-not-resumes", "https://www.elastic.co/products/elasticsearch", "https://github.com/jprante/elasticsearch-langdetect/pull/69", "https://github.com/jprante/elasticsearch-langdetect/pull/69", "https://yanirseroussi.com/2016/01/24/the-joys-of-offline-data-collection/", "https://www.whitsundaytimes.com.au/news/boat-caught-in-eye-of-cyclone-cruises-home/3164170/", "https://data.blog/2017/06/29/data-coalesce-automattic-data-division-meets-in-montreal/", "http://daynebatten.com/2016/06/counting-hard-data-science/", "http://powerfulpython.com/blog/magic-word-legacy-python/", "https://data.blog/2017/05/24/may-the-bot-be-with-you-how-algorithms-are-supporting-happiness-at-wordpress-com/", "https://data.blog/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=twitter", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=facebook", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=google-plus-1", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=linkedin", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=reddit", "https://yanirseroussi.com/category/data-science-2/", "https://yanirseroussi.com/tag/automattic/", "https://yanirseroussi.com/tag/career/", "https://yanirseroussi.com/tag/data-science/", "https://yanirseroussi.com/tag/elasticsearch/", "https://yanirseroussi.com/tag/personal/", "https://yanirseroussi.com/tag/wordpress/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "https://yanirseroussi.com/author/yanirseroussi/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comments", "https://yanirseroussi.com/2017/06/03/exploring-and-visualising-reef-life-survey-data/", "https://dotnetmeditations.com/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1698", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1698&_wpnonce=6c197b2684", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1698#respond", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1700", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1700&_wpnonce=350fa40ca9", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1700#respond", "http://yanirseroussi.com", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1705", "https://www.linkedin.com/in/yanirseroussi/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1705&_wpnonce=7f1e8d7061", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1705#respond", "https://gravatar.com/site/signup/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "https://yanirseroussi.com/2017/06/03/exploring-and-visualising-reef-life-survey-data/", "https://yanirseroussi.com/2017/01/08/customer-lifetime-value-and-the-proliferation-of-misinformation-on-the-internet/", "https://yanirseroussi.com/2016/09/19/ask-why-finding-motives-causes-and-purpose-in-data-science/", "https://yanirseroussi.com/2016/08/21/seven-ways-to-be-data-driven-off-a-cliff/", "https://yanirseroussi.com/category/bcrecommender/", "https://yanirseroussi.com/category/data-science-2/", "https://yanirseroussi.com/category/environment/", "https://yanirseroussi.com/category/general/", "https://yanirseroussi.com/category/kaggle-2/", "https://yanirseroussi.com/category/machine-intelligence/", "https://yanirseroussi.com/category/phd-work/", "https://wordpress.com/?ref=footer_blog"]}, "89": {"url": "https://medium.com/airbnb-engineering/using-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d", "title": "Using Machine Learning to Predict Value of Homes On\u00a0Airbnb", "text": "by Robert Chang Data products have always been an instrumental part of Airbnb\u2019s service. However, we have long recognized that it\u2019s costly to make data products. For example, personalized search ranking enables guests to more easily discover homes, and smart pricing allows hosts to set more competitive prices according to supply and demand. However, these projects each required a lot of dedicated data science and engineering time and effort. Recently, advances in Airbnb\u2019s machine learning infrastructure have lowered the cost significantly to deploy new machine learning models to production. For example, our ML Infra team built a general feature repository that allows users to leverage high quality, vetted, reusable features in their models. Data scientists have started to incorporate several AutoML tools into their workflows to speed up model selection and performance benchmarking. Additionally, ML infra created a new framework that will automatically translate Jupyter notebooks into Airflow pipelines. In this post, I will describe how these tools worked together to expedite the modeling process and hence lower the overall development costs for a specific use case of LTV modeling\u200a\u2014\u200apredicting the value of homes on Airbnb. Customer Lifetime Value (LTV), a popular concept among e-commerce and marketplace companies, captures the projected value of a user for a fixed time horizon, often measured in dollar terms. At e-commerce companies like Spotify or Netflix, LTV is often used to make pricing decisions like setting subscription fees. At marketplace companies like Airbnb, knowing users\u2019 LTVs enable us to allocate budget across different marketing channels more efficiently, calculate more precise bidding prices for online marketing based on keywords, and create better listing segments. While one can use past data to calculate the historical value of existing listings, we took one step further to predict LTV of new listings using machine learning. Data scientists are typically accustomed to machine learning related tasks such as feature engineering, prototyping, and model selection. However, taking a model prototype to production often requires an orthogonal set of data engineering skills that data scientists might not be familiar with. Luckily, At Airbnb we have machine learning tools that abstract away the engineering work behind productionizing ML models. In fact, we could not have put our model into production without these amazing tools. The remainder of this post is organized into four topics, along with the tools we used to tackle each task: One of the first steps of any supervised machine learning project is to define relevant features that are correlated with the chosen outcome variable, a process called feature engineering. For example, in predicting LTV, one might compute the percentage of the next 180 calendar dates that a listing is available or a listing\u2019s price relative to comparable listings in the same market. At Airbnb, feature engineering often means writing Hive queries to create features from scratch. However, this work is tedious and time consuming as it requires specific domain knowledge and business logic, which means the feature pipelines are often not easily sharable or even reusable. To make this work more scalable, we developed Zipline\u200a\u2014\u200aa training feature repository that provides features at different levels of granularity, such as at the host, guest, listing, or market level. The crowdsourced nature of this internal tool allows data scientists to use a wide variety of high quality, vetted features that others have prepared for past projects. If a desired feature is not available, a user can create her own feature with a feature configuration file like the following: When multiple features are required for the construction of a training set, Zipline will automatically perform intelligent key joins and backfill the training dataset behind the scenes. For the listing LTV model, we used existing Zipline features and also added a handful of our own. In sum, there were over 150 features in our model, including: With our features and outcome variable defined, we can now train a model to learn from our historical data. As in the example training dataset above, we often need to perform additional data processing before we can fit a model: In this step, we don\u2019t quite know what is the best set of features to use, so writing code that allows us to rapidly iterate is essential. The pipeline construct, commonly available in open-source tools like Scikit-Learn and Spark, is a very convenient tool for prototyping. Pipelines allow data scientists to specify high-level blueprints that describe how features should be transformed, and which models to train. To make it more concrete, below is a code snippet from our LTV model pipeline: At a high level, we use pipelines to specify data transformations for different types of features, depending on whether those features are of type binary, categorical, or numeric. FeatureUnion at the end simply combines the features column-wise to create the final training dataset. The advantage of writing prototypes with pipelines is that it abstracts away tedious data transformations using data transforms. Collectively, these transforms ensure that data will be transformed consistently across training and scoring, which solves a common problem of data transformation inconsistency when translating a prototype into production. Furthermore, pipelines also separates data transformations from model fitting. While not shown in the code above, data scientists can add a final step to specify an estimator for model fitting. By exploring different estimators, data scientists can perform model selection to pick the best model to improve the model\u2019s out of sample error. As mentioned in the previous section, we need to decide which candidate model is the best to put into production. To make such a decision, we need to weigh the tradeoffs between model interpretability and model complexity. For example, a sparse linear model might be very interpretable but not complex enough to generalize well. A tree based model might be flexible enough to capture non-linear patterns but not very interpretable. This is known as the Bias-Variance tradeoff. In applications such as insurance or credit screening, a model needs to be interpretable because it\u2019s important for the model to avoid inadvertently discriminating against certain customers. In applications such as image classification, however, it is much more important to have a performant classifier than an interpretable model. Given that model selection can be quite time consuming, we experimented with using various AutoML tools to speed up the process. By exploring a wide variety of models, we found which types of models tended to perform best. For example, we learned that eXtreme gradient boosted trees (XGBoost) significantly outperformed benchmark models such as mean response models, ridge regression models, and single decision trees. Given that our primary goal was to predict listing values, we felt comfortable productionizing our final model using XGBoost, which favors flexibility over interpretability. As we alluded to earlier, building a production pipeline is quite different from building a prototype on a local laptop. For example, how can we perform periodic re-training? How do we score a large number of examples efficiently? How do we build a pipeline to monitor model performance over time? At Airbnb, we built a framework called ML Automator that automagically translates a Jupyter notebook into an Airflow machine learning pipeline. This framework is designed specifically for data scientists who are already familiar with writing prototypes in Python, and want to take their model to production with limited experience in data engineering. Here is a code snippet demonstrating how the fit and transform functions are defined in our LTV model. The fit function tells the framework that a XGBoost model will be trained, and that data transformations will be carried out according to the pipeline we defined previously. Once the notebook is merged, ML Automator will wrap the trained model inside a Python UDF and create an Airflow pipeline like the one below. Data engineering tasks such as data serialization, scheduling of periodic re-training, and distributed scoring are all encapsulated as a part of this daily batch job. As a result, this framework significantly lowers the cost of model development for data scientists, as if there was a dedicated data engineer working alongside the data scientists to take the model into production! Note: Beyond productionization, there are other topics, such as tracking model performance over time or leveraging elastic compute environment for modeling, which we will not cover in this post. Rest assured, these are all active areas under development. In the past few months, data scientists have partnered very closely with ML Infra, and many great patterns and ideas arose out of this collaboration. In fact, we believe that these tools will unlock a new paradigm for how to develop machine learning models at Airbnb. We are very excited about the future of this framework and the new paradigm it brought along. By bridging the gap between prototyping and productionization, we can truly enable data scientists and engineers to pursue end-to-end machine learning projects and make our product better. Want to use or build these ML tools? We\u2019re always looking for talented people to join our Data Science and Analytics team! Special thanks to members of Data Science & ML Infra team who were involved in this work: Aaron Keys, Brad Hunter, Hamel Husain, Jiaying Shi, Krishna Puttaswamy, Michael Musson, Nick Handel, Varant Zanoyan, Vaughn Quoss et al. Additional thanks to Gary Tang, Jason Goodman, Jeff Feng, Lindsay Pettingill for reviewing this blog post. Clapping shows how much you appreciated Robert Chang\u2019s story.", "links": ["https://medium.com/", "https://medium.com/airbnb-engineering?source=logo-lo_3d9f3e1f2cba---53c7c27702d5", "https://twitter.com/AirbnbEng", "https://medium.com/m/signin?redirect=https%3A%2F%2Fmedium.com%2Fairbnb-engineering%2Fusing-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d", "https://medium.com/airbnb-engineering", "https://medium.com/airbnb-engineering/ai/home", "https://medium.com/airbnb-engineering/airbnb-engineering-backend/home", "https://medium.com/airbnb-engineering/data/home", "https://medium.com/airbnb-engineering/airbnb-engineering-infrastructure/home", "https://medium.com/airbnb-engineering/tagged/mobile", "https://medium.com/airbnb-engineering/web/home", "http://airbnb.io/projects/", "https://medium.com/airbnb-engineering/search", "https://medium.com/@rchang?source=post_header_lockup", "https://medium.com/@rchang?source=post_header_lockup", "https://twitter.com/_rchang", "https://medium.com/swlh/diligence-at-social-capital-part-3-cohorts-and-revenue-ltv-ab65a07464e1", "http://scikit-learn.org/stable/", "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html", "https://www.kaggle.com/general/16927", "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html", "https://spark.apache.org/docs/latest/ml-pipeline.html", "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html", "http://scikit-learn.org/stable/data_transforms.html", "http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html", "https://medium.com/airbnb-engineering/automated-machine-learning-a-paradigm-shift-that-accelerates-data-scientist-productivity-airbnb-f1f8a10d61f8", "http://scott.fortmann-roe.com/docs/BiasVariance.html", "https://medium.com/airbnb-engineering/automated-machine-learning-a-paradigm-shift-that-accelerates-data-scientist-productivity-airbnb-f1f8a10d61f8", "https://github.com/dmlc/xgboost", "https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8", "http://www.florianwilhelm.info/2016/10/python_udf_in_hive/", "https://airflow.incubator.apache.org/", "https://www.airbnb.com/careers/departments/data-science-analytics", "https://www.linkedin.com/in/aaronkeys/", "https://www.linkedin.com/in/brad-hunter-497621a/", "https://www.linkedin.com/in/hamelhusain/", "https://www.linkedin.com/in/jiaying-shi-a2142733/", "https://www.linkedin.com/in/krishnaputtaswamy/", "https://www.linkedin.com/in/michael-m-a37b1932/", "https://www.linkedin.com/in/nicholashandel/", "https://www.linkedin.com/in/vzanoyan/", "https://www.linkedin.com/in/vquoss/", "https://www.linkedin.com/in/thegarytang/", "https://medium.com/@jasonkgoodman", "https://twitter.com/jtfeng", "https://medium.com/@lpettingill", "https://medium.com/@jtfeng?source=post_page", "https://medium.com/@jasonkgoodman?source=post_page", "https://medium.com/@gary.tang_94319?source=post_page", "https://medium.com/@lpettingill?source=post_page", "https://medium.com/@vquoss?source=post_page", "https://medium.com/@eddie.santos.3?source=post_page", "https://medium.com/@NicholasHandel?source=post_page", "https://medium.com/tag/machine-learning?source=post", "https://medium.com/tag/data-science?source=post", "https://medium.com/tag/airbnb?source=post", "https://medium.com/tag/technology?source=post", "https://medium.com/@rchang?source=footer_card", "https://medium.com/@rchang", "http://twitter.com/Airbnb", "http://twitter.com/Twitter", "https://medium.com/airbnb-engineering?source=footer_card", "https://medium.com/airbnb-engineering?source=footer_card", "http://airbnb.io", "https://medium.com/airbnb-engineering", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "90": {"url": "https://www.ayasdi.com/blog/artificial-intelligence/why_ai_is_the_new_bigdata/", "title": "The \u201cBig Data-ization\u201d of Artificial Intelligence", "text": "\u2039 Back to Blog  Artificial Intelligence, Data, Machine Intelligence, Machine Learning It seems like it was only a few years ago that the term \u201cbig data\u201d went from a promising area of research and interest to something so ubiquitous that it lost all meaning, descending ultimately into the butt of jokes.  As everyone piled onto the big data bandwagon, it became impossible to separate truth from fiction. Every executive and entrepreneur that I ran into was doing a \u201cbig data\u201d thing. I recall meeting someone whose company shot videos for enterprise customers and was pitching it as a \u2018Big Data play\u2019 \u2013 because video files, you know, are huge \u2013 they take up lots of space.  Thankfully, the noise associated with \u201cbig data\u201d is abating as sophistication and common sense take hold. In fact, in many circles the term actually exposes the user as someone who doesn\u2019t really understand the space.  Unfortunately, the same malady has now afflicted AI. Everyone I meet is doing an \u2018AI play\u2019 \u2013 even if all they did was to build a simple linear regressor in Excel.  AI is, unfortunately, the new \u201cbig data.\u201d While not good, it is not all bad either.  After all, the data ecosystem benefited from all of the \u201cbig data\u201d attention and investment \u2013 creating some amazing software and producing some exceptional productivity gains.  The same will happen with AI \u2013 with the increased attention comes investment dollars which in turn will drive adoption \u2013 enhancing the ecosystem. Having said that we need to stop calling regression on excel AI \u2013 it\u2019s ridiculous and undermines some incredible work being done in the space.  Regression has been around for 200+ years. Gauss and Legendre didn\u2019t don the AI cloak when they discovered regression. Neither should the recent stats graduate trying to raise money for his/her startup.  I will tell you what I think qualifies as AI in a moment \u2013 but here are some thoughts that influence that discussion. First, the AI definition I use is focused on the narrow, application specific AI, not the more general problem of artificial general intelligence (AGI) where simulating a person using software is the equivalent of intelligence.  Second, the vast, vast majority of the data that exists in the world is unlabeled. It is not practical to label that data manually and doing so would likely create bias anyway. One can argue that the Internet as a whole is simply a mechanism for humans to entertain ourselves while providing label data for machines.  Unlabeled data presents a different challenge, one we will address shortly, but the key point here is that it is everywhere and represents the key to extracting business value (or any value).  Third, we are not producing data scientists at a rate that can keep pace with the growth of data. Even with the moniker as a the \u201csexiest job of the 21st century\u201d the pace at which data scientists are created doesn\u2019t begin to approach the growth rate we are seeing in data.  Fourth, data scientists, for the most part, are not UX designers or product managers or, in many cases even engineers. As a result, the subject matter experts, those that sit in the business, don\u2019t have effective interfaces to the data science outputs. The interfaces that they have \u2013 powerpoint, excel, or PDF reports have limited utility in transforming the behavior of a company. What is required is something to shape behavior is something more \u2013 applications.  So what does qualify as intelligence? In \u2018On Intelligence\u2019 Jeff Hawkins says that all AI boils down to memory and prediction. The argument is very persuasive, but a bit reductive. We have a slightly different take for what an AI should display and it encompasses a framework: Here are characteristics that I think any AI should display. While some of these elements may seem self-evident that is because they are taken as a single item. Intelligence has a broader context. All the elements must work in conjunction with each other to qualify as AI. \u00a0\u00a0\u00a0 The five elements are: Let\u2019s take each of these concepts in turn. Discovery is the ability of an intelligent system to learn from data without upfront human intervention. Often, this needs to be done without being presented with an explicit target. It relies on the use of unsupervised and semi-supervised machine learning techniques (such as segmentation, dimensionality reduction, anomaly detection, etc.), as well as more supervised techniques where there is an outcome or there are several outcomes of interest. Usually, in enterprise software, the term discovery refers to the ability of ETL/MDM solutions to discover the various schemas of tables in large databases and automatically find join keys etc. This is not what we mean by discovery. We use of the term very differently and has this has important implications. In complex datasets, it is nearly impossible to ask the \u201cright\u201d questions. To discover what value lies within the data one must \u00a0understand all the relationships that are inherent and important in the data. That requires a principled approach to hypothesis generation. \u00a0 One technique, topological data analysis (TDA), is exceptional at surfacing hidden relationships that exist in the data and identifying those relationships that are meaningful without having to ask specific questions of the data. The result is an output that is able to represent complex phenomena, and is therefore able to surface weaker signals as well as the stronger signals. \u00a0 This permits the detection of emergent phenomena. \u00a0 As a result, enterprises can now discover answers to questions they didn\u2019t even know to ask and do so with data that is unlabeled. Once the data set is understood through intelligent discovery, supervised approaches are applied to predict what will happen in the future. These types of problems include classification, regression and ranking.  For this pillar, most companies use a standard set of supervised machine learning algorithms including random forests, gradient boosting, linear/sparse learners. It should be noted, however, that the unsupervised work from the previous step is highly useful in many ways. For example, it can generate relevant features for use in prediction tasks or finding local patches of data where supervised algorithms may struggle (systematic errors). The predict phase is an important part of the business value associated with data science, however, generally, in predictive analytics, there exists a notion that this is the sum total of machine learning.  This is not the case by far.  Prediction, while important, is pretty well understood and does not, on its own qualify as \u201cintelligence.\u201d \u00a0It goes back to calling a maxed out Excel table and a linear regressor AI. It is just not the case. Further, Prediction can go wrong along a number of dimensions, particularly if the groups on which you are predicting are racked with some type of bias (algorithmic, sampling etc.)  Again, Prediction is key and provides tremendous business value done correctly, but in and of itself it is not AI. We need to stop calling it as such. Applications need to support interaction with humans in a way which makes outcomes recognizable and believable. For example, when one builds a predictive model, it is important to have an explanation of how the model is doing what it is doing, i.e. what the features in the model are doing in terms that are familiar to the users of the model. \u00a0This level of familiarity is important in generating trust and intuition.  Similarly, in the same way that automobiles have mechanisms not just for detecting the presence of a malfunction, but also for specifying the nature of the malfunction and suggesting a method for correcting it, so one needs to have a \u201cnuts and bolts\u201d understanding of how an application is working in order to \u201crepair\u201d it when it goes awry. \u00a0 Transparency AND Justification. There is a difference. Transparency tells you what algorithms and parameters were used, while, Justification tells you why. For intelligence to be meaningful, it must be able to justify and explain its assertions, as well as to be able to diagnose failures.  No business leader should deploy intelligent and autonomous applications against critical business problems without a thorough understanding of what variables power the model.  Enterprises cannot move to a model of intelligent applications without trust and transparency. \u00a0 AI without UX is of limited utility.  UX is what distributes that intelligence across the organization and pushes it to the edge \u2013 where it can consumed by practitioners and subject matter experts.  Ultimately, the process of operationalizing an intelligent application within the enterprise requires some change in the organization, an acceptance that the application will evolve over time and that will demand downstream changes \u2013 automated or otherwise.  \u00a0For this to happen, intelligent applications need to be \u201clive\u201d in the business process, seeing new data and automatically executing the loop of Discover, Predict, Justify on a frequency that makes sense for that business process. For some processes that may be quarterly, for others daily. That loop can even be measured in seconds. \u00a0 Intelligent systems are designed to detect and react as the data evolves. An intelligent system is one that is always learning, live in the workflow and constantly improving. \u00a0In the modern data world, an application that is not getting more intelligent is getting dumber.  Intelligent applications are designed to detect and react when data distributions evolve. As a result, they need to be \u201con the wire\u201d in order to detect that phenomena before it becomes a problem. Too many solutions provide an answer in a point of time, an intelligent system is one that is always learning through the framework outlined here. This is what defines intelligence \u2013 not a machine learning algorithm kicking out predictions or the results of a data scientist\u2019s work as a PDF. For the industry to continue to grow and evolve we need to start doing a better job of recognizing what is truly AI and what is imposter-ware.  Let\u2019s make it a point of celebrating the real AI work.", "links": ["https://www.ayasdi.com", "https://www.ayasdi.com/platform/", "https://www.ayasdi.com/industry-overview/", "https://www.ayasdi.com/financial-services/", "https://www.ayasdi.com/healthcare/", "https://www.ayasdi.com/public-sector/", "https://www.ayasdi.com/solutions/anti-money-laundering/", "https://www.ayasdi.com/solutions/clinical-variation-management/", "https://www.ayasdi.com/solutions/denials/", "https://www.ayasdi.com/solutions/regulatory-risk/", "https://www.ayasdi.com/solutions/population-health/", "https://www.ayasdi.com/company/", "https://www.ayasdi.com/company/leadership/", "https://www.ayasdi.com/company/careers/", "https://www.ayasdi.com/company/collaborators/", "https://www.ayasdi.com/company/customers/", "https://www.ayasdi.com/company/news-and-events/", "https://www.ayasdi.com/company/contact-us/", "https://www.ayasdi.com/blog/", "https://www.ayasdi.com/request-a-demo/", "https://www.ayasdi.com/blog/author/gurjeet-singh/", "https://www.ayasdi.com/blog/category/artificial-intelligence/", "https://www.ayasdi.com/blog/category/bigdata/", "https://www.ayasdi.com/blog/category/machine-intelligence/", "https://www.ayasdi.com/blog/category/machine-learning/", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/08/02144138/BG2.png", "https://priceonomics.com/the-discovery-of-statistical-regression/", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/05/12132340/Machine_Intelligence_Apps_WP_051617v01.pdf", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/08/02144141/DS1.png", "https://www.wired.com/brandlab/2015/05/jeff-hawkins-firing-silicon-brain/", "http://cdn.ayasdi.com/wp-content/uploads/2015/02/wp-tda-and-machine-learning.pdf", "https://www.ayasdi.com/blog/aml/intelligent-segmentation-as-the-attack-point-for-aml/", "http://feedburner.google.com/fb/a/mailverify?uri=AyasdiBlog", "https://ayasdicommunity.force.com/login", "https://www.ayasdi.com/terms-of-service/", "https://www.ayasdi.com/privacy-policy/", "https://www.ayasdi.com/pilot-terms/", "http://www.youtube.com/user/ayasdi", "http://feeds.feedburner.com/AyasdiBlog", "http://www.linkedin.com/company/ayasdi", "https://www.facebook.com/ayasdi", "https://twitter.com/ayasdi"]}, "91": {"url": "https://www.mapd.com/blog/2017/08/03/whos-got-the-best-mobile-network-in-the-us/", "title": "", "text": "Based on the deluge of commercials and ads in this competitive industry, it seems every major player claims that they do. The competition is apparently so fierce that even their spokesmen are being headhunted. Admittedly, it can be a bit confusing when you find these results in one city:  And these in another:  By the way, how do you even define \u2018best\u2019 network? It\u2019s a loaded question for sure. We\u2019ll get back to that in a minute. It\u2019s a question with relevance to nearly every American, considering the vast majority of us own a cellphone of some kind (over 95% according to Pew). And we don\u2019t leave them alone; we touch them thousands of times each day. In fact, a good mobile network can affect our very quality of life. Ever travelled any significant distance with children and poor mobile data speeds? The same question is also relevant to those very telecommunications providers themselves, of course, and particularly in light of the fact that obtaining new subscribers will increasingly mean winning them away from competitors. Ensuring you are outpacing your rivals with superior network metrics is a key ingredient to both marketing and market share success. Our friends and partners at Tutela have created the world\u2019s largest mobile information platform, collecting billions of crowdsourced data points anonymously. These span device, network, and application information to allow us to get from data to insights. They\u2019ve graciously given us (and now you) a slice of their treasure trove. Our demo contains a few months of US data (~112M rows), although Tutela has data for more metrics, for nearly every country, and for a much greater timeframe. There are obviously many variables to consider when it comes to a great network, and one carrier will almost certainly not take the top spot in each category. But we can identify areas of significance where we hope to find favorable results: Spoiler alert: we\u2019re not actually going to attempt to find a winner. Those are just some of the metrics you can see at a glance on our MapD Immerse demo dashboard:  This is immediately useful. Coverage, speed, signal strength, and latency all matter, so MapD allows you to visualize and correlate them for effective consumption. Not coincidentally, the MapD Immerse visualization tool is purpose-built to leverage the MapD Core backend GPU-powered, in-memory, columnar SQL database at scale. This allows  interactive analysis over multi-billion row data sets, with the ability to drill down to an individual row...in milliseconds. It\u2019s also why Tutela has selected to partner with MapD; no need for indexing, sampling, or pre-computation. This means you can zoom in and out, filter, and more in an ad hoc fashion. Let\u2019s take it for a spin. As a consumer, you might be interested in how well each service provider performs in a certain geographic region (i.e., where you live and work) to aid in making a purchasing decision.  For example, to zoom from country level to the Washington, DC area, just type the city name into the searchbox of the pointmap. MapD Immerse will automatically zoom in when you click enter. It looks like in this region, you might lean toward Verizon:  But it may not be as clear cut if you live in San Francisco:  In either case, Sprint better have some compelling discounts or incentives to entice new subscribers. Further, both Sprint and T-Mobile might leverage this information to prioritize buildout or upgrade of cell towers. If they\u2019re analyzing subscriber loss in these areas, this also may give them insight into why that could be occurring. Another valuable feature of the MapD platform is the ability to easily identify trends, anomalies, or outliers. Visualization can often be an invaluable enabler for these situations. Sometimes new revenue opportunities or cost savings can be had by capitalizing on correlations in data that you didn\u2019t even realize existed! See the spike in data downloads in the line chart? By highlighting the time frame with your mouse, MapD will cross filter all charts using the selected time slice. Doing the same with the line chart for data uploads further reveals that the largest spikes occur on May 28 and 30. Ah, that\u2019s Memorial Day weekend in the United States. So this makes sense.  We can get even more granular and note that the bulk of data transfer occurred between 12-1am local time (4-5am UTC) as the heatmap indicates. I wonder if Uber and Lyft also experienced a spike around that time, as celebrations came to an end?  When data exploration at scale is so easy, what other interesting insights will you find? Explore this demo using Tutela's dataset on MapD\u2019s platform on your own here.  If you\u2019d like to learn more about MapD\u2019s recently announced partnership with Tutela, watch our on-demand webinar, introducing and demonstrating Tutela Explorer, a new mobile data analytics solution that provides real-time, interactive and highly visual insight into the performance of mobile networks and device usage. Tutela Explorer is available now as-a-service with global data coverage from Tutela.", "links": ["http://www.mapd.com/blog", "https://www.nytimes.com/2016/10/15/business/sprint-verizon-hear-me-now-paul-marcarelli.html", "http://www.pewinternet.org/fact-sheet/mobile/", "http://www.networkworld.com/article/3092446/smartphones/we-touch-our-phones-2617-times-a-day-says-study.html", "https://www.tutela.com/explorer/", "https://www.mapd.com/demos/telecom", "https://www.mapd.com/demos/telecom", "https://www.mapd.com/platform/core/", "https://www.mapd.com/demos/telecom", "https://www.brighttalk.com/webcast/14525/267851", "https://www.tutela.com/explorer"]}, "92": {"url": "https://www.oreilly.com/learning/building-a-simple-graphql-server-with-neo4j", "title": "Building a simple GraphQL server with Neo4j", "text": "How to implement a GraphQL API that queries Neo4j for a simple movie app. If you're interested in making the move from REST to GraphQL, check out this OSCON video from the team at Github. GraphQL is a powerful new tool for building APIs that allows clients to ask for only the data they need. Originally designed at Facebook to minimize data sent over the wire and reduce round-trip API requests for rendering views in native mobile apps, GraphQL has since been open sourced to a healthy community that is building developer tools. There are also a number of large companies and startups such as GitHub, Yelp, Coursera, Shopify, and Mattermark building public and internal GraphQL APIs. Despite what the name seems to imply, GraphQL is not a query language for graph databases, it is instead an API query language and runtime for building APIs. The \u201cGraph\u201d component of the name comes from the graph data model that GraphQL uses in the frontend. GraphQL itself is simply a specification, and there are many great tools available for building GraphQL APIs in almost every language. In this post we'll make use of graphql-tools by Apollo to build a simple GraphQL API in JavaScript that queries a Neo4j graph database for movies and movie recommendations. We will follow a recipe approach: first, exploring the problem in more detail, then developing our solution, and finally we discuss our approach. Good resources for learning more about GraphQL are GraphQL.org and the Apollo Dev Blog. GraphQL by design can work with any database or backend system, however in this example we'll be using the Neo4j graph database as our persistence layer. Why use a graph database with GraphQL? The idea of application data as a graph is an underpinning design choice of GraphQL. For example, think of customers who have placed orders that contain products\u2014that's a graph! GraphQL enables developers to translate their backend data into the application data graph on the frontend, but if we use a graph database on the backend we can do away with this impedance mismatch and we have graphs all the way down. We'd like to build a simple GraphQL API for a movie app that can do two things: In a traditional REST-ish API approach we might create two separate endpoints, perhaps /movies/search and /movies/similar. One endpoint to search for movies by substring of the title, and another to return a list of similar movies. As we add additional features we might keep adding endpoints. With GraphQL, our API is served from a single endpoint /graphql that takes one or more GraphQL queries and returns JSON data in a shape that is specified by the GraphQL query - only the data requested by the client is returned. Let's see how we can build this simple API. We'll be building a simple node.js JavaScript web server using Express.js to serve our GraphQL endpoint1. First of all we\u2019ll need a Neo4j database (with data) for our GraphQL server to query. For this example we\u2019ll make use of a Neo4j Sandbox instance. Neo4j Sandbox allows us to quickly spin up a hosted Neo4j instance, optionally with existing datasets focused around specific use cases. We\u2019ll use the Recommendations Neo4j Sandbox which includes data about movies and movie reviews and is designed to be used for generating personalized recommendations (for example, by using collaborative filtering to recommend movies based on similar users\u2019 reviews). We'll be making use of a few dependencies for this GraphQL server. I won't list them all here (you can see the full list in the package.json for this project, but there a few worth noting: We\u2019ll follow the \u201cGraphQL First\u201d development paradigm. In this approach, we start by defining a GraphQL schema. This schema defines the types and queries available in our API and then becomes the specification for the API. If we were building a complete application, the frontend developers could use this schema to build out the frontend while the backend team builds the backend in parallel, speeding development. Once we\u2019ve defined our schema we\u2019ll need to create resolver functions that are responsible for fetching data from Neo4j. This schema will define the types and GraphQL queries that we'll be able to use in our API. You can think of the schema as the API blueprint. schema.js Each GraphQL field is resolved by a resolver function. The resolver function defines how data is fetched for that field. resolvers.js Now that we've defined the GraphQL schema and resolver functions we are ready to serve the GraphQL endpoint, using Express.js. server.js If you've ever used Express this should look familiar, you'll notice that we're creating two endpoints and serving them on localhost:8080 Let's take a look at what we just did: Now that we have our API running, let's use the GraphiQL in-browser IDE for GraphQL to search for movies whose titles contain \"Matrix\" and find similar movies for each matching movie. We can load GraphiQL by opening http://loalhost:8080/graphiql in our browser and querying with this GraphQL query:   Querying our GraphQL movies API using GraphiQL, the in-browser IDE for GraphQL GraphQL is still new to almost everyone with the exception of Facebook, where it has been used since 2012, so many conventions and best practices are still being developed. However, GraphQL First Development is a philosophy the community has adopted which gives structure to the process of building a GraphQL API. By defining a contract for the API, expressed as a GraphQL schema, the frontend and backend developer teams can independently implement their applications, using the schema as a guide. To make GraphQL First Development even easier with Neo4j, the Neo4j team has built neo4j-graphql-cli, a command line tool for easily spinning up a Neo4j backed GraphQL endpoint based on a user defined GraphQL schema that can optionally be annotated with Cypher queries - exposing the power of Cypher from within GraphQL. You can learn more about this here. 1 All code for this project is available on GitHub . This example is also available as an Apollo Launchpad Pad, which you can run live in the browser.\u21a9 William Lyon is an engineer on the Developer Relations team at Neo4j, the open source graph database, where he builds tools for integrating Neo4j with other technologies and helps users be successful with graphs. He also leads Neo4j's Data Journalism Accelerator Program. Prior to Neo4j, he worked as a software engineer for a variety of startups, building mobile apps for the real estate industry, trading tools for quantitative finance, and predictive APIs. William holds a masters degree in Computer Science from the University of Montana. You can... How to package up your own repositories for use via PHP's Composer. Will content-blocking change the Web? Learn how to use Node.js and Express.js to quickly bootstrap a web server, tie in MongoDB, and display it all using dynamic HTML templates Diversity and inclusion in the tech industry has experienced a severe failure. Erica Baker approaches the topic like we would for any other service failure: with a postmortem. \u00a9 2017 O'Reilly Media, Inc. All trademarks and registered trademarks appearing on oreilly.com are the property of their respective owners.", "links": ["https://www.safaribooksonline.com/home/?utm_medium=content&utm_source=oreilly.com&utm_campaign=lgen&utm_content=20170601+nav", "http://www.oreilly.com/conferences/", "http://shop.oreilly.com/", "https://www.safaribooksonline.com/home/?utm_medium=content&utm_source=oreilly.com&utm_campaign=lgen&utm_content=20170601+nav", "http://www.oreilly.com/conferences/", "http://shop.oreilly.com/", "https://pixabay.com/en/louvre-pyramid-mesh-perspective-2189967/", "https://www.safaribooksonline.com/library/view/oscon-2017-/9781491976227/video306664.html?utm_source=oreilly&utm_medium=newsite&utm_campaign=building-a-simple-graphql-server-with-neo4j", "https://github.com/apollographql/graphql-tools", "http://www.apollodata.com/", "http://graphql.org/", "https://dev-blog.apollodata.com/", "https://neo4j.com/sandbox-v2/", "https://github.com/johnymontana/movies-graphql-neo4j-server/blob/master/package.json", "http://dev.apollodata.com/tools/graphql-tools/index.html", "https://expressjs.com/", "https://github.com/neo4j/neo4j-javascript-driver", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://www.graph.cool/docs/faq/graphql-sdl-schema-definition-language-kr84dktnp0/", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "http://loalhost:8080/graphiql", "https://www.npmjs.com/package/neo4j-graphql-cli", "http://neo4j.com/developer/graphql", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://pixabay.com/en/louvre-pyramid-mesh-perspective-2189967/", "https://twitter.com/share", "http://oreilly.com/about/", "http://oreilly.com/work-with-us.html", "http://oreilly.com/careers/", "http://shop.oreilly.com/category/customer-service.do", "http://shop.oreilly.com/category/customer-service.do", "http://fb.co/OReilly", "http://twitter.com/oreillymedia", "https://www.youtube.com/user/OreillyMedia", "https://plus.google.com/+oreillymedia", "https://www.linkedin.com/company/oreilly-media", "http://oreilly.com/terms/", "http://oreilly.com/privacy.html", "http://www.oreilly.com/about/editorial_independence.html"]}, "93": {"url": "http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "title": "", "text": "", "links": ["http://www.getrevue.co/?ref=Revue+Profile", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profileimage&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profiledate&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profilename&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lXPZA?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lXPZA?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blogs.technet.microsoft.com/machinelearning/2017/07/25/lessons-learned-benchmarking-fast-machine-learning-algorithms/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/DM9WJ?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/DM9WJ?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://medium.com/@stathis/design-by-evolution-393e41863f98?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/XYWKo?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/XYWKo?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.datasciencelearner.com/complete-overview-learning-python-data-analysis/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/6nba0?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/6nba0?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://reinforce.io/blog/introduction-to-tensorforce/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xekd3?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xekd3?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.kdnuggets.com/2017/05/top-10-machine-learning-videos-on-youtube-updated.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Ka8JK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Ka8JK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://arxiv.org/abs/1704.01568?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/96KED?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/96KED?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://datascienceplus.com/random-forests-in-r/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Vd1wK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Vd1wK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blog.keras.io/the-future-of-deep-learning.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/eKr5e?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/eKr5e?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://medium.com/airbnb-engineering/using-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/odRNK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/odRNK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://machinelearning.apple.com/2017/07/07/GAN.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xekoa?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xekoa?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://arxiv.org/abs/1707.06642?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/mxRw2?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/mxRw2?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.techleer.com/articles/200-naive-bayes-machine-learning-algorithm-for-classification-problems/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/3K1nx?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/3K1nx?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463?gi=7fe02170b69e&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/OnXK7?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/OnXK7?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://aiukraine.com/en/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/nqRVq?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/nqRVq?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.bonaccorso.eu/2017/07/23/machine-learning-algorithms/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Z4bBj?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Z4bBj?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/jobs", "http://bit.ly/2uqHV4J", "https://flyelephant.net/jobs/Spinbackup-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/jobs/Spinbackup-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/yJRmd?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/yJRmd?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/jobs/Snap-Ukraine-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/jobs/Snap-Ukraine-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/jAR3B?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/jAR3B?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%238%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "https://www.getrevue.co/?utm_source=Data Science Digest&utm_medium=email&utm_content=footerlink&utm_campaign=Issue", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=forwarded&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%238%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "http://www.getrevue.co/?ref=Revue+Profile"]}, "94": {"url": "https://blog.insightdatascience.com/preparing-for-the-transition-to-applied-ai-d41e48403447", "title": "Transitioning from Software Engineering to Artificial Intelligence", "text": "Emmanuel Ameisen and Jeremy Karnowski, Insight AI A significant part of the Software Engineer role requires staying up-to-date with evolving frameworks, standards, and paradigms. Software Engineers strive to constantly learn, in order to always use the best tool for the job. As Machine Learning finds footholds in more applications every day, it has naturally become a topic that many Engineers want to master. Machine Learning, though, is harder to pick up than a new framework. To be an efficient practitioner, you require a solid understanding of the theory of the field, broad knowledge of the current state of the art, and an ability to frame problems in a non deterministic way. Many guides you can find online will simply teach you how to train an out-of-the-box model on a curated data set to achieve good accuracy and call it a day. The truth is that a much more extensive skillset is essential in becoming an effective Machine Learning Engineer. Below is a distillation of the many conversations we\u2019ve had with over 50 top Machine Learning teams all over The Bay Area and New York, who\u2019ve come to Insight to find AI Practitioners poised to tackle their problems and accelerate their expansion into Applied AI. Deploying a Machine Learning solution requires much more than just training an arbitrary model on your data. It requires an understanding of: In other words, in addition to engineering chops, you need to understand the fundamentals of statistics, linear algebra, and optimization theory in order to integrate, deploy, and debug models. Building a custom Machine Learning solution for a problem requires that you consider issues ranging from acquiring, labeling and pre-processing your data to building, updating, and serving an inference model, and everything in between. Finally, building a REST API for a standard web-app is a task that we can deem feasible ahead of time. Machine Learning models, on the other hand are not always guaranteed to converge, or produce usable outputs. The best way to learn how to scope and deliver impactful Machine Learning products, is to understand how their theoretical underpinnings relate to the taxonomy of your data. In order to understand Machine Learning, a solid knowledge of statistics fundamentals is essential. This involves understanding the following: When you are training a neural network, what is actually happening? What makes some tasks doable and others not? A good approach to this might be to first try to understand Machine Learning through graphics and examples, before diving deeper into the theory. Concepts to understand range from how different loss functions work, why back propagation is useful, or what a computational graph is. A deep understanding is crucial both for building a functional model, and to communicate about it efficiently to the rest of the organization. Following are a few resources, starting with high level overviews, and diving deeper. Another fundamental skill is the ability to read, understand and implement research papers. It can seem like a daunting task at first, so a good way to start is to look up a paper that already has code attached to it (on GitXiv for example) and try to understand the implementation in depth. Ask any Data Scientist and they\u2019ll tell you 90% of the work they do is data munging. This is just as important for Applied AI, as the success of your model correlates hugely with the quality (and quantity) of your data. Data work comes in many aspects, and falls within a few categories: The best way to get familiar with data wrangling is to grab a dataset in the wild and try to use it. There are many datasets online and many social media and news outlets sites have great APIs. Following the steps above, a good way to learn is to: Debugging Machine Learning algorithms that fail to converge or to give sensible results involves a very different process from debugging code. In the same vein, finding the right architecture and hyperparameters requires solid theoretical fundamentals, but also good infrastructure work to be able to test different configurations out. Because of the pace at which the fields evolve, the methods to debug models are constantly evolving. Here are a few \u201csanity checks\u201d from our discussions and experience deploying models that mirror in some ways the principles of KISS familiar to many Software Engineers. A lot of those steps can be accelerated significantly by your development skills, which brings us to our last skill. A lot of Applied Machine Learning will allow you to leverage Software Engineering skills, sometimes with a little twist. These skills include: For more details on some of the software skills we recommend acquiring to become a quality Machine Learning Engineer, check out our post dedicated to transitioning to Applied AI from Academia. The resources above will help you approach and tackle actual Machine Learning problems. But the field of Applied AI changes extremely quickly, and the best way to learn, is to get your hands dirty and actually try to build out an end-to-end solution to solve a real problem. Action Items: Find some inspiration, then dive in! Remember that while Machine Learning Engineering is about building products at heart, there is a research aspect to it. You will explore models and paradigms that will prove unsuccessful, and that is perfectly fine, as it will lead you to understand the intricacies of the problem better. AI is an exciting, ever-changing field. The demand for Machine Learning Engineers is strong, and it is easy to get overwhelmed with the amount of news surrounding the topic. We recommend following a few serious sources and newsletters, to be able to separate PR and abstract research from innovations that are immediately relevant to the field. Here are some sources to help out: Want to learn applied Artificial Intelligence from top professionals in Silicon Valley or New York? Learn more about the Artificial Intelligence program.", "links": ["https://blog.insightdatascience.com?source=logo-lo_25cba469823c---d02e65779d7b", "https://twitter.com/InsightDataSci", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.insightdatascience.com%2Fpreparing-for-the-transition-to-applied-ai-d41e48403447", "https://blog.insightdatascience.com", "https://blog.insightdatascience.com/tagged/about-insight", "https://blog.insightdatascience.com/tagged/insight-data-science", "https://blog.insightdatascience.com/tagged/insight-data-engineering", "https://blog.insightdatascience.com/tagged/insight-health-data", "https://blog.insightdatascience.com/tagged/insight-ai", "https://blog.insightdatascience.com/search", "https://blog.insightdatascience.com/@emmanuelameisen?source=post_header_lockup", "https://blog.insightdatascience.com/@emmanuelameisen?source=post_header_lockup", "https://www.udacity.com/course/deep-learning--ud730", "http://cs231n.stanford.edu/", "http://cs224d.stanford.edu/", "http://www.deeplearningbook.org/", "http://www.gitxiv.com/", "https://data.sfgov.org/", "https://www.data.gov/", "https://dev.twitter.com/streaming/public", "http://developer.nytimes.com/docs", "http://people.apache.org/~fhanik/kiss.html", "http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf", "https://github.com/JasperSnoek/spearmint", "https://blog.insightdatascience.com/preparing-for-the-transition-to-applied-ai-8eaf53624079", "https://news.ycombinator.com/", "https://jack-clark.net/import-ai/", "http://www.insightdatascience.com/blog/", "https://blog.insightdatascience.com/tagged/insight-ai", "https://blog.insightdatascience.com/using-deep-learning-to-reconstruct-high-resolution-audio-29deee8b7ccd", "https://blog.insightdatascience.com/separating-overlapping-chromosomes-with-deep-learning-based-image-segmentation-22f97afd3283", "http://insightdata.ai/notifications-list", "http:insightdata.ai", "https://medium.com/@jkarnows?source=post_page", "https://medium.com/@InsightData?source=post_page", "https://medium.com/@rossfadely?source=post_page", "https://blog.insightdatascience.com/tagged/machine-learning?source=post", "https://blog.insightdatascience.com/tagged/ai?source=post", "https://blog.insightdatascience.com/tagged/software-engineering?source=post", "https://blog.insightdatascience.com/tagged/insight-ai?source=post", "https://blog.insightdatascience.com/tagged/deep-learning?source=post", "https://blog.insightdatascience.com/@emmanuelameisen?source=footer_card", "https://blog.insightdatascience.com/@emmanuelameisen", "http://twitter.com/EmmanuelAmeisen", "https://blog.insightdatascience.com?source=footer_card", "https://blog.insightdatascience.com?source=footer_card", "https://blog.insightdatascience.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "95": {"url": "http://vooban.com/en/tips-articles-geek-stuff/hyperopt-tutorial-for-optimizing-neural-networks-hyperparameters/", "title": "", "text": "Hyperopt is a way to search through an hyperparameter space. For example, it can use the Tree-structured Parzen Estimator (TPE) algorithm, which explore intelligently the search space while narrowing down to the estimated best parameters. It is hence a good method for meta-optimizing a neural network which is itself an optimisation problem: tuning a neural network uses gradient descent methods, and tuning the hyperparameters needs to be done differently since gradient descent can\u2019t apply. Therefore, Hyperopt can be useful not only for tuning hyperparameters such as the learning rate, but also to tune more fancy parameters in a flexible way, such as changing the number of layers of certain types, or the number of neurons in a layer, or even the type of layer to use at a certain place in the network given an array of choices, each with nested tunable hyperparameters. This is an oriented random search, in contrast with a Grid Search where hyperparameters are pre-established with fixed steps increase. Random Search for Hyper-Parameter Optimization (such as what Hyperopt do) has proven to be an effective search technique. The paper about this technique sits among the most cited deep learning papers. To sum up, it is more efficient to search randomly through values and to intelligently narrow the search space rather than looping on fixed sets of values for the hyperparameters. Note that this blog post is also available as a Notebook on GitHub. It contains code that can be run with Jupyter. A parameter is defined with a certain uniformrange or else a probability distribution, such as: There is also a few quantized versions of those functions, which rounds the generated values at each step of \u201cq\u201d: It is also possible to use a \u201cchoice\u201d which can lead to hyperparameter nesting: Visualisations of the parameters for probability distributions can be found below. Then, more details on choices and parameter nesting will come. Note on the above charts (especially for the loguniform and uniform distributions): the blurred line averaging the values fades out toward the ends of the signal since it is zero-padded. The line ideally would not fade out by using techniques such as mirror-padding. Those are the best distributions for modeling the values a learning rate. That\u2019s because we want to observe changes in the learning rate according to changing it with multiplications rather than additions, e.g.: when adjusting the learning rate, we\u2019ll want to try to divide it or multiply it by 2 rather than adding and substracting a finite value. To proove this, let\u2019s generate a loguniform distribution for a multiplier of the learning rate, centered at 1.0. Dividing 1 by those values should yield the same distribution. Let\u2019s now define a simple search space and solve for f(x) = x^2 - x + 1, where x is an hyperparameter. Let\u2019s solve for minimizing f(x, y) = x^2 + y^2 using a space using a python dict as structure. Later, this will neable us to nest hyperparameters with choices in a clean way. Yet, we have defined spaces as a single parameter. But that is 1D. Normally, spaces contain many parameters. Let\u2019s define a more complex one and with one nested hyperparameter choice for an uniform float: This will require us to import a few more things, and return the results with a dict that has a \u201cstatus\u201d and \u201closs\u201d key at least. Let\u2019s keep in our return dict the evaluated space too as this may come in handy if we save results to disk. Note that the optimization could be parallelized by using MongoDB and storing the trials\u2019 state here. Althought this is a built-in feature of hyperopt, let\u2019s keep things simple for our examples here. Indeed, the TPE algorithm used by the fmin function has state which is stored in the trials and which is useful to narrow the search space dynamically once we have a few trials. It is then interesting to pause and resume a training, and to apply that to a real problem. This is what\u2019s done inside the hyperopt_optimize.py file of the GitHub repository for this project. There, as an example, we optimize a convolutional neural network for solving the CIFAR-100 problem. Star Fork You might as well like this other blog post of mine on how to use Git Large File Storage (Git LFS) to handle the versioning of huge files when working with machine learning projects. 510-1015 av. Wilfrid-Pelletier  Quebec, QC, Canada  G1W 0C4 418 800.0027 TOLL FREE: 1 844 800.0027 info@vooban.com", "links": ["http://vooban.com/en/", "http://vooban.com/en/", "http://vooban.com/en/", "http://vooban.com/en/products/", "http://vooban.com/en/products/#tpti", "http://vooban.com/en/products/#tdti", "http://vooban.com/en/approach/", "http://vooban.com/en/tactical-squad/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/", "http://vooban.com/en/tips-articles-geek-stuff/", "https://pdfs.semanticscholar.org/9f2a/efc3821853e963beda011ed770f740385b77.pdf", "http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a", "https://github.com/terryum/awesome-deep-learning-papers#optimization--training-techniques", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100/blob/master/IntroductionToHyperopt.ipynb", "http://jupyter.org/", "http://vooban.com/wp-content/uploads/2017/08/histogram_1_int.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_2_uniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_3_loguniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_4_normal.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_5_lognormal.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_6_inverse_loguniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_7_inverse_lognormal.png", "http://vooban.com/wp-content/uploads/2017/08/fx2.png", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100/fork", "http://vooban.com/en/tips-articles-geek-stuff/migrating-to-git-lfs-for-developing-deep-learning-applications-with-large-files/", "http://vooban.com/en/tips-articles-geek-stuff/discover-structure-behind-data-with-decision-trees/", "http://vooban.com/en/tips-articles-geek-stuff/", "https://www.google.ca/maps/place/Vooban/@46.7694849,-71.3055195,17z/data=!3m1!4b1!4m5!3m4!1s0x4cb890d721a9234b:0x979022270b4a9fa4!8m2!3d46.7694849!4d-71.3033308", "http://vooban.com/en/products/", "http://vooban.com/en/approach/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/", "https://twitter.com/vooban", "https://www.instagram.com/voobanvibe/", "https://www.facebook.com/vooban", "https://www.linkedin.com/company/vooban", "http://vooban.com/en/", "http://vooban.com/en/products/", "http://vooban.com/en/products/#tpti", "http://vooban.com/en/products/#tdti", "http://vooban.com/en/approach/", "http://vooban.com/en/tactical-squad/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/"]}, "96": {"url": "https://elitedatascience.com/learn-python-for-data-science", "title": "How to Learn Python for Data Science in 2017 (Updated)", "text": "In this guide, we\u2019ll cover how to learn Python for data science, including our favorite curriculum for self-study. You see, data science is about problem solving, exploration, and extracting valuable information from data. To do so effectively, you\u2019ll need to wrangle\u00a0datasets, train machine learning models, visualize results, and much more. Enter Python. This is the best time ever to learn Python. In fact, Forbes named it a top 10 technical skill in terms of job demand growth. Let\u2019s discuss why\u2026 Python is one of the most widespread languages in the world, and it has a passionate community of users: Python Popularity, TIOBE Index It has an even more loyal following within the data science profession. Some people judge the quality of a programming language by the simplicity of its \"hello, world!\" program. Python does pretty well by this standard: For comparison, here's the same output in Java: Great, case closed. See you back here\u00a0after you've mastered Python? Well, in all seriousness, simplicity is one of Python's greatest strengths. Thanks to its precise and efficient syntax, Python can accomplish the same tasks with less code than other languages. This makes implementing\u00a0solutions refreshingly fast. In addition, Python's vibrant data science community means you'll be able to find plenty of tutorials, code snippets, and people to commiserate with\u00a0fixes to common bugs. Stackoverflow will be one of your best friends. Finally,\u00a0Python\u00a0has an\u00a0all-star lineup of libraries (a.k.a. packages) for data analysis and machine learning, which drastically reduce the time it takes to produce results. More on these later. Before we go into what you'll need to learn, let's discuss what you won't need. You won't need a C.S. degree. Most data scientists will never deal with topics such as memory leaks, cryptography, or \"Big O\" notation. You'll be fine as long as you can write clean, logical code in a scripting language such as Python or R. You won't need a complete course on Python. Python and data science are\u00a0not synonymous. You won't need to memorize all the syntax. Instead, focus on grasping the intuition, such as when function is appropriate or how conditional statements work. You'll gradually remember the syntax after Googling, reading documentation, and good ol' fashioned practice. We recommend a top-down approach. We advocate\u00a0a top-down approach with the goal of getting results first and then solidifying concepts over time. In fact, we prefer to cut out \"classroom\" study in favor of real-world practice. This approach will allow you to build mastery over time while having more fun. There are many ways to install Python on your computer, but we recommend the Anaconda bundle, which comes with the libraries you'll need for data science. Effective programming is not about memorizing syntax, but rather mastering a new way of thinking. Therefore, take your time in building a solid foundation of core programming concepts. These will help you translate solutions in your head into instructions for a computer. If you are completely new to programming, we recommend the excellent Automate the Boring Stuff with Python book, which has been released for free online under a creative commons license. The book promises \"practical programming for total beginners,\" and it keeps each lesson down-to-earth. Read up to Chapter 6 - Manipulating Strings and complete the practice questions along the way. If you only need to brush up on Python syntax, then we recommend the following video, aptly named \"Learn Python in One Video:\" Again, the goal of this step is not to learn everything about Python and programming. Instead, focus on the intuition. You should be able to answer questions such as: If you'd like more practice with the core programming concepts, check out the following resources. Next, we're going to focus on the for data science part of \"how to learn Python for data science.\" As we mentioned earlier, Python has an all-star lineup of libraries for data science. Libraries are simply bundles of pre-existing functions and objects that you can import into your script to save time. These are the action steps we recommend for efficiently picking up a new library: We don't recommend diving much deeper into a library right now because you'll likely forget most of what you've learned by the time you jump into projects. Instead, aim to discover what each library is capable of. If you installed Python through the Anaconda bundle as we recommended above, it will also come with Jupyter Notebook. Jupyter Notebook is a lightweight IDE that's a favorite among data scientists. We recommend it for your projects. You can open a new notebook through Anaconda Navigator, which came with Anaconda. Check out this short video for instructions. These are the essential libraries you'll need: NumPy allows easy and efficient numeric computation, and many other data science libraries are built on top of it. Pandas is high-performance library for data structures and exploratory analysis. It's built on top of NumPy. Matplotlib is a flexible plotting and visualization library. It's powerful but somewhat cumbersome. You have the option of skipping Matplotlib for now and using Seaborn to get started (see our Seaborn recommendation below). Scikit-Learn is the premier general-purpose machine learning library in Python. It has many popular algorithms and modules for pre-processing, cross-validation, and much more. Seaborn makes it much easier to plot common data visualizations. It's built on top of Matplotlib and offers a more pleasant high-level wrapper. By now, you'll have a basic understanding of programming and a working knowledge of essential libraries. This actually covers most of the Python you'll need to get started with data science. At this point, some students will feel a bit overwhelmed. That's OK, and it's perfectly normal. If you were to take the slow and traditional bottom-up approach, you might feel less overwhelmed, but it would have taken you 10 times as long to get here. Now the key is to dive in immediately and start gluing everything together. Again, our goal up to here has been to just learn enough to get started. Next, it's time to solidify your knowledge through plenty of practice and projects. You have several options. The first option is to participate on Kaggle, a site that hosts data science competitions. The main advantage of Kaggle is that every project is self-contained. You're given the dataset, a goal, and tutorials to get you started. The major disadvantage of competitions is that they're usually not representative of real-world data science. The \"Getting Started\" competitions are way too basic while the standard competitions (i.e. those with prize pools) are usually too tough for beginners. If you're interested in this path, check out our Beginner's Guide to Kaggle. The next option is to structure your own projects and pick datasets that interest you. The main advantage of this approach is that the projects are more representative of real-world data science. You'll likely need to define your own goals, collect data, clean your dataset, engineer features, and so on. The disadvantage of DIY projects is that you'll need to already be familiar with a proper data science workflow. Without one, you could miss important steps or get stuck without knowing how to proceed. If you go with this path, check out our free 7-day crash course on applied machine learning, which covers the key steps in a data science workflow. We also have another article with several\u00a0DIY project ideas. Finally, there are guided end-to-end projects. Proper guided projects should combine the best of both words - they should be representative of real-world data science and allow you to solidify your skills through a carefully planned learning curve. Many data science bootcamps offer this as a main benefit. Bootcamps usually conclude with a \"capstone project\" that allows you to see all the moving pieces together, from start to finish. We've also crafted our own\u00a0Machine Learning Masterclass to solve this exact need. It will provide you over-the-shoulder mentorship for real-world projects while teaching you all of the key concepts in context. The masterclass also includes a comprehensive Python course that gets you up to speed ASAP. In fact, many successful students have enrolled without any prior programming experience. Learn more about it here. [\u2026] In terms of programming languages \u2013 Python is pretty powerful language with integration to all technologies in the CDH stack. For a basic course check out the link below \u2013 Python For Data Science [\u2026] * Denotes Required Field Copyright \u00a9 2017 \u00b7 EliteDataScience.com \u00b7 All Rights Reserved", "links": ["https://elitedatascience.com/", "https://elitedatascience.com/", "https://elitedatascience.com/blog", "https://elitedatascience.com/machine-learning-masterclass", "https://elitedatascience.com/about", "https://elitedatascience.com/about", "https://elitedatascience.com/guest-post-submissions", "https://elitedatascience.com/author/slice2o", "https://elitedatascience.com/learn-python-for-data-science#comments", "https://www.facebook.com/sharer.php?u=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://plus.google.com/share?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://www.linkedin.com/shareArticle?trk=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://twitter.com/intent/tweet?text=How%20to%20Learn%20Python%20for%20Data%20Science%20%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "https://www.forbes.com/sites/jeffkauflin/2017/01/08/the-10-technical-skills-with-explosive-growth-in-job-demand/#6fa6bf344f5c", "http://www.tiobe.com/tiobe-index/", "https://stackoverflow.com/questions/tagged/python?sort=votes&pageSize=15", "https://www.continuum.io/downloads", "https://automatetheboringstuff.com/", "https://automatetheboringstuff.com/", "https://codefights.com/", "http://www.pythonchallenge.com/index.php", "http://www.practicepython.org/", "http://interactivepython.org/runestone/static/thinkcspy/index.html", "http://jupyter.org/", "https://www.youtube.com/watch?v=-MyjG00la2k", "https://docs.scipy.org/doc/numpy/user/", "https://docs.scipy.org/doc/numpy-dev/user/quickstart.html", "https://pandas.pydata.org/pandas-docs/stable/", "https://pandas.pydata.org/pandas-docs/stable/10min.html", "https://matplotlib.org/contents.html", "https://matplotlib.org/users/pyplot_tutorial.html", "http://scikit-learn.org/stable/documentation.html", "http://elitedatascience.com/python-machine-learning-tutorial-scikit-learn", "https://seaborn.pydata.org/", "http://elitedatascience.com/python-seaborn-tutorial", "http://elitedatascience.com/beginner-kaggle", "http://elitedatascience.com/", "http://elitedatascience.com/machine-learning-projects-for-beginners", "http://elitedatascience.com/machine-learning-masterclass", "http://elitedatascience.com/machine-learning-masterclass", "https://www.facebook.com/sharer.php?u=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://plus.google.com/share?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://www.linkedin.com/shareArticle?trk=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://twitter.com/intent/tweet?text=How%20to%20Learn%20Python%20for%20Data%20Science%20%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "https://datascienceandmore.wordpress.com/2017/03/04/getting-the-basics-down/", "https://elitedatascience.com/learn-machine-learning", "https://elitedatascience.com/learn-python-for-data-science", "https://elitedatascience.com/feature-engineering-best-practices", "https://elitedatascience.com/beginner-kaggle", "https://elitedatascience.com/imbalanced-classes", "https://elitedatascience.com/beginner-mistakes", "https://elitedatascience.com/bias-variance-tradeoff", "https://elitedatascience.com/data-science-resources", "https://elitedatascience.com/guest-post-submissions", "https://elitedatascience.com/", "https://elitedatascience.com/terms-of-service", "https://elitedatascience.com/privacy-policy", "https://elitedatascience.com/about", "https://www.linkedin.com/shareArticle?trk=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science", "http://www.facebook.com/sharer.php?u=https://elitedatascience.com/learn-python-for-data-science", "https://plus.google.com/share?text=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science", "https://twitter.com/intent/tweet?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "http://service.weibo.com/share/share.php?url=https://elitedatascience.com/learn-python-for-data-science&title=How to Learn Python for Data Science (Updated)", "https://getpocket.com/save?title=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science"]}, "97": {"url": "https://stats.stackexchange.com/questions/297380/why-not-just-dump-the-neural-networks-and-deep-learning", "title": "", "text": "Fundamental problem with deep learning and neural networks in general. The solutions that fit training data are infinite. We don't have precise mathematical equation that is satisfied by only a single one and that we can say generalizes best. Simply speaking we don't know which generalizes best. Optimizing weights is not a convex problem, so we never know we end up with a global or a local minimum. So why not just dump the neural networks and instead search for a better ML model? Something that we understand, and something that is consistent with a set of mathematical equations? Linear and SVM do not have this mathematical drawbacks and are fully consistent with a a set of mathematical equations. Why not just think on same lines (need not be linear though) and come up with a new ML model better than Linear and SVM and neural networks and deep learning? Not being able to know what solution generalizes best is an issue, but it shouldn't deter us from otherwise using a good solution. Humans themselves often do not known what generalizes best (consider, for example, competing unifying theories of physics), but that doesn't cause us too many problems. It has been shown that it is extremely rare for training to fail because of local minimums. Most of the local minimums in a deep neural network are close in value to the global minimum, so this is not an issue. source But the broader answer is that you can talk all day about nonconvexity and model selection, and people will still use neural networks simply because they work better than anything else (at least on things like image classification).  Of course there are also people arguing that we shouldn't get too focused on CNNs like the community was focused on SVMs a few decades ago, and instead keep looking for the next big thing. In particular, I think I remember Hinton regretting the effectiveness of CNNs as something which might hinder research. related post As the comments to your question point out, there are a lot of people working on finding something better. I would though like to answer this question by expanding the comment left by @josh All models are wrong but some are useful (Wiki) The above statement is a general truth used to describe the nature of statistical models. Using data that we have available, we can create models that let us do useful things such as approximate a predicted value. Take for example Linear Regression  Using a number of observations, we can fit a model to give us an approximate value for a dependent variable given any value(s) for the independent variable(s).  Burnham, K. P.; Anderson, D. R. (2002), Model Selection and Multimodel > Inference: A Practical Information-Theoretic Approach (2nd ed.): \"A model is a simplification or approximation of reality and hence   will not reflect all of reality. ... Box noted that \u201call models are   wrong, but some are useful.\u201d While a model can never be \u201ctruth,\u201d a   model might be ranked from very useful, to useful, to somewhat useful   to, finally, essentially useless.\" Deviations from our model (as can be seen in the image above) appear random, some observations are below the line and some are above, but our regression line shows a general correlation. Whilst deviations in our model appear random, in realistic scenarios there will be other factors at play which cause this deviation. For example, imagine watching cars as they drove through a junction where they must turn either left or right to continue, the cars turn in no particular pattern. Whilst we could say that the direction the cars turn is completely random, does every driver reach the junction and at that point make a random decision of which way to turn? In reality they are probably heading somewhere specific for a specific reason, and without attempting to stop each car to ask them about their reasoning, we can only describe their actions as random.  Where we are able to fit a model with minimal deviation, how certain can we be that an unknown, unnoticed or immeasurable variable wont at some point throw our model? Does the flap of a butterfly\u2019s wings in Brazil set off a tornado in Texas? The problem with using the Linear and SVN models you mention alone is that we are somewhat required to manually observe our variables and how they each affect each other. We then need to decide what variables are important and write a task-specific algorithm. This can be straight forward if we only have a few variables, but what if we had thousands? What if we wanted to create a generalised image recognition model, could this realistically be achieved with this approach?  Deep Learning and Artificial Neural Networks (ANNs) can help us create useful models for huge data sets containing huge amounts of variables (e.g. image libraries). As you mention, there's an incomprehensible number of solutions which could fit the data using ANNs, but is this number really any different to the amount of solutions we would need to develop ourselves through trial and error? The application of ANNs do much of the work for us, we can specify our inputs and our desired outputs (and tweak them later to make improvements) and leave it up to the ANN to figure out the solution. This is why ANNs are often described as \"black boxes\". From a given input they output an approximation, however (in general terms) these approximations don't include details on how they were approximated. And so it really comes down to what problem you are trying to solve, as the problem will dictate what model approach is more useful. Models are not absolutely accurate and so there is always an element of being 'wrong', however the more accurate your results the more useful they are. Having more detail in the results on how the approximation was made may also be useful, depending on the problem it may even be more useful than increased accuracy. If for example you are calculating a persons credit score, using regression and SVMs provides calculations that can be better explored. Being able to both tweak the model directly and explain to customers the effect separate independent variables have on their overall score is very useful. An ANN may aid in processing larger amounts of variables to achieve a more accurate score, but would this accuracy be more useful? I guess for some problem we care less for the mathematical rigor and simplicity but more for its utility, current status is neural network is better in performing certain task like pattern recognition in image processing.  The global minimum may as well as be useless, so we don't really care if we find it or not. The reason is that, for deep networks, not only the time to find it becomes exponentially longer as the network size increases, but also the global minimum often corresponds to overfitting the training set. Thus the generalization ability of the DNN (which is what we really care about) would suffer. Also, often we prefer flatter minima corresponding to a higher value of the loss function, than sharper minima corresponding to a lower value of the loss function, because the second one will deal very badly with uncertainty in the inputs. This is becoming increasingly clear with the development of Bayesian Deep Learning. Robust Optimization beats Determinist Optimization very often, when applied to real world problems where uncertainty is important. Finally, it's a fact that DNNs just kick the ass of methods such as XGBoost at image classification and NLP. A company which must make a profit out of image classification will correctly select them as modelsto be deployed in production (and invest a significant amount of money on feature engineering, data pipeline, etc. but I digress). This doesn't mean that they dominate all the ML environment: for example, they do worse than XGBoost on structured data (see the last winners of Kaggle competitions) and they seem to not still do as well as particle filters on time series modelling. However, some very recent innovations on RNNs may modify this situation. There are excellent answers, mostly weighing in with the usefulness of DL and ANNs. But I would like to object the OP in a more fundamental way, since the question already takes for granted the mathematical inconsistency of neural networks. First of all, there is a mathematical theory behind (most models of) Neural Networks. You could likewise argue that linear regression does not generalize, unless the underlying model is... well, linear. In neural algorithms, a model is assumed (even if not explicitly) and the fitting error is computed. The fact that algorithms are modified with various heuristics does not void the original mathematical support. BTW, local optimization is also a mathematically consistent, let alone useful, theory. Along this line, if Neural Networks just constitute one class of methods within the whole toolbox of scientists, which is the line that separates Neural Networks from the rest of techniques? In fact, SVMs were once considered a class of NNs and they still appear in the same books. On the other hand, NNs could be regarded as a (nonlinear) regression technique, maybe with some simplification. I agree with the OP that we must search better, well founded, efficient algorithms, regardless you label them as NNs or not. Sign up using Google Sign up using Facebook Sign up using Email and Password   By posting your answer, you agree to the privacy policy and terms of service. asked 3 days ago viewed  7,704 times  active yesterday                                   site design / logo \u00a9 2017 Stack Exchange Inc; user contributions licensed under cc by-sa 3.0                                  with attribution required.                                  rev 2017.8.11.26777", "links": ["https://stats.stackexchange.com", "https://chat.stackexchange.com?tab=site&host=stats.stackexchange.com", "https://stats.stackexchange.com", "https://stats.meta.stackexchange.com", "https://stats.stackexchange.com/users/signup?ssrc=site_switcher&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.stackexchange.com/users/login?ssrc=site_switcher&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stackexchange.com/sites", "https://stackoverflow.blog", "https://stackexchange.com", "https://stats.stackexchange.com/users/signup?ssrc=head&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.stackexchange.com/users/login?ssrc=head&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.meta.stackexchange.com", "https://stackoverflow.com/company/about", "https://www.stackoverflowbusiness.com/?ref=topbar_help", "https://stats.stackexchange.com", "https://arxiv.org/pdf/1406.2572.pdf", "https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/clyohpf/?context=3", "https://arxiv.org/pdf/1406.2572.pdf", "https://arxiv.org/pdf/1412.0233.pdf", "https://en.wikipedia.org/wiki/All_models_are_wrong", "https://en.wikipedia.org/wiki/Butterfly_effect", "https://stats.stackexchange.com/questions/93705/meaning-of-a-neural-network-as-a-black-box", "https://stackexchange.com/legal/privacy-policy", "https://stackexchange.com/legal/terms-of-service", "http://stats.stackexchange.com/election", "http://stats.stackexchange.com/election", "https://stackexchange.com/questions?tab=hot", "https://electronics.stackexchange.com/questions/322908/batteries-why-use-9v", "https://mathematica.stackexchange.com/questions/153694/difference-between-and", "https://money.stackexchange.com/questions/83759/are-my-parents-ripping-me-off-with-this-deal-that-doesnt-allow-me-to-build-my-e", "https://interpersonal.stackexchange.com/questions/1737/how-do-i-tell-a-co-worker-that-just-because-something-is-a-hobby-doesnt-mean-i", "https://latin.stackexchange.com/questions/5045/why-is-the-roman-acronym-spqr-and-not-spr", "https://math.stackexchange.com/questions/2392029/why-does-rudin-say-the-rational-number-system-is-inadequate-as-a-field", "https://money.stackexchange.com/questions/83755/do-people-tend-to-spend-less-when-using-cash-than-credit-cards", "https://math.stackexchange.com/questions/2392728/proving-that-the-limit-of-an-abstract-function-with-certain-properties-is-zero", "https://money.stackexchange.com/questions/83708/what-emergencies-could-justify-a-highly-liquid-emergency-fund", "https://movies.stackexchange.com/questions/78741/why-is-the-movie-titled-the-phantom-menace", "https://space.stackexchange.com/questions/22590/how-did-people-know-how-to-build-the-first-space-ship", "https://rpg.stackexchange.com/questions/105287/when-do-you-need-a-free-hand-for-casting-shield", "https://math.stackexchange.com/questions/2392440/is-this-an-issue-with-the-law-of-the-excluded-middle-or-an-issue-with-the-proof", "https://math.stackexchange.com/questions/2392479/when-is-a-vector-glued-to-the-origin", "https://math.stackexchange.com/questions/2392823/identity-elements-in-a-cyclic-group", "https://superuser.com/questions/1239044/can-i-write-200-mb-150-min-music-to-a-700-mb-80-min-cd", "https://security.stackexchange.com/questions/167412/isnt-the-ubuntus-system-prompt-for-my-password-a-bit-unsafe", "https://puzzling.stackexchange.com/questions/54195/measure-22-minutes-with-7-and-13-minute-hourglasses", "https://tex.stackexchange.com/questions/386239/tikz-fractal-uniform-cantor-set", "https://stats.stackexchange.com/questions/297711/what-is-the-difference-between-moment-generating-function-and-probability-genera", "https://salesforce.stackexchange.com/questions/188321/sandbox-verification-emails", "https://travel.stackexchange.com/questions/100105/what-power-supply-is-needed-to-use-a-cpap-machine-in-the-eu", "https://apple.stackexchange.com/questions/295071/c-programming-on-macos", "https://physics.stackexchange.com/questions/350805/seeing-something-from-only-one-angle-means-you-have-only-seen-what-of-its-su", "https://chat.stackexchange.com?tab=site&host=stats.stackexchange.com", "https://stats.meta.stackexchange.com", "https://stackoverflow.com/company/about", "https://stackoverflow.com", "https://www.stackoverflowbusiness.com/?utm_source=so-footer&utm_medium=referral&utm_campaign=brand-activation", "https://stackoverflow.com/jobs", "https://stackoverflow.com/company/about", "https://stackoverflow.com/company/press", "https://stackexchange.com/legal", "https://stackexchange.com/legal/privacy-policy", "https://stackexchange.com", "https://stackexchange.com/sites#technology", "https://stackexchange.com/sites#lifearts", "https://stackexchange.com/sites#culturerecreation", "https://stackexchange.com/sites#science", "https://api.stackexchange.com", "https://data.stackexchange.com", "https://stackoverflow.blog?blb=1", "https://www.facebook.com/officialstackoverflow/", "https://twitter.com/stackoverflow", "https://linkedin.com/company/stack-overflow", "https://creativecommons.org/licenses/by-sa/3.0/", "https://stackoverflow.blog/2009/06/25/attribution-required/"]}, "98": {"url": "https://blog.godatadriven.com/practical-airflow-tutorial", "title": "", "text": "Airflow is a scheduler for workflows such as data pipelines, similar to Luigi and Oozie. It's written in Python and we at GoDataDriven have been contributing to it in the last few months. This tutorial is loosely based on the Airflow tutorial in the official documentation. It will walk you through the basics of setting up Airflow and creating an Airflow workflow, and it will give you some practical tips. A (possibly) more up-to-date version of this blog can be found in my git repo. Setting up a basic configuration of Airflow is pretty straightforward. After installing the Python package, we'll need a database to store some data and start the core Airflow services. You can skip this section if Airflow is already set up. Make sure that you can run airflow commands, know where to put your DAGs and have access to the web UI. Airflow is installable with pip via a simple pip install apache-airflow. Either use a separate Python virtual environment or install it in your default python environment. To use the conda virtual environment as defined in environment.yml from my git repo: You should now have an (almost) working Airflow installation. Alternatively, install Airflow yourself by running: Airflow used to be packaged as airflow but is packaged as apache-airflow since version 1.8.1. Make sure that you install any extra packages with the right Python package: e.g. use pip install apache-airflow[dask] if you've installed apache-airflow and do not use pip install airflow[dask]. Leaving out the prefix apache- will install an old version of Airflow next to your current version, leading to a world of hurt. You may run into problems if you don't have the right binaries or Python packages installed for certain backends or operators. When specifying support for e.g. PostgreSQL when installing extra Airflow packages, make sure the database is installed; do a brew install postgresql or apt-get install postgresql before the pip install apache-airflow[postgres]. Similarly, when running into HiveOperator errors, do a pip install apache-airflow[hive] and make sure you can use Hive. Before you can use Airflow you have to initialize its database. The database contains information about historical & running workflows, connections to external data sources,  user management, etc. Once the database is set up, Airflow's UI can be accessed by running a web server and workflows can be started. The default database is a SQLite database, which is fine for this tutorial. In a production setting you'll probably be using something like MySQL or PostgreSQL. You'll probably want to back it up as this database stores the state of everything related to Airflow. Airflow will use the directory set in the environment variable AIRFLOW_HOME to store its configuration and our SQlite database. This directory will be used after your first Airflow command. If you don't set the environment variable AIRFLOW_HOME, Airflow will create the directory ~/airflow/ to put its files in. Set environment variable AIRFLOW_HOME to e.g. your current directory $(pwd): or any other suitable directory. Next, initialize the database: Now start the web server and go to localhost:8080 to check out the UI: It should look something like this:  With the web server running workflows can be started from a new terminal window. Open a new terminal, activate the virtual environment and set the environment variable AIRFLOW_HOME for this terminal as well: Make sure that you're an in the same directory as before when using $(pwd). Run a supplied example: And check in the web UI that it has run by going to Browse -> Task Instances. This concludes all the setting up that you need for this tutorial. We'll create a workflow by specifying actions as a Directed Acyclic Graph (DAG) in Python. The tasks of a workflow make up a Graph; the graph is Directed because the tasks are ordered; and we don't want to get stuck in an eternal loop so the graph also has to be Acyclic. The figure below shows an example of a DAG:  The DAG of this tutorial is a bit easier. It will consist of the following tasks: and we'll plan daily execution of this workflow. Go to the folder that you've designated to be your AIRFLOW_HOME and find the DAGs folder located in subfolder dags/ (if you cannot find, check the setting dags_folder in $AIRFLOW_HOME/airflow.cfg). Create a Python file with the name airflow_tutorial.py that will contain your DAG. Your workflow will automatically be picked up and scheduled to run. First we'll configure settings that are shared by all our tasks. Settings for tasks can be passed as arguments when creating them, but we can also pass a dictionary with default values to the DAG. This allows us to share default arguments for all the tasks in our DAG is the best place to set e.g. the owner and start date of our DAG. Add the following import and dictionary to airflow_tutorial.py to specify the owner, start time, and retry settings that are shared by our tasks: These settings tell Airflow that this workflow is owned by 'me', that the workflow is valid since June 1st of 2017, it should not send emails and it is allowed to retry the workflow once if it fails with a delay of 5 minutes. Other common default arguments are email settings on failure and the end time. We'll now create a DAG object that will contain our tasks. Name it airflow_tutorial_v01 and pass default_args: With schedule_interval='0 * * * *' we've specified a run at every hour 0; the DAG will run each day at 00:00. See crontab.guru for help deciphering cron schedule expressions. Alternatively, you can use strings like '@daily' and '@hourly'. We've used a context manager to create a DAG (new since 1.8). All the tasks for the DAG should be indented to indicate that they are part of this DAG. Without this context manager you'd have to set the dag parameter for each of your tasks. Airflow will generate DAG runs from the start_date with the specified schedule_interval. Once a DAG is active, Airflow continuously checks in the database if all the DAG runs have successfully ran since the start_date. Any missing DAG runs are automatically scheduled. When you initialize on 2016-01-04 a DAG with a start_date at 2016-01-01 and a daily schedule_interval, Airflow will schedule DAG runs for all the days between 2016-01-01 and 2016-01-04. A run starts after the time for the run has passed. The time for which the workflow runs is called the execution_date. The daily workflow for 2016-06-02 runs after 2016-06-02 23:59 and the hourly workflow for 2016-07-03 01:00 starts after 2016-07-03 01:59. From the ETL viewpoint this makes sense: you can only process the daily data for a day after it has passed. This can, however, ask for some juggling with date for other workflows. For Machine Learning models you may want to use all the data up to a given date, you'll have to add the schedule_interval to your execution_date somewhere in the workflow logic. Because Airflow saves all the (scheduled) DAG runs in its database, you should not change the start_date and schedule_interval of a DAG. Instead, up the version number of the DAG (e.g. airflow_tutorial_v02) and avoid running unnecessary tasks by using the web interface or command line tools Timezones and especially daylight savings can mean trouble when scheduling things, so keep your Airflow machine in UTC. You don't want to skip an hour because daylight savings kicks in (or out). Tasks are represented by operators that either perform an action, transfer data, or sense if something has been done. Examples of actions are running a bash script or calling a Python function; of transfers are copying tables between databases or uploading a file; and of sensors are checking if a file exists or data has been added to a database. We'll create a workflow consisting of three tasks: we'll print 'hello', wait for 10 seconds and finally print 'world'.  The first two are done with the BashOperator and the latter with the PythonOperator. Give each operator an unique task ID and something to do: Note how we can pass bash commands in the BashOperator and that the PythonOperator asks for a Python function that can be called. Dependencies in tasks are added by setting other actions as upstream (or downstream).  Link the operations in a chain so that sleep will be run after print_hello and is followed by print_world; print_hello -> sleep -> print_world: After rearranging the code your final DAG should look something like: First check that DAG file contains valid Python code by executing the file with Python: You can manually test a single task for a given execution_date with airflow test: This runs the task locally as if it was for 2017-07-01, ignoring other tasks and without communicating to the database. Now that you're confident that your dag works, turn on the DAG in the web UI and sit back while Airflow starts backfilling the dag runs! You now know the basics of setting up Airflow, creating a DAG and turning it on; time to go deeper!  Address: Wibautstraat 202, 1091 GS Amsterdam, The Netherlands           Phone: +31 (0)35 672 9069           Email: signal@godatadriven.com", "links": ["http://godatadriven.com/contact", "http://godatadriven.com/careers", "http://blog.godatadriven.com", "http://godatadriven.com/business-consultancy", "http://godatadriven.com/big-data-infrastructure", "http://godatadriven.com/predictive-modelling", "http://godatadriven.com/products-overview", "http://godatadriven.com/data-discovery", "http://godatadriven.com/data-science-audit", "http://godatadriven.com/data-science-suite", "http://godatadriven.com/data-science-box", "http://godatadriven.com/divolte", "http://godatadriven.com/customers", "http://godatadriven.com/casestudy-airfranceklm", "http://godatadriven.com/casestudy-bakkersland", "http://godatadriven.com/casestudy-bol", "http://godatadriven.com/casestudy-eneco", "http://godatadriven.com/casestudy-eretail", "http://godatadriven.com/casestudy-ing", "http://godatadriven.com/casestudy-nuon", "http://godatadriven.com/casestudy-npo", "http://godatadriven.com/casestudy-schipholgroup", "http://godatadriven.com/casestudy-transavia", "http://godatadriven.com/technology-overview", "http://godatadriven.com/databricks", "http://godatadriven.com/hadoop", "http://godatadriven.com/python", "http://godatadriven.com/r", "http://godatadriven.com/spark", "http://godatadriven.com/training-overview", "http://godatadriven.com/training-schedule", "http://godatadriven.com/bigdata-training", "http://godatadriven.com/datascience-training", "http://godatadriven.com/data-science-accelerator-program", "http://godatadriven.com/meetups", "http://godatadriven.com/events", "http://blog.godatadriven.com", "http://godatadriven.com/our-story", "http://godatadriven.com/team", "http://godatadriven.com/careers", "http://godatadriven.com/media-resources", "https://blog.godatadriven.com", "http://godatadriven.com/contact", "http://godatadriven.com", "http://blog.godatadriven.com", "https://airflow.incubator.apache.org/", "https://github.com/spotify/luigi", "https://oozie.apache.org/", "https://blog.godatadriven.com/open-source-201702", "https://blog.godatadriven.com/open-source-201703", "https://blog.godatadriven.com/open-source-201704", "https://blog.godatadriven.com/open-source-201705", "https://blog.godatadriven.com/open-source-201707", "https://pythonhosted.org/airflow/tutorial.html", "https://github.com/hgrif/airflow-tutorial", "https://github.com/hgrif/airflow-tutorial", "http://conda.pydata.org/miniconda.html", "http://localhost:8080/", "https://airflow.incubator.apache.org/configuration.html", "https://airflow.incubator.apache.org/security.html", "https://github.com/apache/incubator-airflow/tree/master/scripts", "https://crontab.guru/#0_*_*_*_*", "https://jeffknupp.com/blog/2016/03/07/python-with-context-managers/", "https://airflow.incubator.apache.org/configuration.html#connections", "https://airflow.incubator.apache.org/ui.html#variable-view", "https://airflow.incubator.apache.org/tutorial.html#templating-with-jinja", "https://github.com/apache/incubator-airflow/blob/master/airflow/example_dags/example_bash_operator.py", "https://airflow.incubator.apache.org/code.html#airflow.operators.PythonOperator", "https://pythonhosted.org/airflow/tutorial.html", "https://gtoonstra.github.io/etl-with-airflow/", "https://medium.com/handy-tech/airflow-tips-tricks-and-pitfalls-9ba53fba14eb", "http://godatadriven.com/players/henk-griffioen", "http://blog.godatadriven.com/practical-airflow-tutorial", "http://blog.godatadriven.com/open-source-201707", "http://blog.godatadriven.com/cd-python-eggs-vsts", "http://blog.godatadriven.com/ldappy", "http://blog.godatadriven.com/open-source-201706", "http://blog.godatadriven.com/vendor-free-ds", "https://twitter.com/GoDataDriven", "http://godatadriven.com/business-consultancy", "http://godatadriven.com/big-data-infrastructure", "http://godatadriven.com/predictive-modelling", "http://godatadriven.com/products-overview", "http://godatadriven.com/data-discovery", "http://godatadriven.com/data-science-audit", "http://godatadriven.com/data-science-suite", "http://godatadriven.com/data-science-box", "http://godatadriven.com/divolte", "http://godatadriven.com/customers", "http://godatadriven.com/casestudy-airfranceklm", "http://godatadriven.com/casestudy-bakkersland", "http://godatadriven.com/casestudy-bol", "http://godatadriven.com/casestudy-eneco", "http://godatadriven.com/casestudy-eretail", "http://godatadriven.com/casestudy-ing", "http://godatadriven.com/casestudy-nuon", "http://godatadriven.com/casestudy-npo", "http://godatadriven.com/casestudy-schipholgroup", "http://godatadriven.com/casestudy-transavia", "http://godatadriven.com/technology-overview", "http://godatadriven.com/hadoop", "http://godatadriven.com/python", "http://godatadriven.com/r", "http://godatadriven.com/spark", "http://godatadriven.com/training-overview", "http://godatadriven.com/training-schedule", "http://godatadriven.com/bigdata-training", "http://godatadriven.com/datascience-training", "http://godatadriven.com/data-science-accelerator-program", "http://godatadriven.com/events", "http://godatadriven.com/meetups", "http://blog.godatadriven.com", "http://godatadriven.com/our-story", "http://godatadriven.com/team", "http://godatadriven.com/careers", "http://godatadriven.com/media-resources", "http://godatadriven.com/news", "http://godatadriven.com/contact", "https://cdn.xebia.com/documents/Cookies+Policy+Xebia+20170130.pdf"]}, "99": {"url": "https://concepttoclinic.drivendata.org/", "title": "", "text": "Lung Cancer Early Detection Challenge  We\u2019re calling on a global community of data scientists, engineers, designers, and researchers to build an open source software application that brings advances from machine learning into the clinic. We\u2019re not just optimizing an algorithm for a single metric\u2014we\u2019re collaborating to build tools which put AI in the hands of clinicians. In addition to pushing forward the cutting-edge of open clinical software, top contributors will be eligible for a share of $100,000 in monetary prizes generously provided by the Bonnie J. Addario Lung Cancer Foundation.          Contribute now by grabbing an issue from the project's GitHub repository and submitting a PR!        Lung cancer causes more deaths each year than any other cancer in the US, and early detection makes a big difference. Recent advances in machine learning \u2013 including open source algorithms from the 2017 Data Science Bowl \u2013 help find and interpret early signs of cancer. Using imagery from CT scans, this application will build on cutting-edge algorithms in machine learning to help clinicians identify and interpret lung nodules. Your guide to contributing throughout the challenge. All you need to start contributing is a passion for building great software and a github account.  Contributions of all sizes can earn points as they add value to the project, by building on pre-designated issues, building out new content, and building up the community. $100,000 in cash, as well as other in-kind prizes, will be available for top contributors filling key roles throughout the challenge. A live leaderboard keeps track of points earned throughout the challenge by top contributors filling key roles. Meet the amazing group of of technical, clinical, and domain experts that are helping to guide the end-to-end design of this challenge. The Addario Lung Cancer Foundation (ALCF) has set the audacious goal of making lung cancer a chronically managed disease by 2023.  Ultimately, the goal of this project is to produce a tool that draws on machine intelligence, is useful to clinicians, and makes a lasting difference in the lives of patients. We hope you will find a way to contribute!  Lung Cancer Early Detection Challenge", "links": ["https://www.drivendata.org/termsofuse/", "https://www.drivendata.org/copyrightpolicy/", "https://www.drivendata.org/privacypolicy/", "https://www.drivendata.org/", "http://www.drivendata.co/"]}, "100": {"url": "http://appsilondatascience.com/blog/rstats/2017/08/09/shiny-collections.html", "title": "shiny.collections,  Google Docs-like live collaboration in ShinyAppsilon Data Science Blog", "text": "A few weeks ago, our CTO Marek Rogala gave an engaging talk at the User Conference in Brussels. Despite being one of the last talks, he drew a crowd that filled the room and had significant viewership online. Marek\u2019s talk was entitled shiny.collections: Google Docs-like live collaboration in Shiny. In short, he went over a package we built that allows for persistence and collaboration without losing reactivity in Shiny. Users can enter information and have continuity, knowing that if they exit and return to the app, their inputs will still be there. Let\u2019s go over his talk. Recently, Shiny users have demanded more from their applications. The advent of cloud applications, especially ones like Google Docs, have accustomed users to collaboration. They expect their Shiny apps to be more than just an interactive tool for data visualisation, but rather, a production ready application that works like any other tool. Their data has to be saved automatically. Interactive collaboration is a must in some use cases. And the application must be delivered as fast as possible. You need to use a reactive database such as rethinkDB, Firebase, or mongoDB to achieve this. mongoDB isn\u2019t really a reactive database, but there are ways where it can be used as one. We are going to use rethinkDB in our case. Also, we are going to use rethinker, which is a rethinkDB driver for R. But using rethinker is not the most ideal solution, as callback do not work very well with Shiny and it is quite painful to configure to our needs. To get around this, we created our own package built upon rethinker called shiny.collections.  It lets you easily connect to your shiny app and takes care of all of the trickiness involved. During his talk, Marek showed a live demo that involved creating a chat application. Check out how to do this yourself. I reccomend watching Marek first, as he has some valuable commentary you won\u2019t find in the blog post. This example is just a simple use case of what you can achieve with shiny.collections. We\u2019ve made it a priority to make it convenient and easily integrable with other powerful tools, including DT, leaflet or rhandsontable. Our goals for the future of this package include getting it on cran and diversifying the functionality of the API. Take a look at the our package and contribute. Get in touch with us for all of you other data science needs as well.  Published 09 Aug 2017", "links": ["https://channel9.msdn.com/Events/useR-international-R-User-conferences/useR-International-R-User-2017-Conference/shinycollections-Google-Docs-like-live-collaboration-in-Shiny#comments", "https://cran.r-project.org/web/packages/rethinker/index.html", "https://appsilon.github.io/shiny.collections/", "http://appsilondatascience.com/blog/rstats/2017/07/02/shiny-chat.html", "http://appsilondatascience.com", "http://disqus.com/?ref_noscript"]}, "101": {"url": "https://blog.statsbot.co/introduction-to-imitation-learning-32334c3b1e7a", "title": "Introduction to Imitation Learning", "text": "This post is the first in the series where we will describe what Imitation Learning is. For today\u2019s article, the Statsbot team asked computer scientist Vitaly Kurin to briefly introduce Imitation Learning and outline the basics of Reinforcement Learning. Living organisms are extremely complex. Even the relatively simple ones such as flies or worms. They are able not only to operate successfully in the real world, but are extremely resilient to changes in conditions. And that\u2019s if we\u2019re not even talking about humans. We can plan ahead, we can alter our plans given new information, and we can collaborate with others and execute our plans more effectively. Science and such projects as CERN or a huge Radio Telescope in Arecibo are perfect examples of human collaboration and the art of planning. Yes, we do a lot of stupid stuff, but let\u2019s not talk about that here and instead focus on the good. Yes, we can now beat the best human in chess or Go, we can get a crazy score in Video Pinball for Atari 2600, we can even already make a pure human broke by challenging him in poker. Can we open a bottle of champagne and celebrate the triumph? I\u2019m afraid not. Yes, machine learning has recently made a significant leap forward. The combination of new Deep Learning ideas with old ones has enabled us to advance in many domains, such as computer vision, speech recognition, and text translation. Reinforcement Learning has also benefited greatly from its marriage with Deep Learning. You\u2019ve definitely heard of Deep Reinforcement Learning success such as achieving superhuman score in Atari 2600 games, solving Go, and making robots learn parkour. Though, we must admit, that operating successfully in the real world is much harder than playing Go or Space Invaders. Many of the tasks are much harder than this. Imagine a kid riding a bicycle in the middle of a crowded city center or a man driving a Porsche 911 on an autobahn at 200 miles per hour. Let\u2019s all admit that we are not there yet. The typical machine learning approach is to train a model from scratch. Give it a million images and some time to figure it out. Give it a week and let it play Space Invaders until it reaches some acceptable score. We, as humans, beg to differ. When a typical human starts to play some game he has never seen, he already has a huge amount of prior information. If he sees a door in Montezuma\u2019s Revenge, he realizes that somewhere there should lie a key and he needs to find it. When he finds the key, he remembers that the closed door is back through the two previous rooms and he returns to open it. When he sees a ladder, he realizes that he can climb it because he has done this hundreds of time already. What if we could somehow transfer human knowledge about the world to an agent? How can we extract all this information? How can we create a model out of it? There is such a way. It\u2019s called Imitation Learning. Imitation Learning is not the only name for leveraging human data for good. Some researchers also call it apprenticeship learning, others refer to it as Learning from Demonstration. From our point of view, there is no substantial difference between all of these titles and we will use Imitation Learning from now on. In order to introduce Imitation Learning, we will need to understand the basics of Reinforcement Learning first. Let\u2019s move on. It\u2019s not hard to get the general idea of the Reinforcement Learning setup. There is some agent and we want this agent to learn some task. Let\u2019s say, we have an Atari 2600 console, the agent has access to the joystick and can see what\u2019s happening on the screen. Let\u2019s say, 60 times per second we give our agent a screenshot of the game and ask him what button he wants to press. If our agent does well, he can see that his score is increasing (positive reinforcement), otherwise we can give him a penalty as a negative reward (negative reinforcement). Gradually, by trial and error, the agent starts to understand that it\u2019s better to avoid some of the actions and do those which bring him a reward. Let\u2019s make it more formal and describe the process stated above mathematically. We can describe the RL framework mentioned above (observe -> act -> get the reward and the next state) as a Markov Decision Process (MDP): where: We also need a definition of a policy function for the next section. Policy is a function, that returns an action given the state: And, actually, our final goal when solving an MDP is to learn such a policy in order to maximize the reward for our agent. Let\u2019s take an example of an MDP. The circles represent the states, arrows with green labels are actions, red labels are the rewards for actions, and the square is the terminal state. The green numeric labels are the transition probabilities. Our student starts in a state with the blue circle. He studies, but this is hard and sometimes boring. He decides to open a Facebook app and once he is there, he can either quit or continue scrolling. He then studies more and more, and finally decides to go to the pub. The state is a smaller filled circle, since now there is an element of randomness, based on the amount of knowledge the student forgets after visiting the pub. He can then either study more and pass the exam (+10 in reward), or he can go to sleep and finish the MDP right now. Since we will use DQN and related ideas in the future, let\u2019s briefly understand what is going on here. The whole approach is built upon approximating the so-called Q function and building the agent\u2019s behavior based on it. The idea of the Q function is the following: it returns you the entire expected discounted reward flow for the particular action and the particular state, given that starting from the next state we will be following our policy \ud835\udfb9. It answers the question: \u201cHow good is to press this button in this state?\u201d The Q function obeys the Bellman equation: And, finally, the Bellman principle of optimality is the following: notwithstanding what happened before, we should always take the action with the highest Q to maximize the reward flow: But how do we get such a Q function, you might ask. Let\u2019s look at an example. Imagine, you want to grab a coffee (+20 in reward) and a chocolate in a vending machine (+10 in reward). Your total reward cannot exceed thirty. Moreover, if you have taken the coffee already, it cannot be higher than 10 (the reward for chocolate) from now on. This is the idea: the Q value for the current step and action is equal to the maximum Q value for the next state (since we behave optimally) + the reward we get for the transition. The value of the quadratic objective function becomes: Q-learning itself is not new. Q-learning which uses neural networks as a function approximators is also not new (e.g. neural fitted-q iteration). A DQN paper was the first to use deep convolutional networks to solve this type of problem and introduced a couple of novelties that make the training process much more stable. First of all, experience replay. The vanilla Q-learning point is to make a step, get the reward and the next state, then update the approximation function parameters based on this transition. The DQN idea is to make the transition and save it in a \u201creplay memory\u201d\u200a\u2014\u200aan array that stores the last 10\u2076 (<insert any large number here>) transitions with the information about the reward, states before and after the transition, and if the event is terminal (game over) or not. Having this experience replay we can randomly sample mini-batches from it and learn more effectively. Another thing that makes the algorithm more stable is that DQN uses two neural networks: the first to compute the Q value for the current state and the second to compute the Q value for the next state. You can see that from the equation with the objective: two different Q functions use \ud835\udfb1 and \ud835\udfb1\u2019, respectively. Each 10,000 steps, the parameters \ud835\udfb1\u2019 are copied from the learned parameters \ud835\udfb1 and this helps a lot in increasing the stability. The problem here with using one function is that when we update the weights, both Q(s,a) and Q(s\u2019,a\u2019) increase and this might lead to oscillations or policy divergence. Using two separate networks adds a delay between an update and computation of the target Q value and reduces such cases. If you have further interest in the phenomena, read the Method section in the DQN Nature paper. Okay, everything described above sounds quite simple. If there is still something you do not understand, please, have a look at David Silver\u2019s lecture where he explains everything perfectly! Knowing all of these, can we build an True AI now? I\u2019m sorry, but we can\u2019t. There are several problems that hinder us from building an agent that will beat ByuN at StarCraft II, bring an autonomous car to the market, or give you an opportunity to buy your grandma a robot that will do the dishes for her after lunch. One of these problems is that the rewards our agent gets might be very sparse in time. Let\u2019s say, you play chess. If you lose, how do you know when you made a catastrophic move? Moreover, it\u2019s highly possible that there was not a catastrophic move, but several average ones. Another problem that is closely connected to the previous one, is the sample-efficiency problem. Or, more honestly, sample-inefficiency. Even to master a simple game such as Space Invaders might take a couple of days in-game time. It\u2019s easy to speed up learning in games since we have access to the simulators, but what if we want to learn something in real life? Unfortunately, physics is not there yet and we cannot speed up time. There is an approach that could potentially solve these problems and a bunch of others\u200a\u2014\u200aImitation Learning, as we mentioned at the beginning of this post. As we said, we, humans, rarely learn something without any prior information. Let\u2019s use this data! What should we do? The idea of Imitation Learning is implicitly giving an agent prior information about the world by mimicking human behavior in some sense. Imitation Learning will not only help us solve the sample-inefficiency or computational feasibility problems, it might potentially make the training process safer. We cannot just put an autonomous car in the middle of the street and let it do whatever it wants. We do not want it to kill humans that are around, destroy someone\u2019s property, or the equipment itself. Pretraining it on a human demonstrator\u2019s data might make the training process faster and avoid undesirable situations. Training a model requires some data. Training a Deep Learning model requires even more data. Training a Deep Reinforcement Learning model requires\u2026 Okay, you get the idea. So, this series is only partly describes what we can do with demonstration data. The main point of all of this is to call for human demonstration datasets, because we do not have many, unfortunately, up to this moment. Okay, we should stop here. The key points of this post are: In the next chapter we will write more about Behavior Cloning\u200a\u2014\u200athe simplest approach to an RL problem that can leverage the human demonstration data. Stay tuned!", "links": ["https://blog.statsbot.co?source=logo-lo_9a5344453a6b---cfc9f21a543a", "https://twitter.com/statsbotco", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.statsbot.co%2Fintroduction-to-imitation-learning-32334c3b1e7a", "https://blog.statsbot.co", "https://blog.statsbot.co/analytics/home", "https://blog.statsbot.co/datascience/home", "https://blog.statsbot.co/design/home", "https://blog.statsbot.co/startups/home", "https://blog.statsbot.co/bots/home", "https://blog.statsbot.co/news/home", "https://blog.statsbot.co/statsbot-digest-b0d7372f842a", "http://statsbot.co?utm_source=blog&utm_medium=navigation&utm_campaign=robot_face", "https://blog.statsbot.co/search", "https://blog.statsbot.co/@yobibyte?source=post_header_lockup", "https://blog.statsbot.co/@yobibyte?source=post_header_lockup", "https://atlas.cern/discover/about", "http://statsbot.co?utm_source=blog&utm_medium=article&utm_campaign=imitation_learning", "https://en.wikipedia.org/wiki/CERN", "https://en.wikipedia.org/wiki/Arecibo_Observatory", "https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning/", "https://deepmind.com/research/alphago/", "https://www.theverge.com/tldr/2017/7/10/15946542/deepmind-parkour-agent-reinforcement-learning", "http://www.ceva-dsp.com/ourblog/wp-content/uploads/sites/3/2016/04/AlphaGo-Lee-Se-dol.png", "https://simple.wikipedia.org/wiki/File:Rl_agent.png", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf", "https://link.springer.com/article/10.1007/BF00992698", "https://pdfs.semanticscholar.org/2820/01869bd502c7917db8b32b75593addfbbc68.pdf", "https://link.springer.com/article/10.1007/BF00992699", "http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html?foxtrotcallback=true", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "http://wiki.teamliquid.net/starcraft2/ByuN", "http://toyoutheartist.co.uk/technique/imitation/", "https://mitpress.mit.edu/books/reinforcement-learning", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html", "https://arxiv.org/abs/1312.5602", "https://www.nature.com/nature/journal/v518/n7540/full/nature14236.html", "https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner", "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/google-analytics-audit-checklist-and-tools-fca7df2f2e7a", "https://blog.statsbot.co/google-analytics-audit-checklist-and-tools-fca7df2f2e7a", "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co/tagged/imitation-learning?source=post", "https://blog.statsbot.co/tagged/data-science?source=post", "https://blog.statsbot.co/tagged/machine-learning?source=post", "https://blog.statsbot.co/tagged/ai?source=post", "https://blog.statsbot.co/tagged/reinforcement-learning?source=post", "https://blog.statsbot.co/@yobibyte?source=footer_card", "https://blog.statsbot.co/@yobibyte", "https://github.com/yobibyte/yobiblog", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "102": {"url": "https://axibase.github.io/atsd-use-cases/DataShorts/Dollar-EX/", "title": "", "text": "Data Source: Federal Reserve Economic Data (FRED) Visualizations: ChartLab Structured Query Language: SQL Console from Axibase Download the Community Edition of Axibase Time Series Database here Data is aggregated daily in visualizations, and annually in SQL queries. Favorable dollar exhange rates are shown in green while less favorable exchange rates are shown in red, based on the overall 5-year average exchange rate. Each section is accompanied by a brief analysis to give the data context. Using computed value settings thresholds have been calculated using the underlying data, the upper 25th percentile of a given exchange rate is considered favorable while the lower 25th percentile is considered less than favorable. The data is coded with an ID which is provided in the index as well. Open any ChartLab display below to navigate through time, or change the metric to reflect the desired exchange rate. Europe Asia North America As the amount of debt carried by various EU member countries grew to increasingly worrisome heights, the Euro plunged in 2015  losing ground against the dollar. Further, as oil prices slumped the European Central Bank began to compensate by launching a program of government bond purchases. The combination of these factors and the growth of the dollar post-recession have all contributed to the currently favorable exchange rate enjoyed by the dollar. Query 1.1 Table 1.1 Figure 1.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index During the worst of the European Debt Crisis which saw European Union per capita debt grow at unprecedented rates across the continent, EU investors sought refuge in the stability of the Swiss Franc, backed by the world-renowned Swiss banking system. Further, the Swiss National Bank removed the peg to the Euro unexpectably in 2015 causing a huge surge in 2015, clearly visible in the visualization below as a sudden change from a favorable exchange rate for dollar holders to an all-time low for the observed time period. Query 1.2 Table 1.2 Figure 1.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Largely influenced by the divisive Brexit vote, the value of the Pound has remained relatively low since the referendum in late June of 2016, which aligns perfectly with the Pound\u2019s slight depreciation in value against the dollar on the chart below. Query 1.3 Table 1.3 Average Exchange Rate: 1.50 Figure 1.3   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index After purchasing trillions of Yen worth of Japanese national debt from the private sector three years ago, the Central Bank of Japan has been watching the gradual decrease in value of the Yen against the dollar. Most analysts, see this fall as controlled and predict a bright future for the Yen as the Central Bank can no longer afford to collect more debt than they already have. However, the Yen\u2019s rise much also be managed as the sudden appreciation of the currency could result in an increase in loan defaults because industries which have taken loans at current Yen prices would be paying them back at a much higher rate. Query 2.1 Table 2.1 Figure 2.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index To prevent dramatic internal inflation, the People\u2019s Bank of China closely regulates local, or onshore, trading of the Yuan by fixing the price each day. Although independent officially, the international, or offshore, price of the Yuan usually remains fairly close to its onshore value. Additionally, the PBoC carefully controls the outflow of capital and often hedges their own currency by maintaining positions in international currencies. Query 2.2 Table 2.2 Figure 2.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index The Hong Kong Dollar is officially pegged to the value of the United States Dollar so any change in relative  value is planned. The \u201cMiracle of the Orient\u201d continues to be one of the most dominant financial markets in the world. Query 2.3 Table 2.3 Figure 2.3   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index One of the financial centers of the world, Singapore is home to the second-busiest port in the world and one of the largest oil refining industries worldwide as well. With one of the highest per capita GDP values, Singapore has cemented its place of international importance despite inexplicably loose financial policy whereby the Monetary Authority of Singapore does not regulate their currency\u2019s value by adjusting interest rates, as is common with most central banking systems. Query 2.4 Table 2.4 Figure 2.4   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Relative to the USD, the Canadian dollar has been making a bit of a resurgence in 2017 as United States GDP numbers missed its targets in quarter two. Although dependant on the price of oil due to their large natural reserves, the diverse Canadian economy is one of the few globally-active nations with a significant trade surplus, although that too have been reduced in recent years. Query 3.1 Table 3.1 Figure 3.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Boasting the highest per capita income of Latin America, the Mexican economy has continued to demonstrate healthy growth over the last several decades. North America\u2019s number one producer of automobiles also enjoys a $46 billion trade surplus with the United States, and is the 9th ranked owner of United States government debt. This dependence on America affords Mexico the ability to spur dramatic internal growth that has led to the creation of a booming electronics sector, but at the cost of a  certain amount of reliance on the United States as roughly half of the country\u2019s imports and exports originate or end up  on U.S. soil. Query 3.2 Table 3.2 Figure 3.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index", "links": ["https://github.com/axibase/atsd-use-cases", "https://fred.stlouisfed.org/categories/94", "https://apps.axibase.com", "https://github.com/axibase/atsd/tree/master/api/sql", "https://axibase.com", "https://axibase.com/products/axibase-time-series-database/", "https://github.com/axibase/atsd/blob/master/installation/README.md#installation", "https://axibase.com/products/axibase-time-series-database/visualization/widgets/configuring-the-widgets/", "https://apps.axibse.com", "https://apps.axibase.com/chartlab/424eb6b2/5/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/9/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/6/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/4/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/2/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/3/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/8/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/7/#fullscreen", "https://github.com/axibase/atsd-use-cases", "https://github.com/axibase", "https://pages.github.com"]}, "103": {"url": "http://byteacademy.co/blog/overview-NLG", "title": "", "text": "NLG (Natural Language Generation), a subfield of Artificial Intelligence, is a hot topic in the technology news today. We hear a lot about AI that can soon replace writers and journalists beginning the era of machine creativity. But, what\u2019s all this fuss about? In this article, we unveil what NLG really is and show that it can bring a lot of benefits to businesses and consumers. In a nutshell, NLG is a sub-field of NLP (Natural Language Processing) that studies methods of automatic transformation of structured data into a human-readable text. In practice, there are two major types of NLG applications: template-based NLG and advanced NLG. \u00a0 Template-based NLG is the simplest solution that uses templates with canned text and placeholders to insert data into them. Such systems heavily rely on hard-coded rules, which makes them less flexible than advanced NLG. Since template-based NLG tools have a limited number of templates and require special data representations, they can not be easily reused across different projects and business use cases. \u00a0 Advanced NLG tools are more flexible thanks to the use of supervised and unsupervised Machine Learning (ML). Rather than tying down structured data to the Procrustean bed of templates, advanced NLG uses neural networks that learn morphological, lexical, and grammar patterns from large corpora of written language. Soft probabilistic methods used in the advanced NLG algorithms allow predicting the likelihood of one word appearing after another, and correcting language errors, such as misspellings. ML algorithms used in the advanced NLG are also better in dealing with new words and expressions not included in the original training samples. \u00a0 Modern NLG service providers such as Narrative Science and Automated Insights prefer advanced NLG methods because they allow creating rich data-driven models that produce intelligent insights from data. These algorithms are much more skillful in making right word choices and writing narratives that reflect intentions and business needs of the NLG users. As an added bonus, advanced NLG models can preprocess and analyze data which makes them not just translators of structured data into text, but automatic analysts able to provide actionable insights. \u00a0 Despite the fact that NLG methods have been used since the 1970s, they got a powerful momentum only recently and thanks to the AI/ML revolution. Today, many startups offer cloud-based NLG services to businesses. NLG is also gaining traction in mass media and journalism. Major American newspapers are already experimenting with the automatic storytelling. For example, in 2016 the Washington Post unveiled its automatic storytelling AI named Heliograph AI. Heliograph was used in the coverage of Rio Olympics and the US Presidential election in 2016. \u00a0 Leveraging data mining techniques and ML models the machine reporter can convert structured statistical data, diagrams, graphs, weather forecasts and other data-rich content into excellent descriptive reports that sound if though they were written by the professional reporters. But, isn\u2019t this dangerous for journalism as a profession? Proponents of automatic storytellers say that they actually free up much time for reporters to add analysis and real insights to stories rather than spending countless hours publishing news and descriptive reports[i]. NLG tools may be used in other innovative ways as well: Benefits of NLG, however, go beyond journalism. There is a growing demand for NLG services among major companies. For example, Quill, an NLG system developed by Narrative Science, is used by such companies as Deloitte, Groupon, and Credit Suisse[iv]. These companies opt for NLG solutions for a reason. Growing acceptance of NLG among businesses makes it a promising field to study. If you want to learn more about NLG, \u00a0Byte Academy offers a Natural Language course that covers Natural Language Processing and Natural Language Generation. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0  Your Name (required)    Your Email (required)    \u00a0 \u00a0 References: [i] \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 WashPost PR Blog (August 5, 2016). The Washington Post Experiments With Automated Storytelling to Help Power 2016 Rio Olympics Coverage. WashPost PR Blog. Retrieved from https://www.washingtonpost.com/pr/wp/2016/08/05/the-washington-post-experiments-with-automated-storytelling-to-help-power-2016-rio-olympics-coverage/?utm_term=.bf63b03c4aeb [ii] \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dayan, Zohar (2015). Hearst, USA Today Sports, & Viralnova Partner With Wibbitz For Video Strategy. Wibbitz Blog. Retrieved from http://blog.wibbitz.com/wibbitz-partners-hearst-usa-today-sports-group-and-viralnova-to-expand-video-strategy [iii] \u00a0\u00a0\u00a0\u00a0\u00a0 Keohane, Joe (2017). What News-Writing Bots Mean for the Future of Journalism. Wired. Retrieved from https://www.wired.com/2017/02/robots-wrote-this-story/ [iv] \u00a0\u00a0\u00a0\u00a0 Narrative Science. Turn Your Data Into Better Decisions With Quill. Retrieved from https://narrativescience.com/Platform info.in@byteacademy.co Byte Dev Licensed by the State of New York, New York State Education Department \u00a9 2017 Byte Academy LLC.  All rights reserved.", "links": ["http://byteacademy.co", "http://byteacademy.co/program/", "http://byteacademy.co/courses/", "http://byteacademy.co/data-science-mini-courses/", "http://byteacademy.co/upcoming-classes/", "http://byteacademy.co/students/", "http://byteacademy.co/hiring/", "http://byteacademy.co/events/", "http://byteacademy.co/blog/", "http://byteacademy.co/india/", "https://byteacademy.fluidreview.com", "http://feeds.feedburner.com/ByteAcademy", "http://byteacademy.co/category/data-science/", "http://byteacademy.co/category/career/", "http://byteacademy.co/category/fintech/", "http://byteacademy.co/category/medtech/", "http://byteacademy.co/category/programming-tips/", "http://byteacademy.co/category/startup/", "http://byteacademy.co/category/student-stories/", "http://byteacademy.co/category/blockchain/", "http://byteacademy.co/category/current-events/", "http://byteacademy.co/category/diversity/", "http://byteacademy.co/category/events/", "http://byteacademy.co/category/finovate/", "http://byteacademy.co/category/light-reads/", "http://byteacademy.co/category/millennials/", "http://byteacademy.co/category/quant/", "http://byteacademy.co/2017/", "http://byteacademy.co/2016/", "http://byteacademy.co/2015/", "http://byteacademy.co/category/data-science/", "http://byteacademy.co/blog/artificial-intelligence-future", "https://www.washingtonpost.com/pr/wp/2016/08/05/the-washington-post-experiments-with-automated-storytelling-to-help-power-2016-rio-olympics-coverage/?utm_term=.bf63b03c4aeb", "http://www.reddit.com/submit?url=http://byteacademy.co/blog/overview-NLG", "http://www.tumblr.com/share?v=3&u=http://byteacademy.co/blog/overview-NLG&t=Overview of Natural Language Generation (NLG)", "http://byteacademy.co/blog/artificial-intelligence-future", "http://byteacademy.co/blog/artificial-intelligence-future", "http://byteacademy.co/blog/data-science-podcasts", "http://byteacademy.co/blog/data-science-podcasts", "http://byteacademy.co/blog/data-science-fintech/", "http://byteacademy.co/blog/data-science-fintech/", "http://byteacademy.co/financial-aid/", "http://byteacademy.co/upcoming-classes/", "http://byteacademy.co/contact-us/", "http://byteacademy.co/corporate-training/", "http://byteacademy.co/faq/", "http://byteacademy.co/press-page/", "http://byteacademy.co/careers-at-byte/", "http://byteacademy.co/brand-ambassador/", "https://www.meetup.com/Byte-Academy-Finance-and-Technology-community/", "http://www.bytedev.co", "https://www.facebook.com/byteacademy/", "https://twitter.com/ByteAcademyCo", "https://www.linkedin.com/edu/school?id=171001", "https://github.com/ByteAcademyCo", "https://www.instagram.com/byteacademy/", "https://www.meetup.com/Byte-Academy-Finance-and-Technology-community/", "https://www.quora.com/topic/Byte-Academy", "https://www.youtube.com/channel/UCrMcJALnO748TSK27bZQzSg", "http://byteacademy.co/privacy-policy/"]}, "104": {"url": "http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "title": "", "text": "", "links": ["http://www.getrevue.co/?ref=Revue+Profile", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profileimage&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profiledate&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profilename&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Mm6Kn?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Mm6Kn?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.datasciencecentral.com/profiles/blogs/automated-machine-learning-for-professionals?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lXBy8?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lXBy8?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.nature.com/articles/s41467-017-00181-8?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/d0nAm?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/d0nAm?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/4mO6D?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/4mO6D?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/eK78N?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/eK78N?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://sourcedexter.com/amazing-tensorflow-github-projects/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xejEQ?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xejEQ?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://techblog.appnexus.com/a-keras-multithreaded-dataframe-generator-for-millions-of-image-files-84d3027f6f43?gi=79cece23ee37&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/6nA72?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/6nA72?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://blog.revolutionanalytics.com/2017/08/a-modern-database-interface-for-r.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/5kyK3?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/5kyK3?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://ruder.io/deep-learning-nlp-best-practices/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/7yxNV?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/7yxNV?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://hyperparameter.space/blog/when-not-to-use-deep-learning/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/8A1NP?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/8A1NP?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278?gi=8e0386afa61e&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/1bKwn?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/1bKwn?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://adeshpande3.github.io/How-I-Used-Deep-Learning-to-Train-a-Chatbot-to-Talk-Like-Me?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lX0x4?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lX0x4?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.youtube.com/channel/UC0n76gicaarsN_Y9YShWwhw/playlists?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/qYVyK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/qYVyK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.youtube.com/watch?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest&v=0fhUJT21-bs", "http://rev.vu/Nkeov?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Nkeov?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%239%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "https://www.getrevue.co/?utm_source=Data Science Digest&utm_medium=email&utm_content=footerlink&utm_campaign=Issue", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=forwarded&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%239%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "http://www.getrevue.co/?ref=Revue+Profile"]}, "105": {"url": "http://rpsychologist.com/d3/bayes/", "title": "", "text": "Created by Kristoffer Magnusson The visualization shows a Bayesian two-sample t test, for simplicity the variance is assumed to be known. It illustrates both Bayesian estimation via the posterior distribution for the effect, and Bayesian hypothesis testing via Bayes factor. The frequentist p-value is also shown. The null hypothesis, H0 is that the effect \u03b4 = 0, and the alternative H1: \u03b4 \u2260 0, just like a two-tailed t test. You can use the sliders to vary the observed effect (Cohen's d), sample size (n per group) and the prior on \u03b4. The prior on the effect is a scaled unit-information prior. The black, and red circle on the curves represents the likelihood of 0 under the prior and posterior. Their likelihood ratio is the Savage-Dickey density ratio, which I use here as to compute Bayes factor. The p-value is the traditional p-value for a two-sample t test with known variance (i.e. a Z test).        HDI is the posterior highest density interval, which in this case is analogous a credible interval. And CI is the traditional frequentist confidence interval.  Check out Alexander Etz's blog series \"Understanding Bayes\" for a really good introduction to Bayes factor. Fabian Dablander also wrote a really good post, \"Bayesian statistics: why and how\", which introduces Bayesian inference in general. If you're interesting in an easy way to perform a Bayesian t test check out JASP, or BayesFactor if you use R. Interactive visualization of Cohen's d effect size Interactive visualization of statistical power and significance testing Interactive visualization of Confidence Intervals Have any suggestion? Or found any bugs? Send them to me, my contact info can be found here. Designed and built by Kristoffer Magnusson. Built with D3.js, jStat and Bootstrap.", "links": ["http://rpsychologist.com", "http://rpsychologist.com", "https://twitter.com/krstoffr", "https://se.linkedin.com/pub/kristoffer-magnusson/b5/133/1b6", "https://twitter.com/share", "http://alexanderetz.com/understanding-bayes/", "http://blog.efpsa.org/2015/08/03/bayesian-statistics-why-and-how/", "https://jasp-stats.org", "http://bayesfactorpcl.r-forge.r-project.org", "http://rpsychologist.com/d3/cohend/", "http://rpsychologist.com/d3/NHST/", "http://rpsychologist.com/d3/CI/", "http://rpsychologist.com/tag/d3js.html", "http://rpsychologist.com/about", "http://rpsychologist.com", "http://d3js.org/", "http://jstat.org/", "http://getbootstrap.com/", "http://twitter.com/krstoffr", "https://github.com/rpsychologist"]}, "106": {"url": "https://blog.datasyndrome.com/generalists-dominate-data-science-f01882f25347", "title": "Generalists Dominate Data\u00a0Science", "text": "Analytics products and systems are best built by small teams of generalists. Large teams of specialists become dominated by communication overhead, and the effect of \u201cChinese whispers\u201d distorts the flow of tasks and stagnates creativity. Data scientists should develop generalist skills to become more efficient members of a data science team. Building data products takes a team covering a broad and diverse skillset. From the customer representative at one end, to the operations engineer at the other, the spectrum of roles in a product analytics team looks like this: Large companies often fill each role with a pair of shoes, resulting in a twelve person team like the one below. The problem with this setup is that it becomes much more difficult to achieve consensus and to perform any task that spans roles. And in data science, most tasks span roles. To look at a particular example, lets focus on the creation of a chart as part of a data product. To begin, a product manager creates a specification, then an interaction designer mocks up the chart, handing it off to a data scientist to fill with data (and hopefully to explore the data and find a chart worth producing), then a back-end engineer to setup an API to grab that data, a front-end web developer to create a web page using the data that matches the mock, and an experience designer to ensure the entire thing feels right and makes sense. Charts take iteration, so this cycle of communication could happen repeatedly for each chart. You can see how communication overhead starts to predominate. A meeting of six people is a full-blown, formal meeting. It is hard to get things done in formal meetings. In the next figure, we see how a data product team might be composed of four generalists: a data engineer, a data scientist/back-end developer, a designer who can build front ends and a product manager that can write marketing copy and cut deals. This is how a startup team would span the skill spectrum, and you can probably see how this makes them more efficient. Revisiting the chart example, creating a chart becomes a collaboration between the product manager, a designer who codes, and a data scientist. This is the kind of ad hoc meeting of 2\u20133 people where \u201cshit gets done\u201d efficiently. This group will be more efficient than the six man group. Put another way: this small team will kick the large team\u2019s ass. In the big company system, sometimes the only way to get anything done efficiently is to go \u201cguerilla generalist\u201d and work with other generalists to cut people out of the chain. This is bad politically, and is part of what drives effective people from big companies. We\u2019ve shown that small teams generalists outperform large teams of specialists. In fact, generalist skills are something every data scientists should work to develop. That doesn\u2019t mean you can\u2019t specialize, but should combine specialization with generalization in order to develop \u201cT-shaped skills.\u201d The T-shaped employee is one that can lend deep expertise across projects while fulfilling multiple roles in his own. It takes time to develop general skills, and that is why the path to becoming a data scientist is not a six month bootcamp, but a ten year journey. Along this path, remember to try to be T-Shaped! Need help building an analytics product or platform? The Data Syndrome team of data scientists and data engineers is available to build your data products and systems as a service. We also offer training in Agile Data Science for all members of data science teams.", "links": ["https://blog.datasyndrome.com?source=logo-lo_63d89b4eabe1---500653fb51a1", "https://twitter.com/datasyndrome", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.datasyndrome.com%2Fgeneralists-dominate-data-science-f01882f25347", "https://blog.datasyndrome.com/@rjurney?source=post_header_lockup", "https://blog.datasyndrome.com/@rjurney?source=post_header_lockup", "https://personalmba.com/communication-overhead/", "https://en.wikipedia.org/wiki/Chinese_whispers", "https://en.wikipedia.org/wiki/T-shaped_skills", "http://datasyndrome.com", "http://datasyndrome.com/training", "https://blog.datasyndrome.com/tagged/data-science?source=post", "https://blog.datasyndrome.com/tagged/agile?source=post", "https://blog.datasyndrome.com/tagged/generalists?source=post", "https://blog.datasyndrome.com/tagged/analytics?source=post", "https://blog.datasyndrome.com/tagged/software-engineering?source=post", "https://blog.datasyndrome.com/@rjurney?source=footer_card", "https://blog.datasyndrome.com/@rjurney", "https://blog.datasyndrome.com?source=footer_card", "https://blog.datasyndrome.com?source=footer_card", "https://blog.datasyndrome.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "107": {"url": "https://github.com/axibase/atsd-use-cases/tree/master/FED_FORDSR#the-average-american-debt-profile", "title": "", "text": "Debt is a complicated concept. After the sub-prime mortgage crisis of the late 2000s, modern Americans are all too familiar with the problems of irresponsible spending on credit. Student loan recipients who queue up to drop off another application for a job in a field they did not study are quick to point to the trappings of deficit spending as a means of wealth creation. Politicians and voters on both sides of the aisle point to the ever-growing United States Government debt with anxiety for the future. And yet despite all the doom and gloom, the American financial system is one of the most stable and robust in the world, in no small part thanks to ingenious monetary policy and hegemonic economic position organized over the entire course of the country's history, modern American consumers are among the wealthiest on the planet. The United States Federal Reserve is the central banking system of the United States, responsible for monitoring the global financial climate and enacting policy that supports the American economy and American consumers. They maintain a number of statistics about these consumers and their monetary practices to better inform their decisions and practices. Provided by the Federal Reserve, this dataset must be correctly parsed during import. The quarterly date format needs to be converted into a monthly format that ATSD can interprete (Q/q letter is not supported). We also need to discard metadata lines contained in the multi-line header. This can be accompilshed with a schema-based parser that provides granular control over the document's rows and columns using RFC 7111 selectors and Javascript: Script 1.1 For step-by-step instructions on data customization with schema based parsing, see this support tutorial. The Financial Obligation Ratio (FOR) is an estimate of the ratio of required debt payments to disposable income. This is a broad calculation and includes all kinds of debt:  mortgage payments, credit cards, property tax and lease payments. Each of these metrics can be expanded further to include associated costs, such as homeowner's insurance for example. The Federal Reserve releases this number each quarter. Figure 1.1   Use the dropdown menus at the top of the visualization screen to navigate through time, selecting the starttime and endtime values to observe a desired period. The data can also be queried with a structured query language in the SQL Console. The data will be aggregated annually, derived from the average value of each quarter within a given year: Query 1.1 Table 1.1 All values are shown as a percent of one hundred, where the whole is representative of the total income of the average person. The Debt Service Ratio (DSR) is more specific than the Financial Obligation Ratio in that it typically does not include non-essential debt payments. Here, it has been parsed into two categories, mortgage debt and consumer debt. These numbers represent the average percent of a person's earned salary each month which much be used to make the required payments associated with consumer credit and mortgage. Typically the DSR is an initial calculation performed to determine a person's eligibility to receive a mortgage. A DSR value of less than 48% is generally preferred, meaning that with a particular mortgage plus other credit obligations at least 52% of a person's gross monthly earning would still be available to them after making the required payments. Figure 2.1   Query 2.1 Table 2.1 Because the FOR value includes the DSR value plus additional non-essential credit values, and the DSR value is parsed into both consumer and mortgage related debt, these three values can be shown in a new visualization that creates a typical consumer profile of the average American. By using the calculated value setting shown below, additional data not specifically included in the set can be displayed: Script 2.1 Shown below is the debt profile of the average American consumer from 1980 to 2017, navigate through time using the dropdown menus at the top of the screen to select a desired span of time and compare how bearing debt has changed over the course of the last three decades. Figure 3.1   The visualization can also be organized to show the amount of each type of debt as it relates to the others: Figure 3.2   Additionally, these values can be compared on an annual basis as shown in the visualization below: Figure 3.3   To view the distribution of these values across time, a histogram is shown below: Figure 3.4   In the following box diagram, explore time with the dropdown menus at the top of the visualization screen. The visualization shows the distribution of debt values as a percentage of total income, with the initial time period set to include the entire data set: Figure 3.5   The following SQL query will detail the above visualizations in one table, displaying averaged annual values of each component described above: non-essential credit payments, mortgage credit payments, and consumer credit payments, as well as the Financial Obligation Ratio (FOR), or total debt obligations. Query 3.1 Table 3.1 The above dataset can illuminate a number of features of the American economy and a number of characteristics of the average American consumer. While modern Americans are quick to denounce the zeitgeist of living outside of one's means, the data shows that in fact, the amount of debt carried by the average American is on par with or even lower in some cases than that of his 1980's counterpart. In fact, the only metric which has demonstrated a legitimate increase in value over the last several decades has been the roughly one percent increase in non-essential credit holdings by the average consumer. According to data from the Economic Research Department of the Saint Louis Branch of the Federal Reserve, the 2015 US median household income was $56,516 per year in 2015 USD. This number can be applied to the above table and visualized in ChartLab to create more comprehensive data. Figure 3.6   The above visualization aggregates the values from Table 3.1 based on a time period of the user's selection. Use the dropdown menu at the top of the screen to select the aggregation period. The initial visualization shows the average values for each metric over the entire period of time in 2015 USD by obligation amount per quarter. The following query summons the same data shown above, but further parses it to show annual average monthly payments instead of quarterly values in 2015 USD for a person making the 2015 median United States income of $56,516 a year. Query 4.3 Table 3.2 As it turns out, the idea that your parents paid less for their house than you will is only true in absolute terms. When compared with current numbers and controlled for inflation, the average 2017 consumer will pay roughly the same portion of their income towards a place to hang their hat up as the average 1980 consumer. The Federal Reserve is able to pull certain levers of power from the Eccles Building in Washington, D.C. such as printing more money, or raising and lowering interest rates to cope with inflation. However, all of these are reactionary measures meant to create small changes that have a butterfly effect over time. Ultimately, the machinations of the Board of Governers have always be something opaque and esoteric to the average man, leading to many people denouncing the Federal Reserve System entirely, occasionally opting for a return of the gold standard or leveling accusations of wrong-doing. However, after reviewing the data above, it seems that at least on a consumer level, the average American actually has more today than they would have had thirty years ago, or even just five years ago. Of course, the Federal Reserve isn't completely responsible for the wise consumer choices made in the current decades, but monetary policy enacted by the various branches of the Federal Reserve are responsible for maintaining the economic conditions that Americans, and consumers the world over, have come to expect from the United States economy.", "links": ["https://github.com/", "https://github.com/axibase/atsd-use-cases/pull/86", "http://www.usdebtclock.org/", "https://www.federalreserve.gov/", "https://www.federalreserve.gov/datadownload/Download.aspx?rel=FOR&series=91e0f9a6b8e6a4b1ef334ce2eaf22860&filetype=csv&label=include&layout=seriescolumn&from=01/01/1980&to=12/31/2017", "https://axibase.com/products/axibase-time-series-database/writing-data/csv/", "https://apps.axibase.com/chartlab/842f1dd9/#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql", "https://apps.axibase.com/chartlab/85522dd3/#fullscreen", "https://apps.axibase.com/chartlab/f25de723/#fullscreen", "https://apps.axibase.com/chartlab/f25de723/3/#fullscreen", "https://apps.axibase.com/chartlab/81ea0ea0/#fullscreen", "https://apps.axibase.com/chartlab/9f74c179/#fullscreen", "https://apps.axibase.com/chartlab/20ff0ade/#fullscreen", "https://fred.stlouisfed.org/series/MEHOINUSA646N", "https://research.stlouisfed.org/", "https://www.stlouisfed.org/", "https://apps.axibase.com", "https://apps.axibase.com/chartlab/da132e01/11/#fullscreen", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "108": {"url": "http://tech.marksblogg.com/billion-nyc-taxi-rides-aws-ec2-p2-16xlarge-brytlytdb.html", "title": "", "text": "I have 14 years of hands-on build and consulting experience with clients in the UK, Ireland & Germany. I've done both back- and frontend work for Bank of America Merrill Lynch, Blackberry, Bloomberg, British Telecom, Ford, Google, ITV, News UK, Pizza Hut, Royal Bank of Scotland, Royal Mail, T-Mobile, Vertu (Nokia subsidiary at the time), Williams Formula 1 Team and Xerox. I hold both a Canadian and a British passport. My CV & my LinkedIn profile.         Home           | Benchmarks           | Archives            | Atom Feed  Posted on Fri 28 July 2017 BrytlytDB is an in-GPU-memory database built on top of PostgreSQL. It's operated using many of PostgreSQL's command line utilities, it's wire protocol compatible so third-party PostgreSQL clients can connect to BrytlytDB and queries are even parsed, planned and optimised by PostgreSQL's regular codebase before the execution plan is passed off to GPU-optimised portions of code BrytlytDB offer. Clustering works right out of the box, GPU-powered JOINs are supported, Stored Procedures are fully functional, Deep- and Machine Learning workloads via Torch are supported and BI visualisation software in the form of SpotLyt is included with BrytlytDB as well. The feature list is too long for one blog to cover so in this post I'll just be taking a look at how performant BrytlytDB's OLAP functionality is. In this benchmark I'll see how well 32 Tesla K80 GPUs spread across two EC2 instances perform when querying 1.1 billion taxi trips. I'll be using the same dataset I've used to benchmark Amazon Athena, BigQuery, ClickHouse, Elasticsearch, EMR, kdb+/q, MapD, PostgreSQL, Redshift and Vertica. I have a single-page summary of all these benchmarks for comparison. For this benchmark I'll be using two p2.16xlarge EC2 instances running Ubuntu 16.04.2 LTS in Amazon Web Services' eu-west-1a region in Ireland. Each machine has 8 Nvidia K80  cards which have 2 GPUs each, 64 virtual CPUs and 732 GB of memory. There's also 20 Gbit/s of networking capacity available to each instance. Below are the specifications of the compute capabilities available each one of the EC2 instances. This is the layout of the GPUs available on one of the two EC2 instances. The first machine has an IP address of 52.214.237.134 and has three roles: Global Transaction Manager, Coordinator and host to the first 16 data nodes. The second machine has an IP address of 34.250.232.38 and has two roles: it is also a coordinator and hosts the second set of 16 data nodes. TCP port 5432 is open between the two machines for communicating via PostgreSQL's wire protocol, TCP port 7777 is open for global transaction manager communication and ports 20,000 through to 20,0031 are open for data node communication. In addition to the default 20 GB EBS volumes on each EC2 instance there are six 500 GB General Purpose SSDs attached which offer a baseline of 100 IOPS each and can burst to 3,000 IOPS if need be. These drives are setup in a RAID 0 configuration on each instance. Before setting up the RAID array I'll install a few dependencies. This was run on both EC2 instances. Below are the commands used to setup the RAID array on each instance. Here's what the RAID layout looked like after it was setup. This RAID 0 setup offers a partition with a capacity of 3.2 TB on each instance: I've run the following to download BrytlytDB's install script. BrytlytDB is commercial software so I cannot divulge the URL I pulled this from at this time. The following was run on both EC2 instances. I've then edited the install script on the first instance with the following instance-specific values: And the install script on the second instance was edited with the following instance-specific values: With those changes in place I then ran the install script on both EC2 instances. The install script conducts the following: Once that's done I can form the cluster with the following registration script. The following was run on the first EC2 instance. The second registration script run on the other EC2 instance is identical with the first with the exception of the coordinator line pointing to 52.214.237.134 instead. For this benchmark I've downloaded and decompressed one half the 500 GB of CSV data I created in my Billion Taxi Rides in Redshift blog post onto each EC2 instance. The data sits across 56 files across both machines but for the quickest load time I want to create 32 reasonably equally-sized CSV files and load 16 files on each EC2 instance simultaneously. The 32 files will pair up with the 32 GPUs available across the cluster and should allow for the best utilisation of the GPU resources when running queries. Here is half of the original 500 GB data set on the first EC2 instance: And this is the other half on the second instance: On each instance I'll decompress the GZIP files. Below took about four minutes to complete on each instance. Then I'll concatenate the ~560 million lines of CSV data on each instance and break it up into files of 35 million lines each. The above completed in 8 minutes on each EC2 instance. I'm now left with 16 CSV files on the first EC2 instance: And 16 files on the other EC2 instance: I'll then connect to the coordinator node on each EC2 instance and setup BrytlytDB's gpu_manager_fdw extension and foreign data wrapper. I can then create the trips table. The above sets the maximum row count per GPU device at 38 million and creates an index on the cab_type column. The cab_type column has very low cardinality so the indexing will be less beneficial versus a column with a higher cardinality of values. Some of the data types used in the above table aren't the types I normally use in my benchmarks. The reason for this is that there is still limited data type support available in this early version of BrytlytDB. I've had to substitute DATETIME with DATE which will truncate the timestamps in the dataset to just the date rather than the full date and time. There is no SMALLINT support yet so I've had to use the larger INT type as an replacement for those fields. DECIMAL types aren't yet supported so I'll be using DOUBLE PRECISION as a replacement there. The above replacements could speed up or slow down the queries I benchmark with so I'm hoping to do another benchmark when data type support is widened in the future. With the table created I'll launch 32 simultaneous load jobs across the two EC2 instances to load the data into BrytlytDB's trips table. This is the import script I ran on the first EC2 instance: And this is the load script for the second instance. The first EC2 instance loaded its half of the dataset in 1 hour, 12 minutes and 21 seconds. The second instance loaded its half in 1 hour, 9 minutes and 57 seconds. After the data was loaded in I could see the PostgreSQL data directory filled up with reasonably evenly-sized data folders. Here's what they look like on the first EC2 instance: I was then able to use PostgreSQL's CLI tool to make sure I can see the table and all 1.1 billion records. The times quoted below are the lowest query times seen during a series of runs. As with all my benchmarks, I use the lowest query time as a way of indicating \"top speed\". The following completed in 0.762 seconds. The following completed in 2.472 seconds. The following completed in 4.131 seconds. The following completed in 6.041 seconds. Given all the features of PostgreSQL are still available I'm blown away at how fast BrytlytDB is able to aggregate data. The cluster I used cost around $30 / hour which means this system out performed other Cloud-based and PostgreSQL-based data warehousing solutions both in terms of wall clock time and in terms of cost per hour of running the cluster. That being said I know Nvidia's K80 chips use extremely fast memory and have thousands of compute cores so I expect further optimisations from BrytlytDB to drive down these already fast query times. \u2190 Back to Index Copyright \u00a9 2014 - 2017 Mark Litwintschik. This site's template is based off a template by Giulio Fidente.", "links": ["https://twitter.com/marklit82", "http://tech.marksblogg.com/theme/cv.pdf", "https://uk.linkedin.com/in/marklitwintschik/", "http://tech.marksblogg.com/feeds/all.atom.xml", "http://www.brytlyt.com/", "https://uk.linkedin.com/in/marklitwintschik/", "https://github.com/giulivo/pelican-svbhack"]}, "109": {"url": "https://blog.datazar.com/why-we-focus-on-design-just-as-much-as-functionality-f6ccb3b2c926", "title": "Why We Focus on Design Just as Much as Functionality", "text": "It\u2019s no secret that scientific software isn\u2019t the most beautiful software in terms of design/aesthetics (generally speaking here). It is very effective though; it gets the job done. It does exactly what you expect it to do. Being a huge movie buff, I\u2019ve always wondered why the software we use in the laboratories or classrooms looks nothing like what we see in the movies. Movies do this thing where tech looks exciting and futuristic and super fun to use. And then I go back to reality and it\u2019s nothing like what I just watched. It\u2019s worth mentioning that not everything in the movies looks practical, especially when it comes to software or even more ridiculous: computer hacking. But that\u2019s another subject. The fact there\u2019s some art missing is something I noticed not only in the software we use in the lab, but also the instruments we build and the papers we write. It seems as if the art has been completely taken out of the sciences. Before you say it, we obviously can\u2019t incorporate art into somethings like highly sensitive instruments as unnecessary additions area big no because then you\u2019re interfering. Another thing to note might be the fact that hundreds or even some thousands years ago, the scientists were also artists. Art came naturally to them as they were also discovering mother nature. At that time, science was also only available to the rich which meant a couple of things: they could afford to spend a little bit of time incorporating art into their scientific work as they weren\u2019t rushing to meet grant deadlines, but it also meant they were already educated in the arts from an early age. These days anyone can be whatever they want to be. To the very least, it\u2019s easier now than it was in that era by an order of magnitude. Today we\u2019re taught that art almost has no place in the sciences and we must be efficient and to the point. Anything extra is generally referred to as fluff. If you like to romanticize science, you\u2019re not really a scientist. But I strongly believe that it\u2019s the artistic and I might add\u200a\u2014\u200aphilosophical\u200a\u2014\u200afluff that gets people interested in science. The hard math and core scientific values obviously being there regardless. All of this results in the absence of art in the sciences which makes art seem dry and boring. Science in its purest form is absolutely beautiful if communicated right. That\u2019s why we love to do our part at Datazar. We\u2019re far, far from perfect or even close to what we want to be. But putting just as much thought in design and UX as we put in functionality is a core principle. There\u2019s a misconception that incorporating art means adding more stuff. Incorporating art can also mean doing less, using less and making the most of what you already have in a beautiful and simple way. An example would be removing that obscene amount of jargon from your paper. So I ask everyone designing software for scientists, design for humans not robots. Scientists are humans too. So go ahead add that CSS animation.", "links": ["https://blog.datazar.com?source=logo-lo_a64d201ed159---e2c7e6e1c75", "https://twitter.com/DatazarHQ", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.datazar.com%2Fwhy-we-focus-on-design-just-as-much-as-functionality-f6ccb3b2c926", "https://blog.datazar.com", "https://blog.datazar.com/tagged/r-language", "https://blog.datazar.com/tagged/open-data", "https://blog.datazar.com/tagged/how-to", "https://www.r-bloggers.com/", "https://blog.datazar.com/search", "https://blog.datazar.com/@ata_aman?source=post_header_lockup", "https://blog.datazar.com/@ata_aman?source=post_header_lockup", "https://blog.datazar.com/tagged/science?source=post", "https://blog.datazar.com/tagged/art?source=post", "https://blog.datazar.com/tagged/research?source=post", "https://blog.datazar.com/tagged/software-development?source=post", "https://blog.datazar.com/tagged/design?source=post", "https://blog.datazar.com/@ata_aman?source=footer_card", "https://blog.datazar.com/@ata_aman", "http://twitter.com/datazarhq", "https://blog.datazar.com?source=footer_card", "https://blog.datazar.com?source=footer_card", "https://blog.datazar.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "110": {"url": "https://news.ycombinator.com/item?id=14950255", "title": "", "text": "", "links": ["https://news.ycombinator.com", "https://edgylabs.com/machine-learning-to-enhance-smartphone-pictures/", "https://hn.algolia.com/?query=Machine%20Learning%20to%20Enhance%20Smartphone%20Pictures&sort=byDate&dateRange=all&type=story&storyText=false&prefix&page=0", "https://www.google.com/search?q=Machine%20Learning%20to%20Enhance%20Smartphone%20Pictures", "https://github.com/HackerNews/API", "http://www.ycombinator.com/apply/"]}, "111": {"url": "http://starmine.ai/datasets/ds02.html", "title": "", "text": "reddit Slack Twitter Instagram Facebook LinkedIn", "links": ["http://starmine.ai", "http://reddit.com/r/datasets", "https://join.slack.com/t/starmineai/shared_invite/MjIwNjg5OTUyODAzLTE1MDE2MTk1MzQtMmJhYTExNDViMA", "https://twitter.com/starmineAI", "https://www.instagram.com/starmine.ai/", "https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Fstarmine.ai/index.html&t=starmine%20ICO", "https://www.linkedin.com/cws/share?url=http%3A%2F%2Fstarmine.ai/index.html&token=&isFramed=true", "http://research.kraeutli.com/index.php/2013/11/the-tate-collection-on-github", "http://54.174.116.134/recommend/datasets/subscribers/free/supercolumns/supercolumns-elements-CMDB-nasdaq-nyse-otcbb-general-2017/supercolumns-elements-nasdaq-nyse-otcbb-general-UPDATE-2017-03-01.csv", "http://54.174.116.134/recommend/datasets/subscribers/free/supercolumns/supercolumns-elements-CMDB-nasdaq-nyse-otcbb-general-2017/supercolumns-elements-nasdaq-nyse-otcbb-general-UPDATE-2017-03-01.csv", "http://54.174.116.134/recommend/app/ai_connect-finance-historicaltrends_api?ccl1=playstation&ccl2=helium&ccl3=korea&ccl4=shampoo&ccl5=coffee&query=concept_column_labels&themesource=theme_CMDB-nasdaq-nyse-otcbb&month=01&year=all&db=CMDB-nasdaq-nyse-otcbb-general", "https://www0.gsb.columbia.edu/mygsb/faculty/research/pubfiles/1555/contagious.pdf", "https://trends.google.com/trends/explore?q=playstation,helium,korea,shampoo,coffee", "https://trends.google.com/trends/explore?q=playstation,helium,korea,shampoo,coffee"]}, "112": {"url": "https://github.com/maciejkula/spotlight/blob/master/examples/movielens_sequence/readme.rst", "title": "", "text": "Using sequences of user-item interactions as an input for recommender models has a number of attractive properties. Firstly, it recognizes that recommending the next item that a user may want to buy or see is precisely the goal we are trying to achieve. Secondly, it's plausible that the ordering of users' interactions carries additional information over and above just the identities of items they have interacted with. For example, a user is more likely to watch the next episode of a given TV series if they've just finished the previous episode. Finally, when the sequence of past interactions rather than the identity of the user is the input to a model, online systems can incorporate new users (and old users' new actions) in real time. They are fed to the existing model, and do not require a new model to be fit to incorporate new information (unlike factorization models). Recurrent neural networks are the most natural way of modelling such sequence problems. In recommendations, gated recurrent units (GRUs) have been used with success in the Session-based recommendations with recurrent neural networks paper. Spotlight implements a similar model using LSTM units as one of its sequence representations. But recurrent neural networks are not the only way of effectively representing sequences: convolutions can also do the job. In particular, we can use causal convolutions: convolutional filters applied to the sequence in a left-to-right fashion, emitting a representation at each step. They are causal in that the their output at time t is conditional on input up to t-1: this is necessary to ensure that they do not have access to the elements of the sequence we are trying to predict. Like LSTMs, causal convolutions can model sequences with long-term dependencies. This is achieved in two ways: stacking convolutional layers (with padding, every convolutional layer preserves the shape of the input), and dilation: insertion of gaps into the convolutional filters (otherwise known as atrous convolutions). Causal convolutions have been used in several recent high-profile papers: Using convolutional rather than recurrent networks for representing sequences has a couple of advantages, as described in this blog post: Spotlight implements causal convolution models as part of its sequence models package, alongside more traditional recurrent and pooling models. The Spotlight implementation has the following characteristics: The model is trained using one of Spotlight's implicit feedback losses, including pointwise (logistic and hinge) and pairwise (BPR as well as WARP-like adaptive hinge) losses. As with other Spotlight sequence models, the loss is computed for all the time steps of the sequence in one pass: for all timesteps t in the sequence, a prediction using elements up to t-1 is made, and the loss is averaged along both the time and the minibatch axis. This leads to siginficant training speed-ups relative to only computing the loss for the last element in the sequence. To see how causal CNNs compare to more traditional sequence models we can have a look at how they perform at predicting the next rated movie on the Movielens 1M dataset. With 1 million interactions spread among 6000 users and around 4000 movies it should be small enough to run quick experiments, but large enough to yield meaningful results. I chose to split the dataset into 80% train, and 10% test and validation sets. I construct 200-long sequences by splitting each user's item sequence into 200-long chunks; if a chunk is shorter than 200 elements, it's padded with zeros. I use mean reciprocal rank (MRR) as the evaluation metric. To choose hyperparameters, I run a quick, coarse grained hyperparameter search, using random sampling to draw 100 hyperparameter sets. With the data and hyperparameters ready, fitting and evaluating the model is relatively simple: Fitting the models is fairly quick, taking at most two or three minutes on a single K80 GPU. The code for the experiments is available in the experiments folder of the Spotlight repo. The results are as follows: It's difficult to draw clear-cut conclusions about the effect of each hyperparameter, but it looks like: To compare causal convolutions with more traditional sequence models I run similar hyperparameter searches for LSTM-based representations and pooling representations. The pooling representation is a simple averaging of item embedding across the sequence; the LSTM-based model runs an LSTM along a user's interactions, using the hidden state for prediction of the next element at each step. The results are as follows: A single layer LSTM seems to outperform causal convolutions, by an over 10% margin, helped by the adaptive hinge loss. Simple pooling performs quite badly. It looks like causal convolutions need some more work before beating recurrent networks. There are a couple of possible avenues for making them better: I'd love to get some input on these. If you have suggestions, let me know on Twitter or open an issue or PR in Spotlight.", "links": ["https://github.com/", "https://arxiv.org/abs/1511.06939", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.LSTMNet", "https://travis-ci.org/maciejkula/spotlight", "https://arxiv.org/pdf/1609.03499.pdf", "http://papers.nips.cc/paper/6527-conditional-image-generation-with-pixelcnn-decoders", "https://arxiv.org/abs/1610.10099", "https://medium.com/@TalPerry/convolutional-methods-for-text-d5260fd5675f", "https://arxiv.org/pdf/1610.10099.pdf", "https://maciejkula.github.io/spotlight/sequence/sequence.html", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.CNNNet", "https://maciejkula.github.io/spotlight/losses.html", "https://grouplens.org/datasets/movielens/1m/", "https://en.wikipedia.org/wiki/Mean_reciprocal_rank", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.LSTMNet", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.PoolNet", "https://maciejkula.github.io/spotlight/losses.html#spotlight.losses.adaptive_hinge_loss", "https://twitter.com/Maciej_Kula", "https://github.com/maciejkula/spotlight", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "113": {"url": "https://github.com/ynqa/word-embedding", "title": "", "text": "GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together.  Sign up             Use Git or checkout with SVN using the web URL.             This is an implementation of word embedding (also referred to as word representation) models in Golang. Word embedding makes words' meaning, structure, and concept mapping into vector space (and low dimension). For representative instance: Like this example, it could calculate word meaning by arithmetic operations between vectors. Listed models for word embedding, and checked it already implemented. Downloading text8 corpus, and training by Skip-Gram with negative sampling.", "links": ["https://github.com/", "https://github.com/ynqa/word-embedding/blob/master/LICENSE", "https://help.github.com/articles/which-remote-url-should-i-use", "https://travis-ci.org/ynqa/word-embedding", "https://godoc.org/github.com/ynqa/word-embedding", "https://goreportcard.com/report/github.com/ynqa/word-embedding", "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf", "http://nlp.stanford.edu/pubs/glove.pdf", "https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf", "http://mattmahoney.net/dc/textdata", "https://github.com/cjlin1/libsvm", "http://www.aclweb.org/anthology/Q15-1016", "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.648.8023&rep=rep1&type=pdf", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "114": {"url": "https://github.com/brannondorsey/keras_weight_animator", "title": "", "text": "GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together.  Sign up             Use Git or checkout with SVN using the web URL.          Save Keras weight matrices as short animated videos to better understand what and how your neural network models are learning. Below are examples of the first LSTM layer and the final output layer of a six-class RNN model trained over one epoch. Blue represents low values and red represents high values.   In order to render videos from the saved weight images you must also have the following packages installed on your machine: This module is named keras_weight_animator. It exposes a Keras callback function that you can include in any model fit(...) method. The two required parameters to image_saver_callback(...) are the Keras model and an output_directory to periodically save weight images to. By default, keras_weight_animator saves layer weights every 100 batches to output_directory as PNGs in folders named epoch_XXX-layer_NAME-weights_YY. Once training is complete, you can optionally create short animated video clips from the image sequences saved in output_directory using [bin/create_image_sequence.sh](bin/create_image_sequence.sh) path/to/output_directory. This will use parallel, mogrify, and ffmpeg to create a .mp4 from the image sequences located in each folder of output_directory. Video files will be named like epoch_XXX-layer_NAME-weights_YY.mp4. You can run this script automatically from your training script by passing the render_videos=True parameter to image_saver_callback(...). weight_image_sequences(...) takes a variety of optional keyword arguments. I've included an example usage of the module in examples/wisdm.py. This example uses smartphone accelerometer data from WISDM to classify human activity tasks like walking, standing, sitting, walking upstairs, etc... This example uses a one layer LSTM to classify a set of 60 data points (representing three seconds of data sampled at 20hz) as belonging to one of six classes. It outputs image sequences and videos to data/wisdm. Using a bash script to leverage parallel, ImageMagick, and FFMPEG isn't necessarily the most elegant solution, but its the one I had time for. The goal of this here lil' project was to write a quick tool that allows me to better understand how weights change over mini-batch updates in a variety of neural networks. Perhaps in the future I will come back and clean up some of the inelegancies. If you have interest in contributing or maintaining a cleaner version of this lib, please reach out at brannon@brannondorsey.com. This module is \u00a9 Brannon Dorsey 2017, released under an \u2665 MIT License \u2665. You are free to use, modify, distribute, sell, etc... this software under those terms. Example data is from the WIreless Sensor Datamining (WISDM) Actitracker dataset published by Fordham University: This idea is tangentially inspired by much of the work on Visualizing and Understanding Recurrent Networks by Andrej Karpathy, Justin Johnson, and Fei-Fei Li. GNU Parallel is adamant about citation to the point of excess IMHO, but for what its worth, here is their bibtex:", "links": ["https://github.com/", "https://github.com/brannondorsey/keras_weight_animator/blob/master/LICENSE", "https://help.github.com/articles/which-remote-url-should-i-use", "https://github.com/brannondorsey/keras_weight_animator/pull/1", "http://keras.io", "https://www.gnu.org/software/parallel/", "https://www.imagemagick.org/script/index.php", "https://ffmpeg.org/download.html", "https://keras.io/callbacks/", "http://matplotlib.org/users/colormaps.html", "http://www.cis.fordham.edu/wisdm/dataset.php", "https://arxiv.org/abs/1506.02078", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "115": {"url": "https://axibase.github.io/atsd-use-cases/Expatriation_Q2/", "title": "", "text": "Expatriation is the temporary or permanent relocation of a person by choice or by force from their native country to any other country in the world for work, pleasure, or purpose. History has seen expatriation from nearly every country in the world up to and including the current epoch. Famous American expatriates throughout history have included American founding father Benjamin Franklin, who worked as the first United States Minister to France for almost a decade after appointment by the Continental Congress in 1778. The  sixth President of the United States John Quincy Adams, who served as a U.S. Diplomat in four countries before winning the Presidency as a Democratic-Republican in 1825, most notably opening American diplomatic ties as the first United States Minister to the Russian Empire, a position he loved  so much he declined a subsequent offer to serve as an Associate Justice on the Supreme Court of the United States a few years later. American expatriates have come from all walks of life: authors, such as Ernest Hemingway who lived in China, Spain, Cuba, and was even present on the beach codenamed Omaha in Normandy during the allied landing. Musicians, like Jimi Hendrix who rose to international fame in London after leaving his native Seattle, directors like Stanley Kubrick, singers like Tina Turner, and even basketball players like  Allen Iverson, who played an incomplete season for a Turkish basketball team in 2010 after retiring from the NBA. For those who decide to make the change permanent, there comes a time when the inconvenience of living abroad as a resident alien outweighs the novelty and many decide to renounce their citizenship for one reason or another. Unsurprisingly, this is a long and complicated process, one of the main features of which is settling up with the Internal Revenue Service (IRS) via the eponymous Expatriation Tax. The Federal Register is a publishing outlet for the United States Government where the IRS officially releases quarterly information about American citizens who renounce their passport in lieu of a new one and other long-term residents who have decided to repatriate, view the raw data here and use the Axibase Data Crawler designed specifically for collecting, parsing, organzing and inserting historical expatriation data and new data as it becomes available.  Axibase covered last quarter\u2019s expatriation release here and is using this quarter\u2019s release to follow-up. Every election cycle countless celebrities and netizens issue their promise to leave if their candidate fails to get elected and the growing notoriety surrounding the 2016 Presidental Election has proven to  be no different. This type of social theater is almost as old as the country itself as there have been a number of similarly divisive elections throughout American history: Andrew Jackson\u2019s victory in 1828 was seen as a populace uprising against the corrupt political elite and a return to the authority of the common man, Abraham Lincoln\u2019s second term was viewed as the tightening of federal authority to excessive levels that ultimately led to the American Civil War, Franklin Roosevelt was called a dangerous communist whose infamous court-packing plan favorably increased the number of seats on the Supreme Court to his  advantage and was seen as abhorrent and traitorous by many, and of course who can forget recent elections that have featured scandal after scandal and two modern Presidents who served after losing the popular vote, but winning in the electorate. Figure 1   Query 1 Data queried in the SQL Console. Table 1 To give the issue further context, Figure 1 and Table 1 use the most current Department of Homeland Security (DHS) data  available to display current naturalization numbers in the United States. Naturalization is the sister process to expatriation, because as a person expatriates from one country they must be naturalized in another or end up a stateless person. Naturalization numbers in America have remained fairly constant throughout the last decade because there are a number of federal regulations that control the amount of long-term immigrants that America accepts each year and that quota is met without fail. As noted in the our Q1 article, the sister value and subject of this examination, expatriation numbers, has been steadily growing for several consecutive years. Figure 2.1   Query 2 The above query features robust syntax and calculated values. See the following tutorials  to understand more about SQL Console. Table 2 Figure 2.2   Expatriation has been increasing each year by roughly 30% since 2010, which featured abnormally high expatriation rates, most likely attributable to the economic turndown of the Great Recession which began in the United States as a result of the sub-prime mortgage crisis. American citizenship is often sought after for the economic opportunity that comes along with the passport, as the ability to work and do business in the country is heavily restricted or regulated, and with the value of that investment or  opportunity in question, it is unsurprising that the number of investors in the system, that is, new citizens, would fluctuate. It seems appropriate to call naturalization an investment because of the nature of the process, which is long, complicated, and often quite expensive similar to a long-position that will cost more at purchasing time but promises high returns after reaching maturity. The peak, or more appropriately, valley of the global recession occurred in 2009 when the global GDP contracted causing a decline in the median familty income of about five percent. Figure 3   Query 3 Table 3 Reducing the timespan to look at data over the last 5 years captures the repetition of this trend for this period of time. Of the eighteen quarters included in this query, only six of them, roughly 33% showed negative growth for this statistic. Growing expatriation numbers provide interesting insight into the perception of America both domestically and abroad, as these numbers not only include U.S. citizens who renounce their citizenship, but also long-term resident aliens who have returned home. Almost unbelievably, the IRS does not strip characteristic information about expatriates from their publications meaning that first and last names are included in the data. Using Social Security data concerning the most common first names chosen throughout the last century and United States Census data concerning the most common last names to compare, the following queries and visualizations show  the number of expatriates by common first and common last name throughout the entire observed period, ranked by their frequency in the IRS publication. The goal of these queries is purely demonstrative and lighthearted. There is no intention to invade privacy, defame people, or otherwise cause harm. Figure 4  Query 4 Table 4 All of these names appear on the list of most common American first names, linked here Figure 5  Query 5 Table 5 Only the name Smith appears on the list of most common American last names, linked here Use the Axibase Data Crawler to collect the data used in this artcle and examine it yourself in ChartLab.", "links": ["https://github.com/axibase/atsd-use-cases", "https://www.irs.gov/", "https://www.irs.gov/individuals/international-taxpayers/expatriation-tax", "https://www.federalregister.gov/", "https://www.federalregister.gov/documents/2017/05/10/2017-09475/quarterly-publication-of-individuals-who-have-chosen-to-expatriate-as-required-by-section-6039g", "https://github.com/axibase/atsd-data-crawlers/tree/irs-expatriation-data-crawler", "https://axibase.com", "https://apps.axibase.com/chartlab/654b9945#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql", "https://www.uscis.gov/news/fact-sheets/naturalization-fact-sheet", "http://www.unhcr.org/stateless-people.html", "https://apps.axibase.com/chartlab/7fa5b643#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql#examples", "https://apps.axibase.com/chartlab/7fa5b643/5/#fullscreen", "https://web.stanford.edu/group/recessiontrends/cgi-bin/web/sites/all/themes/barron/pdf/IncomeWealthDebt_fact_sheet.pdf", "https://apps.axibase.com/chartlab/7fa5b643/2/#fullscreen", "https://www.ssa.gov/oact/babynames/decades/century.html", "http://www.census.gov/main/www/cen2000.html", "https://www.ssa.gov/oact/babynames/decades/century.html", "http://www.census.gov/main/www/cen2000.html", "https://github.com/axibase/atsd-data-crawlers/tree/irs-expatriation-data-crawler", "https://apps.axibase.com", "https://github.com/axibase/atsd-use-cases", "https://github.com/axibase", "https://pages.github.com"]}, "116": {"url": "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/", "title": "", "text": "Machine learning is eating the world right now. Everyone and their mother are learning about machine learning models, classification, neural networks, and Andrew Ng. You\u2019ve decided you want to be a part of it, but where to start? In this article we\u2019ll cover some important characteristics of Python and why it\u2019s great for machine learning. We\u2019ll also cover some of the most important libraries it has for ML, and if it piques your interest, some places where you can learn more. Python is a great choice for machine learning for several reasons. First and foremost, it\u2019s a simple language\u00a0on the surface; even if you\u2019re not familiar with Python, getting up to speed is very quick if you\u2019ve ever used any other language with C-like syntax (i.e. every language out there). Second, Python has a great community, which results in good documentation and friendly, comprehensive answers in StackOverflow (fundamental!). Third, also stemming from the great community, there are plenty of useful libraries for Python (both as \u201cbatteries included\u201d and third party), which solve basically any problem that you can have (including machine learning). Yeah and it\u2019s true. Python isn\u2019t the fastest language out there: all those handy abstractions come at a cost. But here\u2019s the trick: libraries can and do offload the expensive calculations to the much more performant (but harder to use) C and C++. For instance, there\u2019s NumPy, which is a library for numerical computation. It\u2019s written in C, and it\u2019s fast. Practically every library out there that involves intensive calculations uses it \u2014 almost all the libraries listed next use it in some form. So if you read NumPy, think fast. Therefore, you can make your scripts run basically as fast as straight up writing them in a lower level language. So there\u2019s really nothing to worry about when it comes to speed. Are you starting out in machine learning? Want something that covers everything from feature engineering to training and testing a model? Look no further than scikit-learn! This fantastic piece of free software provides every tool necessary for machine learning and data mining. It\u2019s the de facto standard library for machine learning in Python, recommended for most of the \u2018old\u2019 ML algorithms. This library does both classification and regression, supporting basically every algorithm out there (support vector machines, random forest, naive bayes, and so on). It\u2019s built in such a way that allows easy switching of algorithms, so experimentation is easy. These \u2018older\u2019 algorithms are surprisingly resilient and work very well in a lot of cases. But that\u2019s not all! Scikit-learn also does dimensionality reduction, clustering, you name it. It\u2019s also blazingly fast since it runs on NumPy and SciPy (meaning that all the heavy number crunching is run on C instead of Python). Check out some examples to see everything this library is capable of, and the tutorials if you want to learn how it works. While not a machine learning library per se, NLTK is a must when working with natural language processing (NLP). It comes with a bundle of datasets and other lexical resources (useful for training models) in addition to libraries for working with text \u2014 for functions such as classification, tokenization, stemming, tagging, parsing and more. The usefulness of having all of this stuff neatly packaged can\u2019t be overstated. So if you are interested in NLP, check out some tutorials! Used widely in research and academia, Theano is the grandfather of all deep learning frameworks. Written in Python, it\u2019s tightly integrated with NumPy. Theano allows you to create neural networks, which are represented as mathematical expressions with multi-dimensional arrays. Theano handles this for you so you don\u2019t have to worry about the actual implementation of the math involved. It supports offloading calculations to the much faster GPU, which is a feature that everyone supports today, but back when they introduced it this wasn\u2019t the case. The library is very mature at this point and supports a very wide range of operations, which is a great plus when it comes to comparing it with other similar libraries. The biggest complaint out there is that the API may be unwieldy for some, making the library hard to use for beginners. However, there are wrappers that ease the pain and make working with Theano simple, such as Keras, Blocks and Lasagne. Interested in learning about Theano? Check out this Jupyter Notebook tutorial. The Google Brain team created TensorFlow for internal use in machine learning applications, and open sourced it in late 2015. They wanted something that could replace their older, closed source machine learning framework, DistBelief, which they said wasn\u2019t flexible enough and too tightly coupled to their infrastructure to be shared with other researchers around the world. And so TensorFlow was created. Learning from the mistakes of the past, many consider this library to be an improvement over Theano, claiming more flexibility and a more intuitive API. Not only can it be used for research but also for production environments, supporting huge clusters of GPUs for training. While it doesn\u2019t support as wide a range of operations as Theano, it has better computational graph visualizations. TensorFlow is very popular nowadays. In fact, if you\u2019ve heard about a single library on this list, it\u2019s probably this one: there isn\u2019t a day that goes by without a new blog post or paper mentioning TensorFlow gets published. This popularity translates into a lot of new users and a lot of tutorials, making it very welcoming to beginners. Keras is a fantastic library that provides a high-level API for neural networks and is capable of running on top of either Theano or TensorFlow. It makes harnessing the full power of these complex pieces of software much easier than using them directly. It\u2019s very user-friendly, putting user experience as a top priority. They manage this by using simple APIs and excellent feedback on errors. It\u2019s also modular, meaning that different models (neural layers, cost functions, and so on) can be plugged together with little restrictions. This also makes it very easy to extend, since it\u2019s simple to add new modules and connect them with the existing ones. Some people have called Keras so good that it is effectively cheating in machine learning. So if you\u2019re starting out with deep learning, go through the examples and documentation to get a feel for what you can do with it. And if you want to learn, start out with this tutorial and see where you can go from there. Two similar alternatives are Lasagne and Blocks, but they only run on Theano. So if you tried Keras and are unhappy with it, maybe try out one of these alternatives to see if they work out for you. Another popular deep learning framework is Torch, which is written in Lua. Facebook open-sourced a Python implementation of Torch called PyTorch, which allows you to conveniently use the same low-level libraries that Torch uses, but from Python instead of Lua. PyTorch is much better for debugging since one of the biggest differences between Theano/TensorFlow and PyTorch is that the former use symbolic computation while the latter doesn\u2019t. Symbolic computation means that coding an operation (say, \u2018x + y\u2019), it\u2019s not computed when that line is interpreted. Before getting executed it has to be compiled (translated to CUDA or C). This makes debugging harder in Theano/TensorFlow, since an error is much harder to associate with the line of code that caused it. Of course, doing things this way has its advantages, but debugging isn\u2019t one of them. If you want to start out with PyTorch the official tutorials are very friendly to beginners but get to advanced topics as well. Alright, you\u2019ve presented me with a lot of alternatives for machine learning libraries in Python. What should I choose? How do I compare these things? Where do I start? Our Ape Advice\u2122 for beginners is to try and not get bogged down by details. If you\u2019ve never done anything machine learning related, try out scikit-learn. You\u2019ll get an idea of how the cycle of tagging, training and testing work and how a model is developed. Now, if you want to try out deep learning, start out with Keras \u2014 which is widely agreed to be the easiest framework \u2014 and see where that takes you. After you have more experience, you will start to see what it is that you actually want from the framework: greater speed, a different API, or maybe something else, and you\u2019ll be able to make a more informed decision. And even then, there is an endless supply of articles out there comparing Theano, Torch, and TensorFlow. There\u2019s no real way to tell which one is the good one. It\u2019s important to take into account that all of them have wide support and are improving constantly, making comparisons harder to make. A six month old benchmark may be outdated, and year old claims of framework X doesn\u2019t support operation Y could no longer be valid. Finally, if you\u2019re interested in doing machine learning specifically applied to NLP, why not check out MonkeyLearn! Our platform provides a unique UX that makes it super easy to build, train and improve NLP models. You can either use pre-trained models for common use cases (like sentiment analysis, topic detection or keyword extraction) or train custom algorithms using your particular data. Also, you don\u2019t have to worry about the underlying infrastructure or deploying your models, our scalable cloud does this for you. You can start for free and integrate right away with our beautiful API. There are plenty of online resources out there to learn about machine learning ! Here are a few: So that was a brief intro to machine learning in Python and some of its libraries. The important part is not getting bogged down by details and just trying stuff out. Follow your curiosity, and don\u2019t be afraid to experiment. Know about a python library that was left out? Share it in the comments below!", "links": ["http://www.monkeylearn.com", "http://monkeylearn.com/use-cases/", "http://monkeylearn.com/docs/", "http://www.monkeylearn.com/pricing", "http://www.monkeylearn.com/blog/", "https://app.monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/?utm_source=ml-blog&utm_medium=header&utm_campaign=blog", "https://monkeylearn.com/blog", "http://monkeylearn.com/use-cases/", "http://monkeylearn.com/docs/", "http://www.monkeylearn.com/pricing", "http://www.monkeylearn.com/blog/", "https://app.monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/?utm_source=ml-blog&utm_medium=header&utm_campaign=blog", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/07/Post_2c.png", "http://scikit-learn.org/", "http://scikit-learn.org/stable/auto_examples/index.html#general-examples", "http://scikit-learn.org/stable/tutorial/index.html", "http://www.nltk.org/", "http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk", "http://deeplearning.net/software/theano/", "https://keras.io/", "https://github.com/mila-udem/blocks", "https://github.com/Lasagne/Lasagne", "http://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/Theano%20Tutorial.ipynb", "https://www.tensorflow.org/", "https://www.tensorflow.org/get_started/graph_viz", "https://www.tensorflow.org/tutorials/", "https://keras.io/", "https://news.ycombinator.com/item?id=13872764", "https://github.com/fchollet/keras#getting-started-30-seconds-to-keras", "https://keras.io/getting-started/functional-api-guide/", "https://elitedatascience.com/keras-tutorial-deep-learning-in-python", "https://github.com/Lasagne/Lasagne", "https://github.com/mila-udem/blocks", "http://torch.ch/", "http://pytorch.org/", "http://pytorch.org/tutorials/", "http://scikit-learn.org/stable/tutorial/basic/tutorial.html", "https://elitedatascience.com/keras-tutorial-deep-learning-in-python", "https://www.quora.com/Is-TensorFlow-better-than-other-leading-libraries-such-as-Torch-Theano", "http://www.ccri.com/2016/12/09/torch-vs-tensorflow-vs-theano/", "https://www.reddit.com/r/MachineLearning/comments/5w3q74/d_so_pytorch_vs_tensorflow_whats_the_verdict_on/", "https://github.com/zer0n/deepframeworks/blob/master/README.md", "https://monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/", "http://nbviewer.jupyter.org/github/rhiever/Data-Analysis-and-Machine-Learning-Projects/blob/master/example-data-science-notebook/Example%20Machine%20Learning%20Notebook.ipynb", "https://blog.monkeylearn.com/a-gentle-guide-to-machine-learning/", "https://www.coursera.org/learn/machine-learning", "http://course.fast.ai/", "https://monkeylearn.com/blog/author/bruno/", "https://monkeylearn.com/blog/category/news/", "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/#respond", "https://monkeylearn.com/blog/author/bruno/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/monkeylearn_zapier.png", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/wp-content/uploads/2017/06/Post_1e.png", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/naive-bayes-classifier.png", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/monkey-startup.png", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/category/applications/", "https://monkeylearn.com/blog/category/guides/", "https://monkeylearn.com/blog/category/howto/", "https://monkeylearn.com/blog/category/news/", "https://monkeylearn.com/blog/category/text-classification/", "https://www.monkeylearn.com", "https://www.monkeylearn.com", "http://slack.monkeylearn.com/", "https://twitter.com/monkeylearn", "https://github.com/monkeylearn/"]}, "117": {"url": "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "title": "Machine Learning Translation and the Google Translate Algorithm", "text": "Every day we use different technologies without even knowing how exactly they work. In fact, it\u2019s not very easy to understand engines powered by machine learning. The Statsbot team wants to make machine learning clear by telling data stories in this blog. Today, we\u2019ve decided to explore machine translators and explain how the Google Translate algorithm works. Years ago, it was very time consuming to translate the text from an unknown language. Using simple vocabularies with word-for-word translation was hard for two reasons: 1) the reader had to know the grammar rules and 2) needed to keep in mind all language versions while translating the whole sentence. Now, we don\u2019t need to struggle so much\u2013 we can translate phrases, sentences, and even large texts just by putting them in Google Translate. But most people don\u2019t actually care how the engine of machine learning translation works. This post is for those who do care. If the Google Translate engine tried to kept the translations for even short sentences, it wouldn\u2019t work because of the huge number of possible variations. The best idea can be to teach the computer sets of grammar rules and translate the sentences according to them. If only it were as easy as it sounds. If you have ever tried learning a foreign language, you know that there are always a lot of exceptions to rules. When we try to capture all these rules, exceptions and exceptions to the exceptions in the program, the quality of translation breaks down. Creating your own simple machine translator would be a great project for any data science resume. Let\u2019s try to investigate what hides in the \u201cblack boxes\u201d that we call machine translators. Deep neural networks can achieve excellent results in very complicated tasks (speech/visual object recognition), but despite their flexibility, they can be applied only for tasks where the input and target have fixed dimensionality. Here is where Long Short-Term Memory networks (LSTMs) come into play, helping us to work with sequences whose length we can\u2019t know a priori. LSTMs are a special kind of recurrent neural network (RNN), capable of learning long-term dependencies. All RNNs look like a chain of repeating modules. So the LSTM transmits data from module to module and, for example, for generating Ht we use not only Xt, but all previous input values X. To learn more about structure and mathematical models of LSTM, you can read the great article \u201cUnderstanding LSTM Networks.\u201d Our next step is bidirectional recurrent neural networks (BRNNs). What a BRNN does, is split the neurons of a regular RNN into two directions. One direction is for positive time, or forward states. The other direction is for negative time, or backward states. The output of these two states are not connected to inputs of the opposite direction states. To understand why BRNNs can work better than a simple RNN, imagine that we have a sentence of 9 words and we want to predict the 5th word. We can make it know either only the first 4 words, or the first 4 words and last 4 words. Of course, the quality in the second case would be better. Now we\u2019re ready to move to sequence to sequence models (also called seq2seq). The basic seq2seq model consist of two RNNs: an encoder network that processes the input and a decoder network that generates the output. Finally, we can make our first machine translator! However, let\u2019s think about one trick. Google Translate currently supports 103 languages, so we should have 103x102 different models for each pair of languages. Of course, the quality of these models varies according to the popularity of languages and the amount of documents needed for training this network. The best that we can do is to make one NN to take any language as input and translate into any language. That very idea was realized by Google engineers at the end of 2016. Architecture of NN was build on the seq2seq model, which we have already studied. The only exception is that between the encoder and decoder there are 8 layers of LSTM-RNN that have residual connections between layers with some tweaks for accuracy and speed. If you want to go deeper with that, take a look at the article Google\u2019s Neural Machine Translation System. The system requires a \u201ctoken\u201d at the beginning of the input sentence which specifies the language you\u2019re trying to translate the phrase into. This improves translation quality and enables translations even between two languages which the system hasn\u2019t seen yet, a method termed \u201cZero-Shot Translation.\u201d When we\u2019re talking about improvements and better results from Google Translate algorithms, how can we correctly evaluate that the first candidate for translation is better than the second? It\u2019s not a trivial problem, because for some commonly used sentences we have the sets of reference translations from the professional translators, that have, of course, some differences. There are a lot of approaches that partly solve this problem, but the most popular and effective metric is BLEU (bilingual evaluation understudy). Imagine, we have two candidates from machine translators: Although they have the same meaning they differ in quality and have different structure. Let\u2019s look at two human translations: Obviously, Candidate 1 is better, sharing more words and phrases compared to Candidate 2. This is a key idea of the simple BLEU approach. We can compare n-grams of the candidate with n-grams of the reference translation and count the number of matches (independent from their position). We use only n-gram precisions, because calculating recall is difficult with multiple refs and the result is the geometric average of n-gram scores. Now you can evaluate the complex engine of machine learning translation. Next time when you translate something with Google Translate, imagine how many millions of documents it analyzed before giving you the best language version.", "links": ["https://blog.statsbot.co?source=logo-lo_7c777f2aa7bc---cfc9f21a543a", "https://twitter.com/statsbotco", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.statsbot.co%2Fmachine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co", "https://blog.statsbot.co/analytics/home", "https://blog.statsbot.co/datascience/home", "https://blog.statsbot.co/design/home", "https://blog.statsbot.co/startups/home", "https://blog.statsbot.co/bots/home", "https://blog.statsbot.co/news/home", "https://blog.statsbot.co/statsbot-digest-b0d7372f842a", "http://statsbot.co?utm_source=blog&utm_medium=navigation&utm_campaign=robot_face", "https://blog.statsbot.co/search", "https://blog.statsbot.co/@daniilkorbut?source=post_header_lockup", "https://blog.statsbot.co/@daniilkorbut?source=post_header_lockup", "https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html", "http://statsbot.co?utm_source=blog&utm_medium=article&utm_campaign=machine_translation", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6?utm_source=blog&utm_medium=article&utm_campaign=machine_translation", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6", "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "https://www.semanticscholar.org/paper/Hybrid-speech-recognition-with-Deep-Bidirectional-Graves-Jaitly/5807664af8e63d5207f59fb263c9e7bd3673be79", "https://research.googleblog.com/2016/09/a-neural-network-for-machine.html", "https://www.newscientist.com/article/2114748-google-translate-ai-invents-its-own-language-to-translate-with/", "https://research.googleblog.com/2016/09/a-neural-network-for-machine.html", "https://arxiv.org/abs/1609.08144", "https://en.wikipedia.org/wiki/BLEU", "https://en.wikipedia.org/wiki/N-gram", "https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3", "https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/tagged/machine-learning?source=post", "https://blog.statsbot.co/tagged/machine-translation?source=post", "https://blog.statsbot.co/tagged/google-translate?source=post", "https://blog.statsbot.co/tagged/data-science?source=post", "https://blog.statsbot.co/tagged/neural-networks?source=post", "https://blog.statsbot.co/@daniilkorbut?source=footer_card", "https://blog.statsbot.co/@daniilkorbut", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "118": {"url": "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "title": "Yanir SeroussiMy 10-step path to becoming a remote data scientist with\u00a0Automattic", "text": "About two years ago, I read the book The Year without Pants, which describes the author\u2019s experience leading a team at Automattic (the company behind WordPress.com, among other products). Automattic is a fully-distributed company, which means that all of its employees work remotely (hence pants are optional). While the book discusses some of the challenges of working remotely, the author\u2019s general experience was very positive. A few months after reading the book, I decided to look for a full-time position after a period of independent work. Ideally, I wanted a well-paid data science-y remote job with an established distributed tech company that offers a good life balance and makes products I care about. Automattic seemed to tick all my boxes, so I decided to apply for a job with them. This post describes my application steps, which ultimately led to me becoming a data scientist with Automattic. Before jumping in, it\u2019s worth noting that this post describes my personal experience. If you apply for a job with Automattic, your experience is likely to be different, as the process varies across teams, and evolves over time. I decided to apply for a data wrangler position with Automattic in October 2015. While data wrangler may sound less sexy than data scientist, reading the job ad led me to believe that the position may involve interesting data science work. This impression was strengthened by some LinkedIn stalking, which included finding current data wranglers and reading through their profiles and websites. I later found out that all the people on the data division start out as data wranglers, and then they may pick their own title. Some data wranglers do data science work, while others are more focused on data engineering, and there are some projects that require a broad range of skills. As the usefulness of the term data scientist is questionable, I\u2019m not too fussed about fancy job titles. It\u2019s more important to do interesting work in a supportive environment. Applying for the job was fairly straightforward. I simply followed the instructions from the ad:  Does this sound interesting? If yes, please send a short email to jobs @ this domain telling us about yourself and attach a resum\u00e9. Let us know what you can contribute to the team. Include the title of the position you\u2019re applying for and your name in the subject. Proofread! Make sure you spell and capitalize WordPress and Automattic correctly. We are lucky to receive hundreds of applications for every position, so try to make your application stand out. If you apply for multiple positions or send multiple emails there will be one reply.  Having been on the receiving side of job applications, I find it surprising that many people don\u2019t bother writing a cover letter, addressing the selection criteria in the ad, or even applying for a job they\u2019re qualified to do. Hence, my cover letter was fairly short, comprising of several bullet points that highlight the similarities between the job requirements and my experience. It was nothing fancy, but simple cover letters have worked well for me in the past. The initial application was followed by a long wait. From my research, this is the typical scenario. This is unsurprising, as Automattic is a fairly small company with a large footprint, which is both distributed and known as a great place to work (e.g., its Glassdoor rating is 4.9). Therefore, it attracts many applicants from all over the world, which take a while to process. In addition, Matt Mullenweg (Automattic\u2019s CEO) reviews job applications before passing them on to the team leads. As I didn\u2019t know that Matt reviewed job applications, I decided to try to shorten the wait by getting introduced to someone in the data division. My first attempt was via a second-degree LinkedIn connection who works for Automattic. He responded quickly when I reached out to him, saying that his experience working with the company is in line with the Glassdoor reviews \u2013 it\u2019s the best job he\u2019s had in his 15-year-long career. However, he couldn\u2019t help me with an intro, because there is no simple way around Automattic\u2019s internal processes. Nonetheless, he reassured me that it is worth waiting patiently, as the strict process means that you end up working with great people. I wasn\u2019t in a huge rush to find a job, but in December 2015 I decided to accept an offer to become the head of data science at Car Next Door. This was a good decision at the time, as I believe in the company\u2019s original vision of reducing the number of cars on the road through car sharing, and it seemed like there would be many interesting projects for me to work on. The position wasn\u2019t completely remote, but as the company was already spread across several cities, I was able to work from home for a day or two every week. In addition, it was a pleasant commute by bike from my Sydney home to the office, so putting the fully-remote job search on hold didn\u2019t seem like a major sacrifice. As I haven\u2019t heard anything from Automattic at that stage, it seemed unwise to reject a good offer, so I started working full-time with Car Next Door in January 2016. I successfully attracted Automattic\u2019s attention with a post I published on the misuse of the word insights by many tech companies, which included an example from WordPress.com. Greg Ichneumon Brown, one of the data wranglers, commented on the post, and invited me to apply to join Automattic and help them address the issues I raised. This happened after I accepted the offer from Car Next Door, and hasn\u2019t resulted in any speed up of the process, so I just gave up on Automattic and carried on with my life. I finally heard back from Automattic in February 2016 (four months after my initial application and a month into my employment with Car Next Door). Martin Remy, who leads the data division, emailed me to enquire if I\u2019m still interested in the position. I informed him that I was no longer looking for a job, but we agreed to have an informal chat, as I\u2019ve been waiting for such a long time. As is often the case with Automattic interviews, the chat with Martin was completely text-based. Working with a distributed team means that voice and video calls can be hard to schedule. Hence, Automattic relies heavily on textual channels, and text-based interviews allow the company to test the written communication skills of candidates. The chat revolved around my past work experience, and Martin also took the time to answer my questions about the company and the data division. At the conclusion of the chat, Martin suggested I contact him directly if I was ever interested in continuing the application process. While I was happy with my position at the time, the chat strengthened my positive impression of Automattic, and I decided that I would reapply if I were to look for a full-time position again. My next job search started earlier than I had anticipated. In October 2016, I decided to leave Car Next Door due to disagreements with the founders over the general direction of the company. In addition, I had more flexibility in choosing where to live, as my personal circumstances had changed. As I\u2019ve always been curious about life outside the capital cities of Australia, I wanted to move away from Sydney. While I could have probably continued working remotely with Car Next Door, I felt that it would be better to find a job with a fully-distributed team. Therefore, I messaged Martin and we scheduled another chat. The second chat with Martin took place in early November. Similarly to the first chat, it was conducted via Skype text messages, and revolved around my work in the time that has passed since the first chat. This time, as I was keen on continuing with the process, I asked more specific questions about what kind of work I\u2019m likely to end up doing and what the next steps would be. The answers were that I\u2019d be joining the data science team, and that the next steps are a pre-trial test, a paid trial, and a final interview with Matt. While this sounds straightforward, it took another six months until I finally became an Automattic employee (but I wasn\u2019t in a rush). The pre-trial test consisted of a data analysis task, where I was given a dataset and a set of questions to answer by Carly Stambaugh, the data science lead. The goal of the test is to evaluate the candidate\u2019s approach to a problem, and assess organisational and communication skills. As such, the focus isn\u2019t on obtaining a specific result, so candidates are given a choice of several potential avenues to explore. The open-ended nature of the task is reminiscent of many real-world data science projects, where you don\u2019t always have a clear idea of what you\u2019re going to discover. While some people may find this kind of uncertainty daunting, I find it interesting, as it is one of the things that makes data science a science. I spent a few days analysing the data and preparing a report, which was submitted as a Jupyter Notebook. After submitting my initial report, there were a few follow-up questions, which I answered by email. The report was reviewed by Carly and Martin, and as they were satisfied with my work, I was invited to proceed to the next stage: A paid trial project. The main part of the application process with Automattic is the paid trial project. The rationale behind doing paid trials was explained a few years ago by Matt in Hire by Auditions, Not Resumes:  Before we hire anyone, they go through a trial process first, on contract. They can do the work at night or over the weekend, so they don\u2019t have to leave their current job in the meantime. We pay a standard rate of $25 per hour, regardless of whether you\u2019re applying to be an engineer or the chief financial officer. During the trials, we give the applicants actual work. If you\u2019re applying to work in customer support, you\u2019ll answer tickets. If you\u2019re an engineer, you\u2019ll work on engineering problems. If you\u2019re a designer, you\u2019ll design. There\u2019s nothing like being in the trenches with someone, working with them day by day. It tells you something you can\u2019t learn from resumes, interviews, or reference checks. At the end of the trial, everyone involved has a great sense of whether they want to work together going forward. And, yes, that means everyone \u2014 it\u2019s a mutual tryout. Some people decide we\u2019re not the right fit for them.  The goal of my trial project was to improve the Elasticsearch language detection algorithm. This took about a month, and ultimately resulted in a pull request that got merged into the language detection plugin. I find this aspect of the process pretty exciting: While the plugin is used to classify millions of documents internally by Automattic, its impact extends beyond the company, as Elasticsearch is used by many other organisations and projects. This stands in contrast to many other technical job interviews, which consist of unpaid work on toy problems under stressful conditions, where the work performed is ultimately thrown away. While the monetary compensation for the trial work is lower than the market rate for data science consulting, I valued the opportunity to work on a real open source project, even if this hadn\u2019t led to me getting hired. There was much more to the trial project than what\u2019s shown in the final pull request. Most of the discussions were held on an internal project thread, primarly under the guidance of Carly (the data science lead), and Greg (the data wrangler who replied to my post a year earlier). The project was kicked off with a general problem statement: There was some evidence that the Elasticsearch language detection plugin doesn\u2019t perform well on short texts, and my mission was to improve it. As the plugin didn\u2019t include any tests for short texts, one of the main contributions of my work was the creation of datasets and tests to measure its accuracy on texts of different lengths. This was followed by some tweaks that improved the plugin\u2019s performance, as summarised in the pull request. Internally, this work consisted of several iterations where I came up with ideas, asked questions, implemented the ideas, shared the results, and discussed further steps. There are still many possible improvements to the work done in the trial. However, as trials generally last around a month, we decided to end it after a few iterations. I enjoyed the trial process, but it is definitely not for everyone. Most notably, there is a strong emphasis on asynchronous text-based communication, which is the main mode by which projects are coordinated at Automattic. People who don\u2019t enjoy written communication may find this aspect challenging, but I have always found that writing helps me organise my thoughts, and that I retain information better when reading than when listening to people speak. That being said, Automatticians do meet in person several times a year, and some teams have video chats for some discussions. While doing the trial, I had a video chat with Carly, which was the first (and last) time in the process that I got to see and hear a live human. However, this was not an essential part of the trial project, as our chat was mostly on the data scientist role and my job expectations. I finished working on the trial project just before Christmas. The feedback I received throughout the trial was positive, but Martin, Carly, and Greg had to go through the work and discuss it among themselves before making a final decision. This took about a month, due to the holiday period, various personal circumstances, and the data science team meetup that was scheduled for January 2017. Eventually, Martin got back to me with positive news: They were satisfied with my trial work, which meant there was only one stage left \u2013 the final interview with Matt Mullenweg, Automattic\u2019s CEO. Like other parts of the process, the interview with Matt is text-based. The way it works is fairly simple: I was instructed to message Matt on Slack and wait for a response, which may take days or weeks. I sent Matt a message on January 25, and was surprised to hear back from him the following morning. However, that day was Australia Day, which is a public holiday here. Therefore, I only got back to him two hours after he messaged me that morning, and by that time he was probably already busy with other things. This was the start of a pretty long wait. I left Car Next Door at the end of January, as I figured that I would be able to line up some other work even if things didn\u2019t work out with Automattic. My plan was to take some time off, and then move up to the Northern Rivers area of New South Wales. I had two Reef Life Survey trips planned, so I wasn\u2019t going to start working again before mid-April. I assumed that I would hear back from Matt before then, which would have allowed me to make an informed decision whether to look for another job or not. After two weeks of waiting, the time for my dive trips was nearing. As I was going to be without mobile reception for a while, I thought it\u2019d be worth letting Matt know my schedule. After discussing the matter with Martin, I messaged Matt. He responded, saying that we might as well do the interview at the beginning of April, as I won\u2019t be starting work before that time anyway. I would have preferred to be done with the interview earlier, but was happy to have some certainty and not worry about missing more chat messages before April. In early April, I returned from my second dive trip (which included a close encounter with Cyclone Debbie), and was hoping to sort out my remote work situation while completing the move up north. Unfortunately, while the move was successful, I was ready to give up on Automattic because I haven\u2019t heard back from Matt at all in April. However, Martin remained optimistic and encouraged me to wait patiently, which I did as I was pretty busy with the move and with some casual freelancing projects. The chat with Matt finally happened on May 2. As is often the case, it took a few hours and covered my background, the trial process, and some other general questions. I asked him about my long wait for the final chat, and he apologised for me being an outlier, as most chats happen within two weeks of a candidate being passed over to him. As the chat was about to conclude, we got to the topic of salary negotiation (which went well), and then the process was finally over! Within a few hours of the chat I was sent an offer letter and an employment contract. As Automattic has an entity in Australia (called Ausomattic), it\u2019s a fairly standard contract. I signed the contract and started work the following week \u2013 over a year and a half after my initial application. Even before I started working, I booked tickets to meet the data division in Montr\u00e9al \u2013 a fairly swift transition from the long wait for the final interview. As noted above, Automatticians get to choose their own job titles, so to become a data scientist with Automattic, I had to set my job title to Data Scientist. This is generally how many people become data scientists these days, even outside Automattic. However, job titles don\u2019t matter as much as job satisfaction. And after 2.5 months with Automattic, I\u2019m very satisfied with my decision to join the company. My first three weeks were spent doing customer support, like all new Automattic employees. Since then, I\u2019ve been involved in projects to make engagement measurement more consistent (harder than it sounds, as counting things is hard), and to improve the data science codebase (e.g., moving away from Legacy Python). Besides that, I also went to Montr\u00e9al for the data division meetup, and have started getting into chatbot work. I\u2019m looking forward to doing more work and sharing my experience here and on data.blog. Very enlightening post! It was very awesome to see that the insights you saw to Elasticsearch went to a PR. I bet that was worth the whole thing!  That\u2019s very exciting, I wanted to ask are you a self learner or do you have a degree,can you please share your background. Thank you  Thanks Mostafa. Yes, I have a BSc in computer science, and a PhD in what you would now call data science. See: https://www.linkedin.com/in/yanirseroussi/  Fill in your details below or click an icon to log in:     Connecting to %s         Enter your email address to follow this blog and receive notifications of new posts by email.", "links": ["https://yanirseroussi.com/", "https://yanirseroussi.com/", "https://yanirseroussi.com/about/", "https://yanirseroussi.com/presentations/", "https://yanirseroussi.com/phd-work/", "https://yanirseroussi.com/2014/04/05/kaggle-competition-summaries/", "http://scottberkun.com/yearwithoutpants/", "https://automattic.com/", "https://yanirseroussi.com/2015/03/22/the-long-road-to-a-lifestyle-business/", "http://web.archive.org/web/20150908140923/https://automattic.com/work-with-us/data-wrangler/", "https://yanirseroussi.com/2016/08/04/is-data-scientist-a-useless-job-title/", "https://automattic.com/about/", "https://www.glassdoor.com.au/Reviews/Automattic-Reviews-E751107.htm", "http://davemart.in/remote-hiring/", "https://www.carnextdoor.com.au/", "https://yanirseroussi.com/2015/12/08/this-holiday-season-give-me-real-insights/", "https://yanirseroussi.com/2015/12/08/this-holiday-season-give-me-real-insights/#comment-957", "http://jupyter.org/", "https://hbr.org/2014/01/hire-by-auditions-not-resumes", "https://www.elastic.co/products/elasticsearch", "https://github.com/jprante/elasticsearch-langdetect/pull/69", "https://github.com/jprante/elasticsearch-langdetect/pull/69", "https://yanirseroussi.com/2016/01/24/the-joys-of-offline-data-collection/", "https://www.whitsundaytimes.com.au/news/boat-caught-in-eye-of-cyclone-cruises-home/3164170/", "https://data.blog/2017/06/29/data-coalesce-automattic-data-division-meets-in-montreal/", "http://daynebatten.com/2016/06/counting-hard-data-science/", "http://powerfulpython.com/blog/magic-word-legacy-python/", "https://data.blog/2017/05/24/may-the-bot-be-with-you-how-algorithms-are-supporting-happiness-at-wordpress-com/", "https://data.blog/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=twitter", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=facebook", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=google-plus-1", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=linkedin", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=reddit", "https://yanirseroussi.com/category/data-science-2/", "https://yanirseroussi.com/tag/automattic/", "https://yanirseroussi.com/tag/career/", "https://yanirseroussi.com/tag/data-science/", "https://yanirseroussi.com/tag/elasticsearch/", "https://yanirseroussi.com/tag/personal/", "https://yanirseroussi.com/tag/wordpress/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "https://yanirseroussi.com/author/yanirseroussi/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comments", "https://yanirseroussi.com/2017/06/03/exploring-and-visualising-reef-life-survey-data/", "https://dotnetmeditations.com/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1698", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1698&_wpnonce=6c197b2684", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1698#respond", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1700", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1700&_wpnonce=350fa40ca9", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1700#respond", "http://yanirseroussi.com", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1705", "https://www.linkedin.com/in/yanirseroussi/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1705&_wpnonce=7f1e8d7061", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1705#respond", "https://gravatar.com/site/signup/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "https://yanirseroussi.com/2017/06/03/exploring-and-visualising-reef-life-survey-data/", "https://yanirseroussi.com/2017/01/08/customer-lifetime-value-and-the-proliferation-of-misinformation-on-the-internet/", "https://yanirseroussi.com/2016/09/19/ask-why-finding-motives-causes-and-purpose-in-data-science/", "https://yanirseroussi.com/2016/08/21/seven-ways-to-be-data-driven-off-a-cliff/", "https://yanirseroussi.com/category/bcrecommender/", "https://yanirseroussi.com/category/data-science-2/", "https://yanirseroussi.com/category/environment/", "https://yanirseroussi.com/category/general/", "https://yanirseroussi.com/category/kaggle-2/", "https://yanirseroussi.com/category/machine-intelligence/", "https://yanirseroussi.com/category/phd-work/", "https://wordpress.com/?ref=footer_blog"]}, "119": {"url": "https://medium.com/airbnb-engineering/using-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d", "title": "Using Machine Learning to Predict Value of Homes On\u00a0Airbnb", "text": "by Robert Chang Data products have always been an instrumental part of Airbnb\u2019s service. However, we have long recognized that it\u2019s costly to make data products. For example, personalized search ranking enables guests to more easily discover homes, and smart pricing allows hosts to set more competitive prices according to supply and demand. However, these projects each required a lot of dedicated data science and engineering time and effort. Recently, advances in Airbnb\u2019s machine learning infrastructure have lowered the cost significantly to deploy new machine learning models to production. For example, our ML Infra team built a general feature repository that allows users to leverage high quality, vetted, reusable features in their models. Data scientists have started to incorporate several AutoML tools into their workflows to speed up model selection and performance benchmarking. Additionally, ML infra created a new framework that will automatically translate Jupyter notebooks into Airflow pipelines. In this post, I will describe how these tools worked together to expedite the modeling process and hence lower the overall development costs for a specific use case of LTV modeling\u200a\u2014\u200apredicting the value of homes on Airbnb. Customer Lifetime Value (LTV), a popular concept among e-commerce and marketplace companies, captures the projected value of a user for a fixed time horizon, often measured in dollar terms. At e-commerce companies like Spotify or Netflix, LTV is often used to make pricing decisions like setting subscription fees. At marketplace companies like Airbnb, knowing users\u2019 LTVs enable us to allocate budget across different marketing channels more efficiently, calculate more precise bidding prices for online marketing based on keywords, and create better listing segments. While one can use past data to calculate the historical value of existing listings, we took one step further to predict LTV of new listings using machine learning. Data scientists are typically accustomed to machine learning related tasks such as feature engineering, prototyping, and model selection. However, taking a model prototype to production often requires an orthogonal set of data engineering skills that data scientists might not be familiar with. Luckily, At Airbnb we have machine learning tools that abstract away the engineering work behind productionizing ML models. In fact, we could not have put our model into production without these amazing tools. The remainder of this post is organized into four topics, along with the tools we used to tackle each task: One of the first steps of any supervised machine learning project is to define relevant features that are correlated with the chosen outcome variable, a process called feature engineering. For example, in predicting LTV, one might compute the percentage of the next 180 calendar dates that a listing is available or a listing\u2019s price relative to comparable listings in the same market. At Airbnb, feature engineering often means writing Hive queries to create features from scratch. However, this work is tedious and time consuming as it requires specific domain knowledge and business logic, which means the feature pipelines are often not easily sharable or even reusable. To make this work more scalable, we developed Zipline\u200a\u2014\u200aa training feature repository that provides features at different levels of granularity, such as at the host, guest, listing, or market level. The crowdsourced nature of this internal tool allows data scientists to use a wide variety of high quality, vetted features that others have prepared for past projects. If a desired feature is not available, a user can create her own feature with a feature configuration file like the following: When multiple features are required for the construction of a training set, Zipline will automatically perform intelligent key joins and backfill the training dataset behind the scenes. For the listing LTV model, we used existing Zipline features and also added a handful of our own. In sum, there were over 150 features in our model, including: With our features and outcome variable defined, we can now train a model to learn from our historical data. As in the example training dataset above, we often need to perform additional data processing before we can fit a model: In this step, we don\u2019t quite know what is the best set of features to use, so writing code that allows us to rapidly iterate is essential. The pipeline construct, commonly available in open-source tools like Scikit-Learn and Spark, is a very convenient tool for prototyping. Pipelines allow data scientists to specify high-level blueprints that describe how features should be transformed, and which models to train. To make it more concrete, below is a code snippet from our LTV model pipeline: At a high level, we use pipelines to specify data transformations for different types of features, depending on whether those features are of type binary, categorical, or numeric. FeatureUnion at the end simply combines the features column-wise to create the final training dataset. The advantage of writing prototypes with pipelines is that it abstracts away tedious data transformations using data transforms. Collectively, these transforms ensure that data will be transformed consistently across training and scoring, which solves a common problem of data transformation inconsistency when translating a prototype into production. Furthermore, pipelines also separates data transformations from model fitting. While not shown in the code above, data scientists can add a final step to specify an estimator for model fitting. By exploring different estimators, data scientists can perform model selection to pick the best model to improve the model\u2019s out of sample error. As mentioned in the previous section, we need to decide which candidate model is the best to put into production. To make such a decision, we need to weigh the tradeoffs between model interpretability and model complexity. For example, a sparse linear model might be very interpretable but not complex enough to generalize well. A tree based model might be flexible enough to capture non-linear patterns but not very interpretable. This is known as the Bias-Variance tradeoff. In applications such as insurance or credit screening, a model needs to be interpretable because it\u2019s important for the model to avoid inadvertently discriminating against certain customers. In applications such as image classification, however, it is much more important to have a performant classifier than an interpretable model. Given that model selection can be quite time consuming, we experimented with using various AutoML tools to speed up the process. By exploring a wide variety of models, we found which types of models tended to perform best. For example, we learned that eXtreme gradient boosted trees (XGBoost) significantly outperformed benchmark models such as mean response models, ridge regression models, and single decision trees. Given that our primary goal was to predict listing values, we felt comfortable productionizing our final model using XGBoost, which favors flexibility over interpretability. As we alluded to earlier, building a production pipeline is quite different from building a prototype on a local laptop. For example, how can we perform periodic re-training? How do we score a large number of examples efficiently? How do we build a pipeline to monitor model performance over time? At Airbnb, we built a framework called ML Automator that automagically translates a Jupyter notebook into an Airflow machine learning pipeline. This framework is designed specifically for data scientists who are already familiar with writing prototypes in Python, and want to take their model to production with limited experience in data engineering. Here is a code snippet demonstrating how the fit and transform functions are defined in our LTV model. The fit function tells the framework that a XGBoost model will be trained, and that data transformations will be carried out according to the pipeline we defined previously. Once the notebook is merged, ML Automator will wrap the trained model inside a Python UDF and create an Airflow pipeline like the one below. Data engineering tasks such as data serialization, scheduling of periodic re-training, and distributed scoring are all encapsulated as a part of this daily batch job. As a result, this framework significantly lowers the cost of model development for data scientists, as if there was a dedicated data engineer working alongside the data scientists to take the model into production! Note: Beyond productionization, there are other topics, such as tracking model performance over time or leveraging elastic compute environment for modeling, which we will not cover in this post. Rest assured, these are all active areas under development. In the past few months, data scientists have partnered very closely with ML Infra, and many great patterns and ideas arose out of this collaboration. In fact, we believe that these tools will unlock a new paradigm for how to develop machine learning models at Airbnb. We are very excited about the future of this framework and the new paradigm it brought along. By bridging the gap between prototyping and productionization, we can truly enable data scientists and engineers to pursue end-to-end machine learning projects and make our product better. Want to use or build these ML tools? We\u2019re always looking for talented people to join our Data Science and Analytics team! Special thanks to members of Data Science & ML Infra team who were involved in this work: Aaron Keys, Brad Hunter, Hamel Husain, Jiaying Shi, Krishna Puttaswamy, Michael Musson, Nick Handel, Varant Zanoyan, Vaughn Quoss et al. Additional thanks to Gary Tang, Jason Goodman, Jeff Feng, Lindsay Pettingill for reviewing this blog post.", "links": ["https://medium.com/", "https://medium.com/airbnb-engineering?source=logo-lo_de271416bdf---53c7c27702d5", "https://twitter.com/AirbnbEng", "https://medium.com/m/signin?redirect=https%3A%2F%2Fmedium.com%2Fairbnb-engineering%2Fusing-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d", "https://medium.com/airbnb-engineering", "https://medium.com/airbnb-engineering/ai/home", "https://medium.com/airbnb-engineering/airbnb-engineering-backend/home", "https://medium.com/airbnb-engineering/data/home", "https://medium.com/airbnb-engineering/airbnb-engineering-infrastructure/home", "https://medium.com/airbnb-engineering/tagged/mobile", "https://medium.com/airbnb-engineering/web/home", "http://airbnb.io/projects/", "https://medium.com/airbnb-engineering/search", "https://medium.com/@rchang?source=post_header_lockup", "https://medium.com/@rchang?source=post_header_lockup", "https://twitter.com/_rchang", "https://medium.com/swlh/diligence-at-social-capital-part-3-cohorts-and-revenue-ltv-ab65a07464e1", "http://scikit-learn.org/stable/", "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html", "https://www.kaggle.com/general/16927", "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html", "https://spark.apache.org/docs/latest/ml-pipeline.html", "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html", "http://scikit-learn.org/stable/data_transforms.html", "http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html", "https://medium.com/airbnb-engineering/automated-machine-learning-a-paradigm-shift-that-accelerates-data-scientist-productivity-airbnb-f1f8a10d61f8", "http://scott.fortmann-roe.com/docs/BiasVariance.html", "https://medium.com/airbnb-engineering/automated-machine-learning-a-paradigm-shift-that-accelerates-data-scientist-productivity-airbnb-f1f8a10d61f8", "https://github.com/dmlc/xgboost", "https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8", "http://www.florianwilhelm.info/2016/10/python_udf_in_hive/", "https://airflow.incubator.apache.org/", "https://www.airbnb.com/careers/departments/data-science-analytics", "https://www.linkedin.com/in/aaronkeys/", "https://www.linkedin.com/in/brad-hunter-497621a/", "https://www.linkedin.com/in/hamelhusain/", "https://www.linkedin.com/in/jiaying-shi-a2142733/", "https://www.linkedin.com/in/krishnaputtaswamy/", "https://www.linkedin.com/in/michael-m-a37b1932/", "https://www.linkedin.com/in/nicholashandel/", "https://www.linkedin.com/in/vzanoyan/", "https://www.linkedin.com/in/vquoss/", "https://www.linkedin.com/in/thegarytang/", "https://medium.com/@jasonkgoodman", "https://twitter.com/jtfeng", "https://medium.com/@lpettingill", "https://medium.com/@jtfeng?source=post_page", "https://medium.com/@jasonkgoodman?source=post_page", "https://medium.com/@gary.tang_94319?source=post_page", "https://medium.com/@lpettingill?source=post_page", "https://medium.com/@vquoss?source=post_page", "https://medium.com/@eddie.santos.3?source=post_page", "https://medium.com/@NicholasHandel?source=post_page", "https://medium.com/tag/machine-learning?source=post", "https://medium.com/tag/data-science?source=post", "https://medium.com/tag/airbnb?source=post", "https://medium.com/tag/technology?source=post", "https://medium.com/@rchang?source=footer_card", "https://medium.com/@rchang", "http://twitter.com/Airbnb", "http://twitter.com/Twitter", "https://medium.com/airbnb-engineering?source=footer_card", "https://medium.com/airbnb-engineering?source=footer_card", "http://airbnb.io", "https://medium.com/airbnb-engineering", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "120": {"url": "https://www.ayasdi.com/blog/artificial-intelligence/why_ai_is_the_new_bigdata/", "title": "The \u201cBig Data-ization\u201d of Artificial Intelligence", "text": "\u2039 Back to Blog  Artificial Intelligence, Data, Machine Intelligence, Machine Learning It seems like it was only a few years ago that the term \u201cbig data\u201d went from a promising area of research and interest to something so ubiquitous that it lost all meaning, descending ultimately into the butt of jokes.  As everyone piled onto the big data bandwagon, it became impossible to separate truth from fiction. Every executive and entrepreneur that I ran into was doing a \u201cbig data\u201d thing. I recall meeting someone whose company shot videos for enterprise customers and was pitching it as a \u2018Big Data play\u2019 \u2013 because video files, you know, are huge \u2013 they take up lots of space.  Thankfully, the noise associated with \u201cbig data\u201d is abating as sophistication and common sense take hold. In fact, in many circles the term actually exposes the user as someone who doesn\u2019t really understand the space.  Unfortunately, the same malady has now afflicted AI. Everyone I meet is doing an \u2018AI play\u2019 \u2013 even if all they did was to build a simple linear regressor in Excel.  AI is, unfortunately, the new \u201cbig data.\u201d While not good, it is not all bad either.  After all, the data ecosystem benefited from all of the \u201cbig data\u201d attention and investment \u2013 creating some amazing software and producing some exceptional productivity gains.  The same will happen with AI \u2013 with the increased attention comes investment dollars which in turn will drive adoption \u2013 enhancing the ecosystem. Having said that we need to stop calling regression on excel AI \u2013 it\u2019s ridiculous and undermines some incredible work being done in the space.  Regression has been around for 200+ years. Gauss and Legendre didn\u2019t don the AI cloak when they discovered regression. Neither should the recent stats graduate trying to raise money for his/her startup.  I will tell you what I think qualifies as AI in a moment \u2013 but here are some thoughts that influence that discussion. First, the AI definition I use is focused on the narrow, application specific AI, not the more general problem of artificial general intelligence (AGI) where simulating a person using software is the equivalent of intelligence.  Second, the vast, vast majority of the data that exists in the world is unlabeled. It is not practical to label that data manually and doing so would likely create bias anyway. One can argue that the Internet as a whole is simply a mechanism for humans to entertain ourselves while providing label data for machines.  Unlabeled data presents a different challenge, one we will address shortly, but the key point here is that it is everywhere and represents the key to extracting business value (or any value).  Third, we are not producing data scientists at a rate that can keep pace with the growth of data. Even with the moniker as a the \u201csexiest job of the 21st century\u201d the pace at which data scientists are created doesn\u2019t begin to approach the growth rate we are seeing in data.  Fourth, data scientists, for the most part, are not UX designers or product managers or, in many cases even engineers. As a result, the subject matter experts, those that sit in the business, don\u2019t have effective interfaces to the data science outputs. The interfaces that they have \u2013 powerpoint, excel, or PDF reports have limited utility in transforming the behavior of a company. What is required is something to shape behavior is something more \u2013 applications.  So what does qualify as intelligence? In \u2018On Intelligence\u2019 Jeff Hawkins says that all AI boils down to memory and prediction. The argument is very persuasive, but a bit reductive. We have a slightly different take for what an AI should display and it encompasses a framework: Here are characteristics that I think any AI should display. While some of these elements may seem self-evident that is because they are taken as a single item. Intelligence has a broader context. All the elements must work in conjunction with each other to qualify as AI. \u00a0\u00a0\u00a0 The five elements are: Let\u2019s take each of these concepts in turn. Discovery is the ability of an intelligent system to learn from data without upfront human intervention. Often, this needs to be done without being presented with an explicit target. It relies on the use of unsupervised and semi-supervised machine learning techniques (such as segmentation, dimensionality reduction, anomaly detection, etc.), as well as more supervised techniques where there is an outcome or there are several outcomes of interest. Usually, in enterprise software, the term discovery refers to the ability of ETL/MDM solutions to discover the various schemas of tables in large databases and automatically find join keys etc. This is not what we mean by discovery. We use of the term very differently and has this has important implications. In complex datasets, it is nearly impossible to ask the \u201cright\u201d questions. To discover what value lies within the data one must \u00a0understand all the relationships that are inherent and important in the data. That requires a principled approach to hypothesis generation. \u00a0 One technique, topological data analysis (TDA), is exceptional at surfacing hidden relationships that exist in the data and identifying those relationships that are meaningful without having to ask specific questions of the data. The result is an output that is able to represent complex phenomena, and is therefore able to surface weaker signals as well as the stronger signals. \u00a0 This permits the detection of emergent phenomena. \u00a0 As a result, enterprises can now discover answers to questions they didn\u2019t even know to ask and do so with data that is unlabeled. Once the data set is understood through intelligent discovery, supervised approaches are applied to predict what will happen in the future. These types of problems include classification, regression and ranking.  For this pillar, most companies use a standard set of supervised machine learning algorithms including random forests, gradient boosting, linear/sparse learners. It should be noted, however, that the unsupervised work from the previous step is highly useful in many ways. For example, it can generate relevant features for use in prediction tasks or finding local patches of data where supervised algorithms may struggle (systematic errors). The predict phase is an important part of the business value associated with data science, however, generally, in predictive analytics, there exists a notion that this is the sum total of machine learning.  This is not the case by far.  Prediction, while important, is pretty well understood and does not, on its own qualify as \u201cintelligence.\u201d \u00a0It goes back to calling a maxed out Excel table and a linear regressor AI. It is just not the case. Further, Prediction can go wrong along a number of dimensions, particularly if the groups on which you are predicting are racked with some type of bias (algorithmic, sampling etc.)  Again, Prediction is key and provides tremendous business value done correctly, but in and of itself it is not AI. We need to stop calling it as such. Applications need to support interaction with humans in a way which makes outcomes recognizable and believable. For example, when one builds a predictive model, it is important to have an explanation of how the model is doing what it is doing, i.e. what the features in the model are doing in terms that are familiar to the users of the model. \u00a0This level of familiarity is important in generating trust and intuition.  Similarly, in the same way that automobiles have mechanisms not just for detecting the presence of a malfunction, but also for specifying the nature of the malfunction and suggesting a method for correcting it, so one needs to have a \u201cnuts and bolts\u201d understanding of how an application is working in order to \u201crepair\u201d it when it goes awry. \u00a0 Transparency AND Justification. There is a difference. Transparency tells you what algorithms and parameters were used, while, Justification tells you why. For intelligence to be meaningful, it must be able to justify and explain its assertions, as well as to be able to diagnose failures.  No business leader should deploy intelligent and autonomous applications against critical business problems without a thorough understanding of what variables power the model.  Enterprises cannot move to a model of intelligent applications without trust and transparency. \u00a0 AI without UX is of limited utility.  UX is what distributes that intelligence across the organization and pushes it to the edge \u2013 where it can consumed by practitioners and subject matter experts.  Ultimately, the process of operationalizing an intelligent application within the enterprise requires some change in the organization, an acceptance that the application will evolve over time and that will demand downstream changes \u2013 automated or otherwise.  \u00a0For this to happen, intelligent applications need to be \u201clive\u201d in the business process, seeing new data and automatically executing the loop of Discover, Predict, Justify on a frequency that makes sense for that business process. For some processes that may be quarterly, for others daily. That loop can even be measured in seconds. \u00a0 Intelligent systems are designed to detect and react as the data evolves. An intelligent system is one that is always learning, live in the workflow and constantly improving. \u00a0In the modern data world, an application that is not getting more intelligent is getting dumber.  Intelligent applications are designed to detect and react when data distributions evolve. As a result, they need to be \u201con the wire\u201d in order to detect that phenomena before it becomes a problem. Too many solutions provide an answer in a point of time, an intelligent system is one that is always learning through the framework outlined here. This is what defines intelligence \u2013 not a machine learning algorithm kicking out predictions or the results of a data scientist\u2019s work as a PDF. For the industry to continue to grow and evolve we need to start doing a better job of recognizing what is truly AI and what is imposter-ware.  Let\u2019s make it a point of celebrating the real AI work.", "links": ["https://www.ayasdi.com", "https://www.ayasdi.com/platform/", "https://www.ayasdi.com/industry-overview/", "https://www.ayasdi.com/financial-services/", "https://www.ayasdi.com/healthcare/", "https://www.ayasdi.com/public-sector/", "https://www.ayasdi.com/solutions/anti-money-laundering/", "https://www.ayasdi.com/solutions/clinical-variation-management/", "https://www.ayasdi.com/solutions/denials/", "https://www.ayasdi.com/solutions/regulatory-risk/", "https://www.ayasdi.com/solutions/population-health/", "https://www.ayasdi.com/company/", "https://www.ayasdi.com/company/leadership/", "https://www.ayasdi.com/company/careers/", "https://www.ayasdi.com/company/collaborators/", "https://www.ayasdi.com/company/customers/", "https://www.ayasdi.com/company/news-and-events/", "https://www.ayasdi.com/company/contact-us/", "https://www.ayasdi.com/blog/", "https://www.ayasdi.com/request-a-demo/", "https://www.ayasdi.com/blog/author/gurjeet-singh/", "https://www.ayasdi.com/blog/category/artificial-intelligence/", "https://www.ayasdi.com/blog/category/bigdata/", "https://www.ayasdi.com/blog/category/machine-intelligence/", "https://www.ayasdi.com/blog/category/machine-learning/", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/08/02144138/BG2.png", "https://priceonomics.com/the-discovery-of-statistical-regression/", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/05/12132340/Machine_Intelligence_Apps_WP_051617v01.pdf", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/08/02144141/DS1.png", "https://www.wired.com/brandlab/2015/05/jeff-hawkins-firing-silicon-brain/", "http://cdn.ayasdi.com/wp-content/uploads/2015/02/wp-tda-and-machine-learning.pdf", "https://www.ayasdi.com/blog/aml/intelligent-segmentation-as-the-attack-point-for-aml/", "http://feedburner.google.com/fb/a/mailverify?uri=AyasdiBlog", "https://ayasdicommunity.force.com/login", "https://www.ayasdi.com/terms-of-service/", "https://www.ayasdi.com/privacy-policy/", "https://www.ayasdi.com/pilot-terms/", "http://www.youtube.com/user/ayasdi", "http://feeds.feedburner.com/AyasdiBlog", "http://www.linkedin.com/company/ayasdi", "https://www.facebook.com/ayasdi", "https://twitter.com/ayasdi"]}, "121": {"url": "https://www.mapd.com/blog/2017/08/03/whos-got-the-best-mobile-network-in-the-us/", "title": "", "text": "Based on the deluge of commercials and ads in this competitive industry, it seems every major player claims that they do. The competition is apparently so fierce that even their spokesmen are being headhunted. Admittedly, it can be a bit confusing when you find these results in one city:  And these in another:  By the way, how do you even define \u2018best\u2019 network? It\u2019s a loaded question for sure. We\u2019ll get back to that in a minute. It\u2019s a question with relevance to nearly every American, considering the vast majority of us own a cellphone of some kind (over 95% according to Pew). And we don\u2019t leave them alone; we touch them thousands of times each day. In fact, a good mobile network can affect our very quality of life. Ever travelled any significant distance with children and poor mobile data speeds? The same question is also relevant to those very telecommunications providers themselves, of course, and particularly in light of the fact that obtaining new subscribers will increasingly mean winning them away from competitors. Ensuring you are outpacing your rivals with superior network metrics is a key ingredient to both marketing and market share success. Our friends and partners at Tutela have created the world\u2019s largest mobile information platform, collecting billions of crowdsourced data points anonymously. These span device, network, and application information to allow us to get from data to insights. They\u2019ve graciously given us (and now you) a slice of their treasure trove. Our demo contains a few months of US data (~112M rows), although Tutela has data for more metrics, for nearly every country, and for a much greater timeframe. There are obviously many variables to consider when it comes to a great network, and one carrier will almost certainly not take the top spot in each category. But we can identify areas of significance where we hope to find favorable results: Spoiler alert: we\u2019re not actually going to attempt to find a winner. Those are just some of the metrics you can see at a glance on our MapD Immerse demo dashboard:  This is immediately useful. Coverage, speed, signal strength, and latency all matter, so MapD allows you to visualize and correlate them for effective consumption. Not coincidentally, the MapD Immerse visualization tool is purpose-built to leverage the MapD Core backend GPU-powered, in-memory, columnar SQL database at scale. This allows  interactive analysis over multi-billion row data sets, with the ability to drill down to an individual row...in milliseconds. It\u2019s also why Tutela has selected to partner with MapD; no need for indexing, sampling, or pre-computation. This means you can zoom in and out, filter, and more in an ad hoc fashion. Let\u2019s take it for a spin. As a consumer, you might be interested in how well each service provider performs in a certain geographic region (i.e., where you live and work) to aid in making a purchasing decision.  For example, to zoom from country level to the Washington, DC area, just type the city name into the searchbox of the pointmap. MapD Immerse will automatically zoom in when you click enter. It looks like in this region, you might lean toward Verizon:  But it may not be as clear cut if you live in San Francisco:  In either case, Sprint better have some compelling discounts or incentives to entice new subscribers. Further, both Sprint and T-Mobile might leverage this information to prioritize buildout or upgrade of cell towers. If they\u2019re analyzing subscriber loss in these areas, this also may give them insight into why that could be occurring. Another valuable feature of the MapD platform is the ability to easily identify trends, anomalies, or outliers. Visualization can often be an invaluable enabler for these situations. Sometimes new revenue opportunities or cost savings can be had by capitalizing on correlations in data that you didn\u2019t even realize existed! See the spike in data downloads in the line chart? By highlighting the time frame with your mouse, MapD will cross filter all charts using the selected time slice. Doing the same with the line chart for data uploads further reveals that the largest spikes occur on May 28 and 30. Ah, that\u2019s Memorial Day weekend in the United States. So this makes sense.  We can get even more granular and note that the bulk of data transfer occurred between 12-1am local time (4-5am UTC) as the heatmap indicates. I wonder if Uber and Lyft also experienced a spike around that time, as celebrations came to an end?  When data exploration at scale is so easy, what other interesting insights will you find? Explore this demo using Tutela's dataset on MapD\u2019s platform on your own here.  If you\u2019d like to learn more about MapD\u2019s recently announced partnership with Tutela, watch our on-demand webinar, introducing and demonstrating Tutela Explorer, a new mobile data analytics solution that provides real-time, interactive and highly visual insight into the performance of mobile networks and device usage. Tutela Explorer is available now as-a-service with global data coverage from Tutela.", "links": ["http://www.mapd.com/blog", "https://www.nytimes.com/2016/10/15/business/sprint-verizon-hear-me-now-paul-marcarelli.html", "http://www.pewinternet.org/fact-sheet/mobile/", "http://www.networkworld.com/article/3092446/smartphones/we-touch-our-phones-2617-times-a-day-says-study.html", "https://www.tutela.com/explorer/", "https://www.mapd.com/demos/telecom", "https://www.mapd.com/demos/telecom", "https://www.mapd.com/platform/core/", "https://www.mapd.com/demos/telecom", "https://www.brighttalk.com/webcast/14525/267851", "https://www.tutela.com/explorer"]}, "122": {"url": "https://www.oreilly.com/learning/building-a-simple-graphql-server-with-neo4j", "title": "Building a simple GraphQL server with Neo4j", "text": "How to implement a GraphQL API that queries Neo4j for a simple movie app. If you're interested in making the move from REST to GraphQL, check out this OSCON video from the team at Github. GraphQL is a powerful new tool for building APIs that allows clients to ask for only the data they need. Originally designed at Facebook to minimize data sent over the wire and reduce round-trip API requests for rendering views in native mobile apps, GraphQL has since been open sourced to a healthy community that is building developer tools. There are also a number of large companies and startups such as GitHub, Yelp, Coursera, Shopify, and Mattermark building public and internal GraphQL APIs. Despite what the name seems to imply, GraphQL is not a query language for graph databases, it is instead an API query language and runtime for building APIs. The \u201cGraph\u201d component of the name comes from the graph data model that GraphQL uses in the frontend. GraphQL itself is simply a specification, and there are many great tools available for building GraphQL APIs in almost every language. In this post we'll make use of graphql-tools by Apollo to build a simple GraphQL API in JavaScript that queries a Neo4j graph database for movies and movie recommendations. We will follow a recipe approach: first, exploring the problem in more detail, then developing our solution, and finally we discuss our approach. Good resources for learning more about GraphQL are GraphQL.org and the Apollo Dev Blog. GraphQL by design can work with any database or backend system, however in this example we'll be using the Neo4j graph database as our persistence layer. Why use a graph database with GraphQL? The idea of application data as a graph is an underpinning design choice of GraphQL. For example, think of customers who have placed orders that contain products\u2014that's a graph! GraphQL enables developers to translate their backend data into the application data graph on the frontend, but if we use a graph database on the backend we can do away with this impedance mismatch and we have graphs all the way down. We'd like to build a simple GraphQL API for a movie app that can do two things: In a traditional REST-ish API approach we might create two separate endpoints, perhaps /movies/search and /movies/similar. One endpoint to search for movies by substring of the title, and another to return a list of similar movies. As we add additional features we might keep adding endpoints. With GraphQL, our API is served from a single endpoint /graphql that takes one or more GraphQL queries and returns JSON data in a shape that is specified by the GraphQL query - only the data requested by the client is returned. Let's see how we can build this simple API. We'll be building a simple node.js JavaScript web server using Express.js to serve our GraphQL endpoint1. First of all we\u2019ll need a Neo4j database (with data) for our GraphQL server to query. For this example we\u2019ll make use of a Neo4j Sandbox instance. Neo4j Sandbox allows us to quickly spin up a hosted Neo4j instance, optionally with existing datasets focused around specific use cases. We\u2019ll use the Recommendations Neo4j Sandbox which includes data about movies and movie reviews and is designed to be used for generating personalized recommendations (for example, by using collaborative filtering to recommend movies based on similar users\u2019 reviews). We'll be making use of a few dependencies for this GraphQL server. I won't list them all here (you can see the full list in the package.json for this project, but there a few worth noting: We\u2019ll follow the \u201cGraphQL First\u201d development paradigm. In this approach, we start by defining a GraphQL schema. This schema defines the types and queries available in our API and then becomes the specification for the API. If we were building a complete application, the frontend developers could use this schema to build out the frontend while the backend team builds the backend in parallel, speeding development. Once we\u2019ve defined our schema we\u2019ll need to create resolver functions that are responsible for fetching data from Neo4j. This schema will define the types and GraphQL queries that we'll be able to use in our API. You can think of the schema as the API blueprint. schema.js Each GraphQL field is resolved by a resolver function. The resolver function defines how data is fetched for that field. resolvers.js Now that we've defined the GraphQL schema and resolver functions we are ready to serve the GraphQL endpoint, using Express.js. server.js If you've ever used Express this should look familiar, you'll notice that we're creating two endpoints and serving them on localhost:8080 Let's take a look at what we just did: Now that we have our API running, let's use the GraphiQL in-browser IDE for GraphQL to search for movies whose titles contain \"Matrix\" and find similar movies for each matching movie. We can load GraphiQL by opening http://loalhost:8080/graphiql in our browser and querying with this GraphQL query:   Querying our GraphQL movies API using GraphiQL, the in-browser IDE for GraphQL GraphQL is still new to almost everyone with the exception of Facebook, where it has been used since 2012, so many conventions and best practices are still being developed. However, GraphQL First Development is a philosophy the community has adopted which gives structure to the process of building a GraphQL API. By defining a contract for the API, expressed as a GraphQL schema, the frontend and backend developer teams can independently implement their applications, using the schema as a guide. To make GraphQL First Development even easier with Neo4j, the Neo4j team has built neo4j-graphql-cli, a command line tool for easily spinning up a Neo4j backed GraphQL endpoint based on a user defined GraphQL schema that can optionally be annotated with Cypher queries - exposing the power of Cypher from within GraphQL. You can learn more about this here. 1 All code for this project is available on GitHub . This example is also available as an Apollo Launchpad Pad, which you can run live in the browser.\u21a9 William Lyon is an engineer on the Developer Relations team at Neo4j, the open source graph database, where he builds tools for integrating Neo4j with other technologies and helps users be successful with graphs. He also leads Neo4j's Data Journalism Accelerator Program. Prior to Neo4j, he worked as a software engineer for a variety of startups, building mobile apps for the real estate industry, trading tools for quantitative finance, and predictive APIs. William holds a masters degree in Computer Science from the University of Montana. You can... How to package up your own repositories for use via PHP's Composer. Will content-blocking change the Web? Learn how to use Node.js and Express.js to quickly bootstrap a web server, tie in MongoDB, and display it all using dynamic HTML templates Diversity and inclusion in the tech industry has experienced a severe failure. Erica Baker approaches the topic like we would for any other service failure: with a postmortem. \u00a9 2017 O'Reilly Media, Inc. All trademarks and registered trademarks appearing on oreilly.com are the property of their respective owners.", "links": ["https://www.safaribooksonline.com/home/?utm_medium=content&utm_source=oreilly.com&utm_campaign=lgen&utm_content=20170601+nav", "http://www.oreilly.com/conferences/", "http://shop.oreilly.com/", "https://www.safaribooksonline.com/home/?utm_medium=content&utm_source=oreilly.com&utm_campaign=lgen&utm_content=20170601+nav", "http://www.oreilly.com/conferences/", "http://shop.oreilly.com/", "https://pixabay.com/en/louvre-pyramid-mesh-perspective-2189967/", "https://www.safaribooksonline.com/library/view/oscon-2017-/9781491976227/video306664.html?utm_source=oreilly&utm_medium=newsite&utm_campaign=building-a-simple-graphql-server-with-neo4j", "https://github.com/apollographql/graphql-tools", "http://www.apollodata.com/", "http://graphql.org/", "https://dev-blog.apollodata.com/", "https://neo4j.com/sandbox-v2/", "https://github.com/johnymontana/movies-graphql-neo4j-server/blob/master/package.json", "http://dev.apollodata.com/tools/graphql-tools/index.html", "https://expressjs.com/", "https://github.com/neo4j/neo4j-javascript-driver", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://www.graph.cool/docs/faq/graphql-sdl-schema-definition-language-kr84dktnp0/", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "http://loalhost:8080/graphiql", "https://www.npmjs.com/package/neo4j-graphql-cli", "http://neo4j.com/developer/graphql", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://pixabay.com/en/louvre-pyramid-mesh-perspective-2189967/", "https://twitter.com/share", "http://oreilly.com/about/", "http://oreilly.com/work-with-us.html", "http://oreilly.com/careers/", "http://shop.oreilly.com/category/customer-service.do", "http://shop.oreilly.com/category/customer-service.do", "http://fb.co/OReilly", "http://twitter.com/oreillymedia", "https://www.youtube.com/user/OreillyMedia", "https://plus.google.com/+oreillymedia", "https://www.linkedin.com/company/oreilly-media", "http://oreilly.com/terms/", "http://oreilly.com/privacy.html", "http://www.oreilly.com/about/editorial_independence.html"]}, "123": {"url": "http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "title": "", "text": "", "links": ["http://www.getrevue.co/?ref=Revue+Profile", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profileimage&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profiledate&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profilename&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lXPZA?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lXPZA?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blogs.technet.microsoft.com/machinelearning/2017/07/25/lessons-learned-benchmarking-fast-machine-learning-algorithms/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/DM9WJ?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/DM9WJ?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://medium.com/@stathis/design-by-evolution-393e41863f98?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/XYWKo?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/XYWKo?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.datasciencelearner.com/complete-overview-learning-python-data-analysis/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/6nba0?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/6nba0?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://reinforce.io/blog/introduction-to-tensorforce/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xekd3?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xekd3?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.kdnuggets.com/2017/05/top-10-machine-learning-videos-on-youtube-updated.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Ka8JK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Ka8JK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://arxiv.org/abs/1704.01568?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/96KED?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/96KED?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://datascienceplus.com/random-forests-in-r/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Vd1wK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Vd1wK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blog.keras.io/the-future-of-deep-learning.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/eKr5e?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/eKr5e?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://medium.com/airbnb-engineering/using-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/odRNK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/odRNK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://machinelearning.apple.com/2017/07/07/GAN.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xekoa?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xekoa?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://arxiv.org/abs/1707.06642?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/mxRw2?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/mxRw2?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.techleer.com/articles/200-naive-bayes-machine-learning-algorithm-for-classification-problems/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/3K1nx?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/3K1nx?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463?gi=7fe02170b69e&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/OnXK7?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/OnXK7?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://aiukraine.com/en/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/nqRVq?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/nqRVq?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.bonaccorso.eu/2017/07/23/machine-learning-algorithms/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Z4bBj?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Z4bBj?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/jobs", "http://bit.ly/2uqHV4J", "https://flyelephant.net/jobs/Spinbackup-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/jobs/Spinbackup-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/yJRmd?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/yJRmd?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/jobs/Snap-Ukraine-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/jobs/Snap-Ukraine-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/jAR3B?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/jAR3B?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%238%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "https://www.getrevue.co/?utm_source=Data Science Digest&utm_medium=email&utm_content=footerlink&utm_campaign=Issue", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=forwarded&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%238%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "http://www.getrevue.co/?ref=Revue+Profile"]}, "124": {"url": "https://blog.insightdatascience.com/preparing-for-the-transition-to-applied-ai-d41e48403447", "title": "Transitioning from Software Engineering to Artificial Intelligence", "text": "Emmanuel Ameisen and Jeremy Karnowski, Insight AI A significant part of the Software Engineer role requires staying up-to-date with evolving frameworks, standards, and paradigms. Software Engineers strive to constantly learn, in order to always use the best tool for the job. As Machine Learning finds footholds in more applications every day, it has naturally become a topic that many Engineers want to master. Machine Learning, though, is harder to pick up than a new framework. To be an efficient practitioner, you require a solid understanding of the theory of the field, broad knowledge of the current state of the art, and an ability to frame problems in a non deterministic way. Many guides you can find online will simply teach you how to train an out-of-the-box model on a curated data set to achieve good accuracy and call it a day. The truth is that a much more extensive skillset is essential in becoming an effective Machine Learning Engineer. Below is a distillation of the many conversations we\u2019ve had with over 50 top Machine Learning teams all over The Bay Area and New York, who\u2019ve come to Insight to find AI Practitioners poised to tackle their problems and accelerate their expansion into Applied AI. Deploying a Machine Learning solution requires much more than just training an arbitrary model on your data. It requires an understanding of: In other words, in addition to engineering chops, you need to understand the fundamentals of statistics, linear algebra, and optimization theory in order to integrate, deploy, and debug models. Building a custom Machine Learning solution for a problem requires that you consider issues ranging from acquiring, labeling and pre-processing your data to building, updating, and serving an inference model, and everything in between. Finally, building a REST API for a standard web-app is a task that we can deem feasible ahead of time. Machine Learning models, on the other hand are not always guaranteed to converge, or produce usable outputs. The best way to learn how to scope and deliver impactful Machine Learning products, is to understand how their theoretical underpinnings relate to the taxonomy of your data. In order to understand Machine Learning, a solid knowledge of statistics fundamentals is essential. This involves understanding the following: When you are training a neural network, what is actually happening? What makes some tasks doable and others not? A good approach to this might be to first try to understand Machine Learning through graphics and examples, before diving deeper into the theory. Concepts to understand range from how different loss functions work, why back propagation is useful, or what a computational graph is. A deep understanding is crucial both for building a functional model, and to communicate about it efficiently to the rest of the organization. Following are a few resources, starting with high level overviews, and diving deeper. Another fundamental skill is the ability to read, understand and implement research papers. It can seem like a daunting task at first, so a good way to start is to look up a paper that already has code attached to it (on GitXiv for example) and try to understand the implementation in depth. Ask any Data Scientist and they\u2019ll tell you 90% of the work they do is data munging. This is just as important for Applied AI, as the success of your model correlates hugely with the quality (and quantity) of your data. Data work comes in many aspects, and falls within a few categories: The best way to get familiar with data wrangling is to grab a dataset in the wild and try to use it. There are many datasets online and many social media and news outlets sites have great APIs. Following the steps above, a good way to learn is to: Debugging Machine Learning algorithms that fail to converge or to give sensible results involves a very different process from debugging code. In the same vein, finding the right architecture and hyperparameters requires solid theoretical fundamentals, but also good infrastructure work to be able to test different configurations out. Because of the pace at which the fields evolve, the methods to debug models are constantly evolving. Here are a few \u201csanity checks\u201d from our discussions and experience deploying models that mirror in some ways the principles of KISS familiar to many Software Engineers. A lot of those steps can be accelerated significantly by your development skills, which brings us to our last skill. A lot of Applied Machine Learning will allow you to leverage Software Engineering skills, sometimes with a little twist. These skills include: For more details on some of the software skills we recommend acquiring to become a quality Machine Learning Engineer, check out our post dedicated to transitioning to Applied AI from Academia. The resources above will help you approach and tackle actual Machine Learning problems. But the field of Applied AI changes extremely quickly, and the best way to learn, is to get your hands dirty and actually try to build out an end-to-end solution to solve a real problem. Action Items: Find some inspiration, then dive in! Remember that while Machine Learning Engineering is about building products at heart, there is a research aspect to it. You will explore models and paradigms that will prove unsuccessful, and that is perfectly fine, as it will lead you to understand the intricacies of the problem better. AI is an exciting, ever-changing field. The demand for Machine Learning Engineers is strong, and it is easy to get overwhelmed with the amount of news surrounding the topic. We recommend following a few serious sources and newsletters, to be able to separate PR and abstract research from innovations that are immediately relevant to the field. Here are some sources to help out: Want to learn applied Artificial Intelligence from top professionals in Silicon Valley or New York? Learn more about the Artificial Intelligence program.", "links": ["https://blog.insightdatascience.com?source=logo-lo_1f1f64d2ba7---d02e65779d7b", "https://twitter.com/InsightDataSci", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.insightdatascience.com%2Fpreparing-for-the-transition-to-applied-ai-d41e48403447", "https://blog.insightdatascience.com", "https://blog.insightdatascience.com/tagged/about-insight", "https://blog.insightdatascience.com/tagged/insight-data-science", "https://blog.insightdatascience.com/tagged/insight-data-engineering", "https://blog.insightdatascience.com/tagged/insight-health-data", "https://blog.insightdatascience.com/tagged/insight-ai", "https://blog.insightdatascience.com/search", "https://blog.insightdatascience.com/@emmanuelameisen?source=post_header_lockup", "https://blog.insightdatascience.com/@emmanuelameisen?source=post_header_lockup", "https://www.udacity.com/course/deep-learning--ud730", "http://cs231n.stanford.edu/", "http://cs224d.stanford.edu/", "http://www.deeplearningbook.org/", "http://www.gitxiv.com/", "https://data.sfgov.org/", "https://www.data.gov/", "https://dev.twitter.com/streaming/public", "http://developer.nytimes.com/docs", "http://people.apache.org/~fhanik/kiss.html", "http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf", "https://github.com/JasperSnoek/spearmint", "https://blog.insightdatascience.com/preparing-for-the-transition-to-applied-ai-8eaf53624079", "https://news.ycombinator.com/", "https://jack-clark.net/import-ai/", "http://www.insightdatascience.com/blog/", "https://blog.insightdatascience.com/tagged/insight-ai", "https://blog.insightdatascience.com/using-deep-learning-to-reconstruct-high-resolution-audio-29deee8b7ccd", "https://blog.insightdatascience.com/separating-overlapping-chromosomes-with-deep-learning-based-image-segmentation-22f97afd3283", "http://insightdata.ai/notifications-list", "http:insightdata.ai", "https://medium.com/@jkarnows?source=post_page", "https://medium.com/@InsightData?source=post_page", "https://medium.com/@rossfadely?source=post_page", "https://blog.insightdatascience.com/tagged/machine-learning?source=post", "https://blog.insightdatascience.com/tagged/ai?source=post", "https://blog.insightdatascience.com/tagged/software-engineering?source=post", "https://blog.insightdatascience.com/tagged/insight-ai?source=post", "https://blog.insightdatascience.com/tagged/deep-learning?source=post", "https://blog.insightdatascience.com/@emmanuelameisen?source=footer_card", "https://blog.insightdatascience.com/@emmanuelameisen", "http://twitter.com/EmmanuelAmeisen", "https://blog.insightdatascience.com?source=footer_card", "https://blog.insightdatascience.com?source=footer_card", "https://blog.insightdatascience.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "125": {"url": "http://vooban.com/en/tips-articles-geek-stuff/hyperopt-tutorial-for-optimizing-neural-networks-hyperparameters/", "title": "", "text": "Hyperopt is a way to search through an hyperparameter space. For example, it can use the Tree-structured Parzen Estimator (TPE) algorithm, which explore intelligently the search space while narrowing down to the estimated best parameters. It is hence a good method for meta-optimizing a neural network which is itself an optimisation problem: tuning a neural network uses gradient descent methods, and tuning the hyperparameters needs to be done differently since gradient descent can\u2019t apply. Therefore, Hyperopt can be useful not only for tuning hyperparameters such as the learning rate, but also to tune more fancy parameters in a flexible way, such as changing the number of layers of certain types, or the number of neurons in a layer, or even the type of layer to use at a certain place in the network given an array of choices, each with nested tunable hyperparameters. This is an oriented random search, in contrast with a Grid Search where hyperparameters are pre-established with fixed steps increase. Random Search for Hyper-Parameter Optimization (such as what Hyperopt do) has proven to be an effective search technique. The paper about this technique sits among the most cited deep learning papers. To sum up, it is more efficient to search randomly through values and to intelligently narrow the search space rather than looping on fixed sets of values for the hyperparameters. Note that this blog post is also available as a Notebook on GitHub. It contains code that can be run with Jupyter. A parameter is defined with a certain uniformrange or else a probability distribution, such as: There is also a few quantized versions of those functions, which rounds the generated values at each step of \u201cq\u201d: It is also possible to use a \u201cchoice\u201d which can lead to hyperparameter nesting: Visualisations of the parameters for probability distributions can be found below. Then, more details on choices and parameter nesting will come. Note on the above charts (especially for the loguniform and uniform distributions): the blurred line averaging the values fades out toward the ends of the signal since it is zero-padded. The line ideally would not fade out by using techniques such as mirror-padding. Those are the best distributions for modeling the values a learning rate. That\u2019s because we want to observe changes in the learning rate according to changing it with multiplications rather than additions, e.g.: when adjusting the learning rate, we\u2019ll want to try to divide it or multiply it by 2 rather than adding and substracting a finite value. To proove this, let\u2019s generate a loguniform distribution for a multiplier of the learning rate, centered at 1.0. Dividing 1 by those values should yield the same distribution. Let\u2019s now define a simple search space and solve for f(x) = x^2 - x + 1, where x is an hyperparameter. Let\u2019s solve for minimizing f(x, y) = x^2 + y^2 using a space using a python dict as structure. Later, this will neable us to nest hyperparameters with choices in a clean way. Yet, we have defined spaces as a single parameter. But that is 1D. Normally, spaces contain many parameters. Let\u2019s define a more complex one and with one nested hyperparameter choice for an uniform float: This will require us to import a few more things, and return the results with a dict that has a \u201cstatus\u201d and \u201closs\u201d key at least. Let\u2019s keep in our return dict the evaluated space too as this may come in handy if we save results to disk. Note that the optimization could be parallelized by using MongoDB and storing the trials\u2019 state here. Althought this is a built-in feature of hyperopt, let\u2019s keep things simple for our examples here. Indeed, the TPE algorithm used by the fmin function has state which is stored in the trials and which is useful to narrow the search space dynamically once we have a few trials. It is then interesting to pause and resume a training, and to apply that to a real problem. This is what\u2019s done inside the hyperopt_optimize.py file of the GitHub repository for this project. There, as an example, we optimize a convolutional neural network for solving the CIFAR-100 problem. Star Fork You might as well like this other blog post of mine on how to use Git Large File Storage (Git LFS) to handle the versioning of huge files when working with machine learning projects. 510-1015 av. Wilfrid-Pelletier  Quebec, QC, Canada  G1W 0C4 418 800.0027 TOLL FREE: 1 844 800.0027 info@vooban.com", "links": ["http://vooban.com/en/", "http://vooban.com/en/", "http://vooban.com/en/", "http://vooban.com/en/products/", "http://vooban.com/en/products/#tpti", "http://vooban.com/en/products/#tdti", "http://vooban.com/en/approach/", "http://vooban.com/en/tactical-squad/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/", "http://vooban.com/en/tips-articles-geek-stuff/", "https://pdfs.semanticscholar.org/9f2a/efc3821853e963beda011ed770f740385b77.pdf", "http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a", "https://github.com/terryum/awesome-deep-learning-papers#optimization--training-techniques", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100/blob/master/IntroductionToHyperopt.ipynb", "http://jupyter.org/", "http://vooban.com/wp-content/uploads/2017/08/histogram_1_int.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_2_uniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_3_loguniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_4_normal.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_5_lognormal.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_6_inverse_loguniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_7_inverse_lognormal.png", "http://vooban.com/wp-content/uploads/2017/08/fx2.png", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100/fork", "http://vooban.com/en/tips-articles-geek-stuff/migrating-to-git-lfs-for-developing-deep-learning-applications-with-large-files/", "http://vooban.com/en/tips-articles-geek-stuff/discover-structure-behind-data-with-decision-trees/", "http://vooban.com/en/tips-articles-geek-stuff/", "https://www.google.ca/maps/place/Vooban/@46.7694849,-71.3055195,17z/data=!3m1!4b1!4m5!3m4!1s0x4cb890d721a9234b:0x979022270b4a9fa4!8m2!3d46.7694849!4d-71.3033308", "http://vooban.com/en/products/", "http://vooban.com/en/approach/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/", "https://twitter.com/vooban", "https://www.instagram.com/voobanvibe/", "https://www.facebook.com/vooban", "https://www.linkedin.com/company/vooban", "http://vooban.com/en/", "http://vooban.com/en/products/", "http://vooban.com/en/products/#tpti", "http://vooban.com/en/products/#tdti", "http://vooban.com/en/approach/", "http://vooban.com/en/tactical-squad/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/"]}, "126": {"url": "https://elitedatascience.com/learn-python-for-data-science", "title": "How to Learn Python for Data Science in 2017 (Updated)", "text": "In this guide, we\u2019ll cover how to learn Python for data science, including our favorite curriculum for self-study. You see, data science is about problem solving, exploration, and extracting valuable information from data. To do so effectively, you\u2019ll need to wrangle\u00a0datasets, train machine learning models, visualize results, and much more. Enter Python. This is the best time ever to learn Python. In fact, Forbes named it a top 10 technical skill in terms of job demand growth. Let\u2019s discuss why\u2026 Python is one of the most widespread languages in the world, and it has a passionate community of users: Python Popularity, TIOBE Index It has an even more loyal following within the data science profession. Some people judge the quality of a programming language by the simplicity of its \"hello, world!\" program. Python does pretty well by this standard: For comparison, here's the same output in Java: Great, case closed. See you back here\u00a0after you've mastered Python? Well, in all seriousness, simplicity is one of Python's greatest strengths. Thanks to its precise and efficient syntax, Python can accomplish the same tasks with less code than other languages. This makes implementing\u00a0solutions refreshingly fast. In addition, Python's vibrant data science community means you'll be able to find plenty of tutorials, code snippets, and people to commiserate with\u00a0fixes to common bugs. Stackoverflow will be one of your best friends. Finally,\u00a0Python\u00a0has an\u00a0all-star lineup of libraries (a.k.a. packages) for data analysis and machine learning, which drastically reduce the time it takes to produce results. More on these later. Before we go into what you'll need to learn, let's discuss what you won't need. You won't need a C.S. degree. Most data scientists will never deal with topics such as memory leaks, cryptography, or \"Big O\" notation. You'll be fine as long as you can write clean, logical code in a scripting language such as Python or R. You won't need a complete course on Python. Python and data science are\u00a0not synonymous. You won't need to memorize all the syntax. Instead, focus on grasping the intuition, such as when function is appropriate or how conditional statements work. You'll gradually remember the syntax after Googling, reading documentation, and good ol' fashioned practice. We recommend a top-down approach. We advocate\u00a0a top-down approach with the goal of getting results first and then solidifying concepts over time. In fact, we prefer to cut out \"classroom\" study in favor of real-world practice. This approach will allow you to build mastery over time while having more fun. There are many ways to install Python on your computer, but we recommend the Anaconda bundle, which comes with the libraries you'll need for data science. Effective programming is not about memorizing syntax, but rather mastering a new way of thinking. Therefore, take your time in building a solid foundation of core programming concepts. These will help you translate solutions in your head into instructions for a computer. If you are completely new to programming, we recommend the excellent Automate the Boring Stuff with Python book, which has been released for free online under a creative commons license. The book promises \"practical programming for total beginners,\" and it keeps each lesson down-to-earth. Read up to Chapter 6 - Manipulating Strings and complete the practice questions along the way. If you only need to brush up on Python syntax, then we recommend the following video, aptly named \"Learn Python in One Video:\" Again, the goal of this step is not to learn everything about Python and programming. Instead, focus on the intuition. You should be able to answer questions such as: If you'd like more practice with the core programming concepts, check out the following resources. Next, we're going to focus on the for data science part of \"how to learn Python for data science.\" As we mentioned earlier, Python has an all-star lineup of libraries for data science. Libraries are simply bundles of pre-existing functions and objects that you can import into your script to save time. These are the action steps we recommend for efficiently picking up a new library: We don't recommend diving much deeper into a library right now because you'll likely forget most of what you've learned by the time you jump into projects. Instead, aim to discover what each library is capable of. If you installed Python through the Anaconda bundle as we recommended above, it will also come with Jupyter Notebook. Jupyter Notebook is a lightweight IDE that's a favorite among data scientists. We recommend it for your projects. You can open a new notebook through Anaconda Navigator, which came with Anaconda. Check out this short video for instructions. These are the essential libraries you'll need: NumPy allows easy and efficient numeric computation, and many other data science libraries are built on top of it. Pandas is high-performance library for data structures and exploratory analysis. It's built on top of NumPy. Matplotlib is a flexible plotting and visualization library. It's powerful but somewhat cumbersome. You have the option of skipping Matplotlib for now and using Seaborn to get started (see our Seaborn recommendation below). Scikit-Learn is the premier general-purpose machine learning library in Python. It has many popular algorithms and modules for pre-processing, cross-validation, and much more. Seaborn makes it much easier to plot common data visualizations. It's built on top of Matplotlib and offers a more pleasant high-level wrapper. By now, you'll have a basic understanding of programming and a working knowledge of essential libraries. This actually covers most of the Python you'll need to get started with data science. At this point, some students will feel a bit overwhelmed. That's OK, and it's perfectly normal. If you were to take the slow and traditional bottom-up approach, you might feel less overwhelmed, but it would have taken you 10 times as long to get here. Now the key is to dive in immediately and start gluing everything together. Again, our goal up to here has been to just learn enough to get started. Next, it's time to solidify your knowledge through plenty of practice and projects. You have several options. The first option is to participate on Kaggle, a site that hosts data science competitions. The main advantage of Kaggle is that every project is self-contained. You're given the dataset, a goal, and tutorials to get you started. The major disadvantage of competitions is that they're usually not representative of real-world data science. The \"Getting Started\" competitions are way too basic while the standard competitions (i.e. those with prize pools) are usually too tough for beginners. If you're interested in this path, check out our Beginner's Guide to Kaggle. The next option is to structure your own projects and pick datasets that interest you. The main advantage of this approach is that the projects are more representative of real-world data science. You'll likely need to define your own goals, collect data, clean your dataset, engineer features, and so on. The disadvantage of DIY projects is that you'll need to already be familiar with a proper data science workflow. Without one, you could miss important steps or get stuck without knowing how to proceed. If you go with this path, check out our free 7-day crash course on applied machine learning, which covers the key steps in a data science workflow. We also have another article with several\u00a0DIY project ideas. Finally, there are guided end-to-end projects. Proper guided projects should combine the best of both words - they should be representative of real-world data science and allow you to solidify your skills through a carefully planned learning curve. Many data science bootcamps offer this as a main benefit. Bootcamps usually conclude with a \"capstone project\" that allows you to see all the moving pieces together, from start to finish. We've also crafted our own\u00a0Machine Learning Masterclass to solve this exact need. It will provide you over-the-shoulder mentorship for real-world projects while teaching you all of the key concepts in context. The masterclass also includes a comprehensive Python course that gets you up to speed ASAP. In fact, many successful students have enrolled without any prior programming experience. Learn more about it here. [\u2026] In terms of programming languages \u2013 Python is pretty powerful language with integration to all technologies in the CDH stack. For a basic course check out the link below \u2013 Python For Data Science [\u2026] * Denotes Required Field Copyright \u00a9 2017 \u00b7 EliteDataScience.com \u00b7 All Rights Reserved", "links": ["https://elitedatascience.com/", "https://elitedatascience.com/", "https://elitedatascience.com/blog", "https://elitedatascience.com/machine-learning-masterclass", "https://elitedatascience.com/about", "https://elitedatascience.com/about", "https://elitedatascience.com/guest-post-submissions", "https://elitedatascience.com/author/slice2o", "https://elitedatascience.com/learn-python-for-data-science#comments", "https://www.facebook.com/sharer.php?u=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://plus.google.com/share?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://www.linkedin.com/shareArticle?trk=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://twitter.com/intent/tweet?text=How%20to%20Learn%20Python%20for%20Data%20Science%20%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "https://www.forbes.com/sites/jeffkauflin/2017/01/08/the-10-technical-skills-with-explosive-growth-in-job-demand/#6fa6bf344f5c", "http://www.tiobe.com/tiobe-index/", "https://stackoverflow.com/questions/tagged/python?sort=votes&pageSize=15", "https://www.continuum.io/downloads", "https://automatetheboringstuff.com/", "https://automatetheboringstuff.com/", "https://codefights.com/", "http://www.pythonchallenge.com/index.php", "http://www.practicepython.org/", "http://interactivepython.org/runestone/static/thinkcspy/index.html", "http://jupyter.org/", "https://www.youtube.com/watch?v=-MyjG00la2k", "https://docs.scipy.org/doc/numpy/user/", "https://docs.scipy.org/doc/numpy-dev/user/quickstart.html", "https://pandas.pydata.org/pandas-docs/stable/", "https://pandas.pydata.org/pandas-docs/stable/10min.html", "https://matplotlib.org/contents.html", "https://matplotlib.org/users/pyplot_tutorial.html", "http://scikit-learn.org/stable/documentation.html", "http://elitedatascience.com/python-machine-learning-tutorial-scikit-learn", "https://seaborn.pydata.org/", "http://elitedatascience.com/python-seaborn-tutorial", "http://elitedatascience.com/beginner-kaggle", "http://elitedatascience.com/", "http://elitedatascience.com/machine-learning-projects-for-beginners", "http://elitedatascience.com/machine-learning-masterclass", "http://elitedatascience.com/machine-learning-masterclass", "https://www.facebook.com/sharer.php?u=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://plus.google.com/share?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://www.linkedin.com/shareArticle?trk=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://twitter.com/intent/tweet?text=How%20to%20Learn%20Python%20for%20Data%20Science%20%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "https://datascienceandmore.wordpress.com/2017/03/04/getting-the-basics-down/", "https://elitedatascience.com/learn-machine-learning", "https://elitedatascience.com/learn-python-for-data-science", "https://elitedatascience.com/feature-engineering-best-practices", "https://elitedatascience.com/beginner-kaggle", "https://elitedatascience.com/imbalanced-classes", "https://elitedatascience.com/beginner-mistakes", "https://elitedatascience.com/bias-variance-tradeoff", "https://elitedatascience.com/data-science-resources", "https://elitedatascience.com/guest-post-submissions", "https://elitedatascience.com/", "https://elitedatascience.com/terms-of-service", "https://elitedatascience.com/privacy-policy", "https://elitedatascience.com/about", "https://www.linkedin.com/shareArticle?trk=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science", "http://www.facebook.com/sharer.php?u=https://elitedatascience.com/learn-python-for-data-science", "https://plus.google.com/share?text=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science", "https://twitter.com/intent/tweet?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "http://service.weibo.com/share/share.php?url=https://elitedatascience.com/learn-python-for-data-science&title=How to Learn Python for Data Science (Updated)", "https://getpocket.com/save?title=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science"]}, "127": {"url": "https://stats.stackexchange.com/questions/297380/why-not-just-dump-the-neural-networks-and-deep-learning", "title": "", "text": "Fundamental problem with deep learning and neural networks in general. The solutions that fit training data are infinite. We don't have precise mathematical equation that is satisfied by only a single one and that we can say generalizes best. Simply speaking we don't know which generalizes best. Optimizing weights is not a convex problem, so we never know we end up with a global or a local minimum. So why not just dump the neural networks and instead search for a better ML model? Something that we understand, and something that is consistent with a set of mathematical equations? Linear and SVM do not have this mathematical drawbacks and are fully consistent with a a set of mathematical equations. Why not just think on same lines (need not be linear though) and come up with a new ML model better than Linear and SVM and neural networks and deep learning? Not being able to know what solution generalizes best is an issue, but it shouldn't deter us from otherwise using a good solution. Humans themselves often do not known what generalizes best (consider, for example, competing unifying theories of physics), but that doesn't cause us too many problems. It has been shown that it is extremely rare for training to fail because of local minimums. Most of the local minimums in a deep neural network are close in value to the global minimum, so this is not an issue. source But the broader answer is that you can talk all day about nonconvexity and model selection, and people will still use neural networks simply because they work better than anything else (at least on things like image classification).  Of course there are also people arguing that we shouldn't get too focused on CNNs like the community was focused on SVMs a few decades ago, and instead keep looking for the next big thing. In particular, I think I remember Hinton regretting the effectiveness of CNNs as something which might hinder research. related post As the comments to your question point out, there are a lot of people working on finding something better. I would though like to answer this question by expanding the comment left by @josh All models are wrong but some are useful (Wiki) The above statement is a general truth used to describe the nature of statistical models. Using data that we have available, we can create models that let us do useful things such as approximate a predicted value. Take for example Linear Regression  Using a number of observations, we can fit a model to give us an approximate value for a dependent variable given any value(s) for the independent variable(s).  Burnham, K. P.; Anderson, D. R. (2002), Model Selection and Multimodel > Inference: A Practical Information-Theoretic Approach (2nd ed.): \"A model is a simplification or approximation of reality and hence   will not reflect all of reality. ... Box noted that \u201call models are   wrong, but some are useful.\u201d While a model can never be \u201ctruth,\u201d a   model might be ranked from very useful, to useful, to somewhat useful   to, finally, essentially useless.\" Deviations from our model (as can be seen in the image above) appear random, some observations are below the line and some are above, but our regression line shows a general correlation. Whilst deviations in our model appear random, in realistic scenarios there will be other factors at play which cause this deviation. For example, imagine watching cars as they drove through a junction where they must turn either left or right to continue, the cars turn in no particular pattern. Whilst we could say that the direction the cars turn is completely random, does every driver reach the junction and at that point make a random decision of which way to turn? In reality they are probably heading somewhere specific for a specific reason, and without attempting to stop each car to ask them about their reasoning, we can only describe their actions as random.  Where we are able to fit a model with minimal deviation, how certain can we be that an unknown, unnoticed or immeasurable variable wont at some point throw our model? Does the flap of a butterfly\u2019s wings in Brazil set off a tornado in Texas? The problem with using the Linear and SVN models you mention alone is that we are somewhat required to manually observe our variables and how they each affect each other. We then need to decide what variables are important and write a task-specific algorithm. This can be straight forward if we only have a few variables, but what if we had thousands? What if we wanted to create a generalised image recognition model, could this realistically be achieved with this approach?  Deep Learning and Artificial Neural Networks (ANNs) can help us create useful models for huge data sets containing huge amounts of variables (e.g. image libraries). As you mention, there's an incomprehensible number of solutions which could fit the data using ANNs, but is this number really any different to the amount of solutions we would need to develop ourselves through trial and error? The application of ANNs do much of the work for us, we can specify our inputs and our desired outputs (and tweak them later to make improvements) and leave it up to the ANN to figure out the solution. This is why ANNs are often described as \"black boxes\". From a given input they output an approximation, however (in general terms) these approximations don't include details on how they were approximated. And so it really comes down to what problem you are trying to solve, as the problem will dictate what model approach is more useful. Models are not absolutely accurate and so there is always an element of being 'wrong', however the more accurate your results the more useful they are. Having more detail in the results on how the approximation was made may also be useful, depending on the problem it may even be more useful than increased accuracy. If for example you are calculating a persons credit score, using regression and SVMs provides calculations that can be better explored. Being able to both tweak the model directly and explain to customers the effect separate independent variables have on their overall score is very useful. An ANN may aid in processing larger amounts of variables to achieve a more accurate score, but would this accuracy be more useful? I guess for some problem we care less for the mathematical rigor and simplicity but more for its utility, current status is neural network is better in performing certain task like pattern recognition in image processing.  The global minimum may as well as be useless, so we don't really care if we find it or not. The reason is that, for deep networks, not only the time to find it becomes exponentially longer as the network size increases, but also the global minimum often corresponds to overfitting the training set. Thus the generalization ability of the DNN (which is what we really care about) would suffer. Also, often we prefer flatter minima corresponding to a higher value of the loss function, than sharper minima corresponding to a lower value of the loss function, because the second one will deal very badly with uncertainty in the inputs. This is becoming increasingly clear with the development of Bayesian Deep Learning. Robust Optimization beats Determinist Optimization very often, when applied to real world problems where uncertainty is important. Finally, it's a fact that DNNs just kick the ass of methods such as XGBoost at image classification and NLP. A company which must make a profit out of image classification will correctly select them as modelsto be deployed in production (and invest a significant amount of money on feature engineering, data pipeline, etc. but I digress). This doesn't mean that they dominate all the ML environment: for example, they do worse than XGBoost on structured data (see the last winners of Kaggle competitions) and they seem to not still do as well as particle filters on time series modelling. However, some very recent innovations on RNNs may modify this situation. There are excellent answers, mostly weighing in with the usefulness of DL and ANNs. But I would like to object the OP in a more fundamental way, since the question already takes for granted the mathematical inconsistency of neural networks. First of all, there is a mathematical theory behind (most models of) Neural Networks. You could likewise argue that linear regression does not generalize, unless the underlying model is... well, linear. In neural algorithms, a model is assumed (even if not explicitly) and the fitting error is computed. The fact that algorithms are modified with various heuristics does not void the original mathematical support. BTW, local optimization is also a mathematically consistent, let alone useful, theory. Along this line, if Neural Networks just constitute one class of methods within the whole toolbox of scientists, which is the line that separates Neural Networks from the rest of techniques? In fact, SVMs were once considered a class of NNs and they still appear in the same books. On the other hand, NNs could be regarded as a (nonlinear) regression technique, maybe with some simplification. I agree with the OP that we must search better, well founded, efficient algorithms, regardless you label them as NNs or not. Sign up using Google Sign up using Facebook Sign up using Email and Password   By posting your answer, you agree to the privacy policy and terms of service. asked 3 days ago viewed  7,704 times  active yesterday                                   site design / logo \u00a9 2017 Stack Exchange Inc; user contributions licensed under cc by-sa 3.0                                  with attribution required.                                  rev 2017.8.11.26777", "links": ["https://stats.stackexchange.com", "https://chat.stackexchange.com?tab=site&host=stats.stackexchange.com", "https://stats.stackexchange.com", "https://stats.meta.stackexchange.com", "https://stats.stackexchange.com/users/signup?ssrc=site_switcher&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.stackexchange.com/users/login?ssrc=site_switcher&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stackexchange.com/sites", "https://stackoverflow.blog", "https://stackexchange.com", "https://stats.stackexchange.com/users/signup?ssrc=head&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.stackexchange.com/users/login?ssrc=head&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.meta.stackexchange.com", "https://stackoverflow.com/company/about", "https://www.stackoverflowbusiness.com/?ref=topbar_help", "https://stats.stackexchange.com", "https://arxiv.org/pdf/1406.2572.pdf", "https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/clyohpf/?context=3", "https://arxiv.org/pdf/1406.2572.pdf", "https://arxiv.org/pdf/1412.0233.pdf", "https://en.wikipedia.org/wiki/All_models_are_wrong", "https://en.wikipedia.org/wiki/Butterfly_effect", "https://stats.stackexchange.com/questions/93705/meaning-of-a-neural-network-as-a-black-box", "https://stackexchange.com/legal/privacy-policy", "https://stackexchange.com/legal/terms-of-service", "http://stats.stackexchange.com/election", "http://stats.stackexchange.com/election", "https://stackexchange.com/questions?tab=hot", "https://academia.stackexchange.com/questions/94385/where-do-bad-papers-go-to-die", "https://interpersonal.stackexchange.com/questions/1539/is-it-rude-to-ask-tourists-where-they-are-from", "https://biology.stackexchange.com/questions/64735/does-any-molecule-other-than-dna-have-a-double-helix-structure", "https://academia.stackexchange.com/questions/94474/will-one-with-record-of-unfinished-phd-on-his-her-cv-be-doubted-of-his-her-abili", "https://travel.stackexchange.com/questions/100105/what-power-supply-is-needed-to-use-a-cpap-machine-in-the-eu", "https://math.stackexchange.com/questions/2392309/closed-circle-as-a-metric-space", "https://politics.stackexchange.com/questions/23595/why-does-the-united-states-call-japan-an-ally", "https://codereview.stackexchange.com/questions/172849/checking-if-a-number-is-power-of-2-or-not", "https://travel.stackexchange.com/questions/99713/in-germany-what-are-you-supposed-to-do-if-your-train-station-doesnt-have-any-w", "https://ell.stackexchange.com/questions/138943/jokes-played-or-cracked", "https://english.stackexchange.com/questions/405590/does-een-denote-endearment", "https://security.stackexchange.com/questions/167422/is-receiving-fake-torrent-data-possible", "https://scifi.stackexchange.com/questions/166938/story-about-humanity-realizing-they-are-in-a-simulation", "https://tex.stackexchange.com/questions/386263/suppress-globally-set-before-end-axis-code-in-pgfplots", "https://tex.stackexchange.com/questions/386239/tikz-fractal-uniform-cantor-set", "https://codegolf.stackexchange.com/questions/138790/no-co-prime-neighbors", "https://codegolf.stackexchange.com/questions/138827/script-that-outputs-a-script-that-prints-a-given-input", "https://codereview.stackexchange.com/questions/172862/linq-inspired-recursive-file-search", "https://physics.stackexchange.com/questions/351628/why-must-entangled-particles-communicate-their-spin-instantaneously", "https://codereview.stackexchange.com/questions/172861/a-simple-c-class-for-packing-dna-strings", "https://softwareengineering.stackexchange.com/questions/355580/how-many-cores-should-i-utilize-for-calculations-cores-or-cores-1", "https://money.stackexchange.com/questions/83708/what-emergencies-could-justify-a-highly-liquid-emergency-fund", "https://interpersonal.stackexchange.com/questions/1556/how-to-ask-a-vegan-to-stop-telling-me-about-veganism-because-i-am-not-interested", "https://movies.stackexchange.com/questions/78741/why-is-the-movie-titled-the-phantom-menace", "https://chat.stackexchange.com?tab=site&host=stats.stackexchange.com", "https://stats.meta.stackexchange.com", "https://stackoverflow.com/company/about", "https://stackoverflow.com", "https://www.stackoverflowbusiness.com/?utm_source=so-footer&utm_medium=referral&utm_campaign=brand-activation", "https://stackoverflow.com/jobs", "https://stackoverflow.com/company/about", "https://stackoverflow.com/company/press", "https://stackexchange.com/legal", "https://stackexchange.com/legal/privacy-policy", "https://stackexchange.com", "https://stackexchange.com/sites#technology", "https://stackexchange.com/sites#lifearts", "https://stackexchange.com/sites#culturerecreation", "https://stackexchange.com/sites#science", "https://api.stackexchange.com", "https://data.stackexchange.com", "https://stackoverflow.blog?blb=1", "https://www.facebook.com/officialstackoverflow/", "https://twitter.com/stackoverflow", "https://linkedin.com/company/stack-overflow", "https://creativecommons.org/licenses/by-sa/3.0/", "https://stackoverflow.blog/2009/06/25/attribution-required/"]}, "128": {"url": "https://blog.godatadriven.com/practical-airflow-tutorial", "title": "", "text": "Airflow is a scheduler for workflows such as data pipelines, similar to Luigi and Oozie. It's written in Python and we at GoDataDriven have been contributing to it in the last few months. This tutorial is loosely based on the Airflow tutorial in the official documentation. It will walk you through the basics of setting up Airflow and creating an Airflow workflow, and it will give you some practical tips. A (possibly) more up-to-date version of this blog can be found in my git repo. Setting up a basic configuration of Airflow is pretty straightforward. After installing the Python package, we'll need a database to store some data and start the core Airflow services. You can skip this section if Airflow is already set up. Make sure that you can run airflow commands, know where to put your DAGs and have access to the web UI. Airflow is installable with pip via a simple pip install apache-airflow. Either use a separate Python virtual environment or install it in your default python environment. To use the conda virtual environment as defined in environment.yml from my git repo: You should now have an (almost) working Airflow installation. Alternatively, install Airflow yourself by running: Airflow used to be packaged as airflow but is packaged as apache-airflow since version 1.8.1. Make sure that you install any extra packages with the right Python package: e.g. use pip install apache-airflow[dask] if you've installed apache-airflow and do not use pip install airflow[dask]. Leaving out the prefix apache- will install an old version of Airflow next to your current version, leading to a world of hurt. You may run into problems if you don't have the right binaries or Python packages installed for certain backends or operators. When specifying support for e.g. PostgreSQL when installing extra Airflow packages, make sure the database is installed; do a brew install postgresql or apt-get install postgresql before the pip install apache-airflow[postgres]. Similarly, when running into HiveOperator errors, do a pip install apache-airflow[hive] and make sure you can use Hive. Before you can use Airflow you have to initialize its database. The database contains information about historical & running workflows, connections to external data sources,  user management, etc. Once the database is set up, Airflow's UI can be accessed by running a web server and workflows can be started. The default database is a SQLite database, which is fine for this tutorial. In a production setting you'll probably be using something like MySQL or PostgreSQL. You'll probably want to back it up as this database stores the state of everything related to Airflow. Airflow will use the directory set in the environment variable AIRFLOW_HOME to store its configuration and our SQlite database. This directory will be used after your first Airflow command. If you don't set the environment variable AIRFLOW_HOME, Airflow will create the directory ~/airflow/ to put its files in. Set environment variable AIRFLOW_HOME to e.g. your current directory $(pwd): or any other suitable directory. Next, initialize the database: Now start the web server and go to localhost:8080 to check out the UI: It should look something like this:  With the web server running workflows can be started from a new terminal window. Open a new terminal, activate the virtual environment and set the environment variable AIRFLOW_HOME for this terminal as well: Make sure that you're an in the same directory as before when using $(pwd). Run a supplied example: And check in the web UI that it has run by going to Browse -> Task Instances. This concludes all the setting up that you need for this tutorial. We'll create a workflow by specifying actions as a Directed Acyclic Graph (DAG) in Python. The tasks of a workflow make up a Graph; the graph is Directed because the tasks are ordered; and we don't want to get stuck in an eternal loop so the graph also has to be Acyclic. The figure below shows an example of a DAG:  The DAG of this tutorial is a bit easier. It will consist of the following tasks: and we'll plan daily execution of this workflow. Go to the folder that you've designated to be your AIRFLOW_HOME and find the DAGs folder located in subfolder dags/ (if you cannot find, check the setting dags_folder in $AIRFLOW_HOME/airflow.cfg). Create a Python file with the name airflow_tutorial.py that will contain your DAG. Your workflow will automatically be picked up and scheduled to run. First we'll configure settings that are shared by all our tasks. Settings for tasks can be passed as arguments when creating them, but we can also pass a dictionary with default values to the DAG. This allows us to share default arguments for all the tasks in our DAG is the best place to set e.g. the owner and start date of our DAG. Add the following import and dictionary to airflow_tutorial.py to specify the owner, start time, and retry settings that are shared by our tasks: These settings tell Airflow that this workflow is owned by 'me', that the workflow is valid since June 1st of 2017, it should not send emails and it is allowed to retry the workflow once if it fails with a delay of 5 minutes. Other common default arguments are email settings on failure and the end time. We'll now create a DAG object that will contain our tasks. Name it airflow_tutorial_v01 and pass default_args: With schedule_interval='0 * * * *' we've specified a run at every hour 0; the DAG will run each day at 00:00. See crontab.guru for help deciphering cron schedule expressions. Alternatively, you can use strings like '@daily' and '@hourly'. We've used a context manager to create a DAG (new since 1.8). All the tasks for the DAG should be indented to indicate that they are part of this DAG. Without this context manager you'd have to set the dag parameter for each of your tasks. Airflow will generate DAG runs from the start_date with the specified schedule_interval. Once a DAG is active, Airflow continuously checks in the database if all the DAG runs have successfully ran since the start_date. Any missing DAG runs are automatically scheduled. When you initialize on 2016-01-04 a DAG with a start_date at 2016-01-01 and a daily schedule_interval, Airflow will schedule DAG runs for all the days between 2016-01-01 and 2016-01-04. A run starts after the time for the run has passed. The time for which the workflow runs is called the execution_date. The daily workflow for 2016-06-02 runs after 2016-06-02 23:59 and the hourly workflow for 2016-07-03 01:00 starts after 2016-07-03 01:59. From the ETL viewpoint this makes sense: you can only process the daily data for a day after it has passed. This can, however, ask for some juggling with date for other workflows. For Machine Learning models you may want to use all the data up to a given date, you'll have to add the schedule_interval to your execution_date somewhere in the workflow logic. Because Airflow saves all the (scheduled) DAG runs in its database, you should not change the start_date and schedule_interval of a DAG. Instead, up the version number of the DAG (e.g. airflow_tutorial_v02) and avoid running unnecessary tasks by using the web interface or command line tools Timezones and especially daylight savings can mean trouble when scheduling things, so keep your Airflow machine in UTC. You don't want to skip an hour because daylight savings kicks in (or out). Tasks are represented by operators that either perform an action, transfer data, or sense if something has been done. Examples of actions are running a bash script or calling a Python function; of transfers are copying tables between databases or uploading a file; and of sensors are checking if a file exists or data has been added to a database. We'll create a workflow consisting of three tasks: we'll print 'hello', wait for 10 seconds and finally print 'world'.  The first two are done with the BashOperator and the latter with the PythonOperator. Give each operator an unique task ID and something to do: Note how we can pass bash commands in the BashOperator and that the PythonOperator asks for a Python function that can be called. Dependencies in tasks are added by setting other actions as upstream (or downstream).  Link the operations in a chain so that sleep will be run after print_hello and is followed by print_world; print_hello -> sleep -> print_world: After rearranging the code your final DAG should look something like: First check that DAG file contains valid Python code by executing the file with Python: You can manually test a single task for a given execution_date with airflow test: This runs the task locally as if it was for 2017-07-01, ignoring other tasks and without communicating to the database. Now that you're confident that your dag works, turn on the DAG in the web UI and sit back while Airflow starts backfilling the dag runs! You now know the basics of setting up Airflow, creating a DAG and turning it on; time to go deeper!  Address: Wibautstraat 202, 1091 GS Amsterdam, The Netherlands           Phone: +31 (0)35 672 9069           Email: signal@godatadriven.com", "links": ["http://godatadriven.com/contact", "http://godatadriven.com/careers", "http://blog.godatadriven.com", "http://godatadriven.com/business-consultancy", "http://godatadriven.com/big-data-infrastructure", "http://godatadriven.com/predictive-modelling", "http://godatadriven.com/products-overview", "http://godatadriven.com/data-discovery", "http://godatadriven.com/data-science-audit", "http://godatadriven.com/data-science-suite", "http://godatadriven.com/data-science-box", "http://godatadriven.com/divolte", "http://godatadriven.com/customers", "http://godatadriven.com/casestudy-airfranceklm", "http://godatadriven.com/casestudy-bakkersland", "http://godatadriven.com/casestudy-bol", "http://godatadriven.com/casestudy-eneco", "http://godatadriven.com/casestudy-eretail", "http://godatadriven.com/casestudy-ing", "http://godatadriven.com/casestudy-nuon", "http://godatadriven.com/casestudy-npo", "http://godatadriven.com/casestudy-schipholgroup", "http://godatadriven.com/casestudy-transavia", "http://godatadriven.com/technology-overview", "http://godatadriven.com/databricks", "http://godatadriven.com/hadoop", "http://godatadriven.com/python", "http://godatadriven.com/r", "http://godatadriven.com/spark", "http://godatadriven.com/training-overview", "http://godatadriven.com/training-schedule", "http://godatadriven.com/bigdata-training", "http://godatadriven.com/datascience-training", "http://godatadriven.com/data-science-accelerator-program", "http://godatadriven.com/meetups", "http://godatadriven.com/events", "http://blog.godatadriven.com", "http://godatadriven.com/our-story", "http://godatadriven.com/team", "http://godatadriven.com/careers", "http://godatadriven.com/media-resources", "https://blog.godatadriven.com", "http://godatadriven.com/contact", "http://godatadriven.com", "http://blog.godatadriven.com", "https://airflow.incubator.apache.org/", "https://github.com/spotify/luigi", "https://oozie.apache.org/", "https://blog.godatadriven.com/open-source-201702", "https://blog.godatadriven.com/open-source-201703", "https://blog.godatadriven.com/open-source-201704", "https://blog.godatadriven.com/open-source-201705", "https://blog.godatadriven.com/open-source-201707", "https://pythonhosted.org/airflow/tutorial.html", "https://github.com/hgrif/airflow-tutorial", "https://github.com/hgrif/airflow-tutorial", "http://conda.pydata.org/miniconda.html", "http://localhost:8080/", "https://airflow.incubator.apache.org/configuration.html", "https://airflow.incubator.apache.org/security.html", "https://github.com/apache/incubator-airflow/tree/master/scripts", "https://crontab.guru/#0_*_*_*_*", "https://jeffknupp.com/blog/2016/03/07/python-with-context-managers/", "https://airflow.incubator.apache.org/configuration.html#connections", "https://airflow.incubator.apache.org/ui.html#variable-view", "https://airflow.incubator.apache.org/tutorial.html#templating-with-jinja", "https://github.com/apache/incubator-airflow/blob/master/airflow/example_dags/example_bash_operator.py", "https://airflow.incubator.apache.org/code.html#airflow.operators.PythonOperator", "https://pythonhosted.org/airflow/tutorial.html", "https://gtoonstra.github.io/etl-with-airflow/", "https://medium.com/handy-tech/airflow-tips-tricks-and-pitfalls-9ba53fba14eb", "http://godatadriven.com/players/henk-griffioen", "http://blog.godatadriven.com/practical-airflow-tutorial", "http://blog.godatadriven.com/open-source-201707", "http://blog.godatadriven.com/cd-python-eggs-vsts", "http://blog.godatadriven.com/ldappy", "http://blog.godatadriven.com/open-source-201706", "http://blog.godatadriven.com/vendor-free-ds", "https://twitter.com/GoDataDriven", "http://godatadriven.com/business-consultancy", "http://godatadriven.com/big-data-infrastructure", "http://godatadriven.com/predictive-modelling", "http://godatadriven.com/products-overview", "http://godatadriven.com/data-discovery", "http://godatadriven.com/data-science-audit", "http://godatadriven.com/data-science-suite", "http://godatadriven.com/data-science-box", "http://godatadriven.com/divolte", "http://godatadriven.com/customers", "http://godatadriven.com/casestudy-airfranceklm", "http://godatadriven.com/casestudy-bakkersland", "http://godatadriven.com/casestudy-bol", "http://godatadriven.com/casestudy-eneco", "http://godatadriven.com/casestudy-eretail", "http://godatadriven.com/casestudy-ing", "http://godatadriven.com/casestudy-nuon", "http://godatadriven.com/casestudy-npo", "http://godatadriven.com/casestudy-schipholgroup", "http://godatadriven.com/casestudy-transavia", "http://godatadriven.com/technology-overview", "http://godatadriven.com/hadoop", "http://godatadriven.com/python", "http://godatadriven.com/r", "http://godatadriven.com/spark", "http://godatadriven.com/training-overview", "http://godatadriven.com/training-schedule", "http://godatadriven.com/bigdata-training", "http://godatadriven.com/datascience-training", "http://godatadriven.com/data-science-accelerator-program", "http://godatadriven.com/events", "http://godatadriven.com/meetups", "http://blog.godatadriven.com", "http://godatadriven.com/our-story", "http://godatadriven.com/team", "http://godatadriven.com/careers", "http://godatadriven.com/media-resources", "http://godatadriven.com/news", "http://godatadriven.com/contact", "https://cdn.xebia.com/documents/Cookies+Policy+Xebia+20170130.pdf"]}, "129": {"url": "https://concepttoclinic.drivendata.org/", "title": "", "text": "Lung Cancer Early Detection Challenge  We\u2019re calling on a global community of data scientists, engineers, designers, and researchers to build an open source software application that brings advances from machine learning into the clinic. We\u2019re not just optimizing an algorithm for a single metric\u2014we\u2019re collaborating to build tools which put AI in the hands of clinicians. In addition to pushing forward the cutting-edge of open clinical software, top contributors will be eligible for a share of $100,000 in monetary prizes generously provided by the Bonnie J. Addario Lung Cancer Foundation.          Contribute now by grabbing an issue from the project's GitHub repository and submitting a PR!        Lung cancer causes more deaths each year than any other cancer in the US, and early detection makes a big difference. Recent advances in machine learning \u2013 including open source algorithms from the 2017 Data Science Bowl \u2013 help find and interpret early signs of cancer. Using imagery from CT scans, this application will build on cutting-edge algorithms in machine learning to help clinicians identify and interpret lung nodules. Your guide to contributing throughout the challenge. All you need to start contributing is a passion for building great software and a github account.  Contributions of all sizes can earn points as they add value to the project, by building on pre-designated issues, building out new content, and building up the community. $100,000 in cash, as well as other in-kind prizes, will be available for top contributors filling key roles throughout the challenge. A live leaderboard keeps track of points earned throughout the challenge by top contributors filling key roles. Meet the amazing group of of technical, clinical, and domain experts that are helping to guide the end-to-end design of this challenge. The Addario Lung Cancer Foundation (ALCF) has set the audacious goal of making lung cancer a chronically managed disease by 2023.  Ultimately, the goal of this project is to produce a tool that draws on machine intelligence, is useful to clinicians, and makes a lasting difference in the lives of patients. We hope you will find a way to contribute!  Lung Cancer Early Detection Challenge", "links": ["https://www.drivendata.org/termsofuse/", "https://www.drivendata.org/copyrightpolicy/", "https://www.drivendata.org/privacypolicy/", "https://www.drivendata.org/", "http://www.drivendata.co/"]}, "130": {"url": "http://appsilondatascience.com/blog/rstats/2017/08/09/shiny-collections.html", "title": "shiny.collections,  Google Docs-like live collaboration in ShinyAppsilon Data Science Blog", "text": "A few weeks ago, our CTO Marek Rogala gave an engaging talk at the User Conference in Brussels. Despite being one of the last talks, he drew a crowd that filled the room and had significant viewership online. Marek\u2019s talk was entitled shiny.collections: Google Docs-like live collaboration in Shiny. In short, he went over a package we built that allows for persistence and collaboration without losing reactivity in Shiny. Users can enter information and have continuity, knowing that if they exit and return to the app, their inputs will still be there. Let\u2019s go over his talk. Recently, Shiny users have demanded more from their applications. The advent of cloud applications, especially ones like Google Docs, have accustomed users to collaboration. They expect their Shiny apps to be more than just an interactive tool for data visualisation, but rather, a production ready application that works like any other tool. Their data has to be saved automatically. Interactive collaboration is a must in some use cases. And the application must be delivered as fast as possible. You need to use a reactive database such as rethinkDB, Firebase, or mongoDB to achieve this. mongoDB isn\u2019t really a reactive database, but there are ways where it can be used as one. We are going to use rethinkDB in our case. Also, we are going to use rethinker, which is a rethinkDB driver for R. But using rethinker is not the most ideal solution, as callback do not work very well with Shiny and it is quite painful to configure to our needs. To get around this, we created our own package built upon rethinker called shiny.collections.  It lets you easily connect to your shiny app and takes care of all of the trickiness involved. During his talk, Marek showed a live demo that involved creating a chat application. Check out how to do this yourself. I reccomend watching Marek first, as he has some valuable commentary you won\u2019t find in the blog post. This example is just a simple use case of what you can achieve with shiny.collections. We\u2019ve made it a priority to make it convenient and easily integrable with other powerful tools, including DT, leaflet or rhandsontable. Our goals for the future of this package include getting it on cran and diversifying the functionality of the API. Take a look at the our package and contribute. Get in touch with us for all of you other data science needs as well.  Published 09 Aug 2017", "links": ["https://channel9.msdn.com/Events/useR-international-R-User-conferences/useR-International-R-User-2017-Conference/shinycollections-Google-Docs-like-live-collaboration-in-Shiny#comments", "https://cran.r-project.org/web/packages/rethinker/index.html", "https://appsilon.github.io/shiny.collections/", "http://appsilondatascience.com/blog/rstats/2017/07/02/shiny-chat.html", "http://appsilondatascience.com", "http://disqus.com/?ref_noscript"]}, "131": {"url": "https://blog.statsbot.co/introduction-to-imitation-learning-32334c3b1e7a", "title": "Introduction to Imitation Learning", "text": "This post is the first in the series where we will describe what Imitation Learning is. For today\u2019s article, the Statsbot team asked computer scientist Vitaly Kurin to briefly introduce Imitation Learning and outline the basics of Reinforcement Learning. Living organisms are extremely complex. Even the relatively simple ones such as flies or worms. They are able not only to operate successfully in the real world, but are extremely resilient to changes in conditions. And that\u2019s if we\u2019re not even talking about humans. We can plan ahead, we can alter our plans given new information, and we can collaborate with others and execute our plans more effectively. Science and such projects as CERN or a huge Radio Telescope in Arecibo are perfect examples of human collaboration and the art of planning. Yes, we do a lot of stupid stuff, but let\u2019s not talk about that here and instead focus on the good. Yes, we can now beat the best human in chess or Go, we can get a crazy score in Video Pinball for Atari 2600, we can even already make a pure human broke by challenging him in poker. Can we open a bottle of champagne and celebrate the triumph? I\u2019m afraid not. Yes, machine learning has recently made a significant leap forward. The combination of new Deep Learning ideas with old ones has enabled us to advance in many domains, such as computer vision, speech recognition, and text translation. Reinforcement Learning has also benefited greatly from its marriage with Deep Learning. You\u2019ve definitely heard of Deep Reinforcement Learning success such as achieving superhuman score in Atari 2600 games, solving Go, and making robots learn parkour. Though, we must admit, that operating successfully in the real world is much harder than playing Go or Space Invaders. Many of the tasks are much harder than this. Imagine a kid riding a bicycle in the middle of a crowded city center or a man driving a Porsche 911 on an autobahn at 200 miles per hour. Let\u2019s all admit that we are not there yet. The typical machine learning approach is to train a model from scratch. Give it a million images and some time to figure it out. Give it a week and let it play Space Invaders until it reaches some acceptable score. We, as humans, beg to differ. When a typical human starts to play some game he has never seen, he already has a huge amount of prior information. If he sees a door in Montezuma\u2019s Revenge, he realizes that somewhere there should lie a key and he needs to find it. When he finds the key, he remembers that the closed door is back through the two previous rooms and he returns to open it. When he sees a ladder, he realizes that he can climb it because he has done this hundreds of time already. What if we could somehow transfer human knowledge about the world to an agent? How can we extract all this information? How can we create a model out of it? There is such a way. It\u2019s called Imitation Learning. Imitation Learning is not the only name for leveraging human data for good. Some researchers also call it apprenticeship learning, others refer to it as Learning from Demonstration. From our point of view, there is no substantial difference between all of these titles and we will use Imitation Learning from now on. In order to introduce Imitation Learning, we will need to understand the basics of Reinforcement Learning first. Let\u2019s move on. It\u2019s not hard to get the general idea of the Reinforcement Learning setup. There is some agent and we want this agent to learn some task. Let\u2019s say, we have an Atari 2600 console, the agent has access to the joystick and can see what\u2019s happening on the screen. Let\u2019s say, 60 times per second we give our agent a screenshot of the game and ask him what button he wants to press. If our agent does well, he can see that his score is increasing (positive reinforcement), otherwise we can give him a penalty as a negative reward (negative reinforcement). Gradually, by trial and error, the agent starts to understand that it\u2019s better to avoid some of the actions and do those which bring him a reward. Let\u2019s make it more formal and describe the process stated above mathematically. We can describe the RL framework mentioned above (observe -> act -> get the reward and the next state) as a Markov Decision Process (MDP): where: We also need a definition of a policy function for the next section. Policy is a function, that returns an action given the state: And, actually, our final goal when solving an MDP is to learn such a policy in order to maximize the reward for our agent. Let\u2019s take an example of an MDP. The circles represent the states, arrows with green labels are actions, red labels are the rewards for actions, and the square is the terminal state. The green numeric labels are the transition probabilities. Our student starts in a state with the blue circle. He studies, but this is hard and sometimes boring. He decides to open a Facebook app and once he is there, he can either quit or continue scrolling. He then studies more and more, and finally decides to go to the pub. The state is a smaller filled circle, since now there is an element of randomness, based on the amount of knowledge the student forgets after visiting the pub. He can then either study more and pass the exam (+10 in reward), or he can go to sleep and finish the MDP right now. Since we will use DQN and related ideas in the future, let\u2019s briefly understand what is going on here. The whole approach is built upon approximating the so-called Q function and building the agent\u2019s behavior based on it. The idea of the Q function is the following: it returns you the entire expected discounted reward flow for the particular action and the particular state, given that starting from the next state we will be following our policy \ud835\udfb9. It answers the question: \u201cHow good is to press this button in this state?\u201d The Q function obeys the Bellman equation: And, finally, the Bellman principle of optimality is the following: notwithstanding what happened before, we should always take the action with the highest Q to maximize the reward flow: But how do we get such a Q function, you might ask. Let\u2019s look at an example. Imagine, you want to grab a coffee (+20 in reward) and a chocolate in a vending machine (+10 in reward). Your total reward cannot exceed thirty. Moreover, if you have taken the coffee already, it cannot be higher than 10 (the reward for chocolate) from now on. This is the idea: the Q value for the current step and action is equal to the maximum Q value for the next state (since we behave optimally) + the reward we get for the transition. The value of the quadratic objective function becomes: Q-learning itself is not new. Q-learning which uses neural networks as a function approximators is also not new (e.g. neural fitted-q iteration). A DQN paper was the first to use deep convolutional networks to solve this type of problem and introduced a couple of novelties that make the training process much more stable. First of all, experience replay. The vanilla Q-learning point is to make a step, get the reward and the next state, then update the approximation function parameters based on this transition. The DQN idea is to make the transition and save it in a \u201creplay memory\u201d\u200a\u2014\u200aan array that stores the last 10\u2076 (<insert any large number here>) transitions with the information about the reward, states before and after the transition, and if the event is terminal (game over) or not. Having this experience replay we can randomly sample mini-batches from it and learn more effectively. Another thing that makes the algorithm more stable is that DQN uses two neural networks: the first to compute the Q value for the current state and the second to compute the Q value for the next state. You can see that from the equation with the objective: two different Q functions use \ud835\udfb1 and \ud835\udfb1\u2019, respectively. Each 10,000 steps, the parameters \ud835\udfb1\u2019 are copied from the learned parameters \ud835\udfb1 and this helps a lot in increasing the stability. The problem here with using one function is that when we update the weights, both Q(s,a) and Q(s\u2019,a\u2019) increase and this might lead to oscillations or policy divergence. Using two separate networks adds a delay between an update and computation of the target Q value and reduces such cases. If you have further interest in the phenomena, read the Method section in the DQN Nature paper. Okay, everything described above sounds quite simple. If there is still something you do not understand, please, have a look at David Silver\u2019s lecture where he explains everything perfectly! Knowing all of these, can we build an True AI now? I\u2019m sorry, but we can\u2019t. There are several problems that hinder us from building an agent that will beat ByuN at StarCraft II, bring an autonomous car to the market, or give you an opportunity to buy your grandma a robot that will do the dishes for her after lunch. One of these problems is that the rewards our agent gets might be very sparse in time. Let\u2019s say, you play chess. If you lose, how do you know when you made a catastrophic move? Moreover, it\u2019s highly possible that there was not a catastrophic move, but several average ones. Another problem that is closely connected to the previous one, is the sample-efficiency problem. Or, more honestly, sample-inefficiency. Even to master a simple game such as Space Invaders might take a couple of days in-game time. It\u2019s easy to speed up learning in games since we have access to the simulators, but what if we want to learn something in real life? Unfortunately, physics is not there yet and we cannot speed up time. There is an approach that could potentially solve these problems and a bunch of others\u200a\u2014\u200aImitation Learning, as we mentioned at the beginning of this post. As we said, we, humans, rarely learn something without any prior information. Let\u2019s use this data! What should we do? The idea of Imitation Learning is implicitly giving an agent prior information about the world by mimicking human behavior in some sense. Imitation Learning will not only help us solve the sample-inefficiency or computational feasibility problems, it might potentially make the training process safer. We cannot just put an autonomous car in the middle of the street and let it do whatever it wants. We do not want it to kill humans that are around, destroy someone\u2019s property, or the equipment itself. Pretraining it on a human demonstrator\u2019s data might make the training process faster and avoid undesirable situations. Training a model requires some data. Training a Deep Learning model requires even more data. Training a Deep Reinforcement Learning model requires\u2026 Okay, you get the idea. So, this series is only partly describes what we can do with demonstration data. The main point of all of this is to call for human demonstration datasets, because we do not have many, unfortunately, up to this moment. Okay, we should stop here. The key points of this post are: In the next chapter we will write more about Behavior Cloning\u200a\u2014\u200athe simplest approach to an RL problem that can leverage the human demonstration data. Stay tuned!", "links": ["https://blog.statsbot.co?source=logo-lo_cc399bdac290---cfc9f21a543a", "https://twitter.com/statsbotco", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.statsbot.co%2Fintroduction-to-imitation-learning-32334c3b1e7a", "https://blog.statsbot.co", "https://blog.statsbot.co/analytics/home", "https://blog.statsbot.co/datascience/home", "https://blog.statsbot.co/design/home", "https://blog.statsbot.co/startups/home", "https://blog.statsbot.co/bots/home", "https://blog.statsbot.co/news/home", "https://blog.statsbot.co/statsbot-digest-b0d7372f842a", "http://statsbot.co?utm_source=blog&utm_medium=navigation&utm_campaign=robot_face", "https://blog.statsbot.co/search", "https://blog.statsbot.co/@yobibyte?source=post_header_lockup", "https://blog.statsbot.co/@yobibyte?source=post_header_lockup", "https://atlas.cern/discover/about", "http://statsbot.co?utm_source=blog&utm_medium=article&utm_campaign=imitation_learning", "https://en.wikipedia.org/wiki/CERN", "https://en.wikipedia.org/wiki/Arecibo_Observatory", "https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning/", "https://deepmind.com/research/alphago/", "https://www.theverge.com/tldr/2017/7/10/15946542/deepmind-parkour-agent-reinforcement-learning", "http://www.ceva-dsp.com/ourblog/wp-content/uploads/sites/3/2016/04/AlphaGo-Lee-Se-dol.png", "https://simple.wikipedia.org/wiki/File:Rl_agent.png", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf", "https://link.springer.com/article/10.1007/BF00992698", "https://pdfs.semanticscholar.org/2820/01869bd502c7917db8b32b75593addfbbc68.pdf", "https://link.springer.com/article/10.1007/BF00992699", "http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html?foxtrotcallback=true", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "http://wiki.teamliquid.net/starcraft2/ByuN", "http://toyoutheartist.co.uk/technique/imitation/", "https://mitpress.mit.edu/books/reinforcement-learning", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html", "https://arxiv.org/abs/1312.5602", "https://www.nature.com/nature/journal/v518/n7540/full/nature14236.html", "https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner", "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/google-analytics-audit-checklist-and-tools-fca7df2f2e7a", "https://blog.statsbot.co/google-analytics-audit-checklist-and-tools-fca7df2f2e7a", "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co/tagged/imitation-learning?source=post", "https://blog.statsbot.co/tagged/data-science?source=post", "https://blog.statsbot.co/tagged/machine-learning?source=post", "https://blog.statsbot.co/tagged/ai?source=post", "https://blog.statsbot.co/tagged/reinforcement-learning?source=post", "https://blog.statsbot.co/@yobibyte?source=footer_card", "https://blog.statsbot.co/@yobibyte", "https://github.com/yobibyte/yobiblog", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "132": {"url": "https://axibase.github.io/atsd-use-cases/DataShorts/Dollar-EX/", "title": "", "text": "Data Source: Federal Reserve Economic Data (FRED) Visualizations: ChartLab Structured Query Language: SQL Console from Axibase Download the Community Edition of Axibase Time Series Database here Data is aggregated daily in visualizations, and annually in SQL queries. Favorable dollar exhange rates are shown in green while less favorable exchange rates are shown in red, based on the overall 5-year average exchange rate. Each section is accompanied by a brief analysis to give the data context. Using computed value settings thresholds have been calculated using the underlying data, the upper 25th percentile of a given exchange rate is considered favorable while the lower 25th percentile is considered less than favorable. The data is coded with an ID which is provided in the index as well. Open any ChartLab display below to navigate through time, or change the metric to reflect the desired exchange rate. Europe Asia North America As the amount of debt carried by various EU member countries grew to increasingly worrisome heights, the Euro plunged in 2015  losing ground against the dollar. Further, as oil prices slumped the European Central Bank began to compensate by launching a program of government bond purchases. The combination of these factors and the growth of the dollar post-recession have all contributed to the currently favorable exchange rate enjoyed by the dollar. Query 1.1 Table 1.1 Figure 1.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index During the worst of the European Debt Crisis which saw European Union per capita debt grow at unprecedented rates across the continent, EU investors sought refuge in the stability of the Swiss Franc, backed by the world-renowned Swiss banking system. Further, the Swiss National Bank removed the peg to the Euro unexpectably in 2015 causing a huge surge in 2015, clearly visible in the visualization below as a sudden change from a favorable exchange rate for dollar holders to an all-time low for the observed time period. Query 1.2 Table 1.2 Figure 1.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Largely influenced by the divisive Brexit vote, the value of the Pound has remained relatively low since the referendum in late June of 2016, which aligns perfectly with the Pound\u2019s slight depreciation in value against the dollar on the chart below. Query 1.3 Table 1.3 Average Exchange Rate: 1.50 Figure 1.3   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index After purchasing trillions of Yen worth of Japanese national debt from the private sector three years ago, the Central Bank of Japan has been watching the gradual decrease in value of the Yen against the dollar. Most analysts, see this fall as controlled and predict a bright future for the Yen as the Central Bank can no longer afford to collect more debt than they already have. However, the Yen\u2019s rise much also be managed as the sudden appreciation of the currency could result in an increase in loan defaults because industries which have taken loans at current Yen prices would be paying them back at a much higher rate. Query 2.1 Table 2.1 Figure 2.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index To prevent dramatic internal inflation, the People\u2019s Bank of China closely regulates local, or onshore, trading of the Yuan by fixing the price each day. Although independent officially, the international, or offshore, price of the Yuan usually remains fairly close to its onshore value. Additionally, the PBoC carefully controls the outflow of capital and often hedges their own currency by maintaining positions in international currencies. Query 2.2 Table 2.2 Figure 2.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index The Hong Kong Dollar is officially pegged to the value of the United States Dollar so any change in relative  value is planned. The \u201cMiracle of the Orient\u201d continues to be one of the most dominant financial markets in the world. Query 2.3 Table 2.3 Figure 2.3   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index One of the financial centers of the world, Singapore is home to the second-busiest port in the world and one of the largest oil refining industries worldwide as well. With one of the highest per capita GDP values, Singapore has cemented its place of international importance despite inexplicably loose financial policy whereby the Monetary Authority of Singapore does not regulate their currency\u2019s value by adjusting interest rates, as is common with most central banking systems. Query 2.4 Table 2.4 Figure 2.4   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Relative to the USD, the Canadian dollar has been making a bit of a resurgence in 2017 as United States GDP numbers missed its targets in quarter two. Although dependant on the price of oil due to their large natural reserves, the diverse Canadian economy is one of the few globally-active nations with a significant trade surplus, although that too have been reduced in recent years. Query 3.1 Table 3.1 Figure 3.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Boasting the highest per capita income of Latin America, the Mexican economy has continued to demonstrate healthy growth over the last several decades. North America\u2019s number one producer of automobiles also enjoys a $46 billion trade surplus with the United States, and is the 9th ranked owner of United States government debt. This dependence on America affords Mexico the ability to spur dramatic internal growth that has led to the creation of a booming electronics sector, but at the cost of a  certain amount of reliance on the United States as roughly half of the country\u2019s imports and exports originate or end up  on U.S. soil. Query 3.2 Table 3.2 Figure 3.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index", "links": ["https://github.com/axibase/atsd-use-cases", "https://fred.stlouisfed.org/categories/94", "https://apps.axibase.com", "https://github.com/axibase/atsd/tree/master/api/sql", "https://axibase.com", "https://axibase.com/products/axibase-time-series-database/", "https://github.com/axibase/atsd/blob/master/installation/README.md#installation", "https://axibase.com/products/axibase-time-series-database/visualization/widgets/configuring-the-widgets/", "https://apps.axibse.com", "https://apps.axibase.com/chartlab/424eb6b2/5/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/9/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/6/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/4/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/2/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/3/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/8/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/7/#fullscreen", "https://github.com/axibase/atsd-use-cases", "https://github.com/axibase", "https://pages.github.com"]}, "133": {"url": "http://byteacademy.co/blog/overview-NLG", "title": "", "text": "NLG (Natural Language Generation), a subfield of Artificial Intelligence, is a hot topic in the technology news today. We hear a lot about AI that can soon replace writers and journalists beginning the era of machine creativity. But, what\u2019s all this fuss about? In this article, we unveil what NLG really is and show that it can bring a lot of benefits to businesses and consumers. In a nutshell, NLG is a sub-field of NLP (Natural Language Processing) that studies methods of automatic transformation of structured data into a human-readable text. In practice, there are two major types of NLG applications: template-based NLG and advanced NLG. \u00a0 Template-based NLG is the simplest solution that uses templates with canned text and placeholders to insert data into them. Such systems heavily rely on hard-coded rules, which makes them less flexible than advanced NLG. Since template-based NLG tools have a limited number of templates and require special data representations, they can not be easily reused across different projects and business use cases. \u00a0 Advanced NLG tools are more flexible thanks to the use of supervised and unsupervised Machine Learning (ML). Rather than tying down structured data to the Procrustean bed of templates, advanced NLG uses neural networks that learn morphological, lexical, and grammar patterns from large corpora of written language. Soft probabilistic methods used in the advanced NLG algorithms allow predicting the likelihood of one word appearing after another, and correcting language errors, such as misspellings. ML algorithms used in the advanced NLG are also better in dealing with new words and expressions not included in the original training samples. \u00a0 Modern NLG service providers such as Narrative Science and Automated Insights prefer advanced NLG methods because they allow creating rich data-driven models that produce intelligent insights from data. These algorithms are much more skillful in making right word choices and writing narratives that reflect intentions and business needs of the NLG users. As an added bonus, advanced NLG models can preprocess and analyze data which makes them not just translators of structured data into text, but automatic analysts able to provide actionable insights. \u00a0 Despite the fact that NLG methods have been used since the 1970s, they got a powerful momentum only recently and thanks to the AI/ML revolution. Today, many startups offer cloud-based NLG services to businesses. NLG is also gaining traction in mass media and journalism. Major American newspapers are already experimenting with the automatic storytelling. For example, in 2016 the Washington Post unveiled its automatic storytelling AI named Heliograph AI. Heliograph was used in the coverage of Rio Olympics and the US Presidential election in 2016. \u00a0 Leveraging data mining techniques and ML models the machine reporter can convert structured statistical data, diagrams, graphs, weather forecasts and other data-rich content into excellent descriptive reports that sound if though they were written by the professional reporters. But, isn\u2019t this dangerous for journalism as a profession? Proponents of automatic storytellers say that they actually free up much time for reporters to add analysis and real insights to stories rather than spending countless hours publishing news and descriptive reports[i]. NLG tools may be used in other innovative ways as well: Benefits of NLG, however, go beyond journalism. There is a growing demand for NLG services among major companies. For example, Quill, an NLG system developed by Narrative Science, is used by such companies as Deloitte, Groupon, and Credit Suisse[iv]. These companies opt for NLG solutions for a reason. Growing acceptance of NLG among businesses makes it a promising field to study. If you want to learn more about NLG, \u00a0Byte Academy offers a Natural Language course that covers Natural Language Processing and Natural Language Generation. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0  Your Name (required)    Your Email (required)    \u00a0 \u00a0 References: [i] \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 WashPost PR Blog (August 5, 2016). The Washington Post Experiments With Automated Storytelling to Help Power 2016 Rio Olympics Coverage. WashPost PR Blog. Retrieved from https://www.washingtonpost.com/pr/wp/2016/08/05/the-washington-post-experiments-with-automated-storytelling-to-help-power-2016-rio-olympics-coverage/?utm_term=.bf63b03c4aeb [ii] \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dayan, Zohar (2015). Hearst, USA Today Sports, & Viralnova Partner With Wibbitz For Video Strategy. Wibbitz Blog. Retrieved from http://blog.wibbitz.com/wibbitz-partners-hearst-usa-today-sports-group-and-viralnova-to-expand-video-strategy [iii] \u00a0\u00a0\u00a0\u00a0\u00a0 Keohane, Joe (2017). What News-Writing Bots Mean for the Future of Journalism. Wired. Retrieved from https://www.wired.com/2017/02/robots-wrote-this-story/ [iv] \u00a0\u00a0\u00a0\u00a0 Narrative Science. Turn Your Data Into Better Decisions With Quill. Retrieved from https://narrativescience.com/Platform info.in@byteacademy.co Byte Dev Licensed by the State of New York, New York State Education Department \u00a9 2017 Byte Academy LLC.  All rights reserved.", "links": ["http://byteacademy.co", "http://byteacademy.co/program/", "http://byteacademy.co/courses/", "http://byteacademy.co/data-science-mini-courses/", "http://byteacademy.co/upcoming-classes/", "http://byteacademy.co/students/", "http://byteacademy.co/hiring/", "http://byteacademy.co/events/", "http://byteacademy.co/blog/", "http://byteacademy.co/india/", "https://byteacademy.fluidreview.com", "http://feeds.feedburner.com/ByteAcademy", "http://byteacademy.co/category/data-science/", "http://byteacademy.co/category/career/", "http://byteacademy.co/category/fintech/", "http://byteacademy.co/category/medtech/", "http://byteacademy.co/category/programming-tips/", "http://byteacademy.co/category/startup/", "http://byteacademy.co/category/student-stories/", "http://byteacademy.co/category/blockchain/", "http://byteacademy.co/category/current-events/", "http://byteacademy.co/category/diversity/", "http://byteacademy.co/category/events/", "http://byteacademy.co/category/finovate/", "http://byteacademy.co/category/light-reads/", "http://byteacademy.co/category/millennials/", "http://byteacademy.co/category/quant/", "http://byteacademy.co/2017/", "http://byteacademy.co/2016/", "http://byteacademy.co/2015/", "http://byteacademy.co/category/data-science/", "http://byteacademy.co/blog/artificial-intelligence-future", "https://www.washingtonpost.com/pr/wp/2016/08/05/the-washington-post-experiments-with-automated-storytelling-to-help-power-2016-rio-olympics-coverage/?utm_term=.bf63b03c4aeb", "http://www.reddit.com/submit?url=http://byteacademy.co/blog/overview-NLG", "http://www.tumblr.com/share?v=3&u=http://byteacademy.co/blog/overview-NLG&t=Overview of Natural Language Generation (NLG)", "http://byteacademy.co/blog/artificial-intelligence-future", "http://byteacademy.co/blog/artificial-intelligence-future", "http://byteacademy.co/blog/data-science-podcasts", "http://byteacademy.co/blog/data-science-podcasts", "http://byteacademy.co/blog/data-science-fintech/", "http://byteacademy.co/blog/data-science-fintech/", "http://byteacademy.co/financial-aid/", "http://byteacademy.co/upcoming-classes/", "http://byteacademy.co/contact-us/", "http://byteacademy.co/corporate-training/", "http://byteacademy.co/faq/", "http://byteacademy.co/press-page/", "http://byteacademy.co/careers-at-byte/", "http://byteacademy.co/brand-ambassador/", "https://www.meetup.com/Byte-Academy-Finance-and-Technology-community/", "http://www.bytedev.co", "https://www.facebook.com/byteacademy/", "https://twitter.com/ByteAcademyCo", "https://www.linkedin.com/edu/school?id=171001", "https://github.com/ByteAcademyCo", "https://www.instagram.com/byteacademy/", "https://www.meetup.com/Byte-Academy-Finance-and-Technology-community/", "https://www.quora.com/topic/Byte-Academy", "https://www.youtube.com/channel/UCrMcJALnO748TSK27bZQzSg", "http://byteacademy.co/privacy-policy/"]}, "134": {"url": "http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "title": "", "text": "", "links": ["http://www.getrevue.co/?ref=Revue+Profile", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profileimage&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profiledate&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profilename&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Mm6Kn?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Mm6Kn?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.datasciencecentral.com/profiles/blogs/automated-machine-learning-for-professionals?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lXBy8?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lXBy8?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.nature.com/articles/s41467-017-00181-8?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/d0nAm?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/d0nAm?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/4mO6D?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/4mO6D?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/eK78N?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/eK78N?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://sourcedexter.com/amazing-tensorflow-github-projects/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xejEQ?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xejEQ?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://techblog.appnexus.com/a-keras-multithreaded-dataframe-generator-for-millions-of-image-files-84d3027f6f43?gi=79cece23ee37&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/6nA72?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/6nA72?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://blog.revolutionanalytics.com/2017/08/a-modern-database-interface-for-r.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/5kyK3?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/5kyK3?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://ruder.io/deep-learning-nlp-best-practices/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/7yxNV?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/7yxNV?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://hyperparameter.space/blog/when-not-to-use-deep-learning/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/8A1NP?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/8A1NP?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278?gi=8e0386afa61e&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/1bKwn?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/1bKwn?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://adeshpande3.github.io/How-I-Used-Deep-Learning-to-Train-a-Chatbot-to-Talk-Like-Me?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lX0x4?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lX0x4?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.youtube.com/channel/UC0n76gicaarsN_Y9YShWwhw/playlists?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/qYVyK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/qYVyK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.youtube.com/watch?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest&v=0fhUJT21-bs", "http://rev.vu/Nkeov?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Nkeov?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%239%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "https://www.getrevue.co/?utm_source=Data Science Digest&utm_medium=email&utm_content=footerlink&utm_campaign=Issue", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=forwarded&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%239%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "http://www.getrevue.co/?ref=Revue+Profile"]}, "135": {"url": "http://rpsychologist.com/d3/bayes/", "title": "", "text": "Created by Kristoffer Magnusson The visualization shows a Bayesian two-sample t test, for simplicity the variance is assumed to be known. It illustrates both Bayesian estimation via the posterior distribution for the effect, and Bayesian hypothesis testing via Bayes factor. The frequentist p-value is also shown. The null hypothesis, H0 is that the effect \u03b4 = 0, and the alternative H1: \u03b4 \u2260 0, just like a two-tailed t test. You can use the sliders to vary the observed effect (Cohen's d), sample size (n per group) and the prior on \u03b4. The prior on the effect is a scaled unit-information prior. The black, and red circle on the curves represents the likelihood of 0 under the prior and posterior. Their likelihood ratio is the Savage-Dickey density ratio, which I use here as to compute Bayes factor. The p-value is the traditional p-value for a two-sample t test with known variance (i.e. a Z test).        HDI is the posterior highest density interval, which in this case is analogous a credible interval. And CI is the traditional frequentist confidence interval.  Check out Alexander Etz's blog series \"Understanding Bayes\" for a really good introduction to Bayes factor. Fabian Dablander also wrote a really good post, \"Bayesian statistics: why and how\", which introduces Bayesian inference in general. If you're interesting in an easy way to perform a Bayesian t test check out JASP, or BayesFactor if you use R. Interactive visualization of Cohen's d effect size Interactive visualization of statistical power and significance testing Interactive visualization of Confidence Intervals Have any suggestion? Or found any bugs? Send them to me, my contact info can be found here. Designed and built by Kristoffer Magnusson. Built with D3.js, jStat and Bootstrap.", "links": ["http://rpsychologist.com", "http://rpsychologist.com", "https://twitter.com/krstoffr", "https://se.linkedin.com/pub/kristoffer-magnusson/b5/133/1b6", "https://twitter.com/share", "http://alexanderetz.com/understanding-bayes/", "http://blog.efpsa.org/2015/08/03/bayesian-statistics-why-and-how/", "https://jasp-stats.org", "http://bayesfactorpcl.r-forge.r-project.org", "http://rpsychologist.com/d3/cohend/", "http://rpsychologist.com/d3/NHST/", "http://rpsychologist.com/d3/CI/", "http://rpsychologist.com/tag/d3js.html", "http://rpsychologist.com/about", "http://rpsychologist.com", "http://d3js.org/", "http://jstat.org/", "http://getbootstrap.com/", "http://twitter.com/krstoffr", "https://github.com/rpsychologist"]}, "136": {"url": "https://blog.datasyndrome.com/generalists-dominate-data-science-f01882f25347", "title": "Generalists Dominate Data\u00a0Science", "text": "Analytics products and systems are best built by small teams of generalists. Large teams of specialists become dominated by communication overhead, and the effect of \u201cChinese whispers\u201d distorts the flow of tasks and stagnates creativity. Data scientists should develop generalist skills to become more efficient members of a data science team. Building data products takes a team covering a broad and diverse skillset. From the customer representative at one end, to the operations engineer at the other, the spectrum of roles in a product analytics team looks like this: Large companies often fill each role with a pair of shoes, resulting in a twelve person team like the one below. The problem with this setup is that it becomes much more difficult to achieve consensus and to perform any task that spans roles. And in data science, most tasks span roles. To look at a particular example, lets focus on the creation of a chart as part of a data product. To begin, a product manager creates a specification, then an interaction designer mocks up the chart, handing it off to a data scientist to fill with data (and hopefully to explore the data and find a chart worth producing), then a back-end engineer to setup an API to grab that data, a front-end web developer to create a web page using the data that matches the mock, and an experience designer to ensure the entire thing feels right and makes sense. Charts take iteration, so this cycle of communication could happen repeatedly for each chart. You can see how communication overhead starts to predominate. A meeting of six people is a full-blown, formal meeting. It is hard to get things done in formal meetings. In the next figure, we see how a data product team might be composed of four generalists: a data engineer, a data scientist/back-end developer, a designer who can build front ends and a product manager that can write marketing copy and cut deals. This is how a startup team would span the skill spectrum, and you can probably see how this makes them more efficient. Revisiting the chart example, creating a chart becomes a collaboration between the product manager, a designer who codes, and a data scientist. This is the kind of ad hoc meeting of 2\u20133 people where \u201cshit gets done\u201d efficiently. This group will be more efficient than the six man group. Put another way: this small team will kick the large team\u2019s ass. In the big company system, sometimes the only way to get anything done efficiently is to go \u201cguerilla generalist\u201d and work with other generalists to cut people out of the chain. This is bad politically, and is part of what drives effective people from big companies. We\u2019ve shown that small teams generalists outperform large teams of specialists. In fact, generalist skills are something every data scientists should work to develop. That doesn\u2019t mean you can\u2019t specialize, but should combine specialization with generalization in order to develop \u201cT-shaped skills.\u201d The T-shaped employee is one that can lend deep expertise across projects while fulfilling multiple roles in his own. It takes time to develop general skills, and that is why the path to becoming a data scientist is not a six month bootcamp, but a ten year journey. Along this path, remember to try to be T-Shaped! Need help building an analytics product or platform? The Data Syndrome team of data scientists and data engineers is available to build your data products and systems as a service. We also offer training in Agile Data Science for all members of data science teams.", "links": ["https://blog.datasyndrome.com?source=logo-lo_ce82027d9b6d---500653fb51a1", "https://twitter.com/datasyndrome", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.datasyndrome.com%2Fgeneralists-dominate-data-science-f01882f25347", "https://blog.datasyndrome.com/@rjurney?source=post_header_lockup", "https://blog.datasyndrome.com/@rjurney?source=post_header_lockup", "https://personalmba.com/communication-overhead/", "https://en.wikipedia.org/wiki/Chinese_whispers", "https://en.wikipedia.org/wiki/T-shaped_skills", "http://datasyndrome.com", "http://datasyndrome.com/training", "https://blog.datasyndrome.com/tagged/data-science?source=post", "https://blog.datasyndrome.com/tagged/agile?source=post", "https://blog.datasyndrome.com/tagged/generalists?source=post", "https://blog.datasyndrome.com/tagged/analytics?source=post", "https://blog.datasyndrome.com/tagged/software-engineering?source=post", "https://blog.datasyndrome.com/@rjurney?source=footer_card", "https://blog.datasyndrome.com/@rjurney", "https://blog.datasyndrome.com?source=footer_card", "https://blog.datasyndrome.com?source=footer_card", "https://blog.datasyndrome.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "137": {"url": "https://github.com/axibase/atsd-use-cases/tree/master/FED_FORDSR#the-average-american-debt-profile", "title": "", "text": "Debt is a complicated concept. After the sub-prime mortgage crisis of the late 2000s, modern Americans are all too familiar with the problems of irresponsible spending on credit. Student loan recipients who queue up to drop off another application for a job in a field they did not study are quick to point to the trappings of deficit spending as a means of wealth creation. Politicians and voters on both sides of the aisle point to the ever-growing United States Government debt with anxiety for the future. And yet despite all the doom and gloom, the American financial system is one of the most stable and robust in the world, in no small part thanks to ingenious monetary policy and hegemonic economic position organized over the entire course of the country's history, modern American consumers are among the wealthiest on the planet. The United States Federal Reserve is the central banking system of the United States, responsible for monitoring the global financial climate and enacting policy that supports the American economy and American consumers. They maintain a number of statistics about these consumers and their monetary practices to better inform their decisions and practices. Provided by the Federal Reserve, this dataset must be correctly parsed during import. The quarterly date format needs to be converted into a monthly format that ATSD can interprete (Q/q letter is not supported). We also need to discard metadata lines contained in the multi-line header. This can be accompilshed with a schema-based parser that provides granular control over the document's rows and columns using RFC 7111 selectors and Javascript: Script 1.1 For step-by-step instructions on data customization with schema based parsing, see this support tutorial. The Financial Obligation Ratio (FOR) is an estimate of the ratio of required debt payments to disposable income. This is a broad calculation and includes all kinds of debt:  mortgage payments, credit cards, property tax and lease payments. Each of these metrics can be expanded further to include associated costs, such as homeowner's insurance for example. The Federal Reserve releases this number each quarter. Figure 1.1   Use the dropdown menus at the top of the visualization screen to navigate through time, selecting the starttime and endtime values to observe a desired period. The data can also be queried with a structured query language in the SQL Console. The data will be aggregated annually, derived from the average value of each quarter within a given year: Query 1.1 Table 1.1 All values are shown as a percent of one hundred, where the whole is representative of the total income of the average person. The Debt Service Ratio (DSR) is more specific than the Financial Obligation Ratio in that it typically does not include non-essential debt payments. Here, it has been parsed into two categories, mortgage debt and consumer debt. These numbers represent the average percent of a person's earned salary each month which much be used to make the required payments associated with consumer credit and mortgage. Typically the DSR is an initial calculation performed to determine a person's eligibility to receive a mortgage. A DSR value of less than 48% is generally preferred, meaning that with a particular mortgage plus other credit obligations at least 52% of a person's gross monthly earning would still be available to them after making the required payments. Figure 2.1   Query 2.1 Table 2.1 Because the FOR value includes the DSR value plus additional non-essential credit values, and the DSR value is parsed into both consumer and mortgage related debt, these three values can be shown in a new visualization that creates a typical consumer profile of the average American. By using the calculated value setting shown below, additional data not specifically included in the set can be displayed: Script 2.1 Shown below is the debt profile of the average American consumer from 1980 to 2017, navigate through time using the dropdown menus at the top of the screen to select a desired span of time and compare how bearing debt has changed over the course of the last three decades. Figure 3.1   The visualization can also be organized to show the amount of each type of debt as it relates to the others: Figure 3.2   Additionally, these values can be compared on an annual basis as shown in the visualization below: Figure 3.3   To view the distribution of these values across time, a histogram is shown below: Figure 3.4   In the following box diagram, explore time with the dropdown menus at the top of the visualization screen. The visualization shows the distribution of debt values as a percentage of total income, with the initial time period set to include the entire data set: Figure 3.5   The following SQL query will detail the above visualizations in one table, displaying averaged annual values of each component described above: non-essential credit payments, mortgage credit payments, and consumer credit payments, as well as the Financial Obligation Ratio (FOR), or total debt obligations. Query 3.1 Table 3.1 The above dataset can illuminate a number of features of the American economy and a number of characteristics of the average American consumer. While modern Americans are quick to denounce the zeitgeist of living outside of one's means, the data shows that in fact, the amount of debt carried by the average American is on par with or even lower in some cases than that of his 1980's counterpart. In fact, the only metric which has demonstrated a legitimate increase in value over the last several decades has been the roughly one percent increase in non-essential credit holdings by the average consumer. According to data from the Economic Research Department of the Saint Louis Branch of the Federal Reserve, the 2015 US median household income was $56,516 per year in 2015 USD. This number can be applied to the above table and visualized in ChartLab to create more comprehensive data. Figure 3.6   The above visualization aggregates the values from Table 3.1 based on a time period of the user's selection. Use the dropdown menu at the top of the screen to select the aggregation period. The initial visualization shows the average values for each metric over the entire period of time in 2015 USD by obligation amount per quarter. The following query summons the same data shown above, but further parses it to show annual average monthly payments instead of quarterly values in 2015 USD for a person making the 2015 median United States income of $56,516 a year. Query 4.3 Table 3.2 As it turns out, the idea that your parents paid less for their house than you will is only true in absolute terms. When compared with current numbers and controlled for inflation, the average 2017 consumer will pay roughly the same portion of their income towards a place to hang their hat up as the average 1980 consumer. The Federal Reserve is able to pull certain levers of power from the Eccles Building in Washington, D.C. such as printing more money, or raising and lowering interest rates to cope with inflation. However, all of these are reactionary measures meant to create small changes that have a butterfly effect over time. Ultimately, the machinations of the Board of Governers have always be something opaque and esoteric to the average man, leading to many people denouncing the Federal Reserve System entirely, occasionally opting for a return of the gold standard or leveling accusations of wrong-doing. However, after reviewing the data above, it seems that at least on a consumer level, the average American actually has more today than they would have had thirty years ago, or even just five years ago. Of course, the Federal Reserve isn't completely responsible for the wise consumer choices made in the current decades, but monetary policy enacted by the various branches of the Federal Reserve are responsible for maintaining the economic conditions that Americans, and consumers the world over, have come to expect from the United States economy.", "links": ["https://github.com/", "https://github.com/axibase/atsd-use-cases/pull/86", "http://www.usdebtclock.org/", "https://www.federalreserve.gov/", "https://www.federalreserve.gov/datadownload/Download.aspx?rel=FOR&series=91e0f9a6b8e6a4b1ef334ce2eaf22860&filetype=csv&label=include&layout=seriescolumn&from=01/01/1980&to=12/31/2017", "https://axibase.com/products/axibase-time-series-database/writing-data/csv/", "https://apps.axibase.com/chartlab/842f1dd9/#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql", "https://apps.axibase.com/chartlab/85522dd3/#fullscreen", "https://apps.axibase.com/chartlab/f25de723/#fullscreen", "https://apps.axibase.com/chartlab/f25de723/3/#fullscreen", "https://apps.axibase.com/chartlab/81ea0ea0/#fullscreen", "https://apps.axibase.com/chartlab/9f74c179/#fullscreen", "https://apps.axibase.com/chartlab/20ff0ade/#fullscreen", "https://fred.stlouisfed.org/series/MEHOINUSA646N", "https://research.stlouisfed.org/", "https://www.stlouisfed.org/", "https://apps.axibase.com", "https://apps.axibase.com/chartlab/da132e01/11/#fullscreen", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "138": {"url": "http://tech.marksblogg.com/billion-nyc-taxi-rides-aws-ec2-p2-16xlarge-brytlytdb.html", "title": "", "text": "I have 14 years of hands-on build and consulting experience with clients in the UK, Ireland & Germany. I've done both back- and frontend work for Bank of America Merrill Lynch, Blackberry, Bloomberg, British Telecom, Ford, Google, ITV, News UK, Pizza Hut, Royal Bank of Scotland, Royal Mail, T-Mobile, Vertu (Nokia subsidiary at the time), Williams Formula 1 Team and Xerox. I hold both a Canadian and a British passport. My CV & my LinkedIn profile.         Home           | Benchmarks           | Archives            | Atom Feed  Posted on Fri 28 July 2017 BrytlytDB is an in-GPU-memory database built on top of PostgreSQL. It's operated using many of PostgreSQL's command line utilities, it's wire protocol compatible so third-party PostgreSQL clients can connect to BrytlytDB and queries are even parsed, planned and optimised by PostgreSQL's regular codebase before the execution plan is passed off to GPU-optimised portions of code BrytlytDB offer. Clustering works right out of the box, GPU-powered JOINs are supported, Stored Procedures are fully functional, Deep- and Machine Learning workloads via Torch are supported and BI visualisation software in the form of SpotLyt is included with BrytlytDB as well. The feature list is too long for one blog to cover so in this post I'll just be taking a look at how performant BrytlytDB's OLAP functionality is. In this benchmark I'll see how well 32 Tesla K80 GPUs spread across two EC2 instances perform when querying 1.1 billion taxi trips. I'll be using the same dataset I've used to benchmark Amazon Athena, BigQuery, ClickHouse, Elasticsearch, EMR, kdb+/q, MapD, PostgreSQL, Redshift and Vertica. I have a single-page summary of all these benchmarks for comparison. For this benchmark I'll be using two p2.16xlarge EC2 instances running Ubuntu 16.04.2 LTS in Amazon Web Services' eu-west-1a region in Ireland. Each machine has 8 Nvidia K80  cards which have 2 GPUs each, 64 virtual CPUs and 732 GB of memory. There's also 20 Gbit/s of networking capacity available to each instance. Below are the specifications of the compute capabilities available each one of the EC2 instances. This is the layout of the GPUs available on one of the two EC2 instances. The first machine has an IP address of 52.214.237.134 and has three roles: Global Transaction Manager, Coordinator and host to the first 16 data nodes. The second machine has an IP address of 34.250.232.38 and has two roles: it is also a coordinator and hosts the second set of 16 data nodes. TCP port 5432 is open between the two machines for communicating via PostgreSQL's wire protocol, TCP port 7777 is open for global transaction manager communication and ports 20,000 through to 20,0031 are open for data node communication. In addition to the default 20 GB EBS volumes on each EC2 instance there are six 500 GB General Purpose SSDs attached which offer a baseline of 100 IOPS each and can burst to 3,000 IOPS if need be. These drives are setup in a RAID 0 configuration on each instance. Before setting up the RAID array I'll install a few dependencies. This was run on both EC2 instances. Below are the commands used to setup the RAID array on each instance. Here's what the RAID layout looked like after it was setup. This RAID 0 setup offers a partition with a capacity of 3.2 TB on each instance: I've run the following to download BrytlytDB's install script. BrytlytDB is commercial software so I cannot divulge the URL I pulled this from at this time. The following was run on both EC2 instances. I've then edited the install script on the first instance with the following instance-specific values: And the install script on the second instance was edited with the following instance-specific values: With those changes in place I then ran the install script on both EC2 instances. The install script conducts the following: Once that's done I can form the cluster with the following registration script. The following was run on the first EC2 instance. The second registration script run on the other EC2 instance is identical with the first with the exception of the coordinator line pointing to 52.214.237.134 instead. For this benchmark I've downloaded and decompressed one half the 500 GB of CSV data I created in my Billion Taxi Rides in Redshift blog post onto each EC2 instance. The data sits across 56 files across both machines but for the quickest load time I want to create 32 reasonably equally-sized CSV files and load 16 files on each EC2 instance simultaneously. The 32 files will pair up with the 32 GPUs available across the cluster and should allow for the best utilisation of the GPU resources when running queries. Here is half of the original 500 GB data set on the first EC2 instance: And this is the other half on the second instance: On each instance I'll decompress the GZIP files. Below took about four minutes to complete on each instance. Then I'll concatenate the ~560 million lines of CSV data on each instance and break it up into files of 35 million lines each. The above completed in 8 minutes on each EC2 instance. I'm now left with 16 CSV files on the first EC2 instance: And 16 files on the other EC2 instance: I'll then connect to the coordinator node on each EC2 instance and setup BrytlytDB's gpu_manager_fdw extension and foreign data wrapper. I can then create the trips table. The above sets the maximum row count per GPU device at 38 million and creates an index on the cab_type column. The cab_type column has very low cardinality so the indexing will be less beneficial versus a column with a higher cardinality of values. Some of the data types used in the above table aren't the types I normally use in my benchmarks. The reason for this is that there is still limited data type support available in this early version of BrytlytDB. I've had to substitute DATETIME with DATE which will truncate the timestamps in the dataset to just the date rather than the full date and time. There is no SMALLINT support yet so I've had to use the larger INT type as an replacement for those fields. DECIMAL types aren't yet supported so I'll be using DOUBLE PRECISION as a replacement there. The above replacements could speed up or slow down the queries I benchmark with so I'm hoping to do another benchmark when data type support is widened in the future. With the table created I'll launch 32 simultaneous load jobs across the two EC2 instances to load the data into BrytlytDB's trips table. This is the import script I ran on the first EC2 instance: And this is the load script for the second instance. The first EC2 instance loaded its half of the dataset in 1 hour, 12 minutes and 21 seconds. The second instance loaded its half in 1 hour, 9 minutes and 57 seconds. After the data was loaded in I could see the PostgreSQL data directory filled up with reasonably evenly-sized data folders. Here's what they look like on the first EC2 instance: I was then able to use PostgreSQL's CLI tool to make sure I can see the table and all 1.1 billion records. The times quoted below are the lowest query times seen during a series of runs. As with all my benchmarks, I use the lowest query time as a way of indicating \"top speed\". The following completed in 0.762 seconds. The following completed in 2.472 seconds. The following completed in 4.131 seconds. The following completed in 6.041 seconds. Given all the features of PostgreSQL are still available I'm blown away at how fast BrytlytDB is able to aggregate data. The cluster I used cost around $30 / hour which means this system out performed other Cloud-based and PostgreSQL-based data warehousing solutions both in terms of wall clock time and in terms of cost per hour of running the cluster. That being said I know Nvidia's K80 chips use extremely fast memory and have thousands of compute cores so I expect further optimisations from BrytlytDB to drive down these already fast query times. \u2190 Back to Index Copyright \u00a9 2014 - 2017 Mark Litwintschik. This site's template is based off a template by Giulio Fidente.", "links": ["https://twitter.com/marklit82", "http://tech.marksblogg.com/theme/cv.pdf", "https://uk.linkedin.com/in/marklitwintschik/", "http://tech.marksblogg.com/feeds/all.atom.xml", "http://www.brytlyt.com/", "https://uk.linkedin.com/in/marklitwintschik/", "https://github.com/giulivo/pelican-svbhack"]}, "139": {"url": "https://blog.datazar.com/why-we-focus-on-design-just-as-much-as-functionality-f6ccb3b2c926", "title": "Why We Focus on Design Just as Much as Functionality", "text": "It\u2019s no secret that scientific software isn\u2019t the most beautiful software in terms of design/aesthetics (generally speaking here). It is very effective though; it gets the job done. It does exactly what you expect it to do. Being a huge movie buff, I\u2019ve always wondered why the software we use in the laboratories or classrooms looks nothing like what we see in the movies. Movies do this thing where tech looks exciting and futuristic and super fun to use. And then I go back to reality and it\u2019s nothing like what I just watched. It\u2019s worth mentioning that not everything in the movies looks practical, especially when it comes to software or even more ridiculous: computer hacking. But that\u2019s another subject. The fact there\u2019s some art missing is something I noticed not only in the software we use in the lab, but also the instruments we build and the papers we write. It seems as if the art has been completely taken out of the sciences. Before you say it, we obviously can\u2019t incorporate art into somethings like highly sensitive instruments as unnecessary additions area big no because then you\u2019re interfering. Another thing to note might be the fact that hundreds or even some thousands years ago, the scientists were also artists. Art came naturally to them as they were also discovering mother nature. At that time, science was also only available to the rich which meant a couple of things: they could afford to spend a little bit of time incorporating art into their scientific work as they weren\u2019t rushing to meet grant deadlines, but it also meant they were already educated in the arts from an early age. These days anyone can be whatever they want to be. To the very least, it\u2019s easier now than it was in that era by an order of magnitude. Today we\u2019re taught that art almost has no place in the sciences and we must be efficient and to the point. Anything extra is generally referred to as fluff. If you like to romanticize science, you\u2019re not really a scientist. But I strongly believe that it\u2019s the artistic and I might add\u200a\u2014\u200aphilosophical\u200a\u2014\u200afluff that gets people interested in science. The hard math and core scientific values obviously being there regardless. All of this results in the absence of art in the sciences which makes art seem dry and boring. Science in its purest form is absolutely beautiful if communicated right. That\u2019s why we love to do our part at Datazar. We\u2019re far, far from perfect or even close to what we want to be. But putting just as much thought in design and UX as we put in functionality is a core principle. There\u2019s a misconception that incorporating art means adding more stuff. Incorporating art can also mean doing less, using less and making the most of what you already have in a beautiful and simple way. An example would be removing that obscene amount of jargon from your paper. So I ask everyone designing software for scientists, design for humans not robots. Scientists are humans too. So go ahead add that CSS animation.", "links": ["https://blog.datazar.com?source=logo-lo_1b983049c843---e2c7e6e1c75", "https://twitter.com/DatazarHQ", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.datazar.com%2Fwhy-we-focus-on-design-just-as-much-as-functionality-f6ccb3b2c926", "https://blog.datazar.com", "https://blog.datazar.com/tagged/r-language", "https://blog.datazar.com/tagged/open-data", "https://blog.datazar.com/tagged/how-to", "https://www.r-bloggers.com/", "https://blog.datazar.com/search", "https://blog.datazar.com/@ata_aman?source=post_header_lockup", "https://blog.datazar.com/@ata_aman?source=post_header_lockup", "https://blog.datazar.com/tagged/science?source=post", "https://blog.datazar.com/tagged/art?source=post", "https://blog.datazar.com/tagged/research?source=post", "https://blog.datazar.com/tagged/software-development?source=post", "https://blog.datazar.com/tagged/design?source=post", "https://blog.datazar.com/@ata_aman?source=footer_card", "https://blog.datazar.com/@ata_aman", "http://twitter.com/datazarhq", "https://blog.datazar.com?source=footer_card", "https://blog.datazar.com?source=footer_card", "https://blog.datazar.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "140": {"url": "https://news.ycombinator.com/item?id=14950255", "title": "", "text": "", "links": ["https://news.ycombinator.com", "https://edgylabs.com/machine-learning-to-enhance-smartphone-pictures/", "https://hn.algolia.com/?query=Machine%20Learning%20to%20Enhance%20Smartphone%20Pictures&sort=byDate&dateRange=all&type=story&storyText=false&prefix&page=0", "https://www.google.com/search?q=Machine%20Learning%20to%20Enhance%20Smartphone%20Pictures", "https://github.com/HackerNews/API", "http://www.ycombinator.com/apply/"]}, "141": {"url": "http://starmine.ai/datasets/ds02.html", "title": "", "text": "reddit Slack Twitter Instagram Facebook LinkedIn", "links": ["http://starmine.ai", "http://reddit.com/r/datasets", "https://join.slack.com/t/starmineai/shared_invite/MjIwNjg5OTUyODAzLTE1MDE2MTk1MzQtMmJhYTExNDViMA", "https://twitter.com/starmineAI", "https://www.instagram.com/starmine.ai/", "https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Fstarmine.ai/index.html&t=starmine%20ICO", "https://www.linkedin.com/cws/share?url=http%3A%2F%2Fstarmine.ai/index.html&token=&isFramed=true", "http://research.kraeutli.com/index.php/2013/11/the-tate-collection-on-github", "http://54.174.116.134/recommend/datasets/subscribers/free/supercolumns/supercolumns-elements-CMDB-nasdaq-nyse-otcbb-general-2017/supercolumns-elements-nasdaq-nyse-otcbb-general-UPDATE-2017-03-01.csv", "http://54.174.116.134/recommend/datasets/subscribers/free/supercolumns/supercolumns-elements-CMDB-nasdaq-nyse-otcbb-general-2017/supercolumns-elements-nasdaq-nyse-otcbb-general-UPDATE-2017-03-01.csv", "http://54.174.116.134/recommend/app/ai_connect-finance-historicaltrends_api?ccl1=playstation&ccl2=helium&ccl3=korea&ccl4=shampoo&ccl5=coffee&query=concept_column_labels&themesource=theme_CMDB-nasdaq-nyse-otcbb&month=01&year=all&db=CMDB-nasdaq-nyse-otcbb-general", "https://www0.gsb.columbia.edu/mygsb/faculty/research/pubfiles/1555/contagious.pdf", "https://trends.google.com/trends/explore?q=playstation,helium,korea,shampoo,coffee", "https://trends.google.com/trends/explore?q=playstation,helium,korea,shampoo,coffee"]}, "142": {"url": "https://github.com/maciejkula/spotlight/blob/master/examples/movielens_sequence/readme.rst", "title": "", "text": "Using sequences of user-item interactions as an input for recommender models has a number of attractive properties. Firstly, it recognizes that recommending the next item that a user may want to buy or see is precisely the goal we are trying to achieve. Secondly, it's plausible that the ordering of users' interactions carries additional information over and above just the identities of items they have interacted with. For example, a user is more likely to watch the next episode of a given TV series if they've just finished the previous episode. Finally, when the sequence of past interactions rather than the identity of the user is the input to a model, online systems can incorporate new users (and old users' new actions) in real time. They are fed to the existing model, and do not require a new model to be fit to incorporate new information (unlike factorization models). Recurrent neural networks are the most natural way of modelling such sequence problems. In recommendations, gated recurrent units (GRUs) have been used with success in the Session-based recommendations with recurrent neural networks paper. Spotlight implements a similar model using LSTM units as one of its sequence representations. But recurrent neural networks are not the only way of effectively representing sequences: convolutions can also do the job. In particular, we can use causal convolutions: convolutional filters applied to the sequence in a left-to-right fashion, emitting a representation at each step. They are causal in that the their output at time t is conditional on input up to t-1: this is necessary to ensure that they do not have access to the elements of the sequence we are trying to predict. Like LSTMs, causal convolutions can model sequences with long-term dependencies. This is achieved in two ways: stacking convolutional layers (with padding, every convolutional layer preserves the shape of the input), and dilation: insertion of gaps into the convolutional filters (otherwise known as atrous convolutions). Causal convolutions have been used in several recent high-profile papers: Using convolutional rather than recurrent networks for representing sequences has a couple of advantages, as described in this blog post: Spotlight implements causal convolution models as part of its sequence models package, alongside more traditional recurrent and pooling models. The Spotlight implementation has the following characteristics: The model is trained using one of Spotlight's implicit feedback losses, including pointwise (logistic and hinge) and pairwise (BPR as well as WARP-like adaptive hinge) losses. As with other Spotlight sequence models, the loss is computed for all the time steps of the sequence in one pass: for all timesteps t in the sequence, a prediction using elements up to t-1 is made, and the loss is averaged along both the time and the minibatch axis. This leads to siginficant training speed-ups relative to only computing the loss for the last element in the sequence. To see how causal CNNs compare to more traditional sequence models we can have a look at how they perform at predicting the next rated movie on the Movielens 1M dataset. With 1 million interactions spread among 6000 users and around 4000 movies it should be small enough to run quick experiments, but large enough to yield meaningful results. I chose to split the dataset into 80% train, and 10% test and validation sets. I construct 200-long sequences by splitting each user's item sequence into 200-long chunks; if a chunk is shorter than 200 elements, it's padded with zeros. I use mean reciprocal rank (MRR) as the evaluation metric. To choose hyperparameters, I run a quick, coarse grained hyperparameter search, using random sampling to draw 100 hyperparameter sets. With the data and hyperparameters ready, fitting and evaluating the model is relatively simple: Fitting the models is fairly quick, taking at most two or three minutes on a single K80 GPU. The code for the experiments is available in the experiments folder of the Spotlight repo. The results are as follows: It's difficult to draw clear-cut conclusions about the effect of each hyperparameter, but it looks like: To compare causal convolutions with more traditional sequence models I run similar hyperparameter searches for LSTM-based representations and pooling representations. The pooling representation is a simple averaging of item embedding across the sequence; the LSTM-based model runs an LSTM along a user's interactions, using the hidden state for prediction of the next element at each step. The results are as follows: A single layer LSTM seems to outperform causal convolutions, by an over 10% margin, helped by the adaptive hinge loss. Simple pooling performs quite badly. It looks like causal convolutions need some more work before beating recurrent networks. There are a couple of possible avenues for making them better: I'd love to get some input on these. If you have suggestions, let me know on Twitter or open an issue or PR in Spotlight.", "links": ["https://github.com/", "https://arxiv.org/abs/1511.06939", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.LSTMNet", "https://travis-ci.org/maciejkula/spotlight", "https://arxiv.org/pdf/1609.03499.pdf", "http://papers.nips.cc/paper/6527-conditional-image-generation-with-pixelcnn-decoders", "https://arxiv.org/abs/1610.10099", "https://medium.com/@TalPerry/convolutional-methods-for-text-d5260fd5675f", "https://arxiv.org/pdf/1610.10099.pdf", "https://maciejkula.github.io/spotlight/sequence/sequence.html", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.CNNNet", "https://maciejkula.github.io/spotlight/losses.html", "https://grouplens.org/datasets/movielens/1m/", "https://en.wikipedia.org/wiki/Mean_reciprocal_rank", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.LSTMNet", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.PoolNet", "https://maciejkula.github.io/spotlight/losses.html#spotlight.losses.adaptive_hinge_loss", "https://twitter.com/Maciej_Kula", "https://github.com/maciejkula/spotlight", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "143": {"url": "https://github.com/ynqa/word-embedding", "title": "", "text": "GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together.  Sign up             Use Git or checkout with SVN using the web URL.             This is an implementation of word embedding (also referred to as word representation) models in Golang. Word embedding makes words' meaning, structure, and concept mapping into vector space (and low dimension). For representative instance: Like this example, it could calculate word meaning by arithmetic operations between vectors. Listed models for word embedding, and checked it already implemented. Downloading text8 corpus, and training by Skip-Gram with negative sampling.", "links": ["https://github.com/", "https://github.com/ynqa/word-embedding/blob/master/LICENSE", "https://help.github.com/articles/which-remote-url-should-i-use", "https://travis-ci.org/ynqa/word-embedding", "https://godoc.org/github.com/ynqa/word-embedding", "https://goreportcard.com/report/github.com/ynqa/word-embedding", "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf", "http://nlp.stanford.edu/pubs/glove.pdf", "https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf", "http://mattmahoney.net/dc/textdata", "https://github.com/cjlin1/libsvm", "http://www.aclweb.org/anthology/Q15-1016", "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.648.8023&rep=rep1&type=pdf", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "144": {"url": "https://github.com/brannondorsey/keras_weight_animator", "title": "", "text": "GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together.  Sign up             Use Git or checkout with SVN using the web URL.          Save Keras weight matrices as short animated videos to better understand what and how your neural network models are learning. Below are examples of the first LSTM layer and the final output layer of a six-class RNN model trained over one epoch. Blue represents low values and red represents high values.   In order to render videos from the saved weight images you must also have the following packages installed on your machine: This module is named keras_weight_animator. It exposes a Keras callback function that you can include in any model fit(...) method. The two required parameters to image_saver_callback(...) are the Keras model and an output_directory to periodically save weight images to. By default, keras_weight_animator saves layer weights every 100 batches to output_directory as PNGs in folders named epoch_XXX-layer_NAME-weights_YY. Once training is complete, you can optionally create short animated video clips from the image sequences saved in output_directory using [bin/create_image_sequence.sh](bin/create_image_sequence.sh) path/to/output_directory. This will use parallel, mogrify, and ffmpeg to create a .mp4 from the image sequences located in each folder of output_directory. Video files will be named like epoch_XXX-layer_NAME-weights_YY.mp4. You can run this script automatically from your training script by passing the render_videos=True parameter to image_saver_callback(...). weight_image_sequences(...) takes a variety of optional keyword arguments. I've included an example usage of the module in examples/wisdm.py. This example uses smartphone accelerometer data from WISDM to classify human activity tasks like walking, standing, sitting, walking upstairs, etc... This example uses a one layer LSTM to classify a set of 60 data points (representing three seconds of data sampled at 20hz) as belonging to one of six classes. It outputs image sequences and videos to data/wisdm. Using a bash script to leverage parallel, ImageMagick, and FFMPEG isn't necessarily the most elegant solution, but its the one I had time for. The goal of this here lil' project was to write a quick tool that allows me to better understand how weights change over mini-batch updates in a variety of neural networks. Perhaps in the future I will come back and clean up some of the inelegancies. If you have interest in contributing or maintaining a cleaner version of this lib, please reach out at brannon@brannondorsey.com. This module is \u00a9 Brannon Dorsey 2017, released under an \u2665 MIT License \u2665. You are free to use, modify, distribute, sell, etc... this software under those terms. Example data is from the WIreless Sensor Datamining (WISDM) Actitracker dataset published by Fordham University: This idea is tangentially inspired by much of the work on Visualizing and Understanding Recurrent Networks by Andrej Karpathy, Justin Johnson, and Fei-Fei Li. GNU Parallel is adamant about citation to the point of excess IMHO, but for what its worth, here is their bibtex:", "links": ["https://github.com/", "https://github.com/brannondorsey/keras_weight_animator/blob/master/LICENSE", "https://help.github.com/articles/which-remote-url-should-i-use", "https://github.com/brannondorsey/keras_weight_animator/pull/1", "http://keras.io", "https://www.gnu.org/software/parallel/", "https://www.imagemagick.org/script/index.php", "https://ffmpeg.org/download.html", "https://keras.io/callbacks/", "http://matplotlib.org/users/colormaps.html", "http://www.cis.fordham.edu/wisdm/dataset.php", "https://arxiv.org/abs/1506.02078", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "145": {"url": "https://axibase.github.io/atsd-use-cases/Expatriation_Q2/", "title": "", "text": "Expatriation is the temporary or permanent relocation of a person by choice or by force from their native country to any other country in the world for work, pleasure, or purpose. History has seen expatriation from nearly every country in the world up to and including the current epoch. Famous American expatriates throughout history have included American founding father Benjamin Franklin, who worked as the first United States Minister to France for almost a decade after appointment by the Continental Congress in 1778. The  sixth President of the United States John Quincy Adams, who served as a U.S. Diplomat in four countries before winning the Presidency as a Democratic-Republican in 1825, most notably opening American diplomatic ties as the first United States Minister to the Russian Empire, a position he loved  so much he declined a subsequent offer to serve as an Associate Justice on the Supreme Court of the United States a few years later. American expatriates have come from all walks of life: authors, such as Ernest Hemingway who lived in China, Spain, Cuba, and was even present on the beach codenamed Omaha in Normandy during the allied landing. Musicians, like Jimi Hendrix who rose to international fame in London after leaving his native Seattle, directors like Stanley Kubrick, singers like Tina Turner, and even basketball players like  Allen Iverson, who played an incomplete season for a Turkish basketball team in 2010 after retiring from the NBA. For those who decide to make the change permanent, there comes a time when the inconvenience of living abroad as a resident alien outweighs the novelty and many decide to renounce their citizenship for one reason or another. Unsurprisingly, this is a long and complicated process, one of the main features of which is settling up with the Internal Revenue Service (IRS) via the eponymous Expatriation Tax. The Federal Register is a publishing outlet for the United States Government where the IRS officially releases quarterly information about American citizens who renounce their passport in lieu of a new one and other long-term residents who have decided to repatriate, view the raw data here and use the Axibase Data Crawler designed specifically for collecting, parsing, organzing and inserting historical expatriation data and new data as it becomes available.  Axibase covered last quarter\u2019s expatriation release here and is using this quarter\u2019s release to follow-up. Every election cycle countless celebrities and netizens issue their promise to leave if their candidate fails to get elected and the growing notoriety surrounding the 2016 Presidental Election has proven to  be no different. This type of social theater is almost as old as the country itself as there have been a number of similarly divisive elections throughout American history: Andrew Jackson\u2019s victory in 1828 was seen as a populace uprising against the corrupt political elite and a return to the authority of the common man, Abraham Lincoln\u2019s second term was viewed as the tightening of federal authority to excessive levels that ultimately led to the American Civil War, Franklin Roosevelt was called a dangerous communist whose infamous court-packing plan favorably increased the number of seats on the Supreme Court to his  advantage and was seen as abhorrent and traitorous by many, and of course who can forget recent elections that have featured scandal after scandal and two modern Presidents who served after losing the popular vote, but winning in the electorate. Figure 1   Query 1 Data queried in the SQL Console. Table 1 To give the issue further context, Figure 1 and Table 1 use the most current Department of Homeland Security (DHS) data  available to display current naturalization numbers in the United States. Naturalization is the sister process to expatriation, because as a person expatriates from one country they must be naturalized in another or end up a stateless person. Naturalization numbers in America have remained fairly constant throughout the last decade because there are a number of federal regulations that control the amount of long-term immigrants that America accepts each year and that quota is met without fail. As noted in the our Q1 article, the sister value and subject of this examination, expatriation numbers, has been steadily growing for several consecutive years. Figure 2.1   Query 2 The above query features robust syntax and calculated values. See the following tutorials  to understand more about SQL Console. Table 2 Figure 2.2   Expatriation has been increasing each year by roughly 30% since 2010, which featured abnormally high expatriation rates, most likely attributable to the economic turndown of the Great Recession which began in the United States as a result of the sub-prime mortgage crisis. American citizenship is often sought after for the economic opportunity that comes along with the passport, as the ability to work and do business in the country is heavily restricted or regulated, and with the value of that investment or  opportunity in question, it is unsurprising that the number of investors in the system, that is, new citizens, would fluctuate. It seems appropriate to call naturalization an investment because of the nature of the process, which is long, complicated, and often quite expensive similar to a long-position that will cost more at purchasing time but promises high returns after reaching maturity. The peak, or more appropriately, valley of the global recession occurred in 2009 when the global GDP contracted causing a decline in the median familty income of about five percent. Figure 3   Query 3 Table 3 Reducing the timespan to look at data over the last 5 years captures the repetition of this trend for this period of time. Of the eighteen quarters included in this query, only six of them, roughly 33% showed negative growth for this statistic. Growing expatriation numbers provide interesting insight into the perception of America both domestically and abroad, as these numbers not only include U.S. citizens who renounce their citizenship, but also long-term resident aliens who have returned home. Almost unbelievably, the IRS does not strip characteristic information about expatriates from their publications meaning that first and last names are included in the data. Using Social Security data concerning the most common first names chosen throughout the last century and United States Census data concerning the most common last names to compare, the following queries and visualizations show  the number of expatriates by common first and common last name throughout the entire observed period, ranked by their frequency in the IRS publication. The goal of these queries is purely demonstrative and lighthearted. There is no intention to invade privacy, defame people, or otherwise cause harm. Figure 4  Query 4 Table 4 All of these names appear on the list of most common American first names, linked here Figure 5  Query 5 Table 5 Only the name Smith appears on the list of most common American last names, linked here Use the Axibase Data Crawler to collect the data used in this artcle and examine it yourself in ChartLab.", "links": ["https://github.com/axibase/atsd-use-cases", "https://www.irs.gov/", "https://www.irs.gov/individuals/international-taxpayers/expatriation-tax", "https://www.federalregister.gov/", "https://www.federalregister.gov/documents/2017/05/10/2017-09475/quarterly-publication-of-individuals-who-have-chosen-to-expatriate-as-required-by-section-6039g", "https://github.com/axibase/atsd-data-crawlers/tree/irs-expatriation-data-crawler", "https://axibase.com", "https://apps.axibase.com/chartlab/654b9945#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql", "https://www.uscis.gov/news/fact-sheets/naturalization-fact-sheet", "http://www.unhcr.org/stateless-people.html", "https://apps.axibase.com/chartlab/7fa5b643#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql#examples", "https://apps.axibase.com/chartlab/7fa5b643/5/#fullscreen", "https://web.stanford.edu/group/recessiontrends/cgi-bin/web/sites/all/themes/barron/pdf/IncomeWealthDebt_fact_sheet.pdf", "https://apps.axibase.com/chartlab/7fa5b643/2/#fullscreen", "https://www.ssa.gov/oact/babynames/decades/century.html", "http://www.census.gov/main/www/cen2000.html", "https://www.ssa.gov/oact/babynames/decades/century.html", "http://www.census.gov/main/www/cen2000.html", "https://github.com/axibase/atsd-data-crawlers/tree/irs-expatriation-data-crawler", "https://apps.axibase.com", "https://github.com/axibase/atsd-use-cases", "https://github.com/axibase", "https://pages.github.com"]}, "146": {"url": "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/", "title": "", "text": "Machine learning is eating the world right now. Everyone and their mother are learning about machine learning models, classification, neural networks, and Andrew Ng. You\u2019ve decided you want to be a part of it, but where to start? In this article we\u2019ll cover some important characteristics of Python and why it\u2019s great for machine learning. We\u2019ll also cover some of the most important libraries it has for ML, and if it piques your interest, some places where you can learn more. Python is a great choice for machine learning for several reasons. First and foremost, it\u2019s a simple language\u00a0on the surface; even if you\u2019re not familiar with Python, getting up to speed is very quick if you\u2019ve ever used any other language with C-like syntax (i.e. every language out there). Second, Python has a great community, which results in good documentation and friendly, comprehensive answers in StackOverflow (fundamental!). Third, also stemming from the great community, there are plenty of useful libraries for Python (both as \u201cbatteries included\u201d and third party), which solve basically any problem that you can have (including machine learning). Yeah and it\u2019s true. Python isn\u2019t the fastest language out there: all those handy abstractions come at a cost. But here\u2019s the trick: libraries can and do offload the expensive calculations to the much more performant (but harder to use) C and C++. For instance, there\u2019s NumPy, which is a library for numerical computation. It\u2019s written in C, and it\u2019s fast. Practically every library out there that involves intensive calculations uses it \u2014 almost all the libraries listed next use it in some form. So if you read NumPy, think fast. Therefore, you can make your scripts run basically as fast as straight up writing them in a lower level language. So there\u2019s really nothing to worry about when it comes to speed. Are you starting out in machine learning? Want something that covers everything from feature engineering to training and testing a model? Look no further than scikit-learn! This fantastic piece of free software provides every tool necessary for machine learning and data mining. It\u2019s the de facto standard library for machine learning in Python, recommended for most of the \u2018old\u2019 ML algorithms. This library does both classification and regression, supporting basically every algorithm out there (support vector machines, random forest, naive bayes, and so on). It\u2019s built in such a way that allows easy switching of algorithms, so experimentation is easy. These \u2018older\u2019 algorithms are surprisingly resilient and work very well in a lot of cases. But that\u2019s not all! Scikit-learn also does dimensionality reduction, clustering, you name it. It\u2019s also blazingly fast since it runs on NumPy and SciPy (meaning that all the heavy number crunching is run on C instead of Python). Check out some examples to see everything this library is capable of, and the tutorials if you want to learn how it works. While not a machine learning library per se, NLTK is a must when working with natural language processing (NLP). It comes with a bundle of datasets and other lexical resources (useful for training models) in addition to libraries for working with text \u2014 for functions such as classification, tokenization, stemming, tagging, parsing and more. The usefulness of having all of this stuff neatly packaged can\u2019t be overstated. So if you are interested in NLP, check out some tutorials! Used widely in research and academia, Theano is the grandfather of all deep learning frameworks. Written in Python, it\u2019s tightly integrated with NumPy. Theano allows you to create neural networks, which are represented as mathematical expressions with multi-dimensional arrays. Theano handles this for you so you don\u2019t have to worry about the actual implementation of the math involved. It supports offloading calculations to the much faster GPU, which is a feature that everyone supports today, but back when they introduced it this wasn\u2019t the case. The library is very mature at this point and supports a very wide range of operations, which is a great plus when it comes to comparing it with other similar libraries. The biggest complaint out there is that the API may be unwieldy for some, making the library hard to use for beginners. However, there are wrappers that ease the pain and make working with Theano simple, such as Keras, Blocks and Lasagne. Interested in learning about Theano? Check out this Jupyter Notebook tutorial. The Google Brain team created TensorFlow for internal use in machine learning applications, and open sourced it in late 2015. They wanted something that could replace their older, closed source machine learning framework, DistBelief, which they said wasn\u2019t flexible enough and too tightly coupled to their infrastructure to be shared with other researchers around the world. And so TensorFlow was created. Learning from the mistakes of the past, many consider this library to be an improvement over Theano, claiming more flexibility and a more intuitive API. Not only can it be used for research but also for production environments, supporting huge clusters of GPUs for training. While it doesn\u2019t support as wide a range of operations as Theano, it has better computational graph visualizations. TensorFlow is very popular nowadays. In fact, if you\u2019ve heard about a single library on this list, it\u2019s probably this one: there isn\u2019t a day that goes by without a new blog post or paper mentioning TensorFlow gets published. This popularity translates into a lot of new users and a lot of tutorials, making it very welcoming to beginners. Keras is a fantastic library that provides a high-level API for neural networks and is capable of running on top of either Theano or TensorFlow. It makes harnessing the full power of these complex pieces of software much easier than using them directly. It\u2019s very user-friendly, putting user experience as a top priority. They manage this by using simple APIs and excellent feedback on errors. It\u2019s also modular, meaning that different models (neural layers, cost functions, and so on) can be plugged together with little restrictions. This also makes it very easy to extend, since it\u2019s simple to add new modules and connect them with the existing ones. Some people have called Keras so good that it is effectively cheating in machine learning. So if you\u2019re starting out with deep learning, go through the examples and documentation to get a feel for what you can do with it. And if you want to learn, start out with this tutorial and see where you can go from there. Two similar alternatives are Lasagne and Blocks, but they only run on Theano. So if you tried Keras and are unhappy with it, maybe try out one of these alternatives to see if they work out for you. Another popular deep learning framework is Torch, which is written in Lua. Facebook open-sourced a Python implementation of Torch called PyTorch, which allows you to conveniently use the same low-level libraries that Torch uses, but from Python instead of Lua. PyTorch is much better for debugging since one of the biggest differences between Theano/TensorFlow and PyTorch is that the former use symbolic computation while the latter doesn\u2019t. Symbolic computation means that coding an operation (say, \u2018x + y\u2019), it\u2019s not computed when that line is interpreted. Before getting executed it has to be compiled (translated to CUDA or C). This makes debugging harder in Theano/TensorFlow, since an error is much harder to associate with the line of code that caused it. Of course, doing things this way has its advantages, but debugging isn\u2019t one of them. If you want to start out with PyTorch the official tutorials are very friendly to beginners but get to advanced topics as well. Alright, you\u2019ve presented me with a lot of alternatives for machine learning libraries in Python. What should I choose? How do I compare these things? Where do I start? Our Ape Advice\u2122 for beginners is to try and not get bogged down by details. If you\u2019ve never done anything machine learning related, try out scikit-learn. You\u2019ll get an idea of how the cycle of tagging, training and testing work and how a model is developed. Now, if you want to try out deep learning, start out with Keras \u2014 which is widely agreed to be the easiest framework \u2014 and see where that takes you. After you have more experience, you will start to see what it is that you actually want from the framework: greater speed, a different API, or maybe something else, and you\u2019ll be able to make a more informed decision. And even then, there is an endless supply of articles out there comparing Theano, Torch, and TensorFlow. There\u2019s no real way to tell which one is the good one. It\u2019s important to take into account that all of them have wide support and are improving constantly, making comparisons harder to make. A six month old benchmark may be outdated, and year old claims of framework X doesn\u2019t support operation Y could no longer be valid. Finally, if you\u2019re interested in doing machine learning specifically applied to NLP, why not check out MonkeyLearn! Our platform provides a unique UX that makes it super easy to build, train and improve NLP models. You can either use pre-trained models for common use cases (like sentiment analysis, topic detection or keyword extraction) or train custom algorithms using your particular data. Also, you don\u2019t have to worry about the underlying infrastructure or deploying your models, our scalable cloud does this for you. You can start for free and integrate right away with our beautiful API. There are plenty of online resources out there to learn about machine learning ! Here are a few: So that was a brief intro to machine learning in Python and some of its libraries. The important part is not getting bogged down by details and just trying stuff out. Follow your curiosity, and don\u2019t be afraid to experiment. Know about a python library that was left out? Share it in the comments below!", "links": ["http://www.monkeylearn.com", "http://monkeylearn.com/use-cases/", "http://monkeylearn.com/docs/", "http://www.monkeylearn.com/pricing", "http://www.monkeylearn.com/blog/", "https://app.monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/?utm_source=ml-blog&utm_medium=header&utm_campaign=blog", "https://monkeylearn.com/blog", "http://monkeylearn.com/use-cases/", "http://monkeylearn.com/docs/", "http://www.monkeylearn.com/pricing", "http://www.monkeylearn.com/blog/", "https://app.monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/?utm_source=ml-blog&utm_medium=header&utm_campaign=blog", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/07/Post_2c.png", "http://scikit-learn.org/", "http://scikit-learn.org/stable/auto_examples/index.html#general-examples", "http://scikit-learn.org/stable/tutorial/index.html", "http://www.nltk.org/", "http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk", "http://deeplearning.net/software/theano/", "https://keras.io/", "https://github.com/mila-udem/blocks", "https://github.com/Lasagne/Lasagne", "http://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/Theano%20Tutorial.ipynb", "https://www.tensorflow.org/", "https://www.tensorflow.org/get_started/graph_viz", "https://www.tensorflow.org/tutorials/", "https://keras.io/", "https://news.ycombinator.com/item?id=13872764", "https://github.com/fchollet/keras#getting-started-30-seconds-to-keras", "https://keras.io/getting-started/functional-api-guide/", "https://elitedatascience.com/keras-tutorial-deep-learning-in-python", "https://github.com/Lasagne/Lasagne", "https://github.com/mila-udem/blocks", "http://torch.ch/", "http://pytorch.org/", "http://pytorch.org/tutorials/", "http://scikit-learn.org/stable/tutorial/basic/tutorial.html", "https://elitedatascience.com/keras-tutorial-deep-learning-in-python", "https://www.quora.com/Is-TensorFlow-better-than-other-leading-libraries-such-as-Torch-Theano", "http://www.ccri.com/2016/12/09/torch-vs-tensorflow-vs-theano/", "https://www.reddit.com/r/MachineLearning/comments/5w3q74/d_so_pytorch_vs_tensorflow_whats_the_verdict_on/", "https://github.com/zer0n/deepframeworks/blob/master/README.md", "https://monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/", "http://nbviewer.jupyter.org/github/rhiever/Data-Analysis-and-Machine-Learning-Projects/blob/master/example-data-science-notebook/Example%20Machine%20Learning%20Notebook.ipynb", "https://blog.monkeylearn.com/a-gentle-guide-to-machine-learning/", "https://www.coursera.org/learn/machine-learning", "http://course.fast.ai/", "https://monkeylearn.com/blog/author/bruno/", "https://monkeylearn.com/blog/category/news/", "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/#respond", "https://monkeylearn.com/blog/author/bruno/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/monkeylearn_zapier.png", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/wp-content/uploads/2017/06/Post_1e.png", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/naive-bayes-classifier.png", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/monkey-startup.png", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/category/applications/", "https://monkeylearn.com/blog/category/guides/", "https://monkeylearn.com/blog/category/howto/", "https://monkeylearn.com/blog/category/news/", "https://monkeylearn.com/blog/category/text-classification/", "https://www.monkeylearn.com", "https://www.monkeylearn.com", "http://slack.monkeylearn.com/", "https://twitter.com/monkeylearn", "https://github.com/monkeylearn/"]}, "147": {"url": "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "title": "Machine Learning Translation and the Google Translate Algorithm", "text": "Every day we use different technologies without even knowing how exactly they work. In fact, it\u2019s not very easy to understand engines powered by machine learning. The Statsbot team wants to make machine learning clear by telling data stories in this blog. Today, we\u2019ve decided to explore machine translators and explain how the Google Translate algorithm works. Years ago, it was very time consuming to translate the text from an unknown language. Using simple vocabularies with word-for-word translation was hard for two reasons: 1) the reader had to know the grammar rules and 2) needed to keep in mind all language versions while translating the whole sentence. Now, we don\u2019t need to struggle so much\u2013 we can translate phrases, sentences, and even large texts just by putting them in Google Translate. But most people don\u2019t actually care how the engine of machine learning translation works. This post is for those who do care. If the Google Translate engine tried to kept the translations for even short sentences, it wouldn\u2019t work because of the huge number of possible variations. The best idea can be to teach the computer sets of grammar rules and translate the sentences according to them. If only it were as easy as it sounds. If you have ever tried learning a foreign language, you know that there are always a lot of exceptions to rules. When we try to capture all these rules, exceptions and exceptions to the exceptions in the program, the quality of translation breaks down. Creating your own simple machine translator would be a great project for any data science resume. Let\u2019s try to investigate what hides in the \u201cblack boxes\u201d that we call machine translators. Deep neural networks can achieve excellent results in very complicated tasks (speech/visual object recognition), but despite their flexibility, they can be applied only for tasks where the input and target have fixed dimensionality. Here is where Long Short-Term Memory networks (LSTMs) come into play, helping us to work with sequences whose length we can\u2019t know a priori. LSTMs are a special kind of recurrent neural network (RNN), capable of learning long-term dependencies. All RNNs look like a chain of repeating modules. So the LSTM transmits data from module to module and, for example, for generating Ht we use not only Xt, but all previous input values X. To learn more about structure and mathematical models of LSTM, you can read the great article \u201cUnderstanding LSTM Networks.\u201d Our next step is bidirectional recurrent neural networks (BRNNs). What a BRNN does, is split the neurons of a regular RNN into two directions. One direction is for positive time, or forward states. The other direction is for negative time, or backward states. The output of these two states are not connected to inputs of the opposite direction states. To understand why BRNNs can work better than a simple RNN, imagine that we have a sentence of 9 words and we want to predict the 5th word. We can make it know either only the first 4 words, or the first 4 words and last 4 words. Of course, the quality in the second case would be better. Now we\u2019re ready to move to sequence to sequence models (also called seq2seq). The basic seq2seq model consist of two RNNs: an encoder network that processes the input and a decoder network that generates the output. Finally, we can make our first machine translator! However, let\u2019s think about one trick. Google Translate currently supports 103 languages, so we should have 103x102 different models for each pair of languages. Of course, the quality of these models varies according to the popularity of languages and the amount of documents needed for training this network. The best that we can do is to make one NN to take any language as input and translate into any language. That very idea was realized by Google engineers at the end of 2016. Architecture of NN was build on the seq2seq model, which we have already studied. The only exception is that between the encoder and decoder there are 8 layers of LSTM-RNN that have residual connections between layers with some tweaks for accuracy and speed. If you want to go deeper with that, take a look at the article Google\u2019s Neural Machine Translation System. The system requires a \u201ctoken\u201d at the beginning of the input sentence which specifies the language you\u2019re trying to translate the phrase into. This improves translation quality and enables translations even between two languages which the system hasn\u2019t seen yet, a method termed \u201cZero-Shot Translation.\u201d When we\u2019re talking about improvements and better results from Google Translate algorithms, how can we correctly evaluate that the first candidate for translation is better than the second? It\u2019s not a trivial problem, because for some commonly used sentences we have the sets of reference translations from the professional translators, that have, of course, some differences. There are a lot of approaches that partly solve this problem, but the most popular and effective metric is BLEU (bilingual evaluation understudy). Imagine, we have two candidates from machine translators: Although they have the same meaning they differ in quality and have different structure. Let\u2019s look at two human translations: Obviously, Candidate 1 is better, sharing more words and phrases compared to Candidate 2. This is a key idea of the simple BLEU approach. We can compare n-grams of the candidate with n-grams of the reference translation and count the number of matches (independent from their position). We use only n-gram precisions, because calculating recall is difficult with multiple refs and the result is the geometric average of n-gram scores. Now you can evaluate the complex engine of machine learning translation. Next time when you translate something with Google Translate, imagine how many millions of documents it analyzed before giving you the best language version.", "links": ["https://blog.statsbot.co?source=logo-lo_878a378bccca---cfc9f21a543a", "https://twitter.com/statsbotco", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.statsbot.co%2Fmachine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co", "https://blog.statsbot.co/analytics/home", "https://blog.statsbot.co/datascience/home", "https://blog.statsbot.co/design/home", "https://blog.statsbot.co/startups/home", "https://blog.statsbot.co/bots/home", "https://blog.statsbot.co/news/home", "https://blog.statsbot.co/statsbot-digest-b0d7372f842a", "http://statsbot.co?utm_source=blog&utm_medium=navigation&utm_campaign=robot_face", "https://blog.statsbot.co/search", "https://blog.statsbot.co/@daniilkorbut?source=post_header_lockup", "https://blog.statsbot.co/@daniilkorbut?source=post_header_lockup", "https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html", "http://statsbot.co?utm_source=blog&utm_medium=article&utm_campaign=machine_translation", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6?utm_source=blog&utm_medium=article&utm_campaign=machine_translation", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6", "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "https://www.semanticscholar.org/paper/Hybrid-speech-recognition-with-Deep-Bidirectional-Graves-Jaitly/5807664af8e63d5207f59fb263c9e7bd3673be79", "https://research.googleblog.com/2016/09/a-neural-network-for-machine.html", "https://www.newscientist.com/article/2114748-google-translate-ai-invents-its-own-language-to-translate-with/", "https://research.googleblog.com/2016/09/a-neural-network-for-machine.html", "https://arxiv.org/abs/1609.08144", "https://en.wikipedia.org/wiki/BLEU", "https://en.wikipedia.org/wiki/N-gram", "https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3", "https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/tagged/machine-learning?source=post", "https://blog.statsbot.co/tagged/machine-translation?source=post", "https://blog.statsbot.co/tagged/google-translate?source=post", "https://blog.statsbot.co/tagged/data-science?source=post", "https://blog.statsbot.co/tagged/neural-networks?source=post", "https://blog.statsbot.co/@daniilkorbut?source=footer_card", "https://blog.statsbot.co/@daniilkorbut", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "148": {"url": "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "title": "Yanir SeroussiMy 10-step path to becoming a remote data scientist with\u00a0Automattic", "text": "About two years ago, I read the book The Year without Pants, which describes the author\u2019s experience leading a team at Automattic (the company behind WordPress.com, among other products). Automattic is a fully-distributed company, which means that all of its employees work remotely (hence pants are optional). While the book discusses some of the challenges of working remotely, the author\u2019s general experience was very positive. A few months after reading the book, I decided to look for a full-time position after a period of independent work. Ideally, I wanted a well-paid data science-y remote job with an established distributed tech company that offers a good life balance and makes products I care about. Automattic seemed to tick all my boxes, so I decided to apply for a job with them. This post describes my application steps, which ultimately led to me becoming a data scientist with Automattic. Before jumping in, it\u2019s worth noting that this post describes my personal experience. If you apply for a job with Automattic, your experience is likely to be different, as the process varies across teams, and evolves over time. I decided to apply for a data wrangler position with Automattic in October 2015. While data wrangler may sound less sexy than data scientist, reading the job ad led me to believe that the position may involve interesting data science work. This impression was strengthened by some LinkedIn stalking, which included finding current data wranglers and reading through their profiles and websites. I later found out that all the people on the data division start out as data wranglers, and then they may pick their own title. Some data wranglers do data science work, while others are more focused on data engineering, and there are some projects that require a broad range of skills. As the usefulness of the term data scientist is questionable, I\u2019m not too fussed about fancy job titles. It\u2019s more important to do interesting work in a supportive environment. Applying for the job was fairly straightforward. I simply followed the instructions from the ad:  Does this sound interesting? If yes, please send a short email to jobs @ this domain telling us about yourself and attach a resum\u00e9. Let us know what you can contribute to the team. Include the title of the position you\u2019re applying for and your name in the subject. Proofread! Make sure you spell and capitalize WordPress and Automattic correctly. We are lucky to receive hundreds of applications for every position, so try to make your application stand out. If you apply for multiple positions or send multiple emails there will be one reply.  Having been on the receiving side of job applications, I find it surprising that many people don\u2019t bother writing a cover letter, addressing the selection criteria in the ad, or even applying for a job they\u2019re qualified to do. Hence, my cover letter was fairly short, comprising of several bullet points that highlight the similarities between the job requirements and my experience. It was nothing fancy, but simple cover letters have worked well for me in the past. The initial application was followed by a long wait. From my research, this is the typical scenario. This is unsurprising, as Automattic is a fairly small company with a large footprint, which is both distributed and known as a great place to work (e.g., its Glassdoor rating is 4.9). Therefore, it attracts many applicants from all over the world, which take a while to process. In addition, Matt Mullenweg (Automattic\u2019s CEO) reviews job applications before passing them on to the team leads. As I didn\u2019t know that Matt reviewed job applications, I decided to try to shorten the wait by getting introduced to someone in the data division. My first attempt was via a second-degree LinkedIn connection who works for Automattic. He responded quickly when I reached out to him, saying that his experience working with the company is in line with the Glassdoor reviews \u2013 it\u2019s the best job he\u2019s had in his 15-year-long career. However, he couldn\u2019t help me with an intro, because there is no simple way around Automattic\u2019s internal processes. Nonetheless, he reassured me that it is worth waiting patiently, as the strict process means that you end up working with great people. I wasn\u2019t in a huge rush to find a job, but in December 2015 I decided to accept an offer to become the head of data science at Car Next Door. This was a good decision at the time, as I believe in the company\u2019s original vision of reducing the number of cars on the road through car sharing, and it seemed like there would be many interesting projects for me to work on. The position wasn\u2019t completely remote, but as the company was already spread across several cities, I was able to work from home for a day or two every week. In addition, it was a pleasant commute by bike from my Sydney home to the office, so putting the fully-remote job search on hold didn\u2019t seem like a major sacrifice. As I haven\u2019t heard anything from Automattic at that stage, it seemed unwise to reject a good offer, so I started working full-time with Car Next Door in January 2016. I successfully attracted Automattic\u2019s attention with a post I published on the misuse of the word insights by many tech companies, which included an example from WordPress.com. Greg Ichneumon Brown, one of the data wranglers, commented on the post, and invited me to apply to join Automattic and help them address the issues I raised. This happened after I accepted the offer from Car Next Door, and hasn\u2019t resulted in any speed up of the process, so I just gave up on Automattic and carried on with my life. I finally heard back from Automattic in February 2016 (four months after my initial application and a month into my employment with Car Next Door). Martin Remy, who leads the data division, emailed me to enquire if I\u2019m still interested in the position. I informed him that I was no longer looking for a job, but we agreed to have an informal chat, as I\u2019ve been waiting for such a long time. As is often the case with Automattic interviews, the chat with Martin was completely text-based. Working with a distributed team means that voice and video calls can be hard to schedule. Hence, Automattic relies heavily on textual channels, and text-based interviews allow the company to test the written communication skills of candidates. The chat revolved around my past work experience, and Martin also took the time to answer my questions about the company and the data division. At the conclusion of the chat, Martin suggested I contact him directly if I was ever interested in continuing the application process. While I was happy with my position at the time, the chat strengthened my positive impression of Automattic, and I decided that I would reapply if I were to look for a full-time position again. My next job search started earlier than I had anticipated. In October 2016, I decided to leave Car Next Door due to disagreements with the founders over the general direction of the company. In addition, I had more flexibility in choosing where to live, as my personal circumstances had changed. As I\u2019ve always been curious about life outside the capital cities of Australia, I wanted to move away from Sydney. While I could have probably continued working remotely with Car Next Door, I felt that it would be better to find a job with a fully-distributed team. Therefore, I messaged Martin and we scheduled another chat. The second chat with Martin took place in early November. Similarly to the first chat, it was conducted via Skype text messages, and revolved around my work in the time that has passed since the first chat. This time, as I was keen on continuing with the process, I asked more specific questions about what kind of work I\u2019m likely to end up doing and what the next steps would be. The answers were that I\u2019d be joining the data science team, and that the next steps are a pre-trial test, a paid trial, and a final interview with Matt. While this sounds straightforward, it took another six months until I finally became an Automattic employee (but I wasn\u2019t in a rush). The pre-trial test consisted of a data analysis task, where I was given a dataset and a set of questions to answer by Carly Stambaugh, the data science lead. The goal of the test is to evaluate the candidate\u2019s approach to a problem, and assess organisational and communication skills. As such, the focus isn\u2019t on obtaining a specific result, so candidates are given a choice of several potential avenues to explore. The open-ended nature of the task is reminiscent of many real-world data science projects, where you don\u2019t always have a clear idea of what you\u2019re going to discover. While some people may find this kind of uncertainty daunting, I find it interesting, as it is one of the things that makes data science a science. I spent a few days analysing the data and preparing a report, which was submitted as a Jupyter Notebook. After submitting my initial report, there were a few follow-up questions, which I answered by email. The report was reviewed by Carly and Martin, and as they were satisfied with my work, I was invited to proceed to the next stage: A paid trial project. The main part of the application process with Automattic is the paid trial project. The rationale behind doing paid trials was explained a few years ago by Matt in Hire by Auditions, Not Resumes:  Before we hire anyone, they go through a trial process first, on contract. They can do the work at night or over the weekend, so they don\u2019t have to leave their current job in the meantime. We pay a standard rate of $25 per hour, regardless of whether you\u2019re applying to be an engineer or the chief financial officer. During the trials, we give the applicants actual work. If you\u2019re applying to work in customer support, you\u2019ll answer tickets. If you\u2019re an engineer, you\u2019ll work on engineering problems. If you\u2019re a designer, you\u2019ll design. There\u2019s nothing like being in the trenches with someone, working with them day by day. It tells you something you can\u2019t learn from resumes, interviews, or reference checks. At the end of the trial, everyone involved has a great sense of whether they want to work together going forward. And, yes, that means everyone \u2014 it\u2019s a mutual tryout. Some people decide we\u2019re not the right fit for them.  The goal of my trial project was to improve the Elasticsearch language detection algorithm. This took about a month, and ultimately resulted in a pull request that got merged into the language detection plugin. I find this aspect of the process pretty exciting: While the plugin is used to classify millions of documents internally by Automattic, its impact extends beyond the company, as Elasticsearch is used by many other organisations and projects. This stands in contrast to many other technical job interviews, which consist of unpaid work on toy problems under stressful conditions, where the work performed is ultimately thrown away. While the monetary compensation for the trial work is lower than the market rate for data science consulting, I valued the opportunity to work on a real open source project, even if this hadn\u2019t led to me getting hired. There was much more to the trial project than what\u2019s shown in the final pull request. Most of the discussions were held on an internal project thread, primarly under the guidance of Carly (the data science lead), and Greg (the data wrangler who replied to my post a year earlier). The project was kicked off with a general problem statement: There was some evidence that the Elasticsearch language detection plugin doesn\u2019t perform well on short texts, and my mission was to improve it. As the plugin didn\u2019t include any tests for short texts, one of the main contributions of my work was the creation of datasets and tests to measure its accuracy on texts of different lengths. This was followed by some tweaks that improved the plugin\u2019s performance, as summarised in the pull request. Internally, this work consisted of several iterations where I came up with ideas, asked questions, implemented the ideas, shared the results, and discussed further steps. There are still many possible improvements to the work done in the trial. However, as trials generally last around a month, we decided to end it after a few iterations. I enjoyed the trial process, but it is definitely not for everyone. Most notably, there is a strong emphasis on asynchronous text-based communication, which is the main mode by which projects are coordinated at Automattic. People who don\u2019t enjoy written communication may find this aspect challenging, but I have always found that writing helps me organise my thoughts, and that I retain information better when reading than when listening to people speak. That being said, Automatticians do meet in person several times a year, and some teams have video chats for some discussions. While doing the trial, I had a video chat with Carly, which was the first (and last) time in the process that I got to see and hear a live human. However, this was not an essential part of the trial project, as our chat was mostly on the data scientist role and my job expectations. I finished working on the trial project just before Christmas. The feedback I received throughout the trial was positive, but Martin, Carly, and Greg had to go through the work and discuss it among themselves before making a final decision. This took about a month, due to the holiday period, various personal circumstances, and the data science team meetup that was scheduled for January 2017. Eventually, Martin got back to me with positive news: They were satisfied with my trial work, which meant there was only one stage left \u2013 the final interview with Matt Mullenweg, Automattic\u2019s CEO. Like other parts of the process, the interview with Matt is text-based. The way it works is fairly simple: I was instructed to message Matt on Slack and wait for a response, which may take days or weeks. I sent Matt a message on January 25, and was surprised to hear back from him the following morning. However, that day was Australia Day, which is a public holiday here. Therefore, I only got back to him two hours after he messaged me that morning, and by that time he was probably already busy with other things. This was the start of a pretty long wait. I left Car Next Door at the end of January, as I figured that I would be able to line up some other work even if things didn\u2019t work out with Automattic. My plan was to take some time off, and then move up to the Northern Rivers area of New South Wales. I had two Reef Life Survey trips planned, so I wasn\u2019t going to start working again before mid-April. I assumed that I would hear back from Matt before then, which would have allowed me to make an informed decision whether to look for another job or not. After two weeks of waiting, the time for my dive trips was nearing. As I was going to be without mobile reception for a while, I thought it\u2019d be worth letting Matt know my schedule. After discussing the matter with Martin, I messaged Matt. He responded, saying that we might as well do the interview at the beginning of April, as I won\u2019t be starting work before that time anyway. I would have preferred to be done with the interview earlier, but was happy to have some certainty and not worry about missing more chat messages before April. In early April, I returned from my second dive trip (which included a close encounter with Cyclone Debbie), and was hoping to sort out my remote work situation while completing the move up north. Unfortunately, while the move was successful, I was ready to give up on Automattic because I haven\u2019t heard back from Matt at all in April. However, Martin remained optimistic and encouraged me to wait patiently, which I did as I was pretty busy with the move and with some casual freelancing projects. The chat with Matt finally happened on May 2. As is often the case, it took a few hours and covered my background, the trial process, and some other general questions. I asked him about my long wait for the final chat, and he apologised for me being an outlier, as most chats happen within two weeks of a candidate being passed over to him. As the chat was about to conclude, we got to the topic of salary negotiation (which went well), and then the process was finally over! Within a few hours of the chat I was sent an offer letter and an employment contract. As Automattic has an entity in Australia (called Ausomattic), it\u2019s a fairly standard contract. I signed the contract and started work the following week \u2013 over a year and a half after my initial application. Even before I started working, I booked tickets to meet the data division in Montr\u00e9al \u2013 a fairly swift transition from the long wait for the final interview. As noted above, Automatticians get to choose their own job titles, so to become a data scientist with Automattic, I had to set my job title to Data Scientist. This is generally how many people become data scientists these days, even outside Automattic. However, job titles don\u2019t matter as much as job satisfaction. And after 2.5 months with Automattic, I\u2019m very satisfied with my decision to join the company. My first three weeks were spent doing customer support, like all new Automattic employees. Since then, I\u2019ve been involved in projects to make engagement measurement more consistent (harder than it sounds, as counting things is hard), and to improve the data science codebase (e.g., moving away from Legacy Python). Besides that, I also went to Montr\u00e9al for the data division meetup, and have started getting into chatbot work. I\u2019m looking forward to doing more work and sharing my experience here and on data.blog. Very enlightening post! It was very awesome to see that the insights you saw to Elasticsearch went to a PR. I bet that was worth the whole thing!  That\u2019s very exciting, I wanted to ask are you a self learner or do you have a degree,can you please share your background. Thank you  Thanks Mostafa. Yes, I have a BSc in computer science, and a PhD in what you would now call data science. See: https://www.linkedin.com/in/yanirseroussi/  Fill in your details below or click an icon to log in:     Connecting to %s         Enter your email address to follow this blog and receive notifications of new posts by email.", "links": ["https://yanirseroussi.com/", "https://yanirseroussi.com/", "https://yanirseroussi.com/about/", "https://yanirseroussi.com/presentations/", "https://yanirseroussi.com/phd-work/", "https://yanirseroussi.com/2014/04/05/kaggle-competition-summaries/", "http://scottberkun.com/yearwithoutpants/", "https://automattic.com/", "https://yanirseroussi.com/2015/03/22/the-long-road-to-a-lifestyle-business/", "http://web.archive.org/web/20150908140923/https://automattic.com/work-with-us/data-wrangler/", "https://yanirseroussi.com/2016/08/04/is-data-scientist-a-useless-job-title/", "https://automattic.com/about/", "https://www.glassdoor.com.au/Reviews/Automattic-Reviews-E751107.htm", "http://davemart.in/remote-hiring/", "https://www.carnextdoor.com.au/", "https://yanirseroussi.com/2015/12/08/this-holiday-season-give-me-real-insights/", "https://yanirseroussi.com/2015/12/08/this-holiday-season-give-me-real-insights/#comment-957", "http://jupyter.org/", "https://hbr.org/2014/01/hire-by-auditions-not-resumes", "https://www.elastic.co/products/elasticsearch", "https://github.com/jprante/elasticsearch-langdetect/pull/69", "https://github.com/jprante/elasticsearch-langdetect/pull/69", "https://yanirseroussi.com/2016/01/24/the-joys-of-offline-data-collection/", "https://www.whitsundaytimes.com.au/news/boat-caught-in-eye-of-cyclone-cruises-home/3164170/", "https://data.blog/2017/06/29/data-coalesce-automattic-data-division-meets-in-montreal/", "http://daynebatten.com/2016/06/counting-hard-data-science/", "http://powerfulpython.com/blog/magic-word-legacy-python/", "https://data.blog/2017/05/24/may-the-bot-be-with-you-how-algorithms-are-supporting-happiness-at-wordpress-com/", "https://data.blog/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=twitter", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=facebook", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=google-plus-1", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=linkedin", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=reddit", "https://yanirseroussi.com/category/data-science-2/", "https://yanirseroussi.com/tag/automattic/", "https://yanirseroussi.com/tag/career/", "https://yanirseroussi.com/tag/data-science/", "https://yanirseroussi.com/tag/elasticsearch/", "https://yanirseroussi.com/tag/personal/", "https://yanirseroussi.com/tag/wordpress/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "https://yanirseroussi.com/author/yanirseroussi/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comments", "https://yanirseroussi.com/2017/06/03/exploring-and-visualising-reef-life-survey-data/", "https://dotnetmeditations.com/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1698", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1698&_wpnonce=6c197b2684", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1698#respond", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1700", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1700&_wpnonce=350fa40ca9", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1700#respond", "http://yanirseroussi.com", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1705", "https://www.linkedin.com/in/yanirseroussi/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1705&_wpnonce=7f1e8d7061", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1705#respond", "https://gravatar.com/site/signup/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "https://yanirseroussi.com/2017/06/03/exploring-and-visualising-reef-life-survey-data/", "https://yanirseroussi.com/2017/01/08/customer-lifetime-value-and-the-proliferation-of-misinformation-on-the-internet/", "https://yanirseroussi.com/2016/09/19/ask-why-finding-motives-causes-and-purpose-in-data-science/", "https://yanirseroussi.com/2016/08/21/seven-ways-to-be-data-driven-off-a-cliff/", "https://yanirseroussi.com/category/bcrecommender/", "https://yanirseroussi.com/category/data-science-2/", "https://yanirseroussi.com/category/environment/", "https://yanirseroussi.com/category/general/", "https://yanirseroussi.com/category/kaggle-2/", "https://yanirseroussi.com/category/machine-intelligence/", "https://yanirseroussi.com/category/phd-work/", "https://wordpress.com/?ref=footer_blog"]}, "149": {"url": "https://medium.com/airbnb-engineering/using-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d", "title": "Using Machine Learning to Predict Value of Homes On\u00a0Airbnb", "text": "by Robert Chang Data products have always been an instrumental part of Airbnb\u2019s service. However, we have long recognized that it\u2019s costly to make data products. For example, personalized search ranking enables guests to more easily discover homes, and smart pricing allows hosts to set more competitive prices according to supply and demand. However, these projects each required a lot of dedicated data science and engineering time and effort. Recently, advances in Airbnb\u2019s machine learning infrastructure have lowered the cost significantly to deploy new machine learning models to production. For example, our ML Infra team built a general feature repository that allows users to leverage high quality, vetted, reusable features in their models. Data scientists have started to incorporate several AutoML tools into their workflows to speed up model selection and performance benchmarking. Additionally, ML infra created a new framework that will automatically translate Jupyter notebooks into Airflow pipelines. In this post, I will describe how these tools worked together to expedite the modeling process and hence lower the overall development costs for a specific use case of LTV modeling\u200a\u2014\u200apredicting the value of homes on Airbnb. Customer Lifetime Value (LTV), a popular concept among e-commerce and marketplace companies, captures the projected value of a user for a fixed time horizon, often measured in dollar terms. At e-commerce companies like Spotify or Netflix, LTV is often used to make pricing decisions like setting subscription fees. At marketplace companies like Airbnb, knowing users\u2019 LTVs enable us to allocate budget across different marketing channels more efficiently, calculate more precise bidding prices for online marketing based on keywords, and create better listing segments. While one can use past data to calculate the historical value of existing listings, we took one step further to predict LTV of new listings using machine learning. Data scientists are typically accustomed to machine learning related tasks such as feature engineering, prototyping, and model selection. However, taking a model prototype to production often requires an orthogonal set of data engineering skills that data scientists might not be familiar with. Luckily, At Airbnb we have machine learning tools that abstract away the engineering work behind productionizing ML models. In fact, we could not have put our model into production without these amazing tools. The remainder of this post is organized into four topics, along with the tools we used to tackle each task: One of the first steps of any supervised machine learning project is to define relevant features that are correlated with the chosen outcome variable, a process called feature engineering. For example, in predicting LTV, one might compute the percentage of the next 180 calendar dates that a listing is available or a listing\u2019s price relative to comparable listings in the same market. At Airbnb, feature engineering often means writing Hive queries to create features from scratch. However, this work is tedious and time consuming as it requires specific domain knowledge and business logic, which means the feature pipelines are often not easily sharable or even reusable. To make this work more scalable, we developed Zipline\u200a\u2014\u200aa training feature repository that provides features at different levels of granularity, such as at the host, guest, listing, or market level. The crowdsourced nature of this internal tool allows data scientists to use a wide variety of high quality, vetted features that others have prepared for past projects. If a desired feature is not available, a user can create her own feature with a feature configuration file like the following: When multiple features are required for the construction of a training set, Zipline will automatically perform intelligent key joins and backfill the training dataset behind the scenes. For the listing LTV model, we used existing Zipline features and also added a handful of our own. In sum, there were over 150 features in our model, including: With our features and outcome variable defined, we can now train a model to learn from our historical data. As in the example training dataset above, we often need to perform additional data processing before we can fit a model: In this step, we don\u2019t quite know what is the best set of features to use, so writing code that allows us to rapidly iterate is essential. The pipeline construct, commonly available in open-source tools like Scikit-Learn and Spark, is a very convenient tool for prototyping. Pipelines allow data scientists to specify high-level blueprints that describe how features should be transformed, and which models to train. To make it more concrete, below is a code snippet from our LTV model pipeline: At a high level, we use pipelines to specify data transformations for different types of features, depending on whether those features are of type binary, categorical, or numeric. FeatureUnion at the end simply combines the features column-wise to create the final training dataset. The advantage of writing prototypes with pipelines is that it abstracts away tedious data transformations using data transforms. Collectively, these transforms ensure that data will be transformed consistently across training and scoring, which solves a common problem of data transformation inconsistency when translating a prototype into production. Furthermore, pipelines also separates data transformations from model fitting. While not shown in the code above, data scientists can add a final step to specify an estimator for model fitting. By exploring different estimators, data scientists can perform model selection to pick the best model to improve the model\u2019s out of sample error. As mentioned in the previous section, we need to decide which candidate model is the best to put into production. To make such a decision, we need to weigh the tradeoffs between model interpretability and model complexity. For example, a sparse linear model might be very interpretable but not complex enough to generalize well. A tree based model might be flexible enough to capture non-linear patterns but not very interpretable. This is known as the Bias-Variance tradeoff. In applications such as insurance or credit screening, a model needs to be interpretable because it\u2019s important for the model to avoid inadvertently discriminating against certain customers. In applications such as image classification, however, it is much more important to have a performant classifier than an interpretable model. Given that model selection can be quite time consuming, we experimented with using various AutoML tools to speed up the process. By exploring a wide variety of models, we found which types of models tended to perform best. For example, we learned that eXtreme gradient boosted trees (XGBoost) significantly outperformed benchmark models such as mean response models, ridge regression models, and single decision trees. Given that our primary goal was to predict listing values, we felt comfortable productionizing our final model using XGBoost, which favors flexibility over interpretability. As we alluded to earlier, building a production pipeline is quite different from building a prototype on a local laptop. For example, how can we perform periodic re-training? How do we score a large number of examples efficiently? How do we build a pipeline to monitor model performance over time? At Airbnb, we built a framework called ML Automator that automagically translates a Jupyter notebook into an Airflow machine learning pipeline. This framework is designed specifically for data scientists who are already familiar with writing prototypes in Python, and want to take their model to production with limited experience in data engineering. Here is a code snippet demonstrating how the fit and transform functions are defined in our LTV model. The fit function tells the framework that a XGBoost model will be trained, and that data transformations will be carried out according to the pipeline we defined previously. Once the notebook is merged, ML Automator will wrap the trained model inside a Python UDF and create an Airflow pipeline like the one below. Data engineering tasks such as data serialization, scheduling of periodic re-training, and distributed scoring are all encapsulated as a part of this daily batch job. As a result, this framework significantly lowers the cost of model development for data scientists, as if there was a dedicated data engineer working alongside the data scientists to take the model into production! Note: Beyond productionization, there are other topics, such as tracking model performance over time or leveraging elastic compute environment for modeling, which we will not cover in this post. Rest assured, these are all active areas under development. In the past few months, data scientists have partnered very closely with ML Infra, and many great patterns and ideas arose out of this collaboration. In fact, we believe that these tools will unlock a new paradigm for how to develop machine learning models at Airbnb. We are very excited about the future of this framework and the new paradigm it brought along. By bridging the gap between prototyping and productionization, we can truly enable data scientists and engineers to pursue end-to-end machine learning projects and make our product better. Want to use or build these ML tools? We\u2019re always looking for talented people to join our Data Science and Analytics team! Special thanks to members of Data Science & ML Infra team who were involved in this work: Aaron Keys, Brad Hunter, Hamel Husain, Jiaying Shi, Krishna Puttaswamy, Michael Musson, Nick Handel, Varant Zanoyan, Vaughn Quoss et al. Additional thanks to Gary Tang, Jason Goodman, Jeff Feng, Lindsay Pettingill for reviewing this blog post.", "links": ["https://medium.com/", "https://medium.com/airbnb-engineering?source=logo-lo_e41ad19e22dd---53c7c27702d5", "https://twitter.com/AirbnbEng", "https://medium.com/m/signin?redirect=https%3A%2F%2Fmedium.com%2Fairbnb-engineering%2Fusing-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d", "https://medium.com/airbnb-engineering", "https://medium.com/airbnb-engineering/ai/home", "https://medium.com/airbnb-engineering/airbnb-engineering-backend/home", "https://medium.com/airbnb-engineering/data/home", "https://medium.com/airbnb-engineering/airbnb-engineering-infrastructure/home", "https://medium.com/airbnb-engineering/tagged/mobile", "https://medium.com/airbnb-engineering/web/home", "http://airbnb.io/projects/", "https://medium.com/airbnb-engineering/search", "https://medium.com/@rchang?source=post_header_lockup", "https://medium.com/@rchang?source=post_header_lockup", "https://twitter.com/_rchang", "https://medium.com/swlh/diligence-at-social-capital-part-3-cohorts-and-revenue-ltv-ab65a07464e1", "http://scikit-learn.org/stable/", "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html", "https://www.kaggle.com/general/16927", "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html", "https://spark.apache.org/docs/latest/ml-pipeline.html", "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html", "http://scikit-learn.org/stable/data_transforms.html", "http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html", "https://medium.com/airbnb-engineering/automated-machine-learning-a-paradigm-shift-that-accelerates-data-scientist-productivity-airbnb-f1f8a10d61f8", "http://scott.fortmann-roe.com/docs/BiasVariance.html", "https://medium.com/airbnb-engineering/automated-machine-learning-a-paradigm-shift-that-accelerates-data-scientist-productivity-airbnb-f1f8a10d61f8", "https://github.com/dmlc/xgboost", "https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8", "http://www.florianwilhelm.info/2016/10/python_udf_in_hive/", "https://airflow.incubator.apache.org/", "https://www.airbnb.com/careers/departments/data-science-analytics", "https://www.linkedin.com/in/aaronkeys/", "https://www.linkedin.com/in/brad-hunter-497621a/", "https://www.linkedin.com/in/hamelhusain/", "https://www.linkedin.com/in/jiaying-shi-a2142733/", "https://www.linkedin.com/in/krishnaputtaswamy/", "https://www.linkedin.com/in/michael-m-a37b1932/", "https://www.linkedin.com/in/nicholashandel/", "https://www.linkedin.com/in/vzanoyan/", "https://www.linkedin.com/in/vquoss/", "https://www.linkedin.com/in/thegarytang/", "https://medium.com/@jasonkgoodman", "https://twitter.com/jtfeng", "https://medium.com/@lpettingill", "https://medium.com/@jtfeng?source=post_page", "https://medium.com/@jasonkgoodman?source=post_page", "https://medium.com/@gary.tang_94319?source=post_page", "https://medium.com/@lpettingill?source=post_page", "https://medium.com/@vquoss?source=post_page", "https://medium.com/@eddie.santos.3?source=post_page", "https://medium.com/@NicholasHandel?source=post_page", "https://medium.com/tag/machine-learning?source=post", "https://medium.com/tag/data-science?source=post", "https://medium.com/tag/airbnb?source=post", "https://medium.com/tag/technology?source=post", "https://medium.com/@rchang?source=footer_card", "https://medium.com/@rchang", "http://twitter.com/Airbnb", "http://twitter.com/Twitter", "https://medium.com/airbnb-engineering?source=footer_card", "https://medium.com/airbnb-engineering?source=footer_card", "http://airbnb.io", "https://medium.com/airbnb-engineering", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "150": {"url": "https://www.ayasdi.com/blog/artificial-intelligence/why_ai_is_the_new_bigdata/", "title": "The \u201cBig Data-ization\u201d of Artificial Intelligence", "text": "\u2039 Back to Blog  Artificial Intelligence, Data, Machine Intelligence, Machine Learning It seems like it was only a few years ago that the term \u201cbig data\u201d went from a promising area of research and interest to something so ubiquitous that it lost all meaning, descending ultimately into the butt of jokes.  As everyone piled onto the big data bandwagon, it became impossible to separate truth from fiction. Every executive and entrepreneur that I ran into was doing a \u201cbig data\u201d thing. I recall meeting someone whose company shot videos for enterprise customers and was pitching it as a \u2018Big Data play\u2019 \u2013 because video files, you know, are huge \u2013 they take up lots of space.  Thankfully, the noise associated with \u201cbig data\u201d is abating as sophistication and common sense take hold. In fact, in many circles the term actually exposes the user as someone who doesn\u2019t really understand the space.  Unfortunately, the same malady has now afflicted AI. Everyone I meet is doing an \u2018AI play\u2019 \u2013 even if all they did was to build a simple linear regressor in Excel.  AI is, unfortunately, the new \u201cbig data.\u201d While not good, it is not all bad either.  After all, the data ecosystem benefited from all of the \u201cbig data\u201d attention and investment \u2013 creating some amazing software and producing some exceptional productivity gains.  The same will happen with AI \u2013 with the increased attention comes investment dollars which in turn will drive adoption \u2013 enhancing the ecosystem. Having said that we need to stop calling regression on excel AI \u2013 it\u2019s ridiculous and undermines some incredible work being done in the space.  Regression has been around for 200+ years. Gauss and Legendre didn\u2019t don the AI cloak when they discovered regression. Neither should the recent stats graduate trying to raise money for his/her startup.  I will tell you what I think qualifies as AI in a moment \u2013 but here are some thoughts that influence that discussion. First, the AI definition I use is focused on the narrow, application specific AI, not the more general problem of artificial general intelligence (AGI) where simulating a person using software is the equivalent of intelligence.  Second, the vast, vast majority of the data that exists in the world is unlabeled. It is not practical to label that data manually and doing so would likely create bias anyway. One can argue that the Internet as a whole is simply a mechanism for humans to entertain ourselves while providing label data for machines.  Unlabeled data presents a different challenge, one we will address shortly, but the key point here is that it is everywhere and represents the key to extracting business value (or any value).  Third, we are not producing data scientists at a rate that can keep pace with the growth of data. Even with the moniker as a the \u201csexiest job of the 21st century\u201d the pace at which data scientists are created doesn\u2019t begin to approach the growth rate we are seeing in data.  Fourth, data scientists, for the most part, are not UX designers or product managers or, in many cases even engineers. As a result, the subject matter experts, those that sit in the business, don\u2019t have effective interfaces to the data science outputs. The interfaces that they have \u2013 powerpoint, excel, or PDF reports have limited utility in transforming the behavior of a company. What is required is something to shape behavior is something more \u2013 applications.  So what does qualify as intelligence? In \u2018On Intelligence\u2019 Jeff Hawkins says that all AI boils down to memory and prediction. The argument is very persuasive, but a bit reductive. We have a slightly different take for what an AI should display and it encompasses a framework: Here are characteristics that I think any AI should display. While some of these elements may seem self-evident that is because they are taken as a single item. Intelligence has a broader context. All the elements must work in conjunction with each other to qualify as AI. \u00a0\u00a0\u00a0 The five elements are: Let\u2019s take each of these concepts in turn. Discovery is the ability of an intelligent system to learn from data without upfront human intervention. Often, this needs to be done without being presented with an explicit target. It relies on the use of unsupervised and semi-supervised machine learning techniques (such as segmentation, dimensionality reduction, anomaly detection, etc.), as well as more supervised techniques where there is an outcome or there are several outcomes of interest. Usually, in enterprise software, the term discovery refers to the ability of ETL/MDM solutions to discover the various schemas of tables in large databases and automatically find join keys etc. This is not what we mean by discovery. We use of the term very differently and has this has important implications. In complex datasets, it is nearly impossible to ask the \u201cright\u201d questions. To discover what value lies within the data one must \u00a0understand all the relationships that are inherent and important in the data. That requires a principled approach to hypothesis generation. \u00a0 One technique, topological data analysis (TDA), is exceptional at surfacing hidden relationships that exist in the data and identifying those relationships that are meaningful without having to ask specific questions of the data. The result is an output that is able to represent complex phenomena, and is therefore able to surface weaker signals as well as the stronger signals. \u00a0 This permits the detection of emergent phenomena. \u00a0 As a result, enterprises can now discover answers to questions they didn\u2019t even know to ask and do so with data that is unlabeled. Once the data set is understood through intelligent discovery, supervised approaches are applied to predict what will happen in the future. These types of problems include classification, regression and ranking.  For this pillar, most companies use a standard set of supervised machine learning algorithms including random forests, gradient boosting, linear/sparse learners. It should be noted, however, that the unsupervised work from the previous step is highly useful in many ways. For example, it can generate relevant features for use in prediction tasks or finding local patches of data where supervised algorithms may struggle (systematic errors). The predict phase is an important part of the business value associated with data science, however, generally, in predictive analytics, there exists a notion that this is the sum total of machine learning.  This is not the case by far.  Prediction, while important, is pretty well understood and does not, on its own qualify as \u201cintelligence.\u201d \u00a0It goes back to calling a maxed out Excel table and a linear regressor AI. It is just not the case. Further, Prediction can go wrong along a number of dimensions, particularly if the groups on which you are predicting are racked with some type of bias (algorithmic, sampling etc.)  Again, Prediction is key and provides tremendous business value done correctly, but in and of itself it is not AI. We need to stop calling it as such. Applications need to support interaction with humans in a way which makes outcomes recognizable and believable. For example, when one builds a predictive model, it is important to have an explanation of how the model is doing what it is doing, i.e. what the features in the model are doing in terms that are familiar to the users of the model. \u00a0This level of familiarity is important in generating trust and intuition.  Similarly, in the same way that automobiles have mechanisms not just for detecting the presence of a malfunction, but also for specifying the nature of the malfunction and suggesting a method for correcting it, so one needs to have a \u201cnuts and bolts\u201d understanding of how an application is working in order to \u201crepair\u201d it when it goes awry. \u00a0 Transparency AND Justification. There is a difference. Transparency tells you what algorithms and parameters were used, while, Justification tells you why. For intelligence to be meaningful, it must be able to justify and explain its assertions, as well as to be able to diagnose failures.  No business leader should deploy intelligent and autonomous applications against critical business problems without a thorough understanding of what variables power the model.  Enterprises cannot move to a model of intelligent applications without trust and transparency. \u00a0 AI without UX is of limited utility.  UX is what distributes that intelligence across the organization and pushes it to the edge \u2013 where it can consumed by practitioners and subject matter experts.  Ultimately, the process of operationalizing an intelligent application within the enterprise requires some change in the organization, an acceptance that the application will evolve over time and that will demand downstream changes \u2013 automated or otherwise.  \u00a0For this to happen, intelligent applications need to be \u201clive\u201d in the business process, seeing new data and automatically executing the loop of Discover, Predict, Justify on a frequency that makes sense for that business process. For some processes that may be quarterly, for others daily. That loop can even be measured in seconds. \u00a0 Intelligent systems are designed to detect and react as the data evolves. An intelligent system is one that is always learning, live in the workflow and constantly improving. \u00a0In the modern data world, an application that is not getting more intelligent is getting dumber.  Intelligent applications are designed to detect and react when data distributions evolve. As a result, they need to be \u201con the wire\u201d in order to detect that phenomena before it becomes a problem. Too many solutions provide an answer in a point of time, an intelligent system is one that is always learning through the framework outlined here. This is what defines intelligence \u2013 not a machine learning algorithm kicking out predictions or the results of a data scientist\u2019s work as a PDF. For the industry to continue to grow and evolve we need to start doing a better job of recognizing what is truly AI and what is imposter-ware.  Let\u2019s make it a point of celebrating the real AI work.", "links": ["https://www.ayasdi.com", "https://www.ayasdi.com/platform/", "https://www.ayasdi.com/industry-overview/", "https://www.ayasdi.com/financial-services/", "https://www.ayasdi.com/healthcare/", "https://www.ayasdi.com/public-sector/", "https://www.ayasdi.com/solutions/anti-money-laundering/", "https://www.ayasdi.com/solutions/clinical-variation-management/", "https://www.ayasdi.com/solutions/denials/", "https://www.ayasdi.com/solutions/regulatory-risk/", "https://www.ayasdi.com/solutions/population-health/", "https://www.ayasdi.com/company/", "https://www.ayasdi.com/company/leadership/", "https://www.ayasdi.com/company/careers/", "https://www.ayasdi.com/company/collaborators/", "https://www.ayasdi.com/company/customers/", "https://www.ayasdi.com/company/news-and-events/", "https://www.ayasdi.com/company/contact-us/", "https://www.ayasdi.com/blog/", "https://www.ayasdi.com/request-a-demo/", "https://www.ayasdi.com/blog/author/gurjeet-singh/", "https://www.ayasdi.com/blog/category/artificial-intelligence/", "https://www.ayasdi.com/blog/category/bigdata/", "https://www.ayasdi.com/blog/category/machine-intelligence/", "https://www.ayasdi.com/blog/category/machine-learning/", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/08/02144138/BG2.png", "https://priceonomics.com/the-discovery-of-statistical-regression/", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/05/12132340/Machine_Intelligence_Apps_WP_051617v01.pdf", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/08/02144141/DS1.png", "https://www.wired.com/brandlab/2015/05/jeff-hawkins-firing-silicon-brain/", "http://cdn.ayasdi.com/wp-content/uploads/2015/02/wp-tda-and-machine-learning.pdf", "https://www.ayasdi.com/blog/aml/intelligent-segmentation-as-the-attack-point-for-aml/", "http://feedburner.google.com/fb/a/mailverify?uri=AyasdiBlog", "https://ayasdicommunity.force.com/login", "https://www.ayasdi.com/terms-of-service/", "https://www.ayasdi.com/privacy-policy/", "https://www.ayasdi.com/pilot-terms/", "http://www.youtube.com/user/ayasdi", "http://feeds.feedburner.com/AyasdiBlog", "http://www.linkedin.com/company/ayasdi", "https://www.facebook.com/ayasdi", "https://twitter.com/ayasdi"]}, "151": {"url": "https://www.mapd.com/blog/2017/08/03/whos-got-the-best-mobile-network-in-the-us/", "title": "", "text": "Based on the deluge of commercials and ads in this competitive industry, it seems every major player claims that they do. The competition is apparently so fierce that even their spokesmen are being headhunted. Admittedly, it can be a bit confusing when you find these results in one city:  And these in another:  By the way, how do you even define \u2018best\u2019 network? It\u2019s a loaded question for sure. We\u2019ll get back to that in a minute. It\u2019s a question with relevance to nearly every American, considering the vast majority of us own a cellphone of some kind (over 95% according to Pew). And we don\u2019t leave them alone; we touch them thousands of times each day. In fact, a good mobile network can affect our very quality of life. Ever travelled any significant distance with children and poor mobile data speeds? The same question is also relevant to those very telecommunications providers themselves, of course, and particularly in light of the fact that obtaining new subscribers will increasingly mean winning them away from competitors. Ensuring you are outpacing your rivals with superior network metrics is a key ingredient to both marketing and market share success. Our friends and partners at Tutela have created the world\u2019s largest mobile information platform, collecting billions of crowdsourced data points anonymously. These span device, network, and application information to allow us to get from data to insights. They\u2019ve graciously given us (and now you) a slice of their treasure trove. Our demo contains a few months of US data (~112M rows), although Tutela has data for more metrics, for nearly every country, and for a much greater timeframe. There are obviously many variables to consider when it comes to a great network, and one carrier will almost certainly not take the top spot in each category. But we can identify areas of significance where we hope to find favorable results: Spoiler alert: we\u2019re not actually going to attempt to find a winner. Those are just some of the metrics you can see at a glance on our MapD Immerse demo dashboard:  This is immediately useful. Coverage, speed, signal strength, and latency all matter, so MapD allows you to visualize and correlate them for effective consumption. Not coincidentally, the MapD Immerse visualization tool is purpose-built to leverage the MapD Core backend GPU-powered, in-memory, columnar SQL database at scale. This allows  interactive analysis over multi-billion row data sets, with the ability to drill down to an individual row...in milliseconds. It\u2019s also why Tutela has selected to partner with MapD; no need for indexing, sampling, or pre-computation. This means you can zoom in and out, filter, and more in an ad hoc fashion. Let\u2019s take it for a spin. As a consumer, you might be interested in how well each service provider performs in a certain geographic region (i.e., where you live and work) to aid in making a purchasing decision.  For example, to zoom from country level to the Washington, DC area, just type the city name into the searchbox of the pointmap. MapD Immerse will automatically zoom in when you click enter. It looks like in this region, you might lean toward Verizon:  But it may not be as clear cut if you live in San Francisco:  In either case, Sprint better have some compelling discounts or incentives to entice new subscribers. Further, both Sprint and T-Mobile might leverage this information to prioritize buildout or upgrade of cell towers. If they\u2019re analyzing subscriber loss in these areas, this also may give them insight into why that could be occurring. Another valuable feature of the MapD platform is the ability to easily identify trends, anomalies, or outliers. Visualization can often be an invaluable enabler for these situations. Sometimes new revenue opportunities or cost savings can be had by capitalizing on correlations in data that you didn\u2019t even realize existed! See the spike in data downloads in the line chart? By highlighting the time frame with your mouse, MapD will cross filter all charts using the selected time slice. Doing the same with the line chart for data uploads further reveals that the largest spikes occur on May 28 and 30. Ah, that\u2019s Memorial Day weekend in the United States. So this makes sense.  We can get even more granular and note that the bulk of data transfer occurred between 12-1am local time (4-5am UTC) as the heatmap indicates. I wonder if Uber and Lyft also experienced a spike around that time, as celebrations came to an end?  When data exploration at scale is so easy, what other interesting insights will you find? Explore this demo using Tutela's dataset on MapD\u2019s platform on your own here.  If you\u2019d like to learn more about MapD\u2019s recently announced partnership with Tutela, watch our on-demand webinar, introducing and demonstrating Tutela Explorer, a new mobile data analytics solution that provides real-time, interactive and highly visual insight into the performance of mobile networks and device usage. Tutela Explorer is available now as-a-service with global data coverage from Tutela.", "links": ["http://www.mapd.com/blog", "https://www.nytimes.com/2016/10/15/business/sprint-verizon-hear-me-now-paul-marcarelli.html", "http://www.pewinternet.org/fact-sheet/mobile/", "http://www.networkworld.com/article/3092446/smartphones/we-touch-our-phones-2617-times-a-day-says-study.html", "https://www.tutela.com/explorer/", "https://www.mapd.com/demos/telecom", "https://www.mapd.com/demos/telecom", "https://www.mapd.com/platform/core/", "https://www.mapd.com/demos/telecom", "https://www.brighttalk.com/webcast/14525/267851", "https://www.tutela.com/explorer"]}, "152": {"url": "https://www.oreilly.com/learning/building-a-simple-graphql-server-with-neo4j", "title": "Building a simple GraphQL server with Neo4j", "text": "How to implement a GraphQL API that queries Neo4j for a simple movie app. If you're interested in making the move from REST to GraphQL, check out this OSCON video from the team at Github. GraphQL is a powerful new tool for building APIs that allows clients to ask for only the data they need. Originally designed at Facebook to minimize data sent over the wire and reduce round-trip API requests for rendering views in native mobile apps, GraphQL has since been open sourced to a healthy community that is building developer tools. There are also a number of large companies and startups such as GitHub, Yelp, Coursera, Shopify, and Mattermark building public and internal GraphQL APIs. Despite what the name seems to imply, GraphQL is not a query language for graph databases, it is instead an API query language and runtime for building APIs. The \u201cGraph\u201d component of the name comes from the graph data model that GraphQL uses in the frontend. GraphQL itself is simply a specification, and there are many great tools available for building GraphQL APIs in almost every language. In this post we'll make use of graphql-tools by Apollo to build a simple GraphQL API in JavaScript that queries a Neo4j graph database for movies and movie recommendations. We will follow a recipe approach: first, exploring the problem in more detail, then developing our solution, and finally we discuss our approach. Good resources for learning more about GraphQL are GraphQL.org and the Apollo Dev Blog. GraphQL by design can work with any database or backend system, however in this example we'll be using the Neo4j graph database as our persistence layer. Why use a graph database with GraphQL? The idea of application data as a graph is an underpinning design choice of GraphQL. For example, think of customers who have placed orders that contain products\u2014that's a graph! GraphQL enables developers to translate their backend data into the application data graph on the frontend, but if we use a graph database on the backend we can do away with this impedance mismatch and we have graphs all the way down. We'd like to build a simple GraphQL API for a movie app that can do two things: In a traditional REST-ish API approach we might create two separate endpoints, perhaps /movies/search and /movies/similar. One endpoint to search for movies by substring of the title, and another to return a list of similar movies. As we add additional features we might keep adding endpoints. With GraphQL, our API is served from a single endpoint /graphql that takes one or more GraphQL queries and returns JSON data in a shape that is specified by the GraphQL query - only the data requested by the client is returned. Let's see how we can build this simple API. We'll be building a simple node.js JavaScript web server using Express.js to serve our GraphQL endpoint1. First of all we\u2019ll need a Neo4j database (with data) for our GraphQL server to query. For this example we\u2019ll make use of a Neo4j Sandbox instance. Neo4j Sandbox allows us to quickly spin up a hosted Neo4j instance, optionally with existing datasets focused around specific use cases. We\u2019ll use the Recommendations Neo4j Sandbox which includes data about movies and movie reviews and is designed to be used for generating personalized recommendations (for example, by using collaborative filtering to recommend movies based on similar users\u2019 reviews). We'll be making use of a few dependencies for this GraphQL server. I won't list them all here (you can see the full list in the package.json for this project, but there a few worth noting: We\u2019ll follow the \u201cGraphQL First\u201d development paradigm. In this approach, we start by defining a GraphQL schema. This schema defines the types and queries available in our API and then becomes the specification for the API. If we were building a complete application, the frontend developers could use this schema to build out the frontend while the backend team builds the backend in parallel, speeding development. Once we\u2019ve defined our schema we\u2019ll need to create resolver functions that are responsible for fetching data from Neo4j. This schema will define the types and GraphQL queries that we'll be able to use in our API. You can think of the schema as the API blueprint. schema.js Each GraphQL field is resolved by a resolver function. The resolver function defines how data is fetched for that field. resolvers.js Now that we've defined the GraphQL schema and resolver functions we are ready to serve the GraphQL endpoint, using Express.js. server.js If you've ever used Express this should look familiar, you'll notice that we're creating two endpoints and serving them on localhost:8080 Let's take a look at what we just did: Now that we have our API running, let's use the GraphiQL in-browser IDE for GraphQL to search for movies whose titles contain \"Matrix\" and find similar movies for each matching movie. We can load GraphiQL by opening http://loalhost:8080/graphiql in our browser and querying with this GraphQL query:   Querying our GraphQL movies API using GraphiQL, the in-browser IDE for GraphQL GraphQL is still new to almost everyone with the exception of Facebook, where it has been used since 2012, so many conventions and best practices are still being developed. However, GraphQL First Development is a philosophy the community has adopted which gives structure to the process of building a GraphQL API. By defining a contract for the API, expressed as a GraphQL schema, the frontend and backend developer teams can independently implement their applications, using the schema as a guide. To make GraphQL First Development even easier with Neo4j, the Neo4j team has built neo4j-graphql-cli, a command line tool for easily spinning up a Neo4j backed GraphQL endpoint based on a user defined GraphQL schema that can optionally be annotated with Cypher queries - exposing the power of Cypher from within GraphQL. You can learn more about this here. 1 All code for this project is available on GitHub . This example is also available as an Apollo Launchpad Pad, which you can run live in the browser.\u21a9 William Lyon is an engineer on the Developer Relations team at Neo4j, the open source graph database, where he builds tools for integrating Neo4j with other technologies and helps users be successful with graphs. He also leads Neo4j's Data Journalism Accelerator Program. Prior to Neo4j, he worked as a software engineer for a variety of startups, building mobile apps for the real estate industry, trading tools for quantitative finance, and predictive APIs. William holds a masters degree in Computer Science from the University of Montana. You can... How to package up your own repositories for use via PHP's Composer. Will content-blocking change the Web? Learn how to use Node.js and Express.js to quickly bootstrap a web server, tie in MongoDB, and display it all using dynamic HTML templates Diversity and inclusion in the tech industry has experienced a severe failure. Erica Baker approaches the topic like we would for any other service failure: with a postmortem. \u00a9 2017 O'Reilly Media, Inc. All trademarks and registered trademarks appearing on oreilly.com are the property of their respective owners.", "links": ["https://www.safaribooksonline.com/home/?utm_medium=content&utm_source=oreilly.com&utm_campaign=lgen&utm_content=20170601+nav", "http://www.oreilly.com/conferences/", "http://shop.oreilly.com/", "https://www.safaribooksonline.com/home/?utm_medium=content&utm_source=oreilly.com&utm_campaign=lgen&utm_content=20170601+nav", "http://www.oreilly.com/conferences/", "http://shop.oreilly.com/", "https://pixabay.com/en/louvre-pyramid-mesh-perspective-2189967/", "https://www.safaribooksonline.com/library/view/oscon-2017-/9781491976227/video306664.html?utm_source=oreilly&utm_medium=newsite&utm_campaign=building-a-simple-graphql-server-with-neo4j", "https://github.com/apollographql/graphql-tools", "http://www.apollodata.com/", "http://graphql.org/", "https://dev-blog.apollodata.com/", "https://neo4j.com/sandbox-v2/", "https://github.com/johnymontana/movies-graphql-neo4j-server/blob/master/package.json", "http://dev.apollodata.com/tools/graphql-tools/index.html", "https://expressjs.com/", "https://github.com/neo4j/neo4j-javascript-driver", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://www.graph.cool/docs/faq/graphql-sdl-schema-definition-language-kr84dktnp0/", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "http://loalhost:8080/graphiql", "https://www.npmjs.com/package/neo4j-graphql-cli", "http://neo4j.com/developer/graphql", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://pixabay.com/en/louvre-pyramid-mesh-perspective-2189967/", "https://twitter.com/share", "http://oreilly.com/about/", "http://oreilly.com/work-with-us.html", "http://oreilly.com/careers/", "http://shop.oreilly.com/category/customer-service.do", "http://shop.oreilly.com/category/customer-service.do", "http://fb.co/OReilly", "http://twitter.com/oreillymedia", "https://www.youtube.com/user/OreillyMedia", "https://plus.google.com/+oreillymedia", "https://www.linkedin.com/company/oreilly-media", "http://oreilly.com/terms/", "http://oreilly.com/privacy.html", "http://www.oreilly.com/about/editorial_independence.html"]}, "153": {"url": "http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "title": "", "text": "", "links": ["http://www.getrevue.co/?ref=Revue+Profile", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profileimage&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profiledate&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profilename&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lXPZA?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lXPZA?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blogs.technet.microsoft.com/machinelearning/2017/07/25/lessons-learned-benchmarking-fast-machine-learning-algorithms/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/DM9WJ?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/DM9WJ?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://medium.com/@stathis/design-by-evolution-393e41863f98?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/XYWKo?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/XYWKo?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.datasciencelearner.com/complete-overview-learning-python-data-analysis/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/6nba0?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/6nba0?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://reinforce.io/blog/introduction-to-tensorforce/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xekd3?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xekd3?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.kdnuggets.com/2017/05/top-10-machine-learning-videos-on-youtube-updated.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Ka8JK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Ka8JK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://arxiv.org/abs/1704.01568?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/96KED?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/96KED?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://datascienceplus.com/random-forests-in-r/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Vd1wK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Vd1wK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blog.keras.io/the-future-of-deep-learning.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/eKr5e?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/eKr5e?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://medium.com/airbnb-engineering/using-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/odRNK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/odRNK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://machinelearning.apple.com/2017/07/07/GAN.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xekoa?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xekoa?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://arxiv.org/abs/1707.06642?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/mxRw2?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/mxRw2?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.techleer.com/articles/200-naive-bayes-machine-learning-algorithm-for-classification-problems/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/3K1nx?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/3K1nx?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463?gi=7fe02170b69e&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/OnXK7?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/OnXK7?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://aiukraine.com/en/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/nqRVq?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/nqRVq?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.bonaccorso.eu/2017/07/23/machine-learning-algorithms/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Z4bBj?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Z4bBj?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/jobs", "http://bit.ly/2uqHV4J", "https://flyelephant.net/jobs/Spinbackup-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/jobs/Spinbackup-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/yJRmd?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/yJRmd?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/jobs/Snap-Ukraine-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/jobs/Snap-Ukraine-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/jAR3B?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/jAR3B?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%238%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "https://www.getrevue.co/?utm_source=Data Science Digest&utm_medium=email&utm_content=footerlink&utm_campaign=Issue", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=forwarded&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%238%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "http://www.getrevue.co/?ref=Revue+Profile"]}, "154": {"url": "https://blog.insightdatascience.com/preparing-for-the-transition-to-applied-ai-d41e48403447", "title": "Transitioning from Software Engineering to Artificial Intelligence", "text": "Emmanuel Ameisen and Jeremy Karnowski, Insight AI A significant part of the Software Engineer role requires staying up-to-date with evolving frameworks, standards, and paradigms. Software Engineers strive to constantly learn, in order to always use the best tool for the job. As Machine Learning finds footholds in more applications every day, it has naturally become a topic that many Engineers want to master. Machine Learning, though, is harder to pick up than a new framework. To be an efficient practitioner, you require a solid understanding of the theory of the field, broad knowledge of the current state of the art, and an ability to frame problems in a non deterministic way. Many guides you can find online will simply teach you how to train an out-of-the-box model on a curated data set to achieve good accuracy and call it a day. The truth is that a much more extensive skillset is essential in becoming an effective Machine Learning Engineer. Below is a distillation of the many conversations we\u2019ve had with over 50 top Machine Learning teams all over The Bay Area and New York, who\u2019ve come to Insight to find AI Practitioners poised to tackle their problems and accelerate their expansion into Applied AI. Deploying a Machine Learning solution requires much more than just training an arbitrary model on your data. It requires an understanding of: In other words, in addition to engineering chops, you need to understand the fundamentals of statistics, linear algebra, and optimization theory in order to integrate, deploy, and debug models. Building a custom Machine Learning solution for a problem requires that you consider issues ranging from acquiring, labeling and pre-processing your data to building, updating, and serving an inference model, and everything in between. Finally, building a REST API for a standard web-app is a task that we can deem feasible ahead of time. Machine Learning models, on the other hand are not always guaranteed to converge, or produce usable outputs. The best way to learn how to scope and deliver impactful Machine Learning products, is to understand how their theoretical underpinnings relate to the taxonomy of your data. In order to understand Machine Learning, a solid knowledge of statistics fundamentals is essential. This involves understanding the following: When you are training a neural network, what is actually happening? What makes some tasks doable and others not? A good approach to this might be to first try to understand Machine Learning through graphics and examples, before diving deeper into the theory. Concepts to understand range from how different loss functions work, why back propagation is useful, or what a computational graph is. A deep understanding is crucial both for building a functional model, and to communicate about it efficiently to the rest of the organization. Following are a few resources, starting with high level overviews, and diving deeper. Another fundamental skill is the ability to read, understand and implement research papers. It can seem like a daunting task at first, so a good way to start is to look up a paper that already has code attached to it (on GitXiv for example) and try to understand the implementation in depth. Ask any Data Scientist and they\u2019ll tell you 90% of the work they do is data munging. This is just as important for Applied AI, as the success of your model correlates hugely with the quality (and quantity) of your data. Data work comes in many aspects, and falls within a few categories: The best way to get familiar with data wrangling is to grab a dataset in the wild and try to use it. There are many datasets online and many social media and news outlets sites have great APIs. Following the steps above, a good way to learn is to: Debugging Machine Learning algorithms that fail to converge or to give sensible results involves a very different process from debugging code. In the same vein, finding the right architecture and hyperparameters requires solid theoretical fundamentals, but also good infrastructure work to be able to test different configurations out. Because of the pace at which the fields evolve, the methods to debug models are constantly evolving. Here are a few \u201csanity checks\u201d from our discussions and experience deploying models that mirror in some ways the principles of KISS familiar to many Software Engineers. A lot of those steps can be accelerated significantly by your development skills, which brings us to our last skill. A lot of Applied Machine Learning will allow you to leverage Software Engineering skills, sometimes with a little twist. These skills include: For more details on some of the software skills we recommend acquiring to become a quality Machine Learning Engineer, check out our post dedicated to transitioning to Applied AI from Academia. The resources above will help you approach and tackle actual Machine Learning problems. But the field of Applied AI changes extremely quickly, and the best way to learn, is to get your hands dirty and actually try to build out an end-to-end solution to solve a real problem. Action Items: Find some inspiration, then dive in! Remember that while Machine Learning Engineering is about building products at heart, there is a research aspect to it. You will explore models and paradigms that will prove unsuccessful, and that is perfectly fine, as it will lead you to understand the intricacies of the problem better. AI is an exciting, ever-changing field. The demand for Machine Learning Engineers is strong, and it is easy to get overwhelmed with the amount of news surrounding the topic. We recommend following a few serious sources and newsletters, to be able to separate PR and abstract research from innovations that are immediately relevant to the field. Here are some sources to help out: Want to learn applied Artificial Intelligence from top professionals in Silicon Valley or New York? Learn more about the Artificial Intelligence program.", "links": ["https://blog.insightdatascience.com?source=logo-lo_2294ebd12772---d02e65779d7b", "https://twitter.com/InsightDataSci", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.insightdatascience.com%2Fpreparing-for-the-transition-to-applied-ai-d41e48403447", "https://blog.insightdatascience.com", "https://blog.insightdatascience.com/tagged/about-insight", "https://blog.insightdatascience.com/tagged/insight-data-science", "https://blog.insightdatascience.com/tagged/insight-data-engineering", "https://blog.insightdatascience.com/tagged/insight-health-data", "https://blog.insightdatascience.com/tagged/insight-ai", "https://blog.insightdatascience.com/search", "https://blog.insightdatascience.com/@emmanuelameisen?source=post_header_lockup", "https://blog.insightdatascience.com/@emmanuelameisen?source=post_header_lockup", "https://www.udacity.com/course/deep-learning--ud730", "http://cs231n.stanford.edu/", "http://cs224d.stanford.edu/", "http://www.deeplearningbook.org/", "http://www.gitxiv.com/", "https://data.sfgov.org/", "https://www.data.gov/", "https://dev.twitter.com/streaming/public", "http://developer.nytimes.com/docs", "http://people.apache.org/~fhanik/kiss.html", "http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf", "https://github.com/JasperSnoek/spearmint", "https://blog.insightdatascience.com/preparing-for-the-transition-to-applied-ai-8eaf53624079", "https://news.ycombinator.com/", "https://jack-clark.net/import-ai/", "http://www.insightdatascience.com/blog/", "https://blog.insightdatascience.com/tagged/insight-ai", "https://blog.insightdatascience.com/using-deep-learning-to-reconstruct-high-resolution-audio-29deee8b7ccd", "https://blog.insightdatascience.com/separating-overlapping-chromosomes-with-deep-learning-based-image-segmentation-22f97afd3283", "http://insightdata.ai/notifications-list", "http:insightdata.ai", "https://medium.com/@jkarnows?source=post_page", "https://medium.com/@InsightData?source=post_page", "https://medium.com/@rossfadely?source=post_page", "https://blog.insightdatascience.com/tagged/machine-learning?source=post", "https://blog.insightdatascience.com/tagged/ai?source=post", "https://blog.insightdatascience.com/tagged/software-engineering?source=post", "https://blog.insightdatascience.com/tagged/insight-ai?source=post", "https://blog.insightdatascience.com/tagged/deep-learning?source=post", "https://blog.insightdatascience.com/@emmanuelameisen?source=footer_card", "https://blog.insightdatascience.com/@emmanuelameisen", "http://twitter.com/EmmanuelAmeisen", "https://blog.insightdatascience.com?source=footer_card", "https://blog.insightdatascience.com?source=footer_card", "https://blog.insightdatascience.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "155": {"url": "http://vooban.com/en/tips-articles-geek-stuff/hyperopt-tutorial-for-optimizing-neural-networks-hyperparameters/", "title": "", "text": "Hyperopt is a way to search through an hyperparameter space. For example, it can use the Tree-structured Parzen Estimator (TPE) algorithm, which explore intelligently the search space while narrowing down to the estimated best parameters. It is hence a good method for meta-optimizing a neural network which is itself an optimisation problem: tuning a neural network uses gradient descent methods, and tuning the hyperparameters needs to be done differently since gradient descent can\u2019t apply. Therefore, Hyperopt can be useful not only for tuning hyperparameters such as the learning rate, but also to tune more fancy parameters in a flexible way, such as changing the number of layers of certain types, or the number of neurons in a layer, or even the type of layer to use at a certain place in the network given an array of choices, each with nested tunable hyperparameters. This is an oriented random search, in contrast with a Grid Search where hyperparameters are pre-established with fixed steps increase. Random Search for Hyper-Parameter Optimization (such as what Hyperopt do) has proven to be an effective search technique. The paper about this technique sits among the most cited deep learning papers. To sum up, it is more efficient to search randomly through values and to intelligently narrow the search space rather than looping on fixed sets of values for the hyperparameters. Note that this blog post is also available as a Notebook on GitHub. It contains code that can be run with Jupyter. A parameter is defined with a certain uniformrange or else a probability distribution, such as: There is also a few quantized versions of those functions, which rounds the generated values at each step of \u201cq\u201d: It is also possible to use a \u201cchoice\u201d which can lead to hyperparameter nesting: Visualisations of the parameters for probability distributions can be found below. Then, more details on choices and parameter nesting will come. Note on the above charts (especially for the loguniform and uniform distributions): the blurred line averaging the values fades out toward the ends of the signal since it is zero-padded. The line ideally would not fade out by using techniques such as mirror-padding. Those are the best distributions for modeling the values a learning rate. That\u2019s because we want to observe changes in the learning rate according to changing it with multiplications rather than additions, e.g.: when adjusting the learning rate, we\u2019ll want to try to divide it or multiply it by 2 rather than adding and substracting a finite value. To proove this, let\u2019s generate a loguniform distribution for a multiplier of the learning rate, centered at 1.0. Dividing 1 by those values should yield the same distribution. Let\u2019s now define a simple search space and solve for f(x) = x^2 - x + 1, where x is an hyperparameter. Let\u2019s solve for minimizing f(x, y) = x^2 + y^2 using a space using a python dict as structure. Later, this will neable us to nest hyperparameters with choices in a clean way. Yet, we have defined spaces as a single parameter. But that is 1D. Normally, spaces contain many parameters. Let\u2019s define a more complex one and with one nested hyperparameter choice for an uniform float: This will require us to import a few more things, and return the results with a dict that has a \u201cstatus\u201d and \u201closs\u201d key at least. Let\u2019s keep in our return dict the evaluated space too as this may come in handy if we save results to disk. Note that the optimization could be parallelized by using MongoDB and storing the trials\u2019 state here. Althought this is a built-in feature of hyperopt, let\u2019s keep things simple for our examples here. Indeed, the TPE algorithm used by the fmin function has state which is stored in the trials and which is useful to narrow the search space dynamically once we have a few trials. It is then interesting to pause and resume a training, and to apply that to a real problem. This is what\u2019s done inside the hyperopt_optimize.py file of the GitHub repository for this project. There, as an example, we optimize a convolutional neural network for solving the CIFAR-100 problem. Star Fork You might as well like this other blog post of mine on how to use Git Large File Storage (Git LFS) to handle the versioning of huge files when working with machine learning projects. 510-1015 av. Wilfrid-Pelletier  Quebec, QC, Canada  G1W 0C4 418 800.0027 TOLL FREE: 1 844 800.0027 info@vooban.com", "links": ["http://vooban.com/en/", "http://vooban.com/en/", "http://vooban.com/en/", "http://vooban.com/en/products/", "http://vooban.com/en/products/#tpti", "http://vooban.com/en/products/#tdti", "http://vooban.com/en/approach/", "http://vooban.com/en/tactical-squad/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/", "http://vooban.com/en/tips-articles-geek-stuff/", "https://pdfs.semanticscholar.org/9f2a/efc3821853e963beda011ed770f740385b77.pdf", "http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a", "https://github.com/terryum/awesome-deep-learning-papers#optimization--training-techniques", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100/blob/master/IntroductionToHyperopt.ipynb", "http://jupyter.org/", "http://vooban.com/wp-content/uploads/2017/08/histogram_1_int.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_2_uniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_3_loguniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_4_normal.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_5_lognormal.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_6_inverse_loguniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_7_inverse_lognormal.png", "http://vooban.com/wp-content/uploads/2017/08/fx2.png", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100/fork", "http://vooban.com/en/tips-articles-geek-stuff/migrating-to-git-lfs-for-developing-deep-learning-applications-with-large-files/", "http://vooban.com/en/tips-articles-geek-stuff/discover-structure-behind-data-with-decision-trees/", "http://vooban.com/en/tips-articles-geek-stuff/", "https://www.google.ca/maps/place/Vooban/@46.7694849,-71.3055195,17z/data=!3m1!4b1!4m5!3m4!1s0x4cb890d721a9234b:0x979022270b4a9fa4!8m2!3d46.7694849!4d-71.3033308", "http://vooban.com/en/products/", "http://vooban.com/en/approach/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/", "https://twitter.com/vooban", "https://www.instagram.com/voobanvibe/", "https://www.facebook.com/vooban", "https://www.linkedin.com/company/vooban", "http://vooban.com/en/", "http://vooban.com/en/products/", "http://vooban.com/en/products/#tpti", "http://vooban.com/en/products/#tdti", "http://vooban.com/en/approach/", "http://vooban.com/en/tactical-squad/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/"]}, "156": {"url": "https://elitedatascience.com/learn-python-for-data-science", "title": "How to Learn Python for Data Science in 2017 (Updated)", "text": "In this guide, we\u2019ll cover how to learn Python for data science, including our favorite curriculum for self-study. You see, data science is about problem solving, exploration, and extracting valuable information from data. To do so effectively, you\u2019ll need to wrangle\u00a0datasets, train machine learning models, visualize results, and much more. Enter Python. This is the best time ever to learn Python. In fact, Forbes named it a top 10 technical skill in terms of job demand growth. Let\u2019s discuss why\u2026 Python is one of the most widespread languages in the world, and it has a passionate community of users: Python Popularity, TIOBE Index It has an even more loyal following within the data science profession. Some people judge the quality of a programming language by the simplicity of its \"hello, world!\" program. Python does pretty well by this standard: For comparison, here's the same output in Java: Great, case closed. See you back here\u00a0after you've mastered Python? Well, in all seriousness, simplicity is one of Python's greatest strengths. Thanks to its precise and efficient syntax, Python can accomplish the same tasks with less code than other languages. This makes implementing\u00a0solutions refreshingly fast. In addition, Python's vibrant data science community means you'll be able to find plenty of tutorials, code snippets, and people to commiserate with\u00a0fixes to common bugs. Stackoverflow will be one of your best friends. Finally,\u00a0Python\u00a0has an\u00a0all-star lineup of libraries (a.k.a. packages) for data analysis and machine learning, which drastically reduce the time it takes to produce results. More on these later. Before we go into what you'll need to learn, let's discuss what you won't need. You won't need a C.S. degree. Most data scientists will never deal with topics such as memory leaks, cryptography, or \"Big O\" notation. You'll be fine as long as you can write clean, logical code in a scripting language such as Python or R. You won't need a complete course on Python. Python and data science are\u00a0not synonymous. You won't need to memorize all the syntax. Instead, focus on grasping the intuition, such as when function is appropriate or how conditional statements work. You'll gradually remember the syntax after Googling, reading documentation, and good ol' fashioned practice. We recommend a top-down approach. We advocate\u00a0a top-down approach with the goal of getting results first and then solidifying concepts over time. In fact, we prefer to cut out \"classroom\" study in favor of real-world practice. This approach will allow you to build mastery over time while having more fun. There are many ways to install Python on your computer, but we recommend the Anaconda bundle, which comes with the libraries you'll need for data science. Effective programming is not about memorizing syntax, but rather mastering a new way of thinking. Therefore, take your time in building a solid foundation of core programming concepts. These will help you translate solutions in your head into instructions for a computer. If you are completely new to programming, we recommend the excellent Automate the Boring Stuff with Python book, which has been released for free online under a creative commons license. The book promises \"practical programming for total beginners,\" and it keeps each lesson down-to-earth. Read up to Chapter 6 - Manipulating Strings and complete the practice questions along the way. If you only need to brush up on Python syntax, then we recommend the following video, aptly named \"Learn Python in One Video:\" Again, the goal of this step is not to learn everything about Python and programming. Instead, focus on the intuition. You should be able to answer questions such as: If you'd like more practice with the core programming concepts, check out the following resources. Next, we're going to focus on the for data science part of \"how to learn Python for data science.\" As we mentioned earlier, Python has an all-star lineup of libraries for data science. Libraries are simply bundles of pre-existing functions and objects that you can import into your script to save time. These are the action steps we recommend for efficiently picking up a new library: We don't recommend diving much deeper into a library right now because you'll likely forget most of what you've learned by the time you jump into projects. Instead, aim to discover what each library is capable of. If you installed Python through the Anaconda bundle as we recommended above, it will also come with Jupyter Notebook. Jupyter Notebook is a lightweight IDE that's a favorite among data scientists. We recommend it for your projects. You can open a new notebook through Anaconda Navigator, which came with Anaconda. Check out this short video for instructions. These are the essential libraries you'll need: NumPy allows easy and efficient numeric computation, and many other data science libraries are built on top of it. Pandas is high-performance library for data structures and exploratory analysis. It's built on top of NumPy. Matplotlib is a flexible plotting and visualization library. It's powerful but somewhat cumbersome. You have the option of skipping Matplotlib for now and using Seaborn to get started (see our Seaborn recommendation below). Scikit-Learn is the premier general-purpose machine learning library in Python. It has many popular algorithms and modules for pre-processing, cross-validation, and much more. Seaborn makes it much easier to plot common data visualizations. It's built on top of Matplotlib and offers a more pleasant high-level wrapper. By now, you'll have a basic understanding of programming and a working knowledge of essential libraries. This actually covers most of the Python you'll need to get started with data science. At this point, some students will feel a bit overwhelmed. That's OK, and it's perfectly normal. If you were to take the slow and traditional bottom-up approach, you might feel less overwhelmed, but it would have taken you 10 times as long to get here. Now the key is to dive in immediately and start gluing everything together. Again, our goal up to here has been to just learn enough to get started. Next, it's time to solidify your knowledge through plenty of practice and projects. You have several options. The first option is to participate on Kaggle, a site that hosts data science competitions. The main advantage of Kaggle is that every project is self-contained. You're given the dataset, a goal, and tutorials to get you started. The major disadvantage of competitions is that they're usually not representative of real-world data science. The \"Getting Started\" competitions are way too basic while the standard competitions (i.e. those with prize pools) are usually too tough for beginners. If you're interested in this path, check out our Beginner's Guide to Kaggle. The next option is to structure your own projects and pick datasets that interest you. The main advantage of this approach is that the projects are more representative of real-world data science. You'll likely need to define your own goals, collect data, clean your dataset, engineer features, and so on. The disadvantage of DIY projects is that you'll need to already be familiar with a proper data science workflow. Without one, you could miss important steps or get stuck without knowing how to proceed. If you go with this path, check out our free 7-day crash course on applied machine learning, which covers the key steps in a data science workflow. We also have another article with several\u00a0DIY project ideas. Finally, there are guided end-to-end projects. Proper guided projects should combine the best of both words - they should be representative of real-world data science and allow you to solidify your skills through a carefully planned learning curve. Many data science bootcamps offer this as a main benefit. Bootcamps usually conclude with a \"capstone project\" that allows you to see all the moving pieces together, from start to finish. We've also crafted our own\u00a0Machine Learning Masterclass to solve this exact need. It will provide you over-the-shoulder mentorship for real-world projects while teaching you all of the key concepts in context. The masterclass also includes a comprehensive Python course that gets you up to speed ASAP. In fact, many successful students have enrolled without any prior programming experience. Learn more about it here. [\u2026] In terms of programming languages \u2013 Python is pretty powerful language with integration to all technologies in the CDH stack. For a basic course check out the link below \u2013 Python For Data Science [\u2026] * Denotes Required Field Copyright \u00a9 2017 \u00b7 EliteDataScience.com \u00b7 All Rights Reserved", "links": ["https://elitedatascience.com/", "https://elitedatascience.com/", "https://elitedatascience.com/blog", "https://elitedatascience.com/machine-learning-masterclass", "https://elitedatascience.com/about", "https://elitedatascience.com/about", "https://elitedatascience.com/guest-post-submissions", "https://elitedatascience.com/author/slice2o", "https://elitedatascience.com/learn-python-for-data-science#comments", "https://www.facebook.com/sharer.php?u=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://plus.google.com/share?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://www.linkedin.com/shareArticle?trk=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://twitter.com/intent/tweet?text=How%20to%20Learn%20Python%20for%20Data%20Science%20%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "https://www.forbes.com/sites/jeffkauflin/2017/01/08/the-10-technical-skills-with-explosive-growth-in-job-demand/#6fa6bf344f5c", "http://www.tiobe.com/tiobe-index/", "https://stackoverflow.com/questions/tagged/python?sort=votes&pageSize=15", "https://www.continuum.io/downloads", "https://automatetheboringstuff.com/", "https://automatetheboringstuff.com/", "https://codefights.com/", "http://www.pythonchallenge.com/index.php", "http://www.practicepython.org/", "http://interactivepython.org/runestone/static/thinkcspy/index.html", "http://jupyter.org/", "https://www.youtube.com/watch?v=-MyjG00la2k", "https://docs.scipy.org/doc/numpy/user/", "https://docs.scipy.org/doc/numpy-dev/user/quickstart.html", "https://pandas.pydata.org/pandas-docs/stable/", "https://pandas.pydata.org/pandas-docs/stable/10min.html", "https://matplotlib.org/contents.html", "https://matplotlib.org/users/pyplot_tutorial.html", "http://scikit-learn.org/stable/documentation.html", "http://elitedatascience.com/python-machine-learning-tutorial-scikit-learn", "https://seaborn.pydata.org/", "http://elitedatascience.com/python-seaborn-tutorial", "http://elitedatascience.com/beginner-kaggle", "http://elitedatascience.com/", "http://elitedatascience.com/machine-learning-projects-for-beginners", "http://elitedatascience.com/machine-learning-masterclass", "http://elitedatascience.com/machine-learning-masterclass", "https://www.facebook.com/sharer.php?u=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://plus.google.com/share?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://www.linkedin.com/shareArticle?trk=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://twitter.com/intent/tweet?text=How%20to%20Learn%20Python%20for%20Data%20Science%20%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "https://datascienceandmore.wordpress.com/2017/03/04/getting-the-basics-down/", "https://elitedatascience.com/learn-machine-learning", "https://elitedatascience.com/learn-python-for-data-science", "https://elitedatascience.com/feature-engineering-best-practices", "https://elitedatascience.com/beginner-kaggle", "https://elitedatascience.com/imbalanced-classes", "https://elitedatascience.com/beginner-mistakes", "https://elitedatascience.com/bias-variance-tradeoff", "https://elitedatascience.com/data-science-resources", "https://elitedatascience.com/guest-post-submissions", "https://elitedatascience.com/", "https://elitedatascience.com/terms-of-service", "https://elitedatascience.com/privacy-policy", "https://elitedatascience.com/about", "https://www.linkedin.com/shareArticle?trk=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science", "http://www.facebook.com/sharer.php?u=https://elitedatascience.com/learn-python-for-data-science", "https://plus.google.com/share?text=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science", "https://twitter.com/intent/tweet?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "http://service.weibo.com/share/share.php?url=https://elitedatascience.com/learn-python-for-data-science&title=How to Learn Python for Data Science (Updated)", "https://getpocket.com/save?title=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science"]}, "157": {"url": "https://stats.stackexchange.com/questions/297380/why-not-just-dump-the-neural-networks-and-deep-learning", "title": "", "text": "Fundamental problem with deep learning and neural networks in general. The solutions that fit training data are infinite. We don't have precise mathematical equation that is satisfied by only a single one and that we can say generalizes best. Simply speaking we don't know which generalizes best. Optimizing weights is not a convex problem, so we never know we end up with a global or a local minimum. So why not just dump the neural networks and instead search for a better ML model? Something that we understand, and something that is consistent with a set of mathematical equations? Linear and SVM do not have this mathematical drawbacks and are fully consistent with a a set of mathematical equations. Why not just think on same lines (need not be linear though) and come up with a new ML model better than Linear and SVM and neural networks and deep learning? Not being able to know what solution generalizes best is an issue, but it shouldn't deter us from otherwise using a good solution. Humans themselves often do not known what generalizes best (consider, for example, competing unifying theories of physics), but that doesn't cause us too many problems. It has been shown that it is extremely rare for training to fail because of local minimums. Most of the local minimums in a deep neural network are close in value to the global minimum, so this is not an issue. source But the broader answer is that you can talk all day about nonconvexity and model selection, and people will still use neural networks simply because they work better than anything else (at least on things like image classification).  Of course there are also people arguing that we shouldn't get too focused on CNNs like the community was focused on SVMs a few decades ago, and instead keep looking for the next big thing. In particular, I think I remember Hinton regretting the effectiveness of CNNs as something which might hinder research. related post As the comments to your question point out, there are a lot of people working on finding something better. I would though like to answer this question by expanding the comment left by @josh All models are wrong but some are useful (Wiki) The above statement is a general truth used to describe the nature of statistical models. Using data that we have available, we can create models that let us do useful things such as approximate a predicted value. Take for example Linear Regression  Using a number of observations, we can fit a model to give us an approximate value for a dependent variable given any value(s) for the independent variable(s).  Burnham, K. P.; Anderson, D. R. (2002), Model Selection and Multimodel > Inference: A Practical Information-Theoretic Approach (2nd ed.): \"A model is a simplification or approximation of reality and hence   will not reflect all of reality. ... Box noted that \u201call models are   wrong, but some are useful.\u201d While a model can never be \u201ctruth,\u201d a   model might be ranked from very useful, to useful, to somewhat useful   to, finally, essentially useless.\" Deviations from our model (as can be seen in the image above) appear random, some observations are below the line and some are above, but our regression line shows a general correlation. Whilst deviations in our model appear random, in realistic scenarios there will be other factors at play which cause this deviation. For example, imagine watching cars as they drove through a junction where they must turn either left or right to continue, the cars turn in no particular pattern. Whilst we could say that the direction the cars turn is completely random, does every driver reach the junction and at that point make a random decision of which way to turn? In reality they are probably heading somewhere specific for a specific reason, and without attempting to stop each car to ask them about their reasoning, we can only describe their actions as random.  Where we are able to fit a model with minimal deviation, how certain can we be that an unknown, unnoticed or immeasurable variable wont at some point throw our model? Does the flap of a butterfly\u2019s wings in Brazil set off a tornado in Texas? The problem with using the Linear and SVN models you mention alone is that we are somewhat required to manually observe our variables and how they each affect each other. We then need to decide what variables are important and write a task-specific algorithm. This can be straight forward if we only have a few variables, but what if we had thousands? What if we wanted to create a generalised image recognition model, could this realistically be achieved with this approach?  Deep Learning and Artificial Neural Networks (ANNs) can help us create useful models for huge data sets containing huge amounts of variables (e.g. image libraries). As you mention, there's an incomprehensible number of solutions which could fit the data using ANNs, but is this number really any different to the amount of solutions we would need to develop ourselves through trial and error? The application of ANNs do much of the work for us, we can specify our inputs and our desired outputs (and tweak them later to make improvements) and leave it up to the ANN to figure out the solution. This is why ANNs are often described as \"black boxes\". From a given input they output an approximation, however (in general terms) these approximations don't include details on how they were approximated. And so it really comes down to what problem you are trying to solve, as the problem will dictate what model approach is more useful. Models are not absolutely accurate and so there is always an element of being 'wrong', however the more accurate your results the more useful they are. Having more detail in the results on how the approximation was made may also be useful, depending on the problem it may even be more useful than increased accuracy. If for example you are calculating a persons credit score, using regression and SVMs provides calculations that can be better explored. Being able to both tweak the model directly and explain to customers the effect separate independent variables have on their overall score is very useful. An ANN may aid in processing larger amounts of variables to achieve a more accurate score, but would this accuracy be more useful? I guess for some problem we care less for the mathematical rigor and simplicity but more for its utility, current status is neural network is better in performing certain task like pattern recognition in image processing.  The global minimum may as well as be useless, so we don't really care if we find it or not. The reason is that, for deep networks, not only the time to find it becomes exponentially longer as the network size increases, but also the global minimum often corresponds to overfitting the training set. Thus the generalization ability of the DNN (which is what we really care about) would suffer. Also, often we prefer flatter minima corresponding to a higher value of the loss function, than sharper minima corresponding to a lower value of the loss function, because the second one will deal very badly with uncertainty in the inputs. This is becoming increasingly clear with the development of Bayesian Deep Learning. Robust Optimization beats Determinist Optimization very often, when applied to real world problems where uncertainty is important. Finally, it's a fact that DNNs just kick the ass of methods such as XGBoost at image classification and NLP. A company which must make a profit out of image classification will correctly select them as modelsto be deployed in production (and invest a significant amount of money on feature engineering, data pipeline, etc. but I digress). This doesn't mean that they dominate all the ML environment: for example, they do worse than XGBoost on structured data (see the last winners of Kaggle competitions) and they seem to not still do as well as particle filters on time series modelling. However, some very recent innovations on RNNs may modify this situation. There are excellent answers, mostly weighing in with the usefulness of DL and ANNs. But I would like to object the OP in a more fundamental way, since the question already takes for granted the mathematical inconsistency of neural networks. First of all, there is a mathematical theory behind (most models of) Neural Networks. You could likewise argue that linear regression does not generalize, unless the underlying model is... well, linear. In neural algorithms, a model is assumed (even if not explicitly) and the fitting error is computed. The fact that algorithms are modified with various heuristics does not void the original mathematical support. BTW, local optimization is also a mathematically consistent, let alone useful, theory. Along this line, if Neural Networks just constitute one class of methods within the whole toolbox of scientists, which is the line that separates Neural Networks from the rest of techniques? In fact, SVMs were once considered a class of NNs and they still appear in the same books. On the other hand, NNs could be regarded as a (nonlinear) regression technique, maybe with some simplification. I agree with the OP that we must search better, well founded, efficient algorithms, regardless you label them as NNs or not. Sign up using Google Sign up using Facebook Sign up using Email and Password   By posting your answer, you agree to the privacy policy and terms of service. asked 3 days ago viewed  7,704 times  active yesterday                                   site design / logo \u00a9 2017 Stack Exchange Inc; user contributions licensed under cc by-sa 3.0                                  with attribution required.                                  rev 2017.8.11.26777", "links": ["https://stats.stackexchange.com", "https://chat.stackexchange.com?tab=site&host=stats.stackexchange.com", "https://stats.stackexchange.com", "https://stats.meta.stackexchange.com", "https://stats.stackexchange.com/users/signup?ssrc=site_switcher&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.stackexchange.com/users/login?ssrc=site_switcher&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stackexchange.com/sites", "https://stackoverflow.blog", "https://stackexchange.com", "https://stats.stackexchange.com/users/signup?ssrc=head&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.stackexchange.com/users/login?ssrc=head&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.meta.stackexchange.com", "https://stackoverflow.com/company/about", "https://www.stackoverflowbusiness.com/?ref=topbar_help", "https://stats.stackexchange.com", "https://arxiv.org/pdf/1406.2572.pdf", "https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/clyohpf/?context=3", "https://arxiv.org/pdf/1406.2572.pdf", "https://arxiv.org/pdf/1412.0233.pdf", "https://en.wikipedia.org/wiki/All_models_are_wrong", "https://en.wikipedia.org/wiki/Butterfly_effect", "https://stats.stackexchange.com/questions/93705/meaning-of-a-neural-network-as-a-black-box", "https://stackexchange.com/legal/privacy-policy", "https://stackexchange.com/legal/terms-of-service", "http://stats.stackexchange.com/election", "http://stats.stackexchange.com/election", "https://stackexchange.com/questions?tab=hot", "https://codegolf.stackexchange.com/questions/138697/maximum-sub-array", "https://askubuntu.com/questions/945919/where-does-gedit-store-the-last-cursor-position", "https://ux.stackexchange.com/questions/110855/how-big-of-a-deal-is-blocking-copy-pasting-on-a-website", "https://workplace.stackexchange.com/questions/96824/how-to-get-a-raise-i-am-denied-to-because-of-sick-leaves", "https://codegolf.stackexchange.com/questions/138840/reliably-broken-sort", "https://interpersonal.stackexchange.com/questions/1737/how-do-i-tell-a-co-worker-that-just-because-something-is-a-hobby-doesnt-mean-i", "https://puzzling.stackexchange.com/questions/54195/measure-22-minutes-with-7-and-13-minute-hourglasses", "https://photo.stackexchange.com/questions/91713/why-is-there-single-in-the-term-dslr-digital-single-lens-reflex", "https://interpersonal.stackexchange.com/questions/1585/how-do-i-react-when-a-girl-i-like-has-a-new-haircut-that-i-dont-like-very-much", "https://worldbuilding.stackexchange.com/questions/88602/could-a-wwi-biplane-fly-if-it-were-dropped-from-a-high-altitude-instead-of-using", "https://scifi.stackexchange.com/questions/167116/tv-series-episode-where-a-stand-up-comedian-is-forced-to-perform-in-hell-for-all", "https://latin.stackexchange.com/questions/5045/why-is-the-roman-acronym-spqr-and-not-spr", "https://aviation.stackexchange.com/questions/42929/what-performance-issues-symptoms-can-a-pilot-expect-while-taking-off-above-mtow", "https://scifi.stackexchange.com/questions/167120/tv-show-where-a-condemned-prisoner-can-miraculously-heal-by-touch", "https://physics.stackexchange.com/questions/350805/seeing-something-from-only-one-angle-means-you-have-only-seen-what-of-its-su", "https://rpg.stackexchange.com/questions/105287/when-do-you-need-a-free-hand-for-casting-shield", "https://space.stackexchange.com/questions/22590/how-did-people-know-how-to-build-the-first-space-ship", "https://math.stackexchange.com/questions/2392440/is-this-an-issue-with-the-law-of-the-excluded-middle-or-an-issue-with-the-proof", "https://blender.stackexchange.com/questions/87969/how-to-scale-two-objects-to-fit-together", "https://money.stackexchange.com/questions/83759/are-my-parents-ripping-me-off-with-this-deal-that-doesnt-allow-me-to-build-my-e", "https://security.stackexchange.com/questions/167422/is-receiving-fake-torrent-data-possible", "https://electronics.stackexchange.com/questions/323794/what-sealing-type-for-cable-tube-enter-casing-for-outdoor-use", "https://mathoverflow.net/questions/278659/a-sum-involving-euler-totient-function", "https://history.stackexchange.com/questions/39549/was-russia-the-only-country-in-wwii-that-succeeded-in-expansionist-war-aims", "https://chat.stackexchange.com?tab=site&host=stats.stackexchange.com", "https://stats.meta.stackexchange.com", "https://stackoverflow.com/company/about", "https://stackoverflow.com", "https://www.stackoverflowbusiness.com/?utm_source=so-footer&utm_medium=referral&utm_campaign=brand-activation", "https://stackoverflow.com/jobs", "https://stackoverflow.com/company/about", "https://stackoverflow.com/company/press", "https://stackexchange.com/legal", "https://stackexchange.com/legal/privacy-policy", "https://stackexchange.com", "https://stackexchange.com/sites#technology", "https://stackexchange.com/sites#lifearts", "https://stackexchange.com/sites#culturerecreation", "https://stackexchange.com/sites#science", "https://api.stackexchange.com", "https://data.stackexchange.com", "https://stackoverflow.blog?blb=1", "https://www.facebook.com/officialstackoverflow/", "https://twitter.com/stackoverflow", "https://linkedin.com/company/stack-overflow", "https://creativecommons.org/licenses/by-sa/3.0/", "https://stackoverflow.blog/2009/06/25/attribution-required/"]}, "158": {"url": "https://blog.godatadriven.com/practical-airflow-tutorial", "title": "", "text": "Airflow is a scheduler for workflows such as data pipelines, similar to Luigi and Oozie. It's written in Python and we at GoDataDriven have been contributing to it in the last few months. This tutorial is loosely based on the Airflow tutorial in the official documentation. It will walk you through the basics of setting up Airflow and creating an Airflow workflow, and it will give you some practical tips. A (possibly) more up-to-date version of this blog can be found in my git repo. Setting up a basic configuration of Airflow is pretty straightforward. After installing the Python package, we'll need a database to store some data and start the core Airflow services. You can skip this section if Airflow is already set up. Make sure that you can run airflow commands, know where to put your DAGs and have access to the web UI. Airflow is installable with pip via a simple pip install apache-airflow. Either use a separate Python virtual environment or install it in your default python environment. To use the conda virtual environment as defined in environment.yml from my git repo: You should now have an (almost) working Airflow installation. Alternatively, install Airflow yourself by running: Airflow used to be packaged as airflow but is packaged as apache-airflow since version 1.8.1. Make sure that you install any extra packages with the right Python package: e.g. use pip install apache-airflow[dask] if you've installed apache-airflow and do not use pip install airflow[dask]. Leaving out the prefix apache- will install an old version of Airflow next to your current version, leading to a world of hurt. You may run into problems if you don't have the right binaries or Python packages installed for certain backends or operators. When specifying support for e.g. PostgreSQL when installing extra Airflow packages, make sure the database is installed; do a brew install postgresql or apt-get install postgresql before the pip install apache-airflow[postgres]. Similarly, when running into HiveOperator errors, do a pip install apache-airflow[hive] and make sure you can use Hive. Before you can use Airflow you have to initialize its database. The database contains information about historical & running workflows, connections to external data sources,  user management, etc. Once the database is set up, Airflow's UI can be accessed by running a web server and workflows can be started. The default database is a SQLite database, which is fine for this tutorial. In a production setting you'll probably be using something like MySQL or PostgreSQL. You'll probably want to back it up as this database stores the state of everything related to Airflow. Airflow will use the directory set in the environment variable AIRFLOW_HOME to store its configuration and our SQlite database. This directory will be used after your first Airflow command. If you don't set the environment variable AIRFLOW_HOME, Airflow will create the directory ~/airflow/ to put its files in. Set environment variable AIRFLOW_HOME to e.g. your current directory $(pwd): or any other suitable directory. Next, initialize the database: Now start the web server and go to localhost:8080 to check out the UI: It should look something like this:  With the web server running workflows can be started from a new terminal window. Open a new terminal, activate the virtual environment and set the environment variable AIRFLOW_HOME for this terminal as well: Make sure that you're an in the same directory as before when using $(pwd). Run a supplied example: And check in the web UI that it has run by going to Browse -> Task Instances. This concludes all the setting up that you need for this tutorial. We'll create a workflow by specifying actions as a Directed Acyclic Graph (DAG) in Python. The tasks of a workflow make up a Graph; the graph is Directed because the tasks are ordered; and we don't want to get stuck in an eternal loop so the graph also has to be Acyclic. The figure below shows an example of a DAG:  The DAG of this tutorial is a bit easier. It will consist of the following tasks: and we'll plan daily execution of this workflow. Go to the folder that you've designated to be your AIRFLOW_HOME and find the DAGs folder located in subfolder dags/ (if you cannot find, check the setting dags_folder in $AIRFLOW_HOME/airflow.cfg). Create a Python file with the name airflow_tutorial.py that will contain your DAG. Your workflow will automatically be picked up and scheduled to run. First we'll configure settings that are shared by all our tasks. Settings for tasks can be passed as arguments when creating them, but we can also pass a dictionary with default values to the DAG. This allows us to share default arguments for all the tasks in our DAG is the best place to set e.g. the owner and start date of our DAG. Add the following import and dictionary to airflow_tutorial.py to specify the owner, start time, and retry settings that are shared by our tasks: These settings tell Airflow that this workflow is owned by 'me', that the workflow is valid since June 1st of 2017, it should not send emails and it is allowed to retry the workflow once if it fails with a delay of 5 minutes. Other common default arguments are email settings on failure and the end time. We'll now create a DAG object that will contain our tasks. Name it airflow_tutorial_v01 and pass default_args: With schedule_interval='0 * * * *' we've specified a run at every hour 0; the DAG will run each day at 00:00. See crontab.guru for help deciphering cron schedule expressions. Alternatively, you can use strings like '@daily' and '@hourly'. We've used a context manager to create a DAG (new since 1.8). All the tasks for the DAG should be indented to indicate that they are part of this DAG. Without this context manager you'd have to set the dag parameter for each of your tasks. Airflow will generate DAG runs from the start_date with the specified schedule_interval. Once a DAG is active, Airflow continuously checks in the database if all the DAG runs have successfully ran since the start_date. Any missing DAG runs are automatically scheduled. When you initialize on 2016-01-04 a DAG with a start_date at 2016-01-01 and a daily schedule_interval, Airflow will schedule DAG runs for all the days between 2016-01-01 and 2016-01-04. A run starts after the time for the run has passed. The time for which the workflow runs is called the execution_date. The daily workflow for 2016-06-02 runs after 2016-06-02 23:59 and the hourly workflow for 2016-07-03 01:00 starts after 2016-07-03 01:59. From the ETL viewpoint this makes sense: you can only process the daily data for a day after it has passed. This can, however, ask for some juggling with date for other workflows. For Machine Learning models you may want to use all the data up to a given date, you'll have to add the schedule_interval to your execution_date somewhere in the workflow logic. Because Airflow saves all the (scheduled) DAG runs in its database, you should not change the start_date and schedule_interval of a DAG. Instead, up the version number of the DAG (e.g. airflow_tutorial_v02) and avoid running unnecessary tasks by using the web interface or command line tools Timezones and especially daylight savings can mean trouble when scheduling things, so keep your Airflow machine in UTC. You don't want to skip an hour because daylight savings kicks in (or out). Tasks are represented by operators that either perform an action, transfer data, or sense if something has been done. Examples of actions are running a bash script or calling a Python function; of transfers are copying tables between databases or uploading a file; and of sensors are checking if a file exists or data has been added to a database. We'll create a workflow consisting of three tasks: we'll print 'hello', wait for 10 seconds and finally print 'world'.  The first two are done with the BashOperator and the latter with the PythonOperator. Give each operator an unique task ID and something to do: Note how we can pass bash commands in the BashOperator and that the PythonOperator asks for a Python function that can be called. Dependencies in tasks are added by setting other actions as upstream (or downstream).  Link the operations in a chain so that sleep will be run after print_hello and is followed by print_world; print_hello -> sleep -> print_world: After rearranging the code your final DAG should look something like: First check that DAG file contains valid Python code by executing the file with Python: You can manually test a single task for a given execution_date with airflow test: This runs the task locally as if it was for 2017-07-01, ignoring other tasks and without communicating to the database. Now that you're confident that your dag works, turn on the DAG in the web UI and sit back while Airflow starts backfilling the dag runs! You now know the basics of setting up Airflow, creating a DAG and turning it on; time to go deeper!  Address: Wibautstraat 202, 1091 GS Amsterdam, The Netherlands           Phone: +31 (0)35 672 9069           Email: signal@godatadriven.com", "links": ["http://godatadriven.com/contact", "http://godatadriven.com/careers", "http://blog.godatadriven.com", "http://godatadriven.com/business-consultancy", "http://godatadriven.com/big-data-infrastructure", "http://godatadriven.com/predictive-modelling", "http://godatadriven.com/products-overview", "http://godatadriven.com/data-discovery", "http://godatadriven.com/data-science-audit", "http://godatadriven.com/data-science-suite", "http://godatadriven.com/data-science-box", "http://godatadriven.com/divolte", "http://godatadriven.com/customers", "http://godatadriven.com/casestudy-airfranceklm", "http://godatadriven.com/casestudy-bakkersland", "http://godatadriven.com/casestudy-bol", "http://godatadriven.com/casestudy-eneco", "http://godatadriven.com/casestudy-eretail", "http://godatadriven.com/casestudy-ing", "http://godatadriven.com/casestudy-nuon", "http://godatadriven.com/casestudy-npo", "http://godatadriven.com/casestudy-schipholgroup", "http://godatadriven.com/casestudy-transavia", "http://godatadriven.com/technology-overview", "http://godatadriven.com/databricks", "http://godatadriven.com/hadoop", "http://godatadriven.com/python", "http://godatadriven.com/r", "http://godatadriven.com/spark", "http://godatadriven.com/training-overview", "http://godatadriven.com/training-schedule", "http://godatadriven.com/bigdata-training", "http://godatadriven.com/datascience-training", "http://godatadriven.com/data-science-accelerator-program", "http://godatadriven.com/meetups", "http://godatadriven.com/events", "http://blog.godatadriven.com", "http://godatadriven.com/our-story", "http://godatadriven.com/team", "http://godatadriven.com/careers", "http://godatadriven.com/media-resources", "https://blog.godatadriven.com", "http://godatadriven.com/contact", "http://godatadriven.com", "http://blog.godatadriven.com", "https://airflow.incubator.apache.org/", "https://github.com/spotify/luigi", "https://oozie.apache.org/", "https://blog.godatadriven.com/open-source-201702", "https://blog.godatadriven.com/open-source-201703", "https://blog.godatadriven.com/open-source-201704", "https://blog.godatadriven.com/open-source-201705", "https://blog.godatadriven.com/open-source-201707", "https://pythonhosted.org/airflow/tutorial.html", "https://github.com/hgrif/airflow-tutorial", "https://github.com/hgrif/airflow-tutorial", "http://conda.pydata.org/miniconda.html", "http://localhost:8080/", "https://airflow.incubator.apache.org/configuration.html", "https://airflow.incubator.apache.org/security.html", "https://github.com/apache/incubator-airflow/tree/master/scripts", "https://crontab.guru/#0_*_*_*_*", "https://jeffknupp.com/blog/2016/03/07/python-with-context-managers/", "https://airflow.incubator.apache.org/configuration.html#connections", "https://airflow.incubator.apache.org/ui.html#variable-view", "https://airflow.incubator.apache.org/tutorial.html#templating-with-jinja", "https://github.com/apache/incubator-airflow/blob/master/airflow/example_dags/example_bash_operator.py", "https://airflow.incubator.apache.org/code.html#airflow.operators.PythonOperator", "https://pythonhosted.org/airflow/tutorial.html", "https://gtoonstra.github.io/etl-with-airflow/", "https://medium.com/handy-tech/airflow-tips-tricks-and-pitfalls-9ba53fba14eb", "http://godatadriven.com/players/henk-griffioen", "http://blog.godatadriven.com/practical-airflow-tutorial", "http://blog.godatadriven.com/open-source-201707", "http://blog.godatadriven.com/cd-python-eggs-vsts", "http://blog.godatadriven.com/ldappy", "http://blog.godatadriven.com/open-source-201706", "http://blog.godatadriven.com/vendor-free-ds", "https://twitter.com/GoDataDriven", "http://godatadriven.com/business-consultancy", "http://godatadriven.com/big-data-infrastructure", "http://godatadriven.com/predictive-modelling", "http://godatadriven.com/products-overview", "http://godatadriven.com/data-discovery", "http://godatadriven.com/data-science-audit", "http://godatadriven.com/data-science-suite", "http://godatadriven.com/data-science-box", "http://godatadriven.com/divolte", "http://godatadriven.com/customers", "http://godatadriven.com/casestudy-airfranceklm", "http://godatadriven.com/casestudy-bakkersland", "http://godatadriven.com/casestudy-bol", "http://godatadriven.com/casestudy-eneco", "http://godatadriven.com/casestudy-eretail", "http://godatadriven.com/casestudy-ing", "http://godatadriven.com/casestudy-nuon", "http://godatadriven.com/casestudy-npo", "http://godatadriven.com/casestudy-schipholgroup", "http://godatadriven.com/casestudy-transavia", "http://godatadriven.com/technology-overview", "http://godatadriven.com/hadoop", "http://godatadriven.com/python", "http://godatadriven.com/r", "http://godatadriven.com/spark", "http://godatadriven.com/training-overview", "http://godatadriven.com/training-schedule", "http://godatadriven.com/bigdata-training", "http://godatadriven.com/datascience-training", "http://godatadriven.com/data-science-accelerator-program", "http://godatadriven.com/events", "http://godatadriven.com/meetups", "http://blog.godatadriven.com", "http://godatadriven.com/our-story", "http://godatadriven.com/team", "http://godatadriven.com/careers", "http://godatadriven.com/media-resources", "http://godatadriven.com/news", "http://godatadriven.com/contact", "https://cdn.xebia.com/documents/Cookies+Policy+Xebia+20170130.pdf"]}, "159": {"url": "https://concepttoclinic.drivendata.org/", "title": "", "text": "Lung Cancer Early Detection Challenge  We\u2019re calling on a global community of data scientists, engineers, designers, and researchers to build an open source software application that brings advances from machine learning into the clinic. We\u2019re not just optimizing an algorithm for a single metric\u2014we\u2019re collaborating to build tools which put AI in the hands of clinicians. In addition to pushing forward the cutting-edge of open clinical software, top contributors will be eligible for a share of $100,000 in monetary prizes generously provided by the Bonnie J. Addario Lung Cancer Foundation.          Contribute now by grabbing an issue from the project's GitHub repository and submitting a PR!        Lung cancer causes more deaths each year than any other cancer in the US, and early detection makes a big difference. Recent advances in machine learning \u2013 including open source algorithms from the 2017 Data Science Bowl \u2013 help find and interpret early signs of cancer. Using imagery from CT scans, this application will build on cutting-edge algorithms in machine learning to help clinicians identify and interpret lung nodules. Your guide to contributing throughout the challenge. All you need to start contributing is a passion for building great software and a github account.  Contributions of all sizes can earn points as they add value to the project, by building on pre-designated issues, building out new content, and building up the community. $100,000 in cash, as well as other in-kind prizes, will be available for top contributors filling key roles throughout the challenge. A live leaderboard keeps track of points earned throughout the challenge by top contributors filling key roles. Meet the amazing group of of technical, clinical, and domain experts that are helping to guide the end-to-end design of this challenge. The Addario Lung Cancer Foundation (ALCF) has set the audacious goal of making lung cancer a chronically managed disease by 2023.  Ultimately, the goal of this project is to produce a tool that draws on machine intelligence, is useful to clinicians, and makes a lasting difference in the lives of patients. We hope you will find a way to contribute!  Lung Cancer Early Detection Challenge", "links": ["https://www.drivendata.org/termsofuse/", "https://www.drivendata.org/copyrightpolicy/", "https://www.drivendata.org/privacypolicy/", "https://www.drivendata.org/", "http://www.drivendata.co/"]}, "160": {"url": "http://appsilondatascience.com/blog/rstats/2017/08/09/shiny-collections.html", "title": "shiny.collections,  Google Docs-like live collaboration in ShinyAppsilon Data Science Blog", "text": "A few weeks ago, our CTO Marek Rogala gave an engaging talk at the User Conference in Brussels. Despite being one of the last talks, he drew a crowd that filled the room and had significant viewership online. Marek\u2019s talk was entitled shiny.collections: Google Docs-like live collaboration in Shiny. In short, he went over a package we built that allows for persistence and collaboration without losing reactivity in Shiny. Users can enter information and have continuity, knowing that if they exit and return to the app, their inputs will still be there. Let\u2019s go over his talk. Recently, Shiny users have demanded more from their applications. The advent of cloud applications, especially ones like Google Docs, have accustomed users to collaboration. They expect their Shiny apps to be more than just an interactive tool for data visualisation, but rather, a production ready application that works like any other tool. Their data has to be saved automatically. Interactive collaboration is a must in some use cases. And the application must be delivered as fast as possible. You need to use a reactive database such as rethinkDB, Firebase, or mongoDB to achieve this. mongoDB isn\u2019t really a reactive database, but there are ways where it can be used as one. We are going to use rethinkDB in our case. Also, we are going to use rethinker, which is a rethinkDB driver for R. But using rethinker is not the most ideal solution, as callback do not work very well with Shiny and it is quite painful to configure to our needs. To get around this, we created our own package built upon rethinker called shiny.collections.  It lets you easily connect to your shiny app and takes care of all of the trickiness involved. During his talk, Marek showed a live demo that involved creating a chat application. Check out how to do this yourself. I reccomend watching Marek first, as he has some valuable commentary you won\u2019t find in the blog post. This example is just a simple use case of what you can achieve with shiny.collections. We\u2019ve made it a priority to make it convenient and easily integrable with other powerful tools, including DT, leaflet or rhandsontable. Our goals for the future of this package include getting it on cran and diversifying the functionality of the API. Take a look at the our package and contribute. Get in touch with us for all of you other data science needs as well.  Published 09 Aug 2017", "links": ["https://channel9.msdn.com/Events/useR-international-R-User-conferences/useR-International-R-User-2017-Conference/shinycollections-Google-Docs-like-live-collaboration-in-Shiny#comments", "https://cran.r-project.org/web/packages/rethinker/index.html", "https://appsilon.github.io/shiny.collections/", "http://appsilondatascience.com/blog/rstats/2017/07/02/shiny-chat.html", "http://appsilondatascience.com", "http://disqus.com/?ref_noscript"]}, "161": {"url": "https://blog.statsbot.co/introduction-to-imitation-learning-32334c3b1e7a", "title": "Introduction to Imitation Learning", "text": "This post is the first in the series where we will describe what Imitation Learning is. For today\u2019s article, the Statsbot team asked computer scientist Vitaly Kurin to briefly introduce Imitation Learning and outline the basics of Reinforcement Learning. Living organisms are extremely complex. Even the relatively simple ones such as flies or worms. They are able not only to operate successfully in the real world, but are extremely resilient to changes in conditions. And that\u2019s if we\u2019re not even talking about humans. We can plan ahead, we can alter our plans given new information, and we can collaborate with others and execute our plans more effectively. Science and such projects as CERN or a huge Radio Telescope in Arecibo are perfect examples of human collaboration and the art of planning. Yes, we do a lot of stupid stuff, but let\u2019s not talk about that here and instead focus on the good. Yes, we can now beat the best human in chess or Go, we can get a crazy score in Video Pinball for Atari 2600, we can even already make a pure human broke by challenging him in poker. Can we open a bottle of champagne and celebrate the triumph? I\u2019m afraid not. Yes, machine learning has recently made a significant leap forward. The combination of new Deep Learning ideas with old ones has enabled us to advance in many domains, such as computer vision, speech recognition, and text translation. Reinforcement Learning has also benefited greatly from its marriage with Deep Learning. You\u2019ve definitely heard of Deep Reinforcement Learning success such as achieving superhuman score in Atari 2600 games, solving Go, and making robots learn parkour. Though, we must admit, that operating successfully in the real world is much harder than playing Go or Space Invaders. Many of the tasks are much harder than this. Imagine a kid riding a bicycle in the middle of a crowded city center or a man driving a Porsche 911 on an autobahn at 200 miles per hour. Let\u2019s all admit that we are not there yet. The typical machine learning approach is to train a model from scratch. Give it a million images and some time to figure it out. Give it a week and let it play Space Invaders until it reaches some acceptable score. We, as humans, beg to differ. When a typical human starts to play some game he has never seen, he already has a huge amount of prior information. If he sees a door in Montezuma\u2019s Revenge, he realizes that somewhere there should lie a key and he needs to find it. When he finds the key, he remembers that the closed door is back through the two previous rooms and he returns to open it. When he sees a ladder, he realizes that he can climb it because he has done this hundreds of time already. What if we could somehow transfer human knowledge about the world to an agent? How can we extract all this information? How can we create a model out of it? There is such a way. It\u2019s called Imitation Learning. Imitation Learning is not the only name for leveraging human data for good. Some researchers also call it apprenticeship learning, others refer to it as Learning from Demonstration. From our point of view, there is no substantial difference between all of these titles and we will use Imitation Learning from now on. In order to introduce Imitation Learning, we will need to understand the basics of Reinforcement Learning first. Let\u2019s move on. It\u2019s not hard to get the general idea of the Reinforcement Learning setup. There is some agent and we want this agent to learn some task. Let\u2019s say, we have an Atari 2600 console, the agent has access to the joystick and can see what\u2019s happening on the screen. Let\u2019s say, 60 times per second we give our agent a screenshot of the game and ask him what button he wants to press. If our agent does well, he can see that his score is increasing (positive reinforcement), otherwise we can give him a penalty as a negative reward (negative reinforcement). Gradually, by trial and error, the agent starts to understand that it\u2019s better to avoid some of the actions and do those which bring him a reward. Let\u2019s make it more formal and describe the process stated above mathematically. We can describe the RL framework mentioned above (observe -> act -> get the reward and the next state) as a Markov Decision Process (MDP): where: We also need a definition of a policy function for the next section. Policy is a function, that returns an action given the state: And, actually, our final goal when solving an MDP is to learn such a policy in order to maximize the reward for our agent. Let\u2019s take an example of an MDP. The circles represent the states, arrows with green labels are actions, red labels are the rewards for actions, and the square is the terminal state. The green numeric labels are the transition probabilities. Our student starts in a state with the blue circle. He studies, but this is hard and sometimes boring. He decides to open a Facebook app and once he is there, he can either quit or continue scrolling. He then studies more and more, and finally decides to go to the pub. The state is a smaller filled circle, since now there is an element of randomness, based on the amount of knowledge the student forgets after visiting the pub. He can then either study more and pass the exam (+10 in reward), or he can go to sleep and finish the MDP right now. Since we will use DQN and related ideas in the future, let\u2019s briefly understand what is going on here. The whole approach is built upon approximating the so-called Q function and building the agent\u2019s behavior based on it. The idea of the Q function is the following: it returns you the entire expected discounted reward flow for the particular action and the particular state, given that starting from the next state we will be following our policy \ud835\udfb9. It answers the question: \u201cHow good is to press this button in this state?\u201d The Q function obeys the Bellman equation: And, finally, the Bellman principle of optimality is the following: notwithstanding what happened before, we should always take the action with the highest Q to maximize the reward flow: But how do we get such a Q function, you might ask. Let\u2019s look at an example. Imagine, you want to grab a coffee (+20 in reward) and a chocolate in a vending machine (+10 in reward). Your total reward cannot exceed thirty. Moreover, if you have taken the coffee already, it cannot be higher than 10 (the reward for chocolate) from now on. This is the idea: the Q value for the current step and action is equal to the maximum Q value for the next state (since we behave optimally) + the reward we get for the transition. The value of the quadratic objective function becomes: Q-learning itself is not new. Q-learning which uses neural networks as a function approximators is also not new (e.g. neural fitted-q iteration). A DQN paper was the first to use deep convolutional networks to solve this type of problem and introduced a couple of novelties that make the training process much more stable. First of all, experience replay. The vanilla Q-learning point is to make a step, get the reward and the next state, then update the approximation function parameters based on this transition. The DQN idea is to make the transition and save it in a \u201creplay memory\u201d\u200a\u2014\u200aan array that stores the last 10\u2076 (<insert any large number here>) transitions with the information about the reward, states before and after the transition, and if the event is terminal (game over) or not. Having this experience replay we can randomly sample mini-batches from it and learn more effectively. Another thing that makes the algorithm more stable is that DQN uses two neural networks: the first to compute the Q value for the current state and the second to compute the Q value for the next state. You can see that from the equation with the objective: two different Q functions use \ud835\udfb1 and \ud835\udfb1\u2019, respectively. Each 10,000 steps, the parameters \ud835\udfb1\u2019 are copied from the learned parameters \ud835\udfb1 and this helps a lot in increasing the stability. The problem here with using one function is that when we update the weights, both Q(s,a) and Q(s\u2019,a\u2019) increase and this might lead to oscillations or policy divergence. Using two separate networks adds a delay between an update and computation of the target Q value and reduces such cases. If you have further interest in the phenomena, read the Method section in the DQN Nature paper. Okay, everything described above sounds quite simple. If there is still something you do not understand, please, have a look at David Silver\u2019s lecture where he explains everything perfectly! Knowing all of these, can we build an True AI now? I\u2019m sorry, but we can\u2019t. There are several problems that hinder us from building an agent that will beat ByuN at StarCraft II, bring an autonomous car to the market, or give you an opportunity to buy your grandma a robot that will do the dishes for her after lunch. One of these problems is that the rewards our agent gets might be very sparse in time. Let\u2019s say, you play chess. If you lose, how do you know when you made a catastrophic move? Moreover, it\u2019s highly possible that there was not a catastrophic move, but several average ones. Another problem that is closely connected to the previous one, is the sample-efficiency problem. Or, more honestly, sample-inefficiency. Even to master a simple game such as Space Invaders might take a couple of days in-game time. It\u2019s easy to speed up learning in games since we have access to the simulators, but what if we want to learn something in real life? Unfortunately, physics is not there yet and we cannot speed up time. There is an approach that could potentially solve these problems and a bunch of others\u200a\u2014\u200aImitation Learning, as we mentioned at the beginning of this post. As we said, we, humans, rarely learn something without any prior information. Let\u2019s use this data! What should we do? The idea of Imitation Learning is implicitly giving an agent prior information about the world by mimicking human behavior in some sense. Imitation Learning will not only help us solve the sample-inefficiency or computational feasibility problems, it might potentially make the training process safer. We cannot just put an autonomous car in the middle of the street and let it do whatever it wants. We do not want it to kill humans that are around, destroy someone\u2019s property, or the equipment itself. Pretraining it on a human demonstrator\u2019s data might make the training process faster and avoid undesirable situations. Training a model requires some data. Training a Deep Learning model requires even more data. Training a Deep Reinforcement Learning model requires\u2026 Okay, you get the idea. So, this series is only partly describes what we can do with demonstration data. The main point of all of this is to call for human demonstration datasets, because we do not have many, unfortunately, up to this moment. Okay, we should stop here. The key points of this post are: In the next chapter we will write more about Behavior Cloning\u200a\u2014\u200athe simplest approach to an RL problem that can leverage the human demonstration data. Stay tuned!", "links": ["https://blog.statsbot.co?source=logo-lo_c410f411d799---cfc9f21a543a", "https://twitter.com/statsbotco", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.statsbot.co%2Fintroduction-to-imitation-learning-32334c3b1e7a", "https://blog.statsbot.co", "https://blog.statsbot.co/analytics/home", "https://blog.statsbot.co/datascience/home", "https://blog.statsbot.co/design/home", "https://blog.statsbot.co/startups/home", "https://blog.statsbot.co/bots/home", "https://blog.statsbot.co/news/home", "https://blog.statsbot.co/statsbot-digest-b0d7372f842a", "http://statsbot.co?utm_source=blog&utm_medium=navigation&utm_campaign=robot_face", "https://blog.statsbot.co/search", "https://blog.statsbot.co/@yobibyte?source=post_header_lockup", "https://blog.statsbot.co/@yobibyte?source=post_header_lockup", "https://atlas.cern/discover/about", "http://statsbot.co?utm_source=blog&utm_medium=article&utm_campaign=imitation_learning", "https://en.wikipedia.org/wiki/CERN", "https://en.wikipedia.org/wiki/Arecibo_Observatory", "https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning/", "https://deepmind.com/research/alphago/", "https://www.theverge.com/tldr/2017/7/10/15946542/deepmind-parkour-agent-reinforcement-learning", "http://www.ceva-dsp.com/ourblog/wp-content/uploads/sites/3/2016/04/AlphaGo-Lee-Se-dol.png", "https://simple.wikipedia.org/wiki/File:Rl_agent.png", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf", "https://link.springer.com/article/10.1007/BF00992698", "https://pdfs.semanticscholar.org/2820/01869bd502c7917db8b32b75593addfbbc68.pdf", "https://link.springer.com/article/10.1007/BF00992699", "http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html?foxtrotcallback=true", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "http://wiki.teamliquid.net/starcraft2/ByuN", "http://toyoutheartist.co.uk/technique/imitation/", "https://mitpress.mit.edu/books/reinforcement-learning", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html", "https://arxiv.org/abs/1312.5602", "https://www.nature.com/nature/journal/v518/n7540/full/nature14236.html", "https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner", "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/google-analytics-audit-checklist-and-tools-fca7df2f2e7a", "https://blog.statsbot.co/google-analytics-audit-checklist-and-tools-fca7df2f2e7a", "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co/tagged/imitation-learning?source=post", "https://blog.statsbot.co/tagged/data-science?source=post", "https://blog.statsbot.co/tagged/machine-learning?source=post", "https://blog.statsbot.co/tagged/ai?source=post", "https://blog.statsbot.co/tagged/reinforcement-learning?source=post", "https://blog.statsbot.co/@yobibyte?source=footer_card", "https://blog.statsbot.co/@yobibyte", "https://github.com/yobibyte/yobiblog", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "162": {"url": "https://axibase.github.io/atsd-use-cases/DataShorts/Dollar-EX/", "title": "", "text": "Data Source: Federal Reserve Economic Data (FRED) Visualizations: ChartLab Structured Query Language: SQL Console from Axibase Download the Community Edition of Axibase Time Series Database here Data is aggregated daily in visualizations, and annually in SQL queries. Favorable dollar exhange rates are shown in green while less favorable exchange rates are shown in red, based on the overall 5-year average exchange rate. Each section is accompanied by a brief analysis to give the data context. Using computed value settings thresholds have been calculated using the underlying data, the upper 25th percentile of a given exchange rate is considered favorable while the lower 25th percentile is considered less than favorable. The data is coded with an ID which is provided in the index as well. Open any ChartLab display below to navigate through time, or change the metric to reflect the desired exchange rate. Europe Asia North America As the amount of debt carried by various EU member countries grew to increasingly worrisome heights, the Euro plunged in 2015  losing ground against the dollar. Further, as oil prices slumped the European Central Bank began to compensate by launching a program of government bond purchases. The combination of these factors and the growth of the dollar post-recession have all contributed to the currently favorable exchange rate enjoyed by the dollar. Query 1.1 Table 1.1 Figure 1.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index During the worst of the European Debt Crisis which saw European Union per capita debt grow at unprecedented rates across the continent, EU investors sought refuge in the stability of the Swiss Franc, backed by the world-renowned Swiss banking system. Further, the Swiss National Bank removed the peg to the Euro unexpectably in 2015 causing a huge surge in 2015, clearly visible in the visualization below as a sudden change from a favorable exchange rate for dollar holders to an all-time low for the observed time period. Query 1.2 Table 1.2 Figure 1.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Largely influenced by the divisive Brexit vote, the value of the Pound has remained relatively low since the referendum in late June of 2016, which aligns perfectly with the Pound\u2019s slight depreciation in value against the dollar on the chart below. Query 1.3 Table 1.3 Average Exchange Rate: 1.50 Figure 1.3   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index After purchasing trillions of Yen worth of Japanese national debt from the private sector three years ago, the Central Bank of Japan has been watching the gradual decrease in value of the Yen against the dollar. Most analysts, see this fall as controlled and predict a bright future for the Yen as the Central Bank can no longer afford to collect more debt than they already have. However, the Yen\u2019s rise much also be managed as the sudden appreciation of the currency could result in an increase in loan defaults because industries which have taken loans at current Yen prices would be paying them back at a much higher rate. Query 2.1 Table 2.1 Figure 2.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index To prevent dramatic internal inflation, the People\u2019s Bank of China closely regulates local, or onshore, trading of the Yuan by fixing the price each day. Although independent officially, the international, or offshore, price of the Yuan usually remains fairly close to its onshore value. Additionally, the PBoC carefully controls the outflow of capital and often hedges their own currency by maintaining positions in international currencies. Query 2.2 Table 2.2 Figure 2.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index The Hong Kong Dollar is officially pegged to the value of the United States Dollar so any change in relative  value is planned. The \u201cMiracle of the Orient\u201d continues to be one of the most dominant financial markets in the world. Query 2.3 Table 2.3 Figure 2.3   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index One of the financial centers of the world, Singapore is home to the second-busiest port in the world and one of the largest oil refining industries worldwide as well. With one of the highest per capita GDP values, Singapore has cemented its place of international importance despite inexplicably loose financial policy whereby the Monetary Authority of Singapore does not regulate their currency\u2019s value by adjusting interest rates, as is common with most central banking systems. Query 2.4 Table 2.4 Figure 2.4   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Relative to the USD, the Canadian dollar has been making a bit of a resurgence in 2017 as United States GDP numbers missed its targets in quarter two. Although dependant on the price of oil due to their large natural reserves, the diverse Canadian economy is one of the few globally-active nations with a significant trade surplus, although that too have been reduced in recent years. Query 3.1 Table 3.1 Figure 3.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Boasting the highest per capita income of Latin America, the Mexican economy has continued to demonstrate healthy growth over the last several decades. North America\u2019s number one producer of automobiles also enjoys a $46 billion trade surplus with the United States, and is the 9th ranked owner of United States government debt. This dependence on America affords Mexico the ability to spur dramatic internal growth that has led to the creation of a booming electronics sector, but at the cost of a  certain amount of reliance on the United States as roughly half of the country\u2019s imports and exports originate or end up  on U.S. soil. Query 3.2 Table 3.2 Figure 3.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index", "links": ["https://github.com/axibase/atsd-use-cases", "https://fred.stlouisfed.org/categories/94", "https://apps.axibase.com", "https://github.com/axibase/atsd/tree/master/api/sql", "https://axibase.com", "https://axibase.com/products/axibase-time-series-database/", "https://github.com/axibase/atsd/blob/master/installation/README.md#installation", "https://axibase.com/products/axibase-time-series-database/visualization/widgets/configuring-the-widgets/", "https://apps.axibse.com", "https://apps.axibase.com/chartlab/424eb6b2/5/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/9/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/6/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/4/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/2/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/3/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/8/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/7/#fullscreen", "https://github.com/axibase/atsd-use-cases", "https://github.com/axibase", "https://pages.github.com"]}, "163": {"url": "http://byteacademy.co/blog/overview-NLG", "title": "", "text": "NLG (Natural Language Generation), a subfield of Artificial Intelligence, is a hot topic in the technology news today. We hear a lot about AI that can soon replace writers and journalists beginning the era of machine creativity. But, what\u2019s all this fuss about? In this article, we unveil what NLG really is and show that it can bring a lot of benefits to businesses and consumers. In a nutshell, NLG is a sub-field of NLP (Natural Language Processing) that studies methods of automatic transformation of structured data into a human-readable text. In practice, there are two major types of NLG applications: template-based NLG and advanced NLG. \u00a0 Template-based NLG is the simplest solution that uses templates with canned text and placeholders to insert data into them. Such systems heavily rely on hard-coded rules, which makes them less flexible than advanced NLG. Since template-based NLG tools have a limited number of templates and require special data representations, they can not be easily reused across different projects and business use cases. \u00a0 Advanced NLG tools are more flexible thanks to the use of supervised and unsupervised Machine Learning (ML). Rather than tying down structured data to the Procrustean bed of templates, advanced NLG uses neural networks that learn morphological, lexical, and grammar patterns from large corpora of written language. Soft probabilistic methods used in the advanced NLG algorithms allow predicting the likelihood of one word appearing after another, and correcting language errors, such as misspellings. ML algorithms used in the advanced NLG are also better in dealing with new words and expressions not included in the original training samples. \u00a0 Modern NLG service providers such as Narrative Science and Automated Insights prefer advanced NLG methods because they allow creating rich data-driven models that produce intelligent insights from data. These algorithms are much more skillful in making right word choices and writing narratives that reflect intentions and business needs of the NLG users. As an added bonus, advanced NLG models can preprocess and analyze data which makes them not just translators of structured data into text, but automatic analysts able to provide actionable insights. \u00a0 Despite the fact that NLG methods have been used since the 1970s, they got a powerful momentum only recently and thanks to the AI/ML revolution. Today, many startups offer cloud-based NLG services to businesses. NLG is also gaining traction in mass media and journalism. Major American newspapers are already experimenting with the automatic storytelling. For example, in 2016 the Washington Post unveiled its automatic storytelling AI named Heliograph AI. Heliograph was used in the coverage of Rio Olympics and the US Presidential election in 2016. \u00a0 Leveraging data mining techniques and ML models the machine reporter can convert structured statistical data, diagrams, graphs, weather forecasts and other data-rich content into excellent descriptive reports that sound if though they were written by the professional reporters. But, isn\u2019t this dangerous for journalism as a profession? Proponents of automatic storytellers say that they actually free up much time for reporters to add analysis and real insights to stories rather than spending countless hours publishing news and descriptive reports[i]. NLG tools may be used in other innovative ways as well: Benefits of NLG, however, go beyond journalism. There is a growing demand for NLG services among major companies. For example, Quill, an NLG system developed by Narrative Science, is used by such companies as Deloitte, Groupon, and Credit Suisse[iv]. These companies opt for NLG solutions for a reason. Growing acceptance of NLG among businesses makes it a promising field to study. If you want to learn more about NLG, \u00a0Byte Academy offers a Natural Language course that covers Natural Language Processing and Natural Language Generation. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0  Your Name (required)    Your Email (required)    \u00a0 \u00a0 References: [i] \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 WashPost PR Blog (August 5, 2016). The Washington Post Experiments With Automated Storytelling to Help Power 2016 Rio Olympics Coverage. WashPost PR Blog. Retrieved from https://www.washingtonpost.com/pr/wp/2016/08/05/the-washington-post-experiments-with-automated-storytelling-to-help-power-2016-rio-olympics-coverage/?utm_term=.bf63b03c4aeb [ii] \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dayan, Zohar (2015). Hearst, USA Today Sports, & Viralnova Partner With Wibbitz For Video Strategy. Wibbitz Blog. Retrieved from http://blog.wibbitz.com/wibbitz-partners-hearst-usa-today-sports-group-and-viralnova-to-expand-video-strategy [iii] \u00a0\u00a0\u00a0\u00a0\u00a0 Keohane, Joe (2017). What News-Writing Bots Mean for the Future of Journalism. Wired. Retrieved from https://www.wired.com/2017/02/robots-wrote-this-story/ [iv] \u00a0\u00a0\u00a0\u00a0 Narrative Science. Turn Your Data Into Better Decisions With Quill. Retrieved from https://narrativescience.com/Platform info.in@byteacademy.co Byte Dev Licensed by the State of New York, New York State Education Department \u00a9 2017 Byte Academy LLC.  All rights reserved.", "links": ["http://byteacademy.co", "http://byteacademy.co/program/", "http://byteacademy.co/courses/", "http://byteacademy.co/data-science-mini-courses/", "http://byteacademy.co/upcoming-classes/", "http://byteacademy.co/students/", "http://byteacademy.co/hiring/", "http://byteacademy.co/events/", "http://byteacademy.co/blog/", "http://byteacademy.co/india/", "https://byteacademy.fluidreview.com", "http://feeds.feedburner.com/ByteAcademy", "http://byteacademy.co/category/data-science/", "http://byteacademy.co/category/career/", "http://byteacademy.co/category/fintech/", "http://byteacademy.co/category/medtech/", "http://byteacademy.co/category/programming-tips/", "http://byteacademy.co/category/startup/", "http://byteacademy.co/category/student-stories/", "http://byteacademy.co/category/blockchain/", "http://byteacademy.co/category/current-events/", "http://byteacademy.co/category/diversity/", "http://byteacademy.co/category/events/", "http://byteacademy.co/category/finovate/", "http://byteacademy.co/category/light-reads/", "http://byteacademy.co/category/millennials/", "http://byteacademy.co/category/quant/", "http://byteacademy.co/2017/", "http://byteacademy.co/2016/", "http://byteacademy.co/2015/", "http://byteacademy.co/category/data-science/", "http://byteacademy.co/blog/artificial-intelligence-future", "https://www.washingtonpost.com/pr/wp/2016/08/05/the-washington-post-experiments-with-automated-storytelling-to-help-power-2016-rio-olympics-coverage/?utm_term=.bf63b03c4aeb", "http://www.reddit.com/submit?url=http://byteacademy.co/blog/overview-NLG", "http://www.tumblr.com/share?v=3&u=http://byteacademy.co/blog/overview-NLG&t=Overview of Natural Language Generation (NLG)", "http://byteacademy.co/blog/artificial-intelligence-future", "http://byteacademy.co/blog/artificial-intelligence-future", "http://byteacademy.co/blog/data-science-podcasts", "http://byteacademy.co/blog/data-science-podcasts", "http://byteacademy.co/blog/data-science-fintech/", "http://byteacademy.co/blog/data-science-fintech/", "http://byteacademy.co/financial-aid/", "http://byteacademy.co/upcoming-classes/", "http://byteacademy.co/contact-us/", "http://byteacademy.co/corporate-training/", "http://byteacademy.co/faq/", "http://byteacademy.co/press-page/", "http://byteacademy.co/careers-at-byte/", "http://byteacademy.co/brand-ambassador/", "https://www.meetup.com/Byte-Academy-Finance-and-Technology-community/", "http://www.bytedev.co", "https://www.facebook.com/byteacademy/", "https://twitter.com/ByteAcademyCo", "https://www.linkedin.com/edu/school?id=171001", "https://github.com/ByteAcademyCo", "https://www.instagram.com/byteacademy/", "https://www.meetup.com/Byte-Academy-Finance-and-Technology-community/", "https://www.quora.com/topic/Byte-Academy", "https://www.youtube.com/channel/UCrMcJALnO748TSK27bZQzSg", "http://byteacademy.co/privacy-policy/"]}, "164": {"url": "http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "title": "", "text": "", "links": ["http://www.getrevue.co/?ref=Revue+Profile", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profileimage&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profiledate&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profilename&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Mm6Kn?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Mm6Kn?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.datasciencecentral.com/profiles/blogs/automated-machine-learning-for-professionals?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lXBy8?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lXBy8?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.nature.com/articles/s41467-017-00181-8?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/d0nAm?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/d0nAm?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/4mO6D?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/4mO6D?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/eK78N?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/eK78N?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://sourcedexter.com/amazing-tensorflow-github-projects/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xejEQ?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xejEQ?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://techblog.appnexus.com/a-keras-multithreaded-dataframe-generator-for-millions-of-image-files-84d3027f6f43?gi=79cece23ee37&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/6nA72?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/6nA72?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://blog.revolutionanalytics.com/2017/08/a-modern-database-interface-for-r.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/5kyK3?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/5kyK3?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://ruder.io/deep-learning-nlp-best-practices/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/7yxNV?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/7yxNV?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://hyperparameter.space/blog/when-not-to-use-deep-learning/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/8A1NP?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/8A1NP?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278?gi=8e0386afa61e&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/1bKwn?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/1bKwn?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://adeshpande3.github.io/How-I-Used-Deep-Learning-to-Train-a-Chatbot-to-Talk-Like-Me?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lX0x4?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lX0x4?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.youtube.com/channel/UC0n76gicaarsN_Y9YShWwhw/playlists?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/qYVyK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/qYVyK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.youtube.com/watch?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest&v=0fhUJT21-bs", "http://rev.vu/Nkeov?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Nkeov?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%239%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "https://www.getrevue.co/?utm_source=Data Science Digest&utm_medium=email&utm_content=footerlink&utm_campaign=Issue", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=forwarded&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%239%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "http://www.getrevue.co/?ref=Revue+Profile"]}, "165": {"url": "http://rpsychologist.com/d3/bayes/", "title": "", "text": "Created by Kristoffer Magnusson The visualization shows a Bayesian two-sample t test, for simplicity the variance is assumed to be known. It illustrates both Bayesian estimation via the posterior distribution for the effect, and Bayesian hypothesis testing via Bayes factor. The frequentist p-value is also shown. The null hypothesis, H0 is that the effect \u03b4 = 0, and the alternative H1: \u03b4 \u2260 0, just like a two-tailed t test. You can use the sliders to vary the observed effect (Cohen's d), sample size (n per group) and the prior on \u03b4. The prior on the effect is a scaled unit-information prior. The black, and red circle on the curves represents the likelihood of 0 under the prior and posterior. Their likelihood ratio is the Savage-Dickey density ratio, which I use here as to compute Bayes factor. The p-value is the traditional p-value for a two-sample t test with known variance (i.e. a Z test).        HDI is the posterior highest density interval, which in this case is analogous a credible interval. And CI is the traditional frequentist confidence interval.  Check out Alexander Etz's blog series \"Understanding Bayes\" for a really good introduction to Bayes factor. Fabian Dablander also wrote a really good post, \"Bayesian statistics: why and how\", which introduces Bayesian inference in general. If you're interesting in an easy way to perform a Bayesian t test check out JASP, or BayesFactor if you use R. Interactive visualization of Cohen's d effect size Interactive visualization of statistical power and significance testing Interactive visualization of Confidence Intervals Have any suggestion? Or found any bugs? Send them to me, my contact info can be found here. Designed and built by Kristoffer Magnusson. Built with D3.js, jStat and Bootstrap.", "links": ["http://rpsychologist.com", "http://rpsychologist.com", "https://twitter.com/krstoffr", "https://se.linkedin.com/pub/kristoffer-magnusson/b5/133/1b6", "https://twitter.com/share", "http://alexanderetz.com/understanding-bayes/", "http://blog.efpsa.org/2015/08/03/bayesian-statistics-why-and-how/", "https://jasp-stats.org", "http://bayesfactorpcl.r-forge.r-project.org", "http://rpsychologist.com/d3/cohend/", "http://rpsychologist.com/d3/NHST/", "http://rpsychologist.com/d3/CI/", "http://rpsychologist.com/tag/d3js.html", "http://rpsychologist.com/about", "http://rpsychologist.com", "http://d3js.org/", "http://jstat.org/", "http://getbootstrap.com/", "http://twitter.com/krstoffr", "https://github.com/rpsychologist"]}, "166": {"url": "https://blog.datasyndrome.com/generalists-dominate-data-science-f01882f25347", "title": "Generalists Dominate Data\u00a0Science", "text": "Analytics products and systems are best built by small teams of generalists. Large teams of specialists become dominated by communication overhead, and the effect of \u201cChinese whispers\u201d distorts the flow of tasks and stagnates creativity. Data scientists should develop generalist skills to become more efficient members of a data science team. Building data products takes a team covering a broad and diverse skillset. From the customer representative at one end, to the operations engineer at the other, the spectrum of roles in a product analytics team looks like this: Large companies often fill each role with a pair of shoes, resulting in a twelve person team like the one below. The problem with this setup is that it becomes much more difficult to achieve consensus and to perform any task that spans roles. And in data science, most tasks span roles. To look at a particular example, lets focus on the creation of a chart as part of a data product. To begin, a product manager creates a specification, then an interaction designer mocks up the chart, handing it off to a data scientist to fill with data (and hopefully to explore the data and find a chart worth producing), then a back-end engineer to setup an API to grab that data, a front-end web developer to create a web page using the data that matches the mock, and an experience designer to ensure the entire thing feels right and makes sense. Charts take iteration, so this cycle of communication could happen repeatedly for each chart. You can see how communication overhead starts to predominate. A meeting of six people is a full-blown, formal meeting. It is hard to get things done in formal meetings. In the next figure, we see how a data product team might be composed of four generalists: a data engineer, a data scientist/back-end developer, a designer who can build front ends and a product manager that can write marketing copy and cut deals. This is how a startup team would span the skill spectrum, and you can probably see how this makes them more efficient. Revisiting the chart example, creating a chart becomes a collaboration between the product manager, a designer who codes, and a data scientist. This is the kind of ad hoc meeting of 2\u20133 people where \u201cshit gets done\u201d efficiently. This group will be more efficient than the six man group. Put another way: this small team will kick the large team\u2019s ass. In the big company system, sometimes the only way to get anything done efficiently is to go \u201cguerilla generalist\u201d and work with other generalists to cut people out of the chain. This is bad politically, and is part of what drives effective people from big companies. We\u2019ve shown that small teams generalists outperform large teams of specialists. In fact, generalist skills are something every data scientists should work to develop. That doesn\u2019t mean you can\u2019t specialize, but should combine specialization with generalization in order to develop \u201cT-shaped skills.\u201d The T-shaped employee is one that can lend deep expertise across projects while fulfilling multiple roles in his own. It takes time to develop general skills, and that is why the path to becoming a data scientist is not a six month bootcamp, but a ten year journey. Along this path, remember to try to be T-Shaped! Need help building an analytics product or platform? The Data Syndrome team of data scientists and data engineers is available to build your data products and systems as a service. We also offer training in Agile Data Science for all members of data science teams. Clapping shows how much you appreciated Russell Jurney\u2019s story.", "links": ["https://blog.datasyndrome.com?source=logo-lo_aa26d5b3dd6a---500653fb51a1", "https://twitter.com/datasyndrome", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.datasyndrome.com%2Fgeneralists-dominate-data-science-f01882f25347", "https://blog.datasyndrome.com/@rjurney?source=post_header_lockup", "https://blog.datasyndrome.com/@rjurney?source=post_header_lockup", "https://personalmba.com/communication-overhead/", "https://en.wikipedia.org/wiki/Chinese_whispers", "https://en.wikipedia.org/wiki/T-shaped_skills", "http://datasyndrome.com", "http://datasyndrome.com/training", "https://blog.datasyndrome.com/tagged/data-science?source=post", "https://blog.datasyndrome.com/tagged/agile?source=post", "https://blog.datasyndrome.com/tagged/generalists?source=post", "https://blog.datasyndrome.com/tagged/analytics?source=post", "https://blog.datasyndrome.com/tagged/software-engineering?source=post", "https://blog.datasyndrome.com/@rjurney?source=footer_card", "https://blog.datasyndrome.com/@rjurney", "https://blog.datasyndrome.com?source=footer_card", "https://blog.datasyndrome.com?source=footer_card", "https://blog.datasyndrome.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "167": {"url": "https://github.com/axibase/atsd-use-cases/tree/master/FED_FORDSR#the-average-american-debt-profile", "title": "", "text": "Debt is a complicated concept. After the sub-prime mortgage crisis of the late 2000s, modern Americans are all too familiar with the problems of irresponsible spending on credit. Student loan recipients who queue up to drop off another application for a job in a field they did not study are quick to point to the trappings of deficit spending as a means of wealth creation. Politicians and voters on both sides of the aisle point to the ever-growing United States Government debt with anxiety for the future. And yet despite all the doom and gloom, the American financial system is one of the most stable and robust in the world, in no small part thanks to ingenious monetary policy and hegemonic economic position organized over the entire course of the country's history, modern American consumers are among the wealthiest on the planet. The United States Federal Reserve is the central banking system of the United States, responsible for monitoring the global financial climate and enacting policy that supports the American economy and American consumers. They maintain a number of statistics about these consumers and their monetary practices to better inform their decisions and practices. Provided by the Federal Reserve, this dataset must be correctly parsed during import. The quarterly date format needs to be converted into a monthly format that ATSD can interprete (Q/q letter is not supported). We also need to discard metadata lines contained in the multi-line header. This can be accompilshed with a schema-based parser that provides granular control over the document's rows and columns using RFC 7111 selectors and Javascript: Script 1.1 For step-by-step instructions on data customization with schema based parsing, see this support tutorial. The Financial Obligation Ratio (FOR) is an estimate of the ratio of required debt payments to disposable income. This is a broad calculation and includes all kinds of debt:  mortgage payments, credit cards, property tax and lease payments. Each of these metrics can be expanded further to include associated costs, such as homeowner's insurance for example. The Federal Reserve releases this number each quarter. Figure 1.1   Use the dropdown menus at the top of the visualization screen to navigate through time, selecting the starttime and endtime values to observe a desired period. The data can also be queried with a structured query language in the SQL Console. The data will be aggregated annually, derived from the average value of each quarter within a given year: Query 1.1 Table 1.1 All values are shown as a percent of one hundred, where the whole is representative of the total income of the average person. The Debt Service Ratio (DSR) is more specific than the Financial Obligation Ratio in that it typically does not include non-essential debt payments. Here, it has been parsed into two categories, mortgage debt and consumer debt. These numbers represent the average percent of a person's earned salary each month which much be used to make the required payments associated with consumer credit and mortgage. Typically the DSR is an initial calculation performed to determine a person's eligibility to receive a mortgage. A DSR value of less than 48% is generally preferred, meaning that with a particular mortgage plus other credit obligations at least 52% of a person's gross monthly earning would still be available to them after making the required payments. Figure 2.1   Query 2.1 Table 2.1 Because the FOR value includes the DSR value plus additional non-essential credit values, and the DSR value is parsed into both consumer and mortgage related debt, these three values can be shown in a new visualization that creates a typical consumer profile of the average American. By using the calculated value setting shown below, additional data not specifically included in the set can be displayed: Script 2.1 Shown below is the debt profile of the average American consumer from 1980 to 2017, navigate through time using the dropdown menus at the top of the screen to select a desired span of time and compare how bearing debt has changed over the course of the last three decades. Figure 3.1   The visualization can also be organized to show the amount of each type of debt as it relates to the others: Figure 3.2   Additionally, these values can be compared on an annual basis as shown in the visualization below: Figure 3.3   To view the distribution of these values across time, a histogram is shown below: Figure 3.4   In the following box diagram, explore time with the dropdown menus at the top of the visualization screen. The visualization shows the distribution of debt values as a percentage of total income, with the initial time period set to include the entire data set: Figure 3.5   The following SQL query will detail the above visualizations in one table, displaying averaged annual values of each component described above: non-essential credit payments, mortgage credit payments, and consumer credit payments, as well as the Financial Obligation Ratio (FOR), or total debt obligations. Query 3.1 Table 3.1 The above dataset can illuminate a number of features of the American economy and a number of characteristics of the average American consumer. While modern Americans are quick to denounce the zeitgeist of living outside of one's means, the data shows that in fact, the amount of debt carried by the average American is on par with or even lower in some cases than that of his 1980's counterpart. In fact, the only metric which has demonstrated a legitimate increase in value over the last several decades has been the roughly one percent increase in non-essential credit holdings by the average consumer. According to data from the Economic Research Department of the Saint Louis Branch of the Federal Reserve, the 2015 US median household income was $56,516 per year in 2015 USD. This number can be applied to the above table and visualized in ChartLab to create more comprehensive data. Figure 3.6   The above visualization aggregates the values from Table 3.1 based on a time period of the user's selection. Use the dropdown menu at the top of the screen to select the aggregation period. The initial visualization shows the average values for each metric over the entire period of time in 2015 USD by obligation amount per quarter. The following query summons the same data shown above, but further parses it to show annual average monthly payments instead of quarterly values in 2015 USD for a person making the 2015 median United States income of $56,516 a year. Query 4.3 Table 3.2 As it turns out, the idea that your parents paid less for their house than you will is only true in absolute terms. When compared with current numbers and controlled for inflation, the average 2017 consumer will pay roughly the same portion of their income towards a place to hang their hat up as the average 1980 consumer. The Federal Reserve is able to pull certain levers of power from the Eccles Building in Washington, D.C. such as printing more money, or raising and lowering interest rates to cope with inflation. However, all of these are reactionary measures meant to create small changes that have a butterfly effect over time. Ultimately, the machinations of the Board of Governers have always be something opaque and esoteric to the average man, leading to many people denouncing the Federal Reserve System entirely, occasionally opting for a return of the gold standard or leveling accusations of wrong-doing. However, after reviewing the data above, it seems that at least on a consumer level, the average American actually has more today than they would have had thirty years ago, or even just five years ago. Of course, the Federal Reserve isn't completely responsible for the wise consumer choices made in the current decades, but monetary policy enacted by the various branches of the Federal Reserve are responsible for maintaining the economic conditions that Americans, and consumers the world over, have come to expect from the United States economy.", "links": ["https://github.com/", "https://github.com/axibase/atsd-use-cases/pull/86", "http://www.usdebtclock.org/", "https://www.federalreserve.gov/", "https://www.federalreserve.gov/datadownload/Download.aspx?rel=FOR&series=91e0f9a6b8e6a4b1ef334ce2eaf22860&filetype=csv&label=include&layout=seriescolumn&from=01/01/1980&to=12/31/2017", "https://axibase.com/products/axibase-time-series-database/writing-data/csv/", "https://apps.axibase.com/chartlab/842f1dd9/#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql", "https://apps.axibase.com/chartlab/85522dd3/#fullscreen", "https://apps.axibase.com/chartlab/f25de723/#fullscreen", "https://apps.axibase.com/chartlab/f25de723/3/#fullscreen", "https://apps.axibase.com/chartlab/81ea0ea0/#fullscreen", "https://apps.axibase.com/chartlab/9f74c179/#fullscreen", "https://apps.axibase.com/chartlab/20ff0ade/#fullscreen", "https://fred.stlouisfed.org/series/MEHOINUSA646N", "https://research.stlouisfed.org/", "https://www.stlouisfed.org/", "https://apps.axibase.com", "https://apps.axibase.com/chartlab/da132e01/11/#fullscreen", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "168": {"url": "http://tech.marksblogg.com/billion-nyc-taxi-rides-aws-ec2-p2-16xlarge-brytlytdb.html", "title": "", "text": "I have 14 years of hands-on build and consulting experience with clients in the UK, Ireland & Germany. I've done both back- and frontend work for Bank of America Merrill Lynch, Blackberry, Bloomberg, British Telecom, Ford, Google, ITV, News UK, Pizza Hut, Royal Bank of Scotland, Royal Mail, T-Mobile, Vertu (Nokia subsidiary at the time), Williams Formula 1 Team and Xerox. I hold both a Canadian and a British passport. My CV & my LinkedIn profile.         Home           | Benchmarks           | Archives            | Atom Feed  Posted on Fri 28 July 2017 BrytlytDB is an in-GPU-memory database built on top of PostgreSQL. It's operated using many of PostgreSQL's command line utilities, it's wire protocol compatible so third-party PostgreSQL clients can connect to BrytlytDB and queries are even parsed, planned and optimised by PostgreSQL's regular codebase before the execution plan is passed off to GPU-optimised portions of code BrytlytDB offer. Clustering works right out of the box, GPU-powered JOINs are supported, Stored Procedures are fully functional, Deep- and Machine Learning workloads via Torch are supported and BI visualisation software in the form of SpotLyt is included with BrytlytDB as well. The feature list is too long for one blog to cover so in this post I'll just be taking a look at how performant BrytlytDB's OLAP functionality is. In this benchmark I'll see how well 32 Tesla K80 GPUs spread across two EC2 instances perform when querying 1.1 billion taxi trips. I'll be using the same dataset I've used to benchmark Amazon Athena, BigQuery, ClickHouse, Elasticsearch, EMR, kdb+/q, MapD, PostgreSQL, Redshift and Vertica. I have a single-page summary of all these benchmarks for comparison. For this benchmark I'll be using two p2.16xlarge EC2 instances running Ubuntu 16.04.2 LTS in Amazon Web Services' eu-west-1a region in Ireland. Each machine has 8 Nvidia K80  cards which have 2 GPUs each, 64 virtual CPUs and 732 GB of memory. There's also 20 Gbit/s of networking capacity available to each instance. Below are the specifications of the compute capabilities available each one of the EC2 instances. This is the layout of the GPUs available on one of the two EC2 instances. The first machine has an IP address of 52.214.237.134 and has three roles: Global Transaction Manager, Coordinator and host to the first 16 data nodes. The second machine has an IP address of 34.250.232.38 and has two roles: it is also a coordinator and hosts the second set of 16 data nodes. TCP port 5432 is open between the two machines for communicating via PostgreSQL's wire protocol, TCP port 7777 is open for global transaction manager communication and ports 20,000 through to 20,0031 are open for data node communication. In addition to the default 20 GB EBS volumes on each EC2 instance there are six 500 GB General Purpose SSDs attached which offer a baseline of 100 IOPS each and can burst to 3,000 IOPS if need be. These drives are setup in a RAID 0 configuration on each instance. Before setting up the RAID array I'll install a few dependencies. This was run on both EC2 instances. Below are the commands used to setup the RAID array on each instance. Here's what the RAID layout looked like after it was setup. This RAID 0 setup offers a partition with a capacity of 3.2 TB on each instance: I've run the following to download BrytlytDB's install script. BrytlytDB is commercial software so I cannot divulge the URL I pulled this from at this time. The following was run on both EC2 instances. I've then edited the install script on the first instance with the following instance-specific values: And the install script on the second instance was edited with the following instance-specific values: With those changes in place I then ran the install script on both EC2 instances. The install script conducts the following: Once that's done I can form the cluster with the following registration script. The following was run on the first EC2 instance. The second registration script run on the other EC2 instance is identical with the first with the exception of the coordinator line pointing to 52.214.237.134 instead. For this benchmark I've downloaded and decompressed one half the 500 GB of CSV data I created in my Billion Taxi Rides in Redshift blog post onto each EC2 instance. The data sits across 56 files across both machines but for the quickest load time I want to create 32 reasonably equally-sized CSV files and load 16 files on each EC2 instance simultaneously. The 32 files will pair up with the 32 GPUs available across the cluster and should allow for the best utilisation of the GPU resources when running queries. Here is half of the original 500 GB data set on the first EC2 instance: And this is the other half on the second instance: On each instance I'll decompress the GZIP files. Below took about four minutes to complete on each instance. Then I'll concatenate the ~560 million lines of CSV data on each instance and break it up into files of 35 million lines each. The above completed in 8 minutes on each EC2 instance. I'm now left with 16 CSV files on the first EC2 instance: And 16 files on the other EC2 instance: I'll then connect to the coordinator node on each EC2 instance and setup BrytlytDB's gpu_manager_fdw extension and foreign data wrapper. I can then create the trips table. The above sets the maximum row count per GPU device at 38 million and creates an index on the cab_type column. The cab_type column has very low cardinality so the indexing will be less beneficial versus a column with a higher cardinality of values. Some of the data types used in the above table aren't the types I normally use in my benchmarks. The reason for this is that there is still limited data type support available in this early version of BrytlytDB. I've had to substitute DATETIME with DATE which will truncate the timestamps in the dataset to just the date rather than the full date and time. There is no SMALLINT support yet so I've had to use the larger INT type as an replacement for those fields. DECIMAL types aren't yet supported so I'll be using DOUBLE PRECISION as a replacement there. The above replacements could speed up or slow down the queries I benchmark with so I'm hoping to do another benchmark when data type support is widened in the future. With the table created I'll launch 32 simultaneous load jobs across the two EC2 instances to load the data into BrytlytDB's trips table. This is the import script I ran on the first EC2 instance: And this is the load script for the second instance. The first EC2 instance loaded its half of the dataset in 1 hour, 12 minutes and 21 seconds. The second instance loaded its half in 1 hour, 9 minutes and 57 seconds. After the data was loaded in I could see the PostgreSQL data directory filled up with reasonably evenly-sized data folders. Here's what they look like on the first EC2 instance: I was then able to use PostgreSQL's CLI tool to make sure I can see the table and all 1.1 billion records. The times quoted below are the lowest query times seen during a series of runs. As with all my benchmarks, I use the lowest query time as a way of indicating \"top speed\". The following completed in 0.762 seconds. The following completed in 2.472 seconds. The following completed in 4.131 seconds. The following completed in 6.041 seconds. Given all the features of PostgreSQL are still available I'm blown away at how fast BrytlytDB is able to aggregate data. The cluster I used cost around $30 / hour which means this system out performed other Cloud-based and PostgreSQL-based data warehousing solutions both in terms of wall clock time and in terms of cost per hour of running the cluster. That being said I know Nvidia's K80 chips use extremely fast memory and have thousands of compute cores so I expect further optimisations from BrytlytDB to drive down these already fast query times. \u2190 Back to Index Copyright \u00a9 2014 - 2017 Mark Litwintschik. This site's template is based off a template by Giulio Fidente.", "links": ["https://twitter.com/marklit82", "http://tech.marksblogg.com/theme/cv.pdf", "https://uk.linkedin.com/in/marklitwintschik/", "http://tech.marksblogg.com/feeds/all.atom.xml", "http://www.brytlyt.com/", "https://uk.linkedin.com/in/marklitwintschik/", "https://github.com/giulivo/pelican-svbhack"]}, "169": {"url": "https://blog.datazar.com/why-we-focus-on-design-just-as-much-as-functionality-f6ccb3b2c926", "title": "Why We Focus on Design Just as Much as Functionality", "text": "It\u2019s no secret that scientific software isn\u2019t the most beautiful software in terms of design/aesthetics (generally speaking here). It is very effective though; it gets the job done. It does exactly what you expect it to do. Being a huge movie buff, I\u2019ve always wondered why the software we use in the laboratories or classrooms looks nothing like what we see in the movies. Movies do this thing where tech looks exciting and futuristic and super fun to use. And then I go back to reality and it\u2019s nothing like what I just watched. It\u2019s worth mentioning that not everything in the movies looks practical, especially when it comes to software or even more ridiculous: computer hacking. But that\u2019s another subject. The fact there\u2019s some art missing is something I noticed not only in the software we use in the lab, but also the instruments we build and the papers we write. It seems as if the art has been completely taken out of the sciences. Before you say it, we obviously can\u2019t incorporate art into somethings like highly sensitive instruments as unnecessary additions area big no because then you\u2019re interfering. Another thing to note might be the fact that hundreds or even some thousands years ago, the scientists were also artists. Art came naturally to them as they were also discovering mother nature. At that time, science was also only available to the rich which meant a couple of things: they could afford to spend a little bit of time incorporating art into their scientific work as they weren\u2019t rushing to meet grant deadlines, but it also meant they were already educated in the arts from an early age. These days anyone can be whatever they want to be. To the very least, it\u2019s easier now than it was in that era by an order of magnitude. Today we\u2019re taught that art almost has no place in the sciences and we must be efficient and to the point. Anything extra is generally referred to as fluff. If you like to romanticize science, you\u2019re not really a scientist. But I strongly believe that it\u2019s the artistic and I might add\u200a\u2014\u200aphilosophical\u200a\u2014\u200afluff that gets people interested in science. The hard math and core scientific values obviously being there regardless. All of this results in the absence of art in the sciences which makes art seem dry and boring. Science in its purest form is absolutely beautiful if communicated right. That\u2019s why we love to do our part at Datazar. We\u2019re far, far from perfect or even close to what we want to be. But putting just as much thought in design and UX as we put in functionality is a core principle. There\u2019s a misconception that incorporating art means adding more stuff. Incorporating art can also mean doing less, using less and making the most of what you already have in a beautiful and simple way. An example would be removing that obscene amount of jargon from your paper. So I ask everyone designing software for scientists, design for humans not robots. Scientists are humans too. So go ahead add that CSS animation. Clapping shows how much you appreciated Aman Tsegai\u2019s story.", "links": ["https://blog.datazar.com?source=logo-lo_8ecd22ee1045---e2c7e6e1c75", "https://twitter.com/DatazarHQ", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.datazar.com%2Fwhy-we-focus-on-design-just-as-much-as-functionality-f6ccb3b2c926", "https://blog.datazar.com", "https://blog.datazar.com/tagged/r-language", "https://blog.datazar.com/tagged/open-data", "https://blog.datazar.com/tagged/how-to", "https://www.r-bloggers.com/", "https://blog.datazar.com/search", "https://blog.datazar.com/@ata_aman?source=post_header_lockup", "https://blog.datazar.com/@ata_aman?source=post_header_lockup", "https://blog.datazar.com/tagged/science?source=post", "https://blog.datazar.com/tagged/art?source=post", "https://blog.datazar.com/tagged/research?source=post", "https://blog.datazar.com/tagged/software-development?source=post", "https://blog.datazar.com/tagged/design?source=post", "https://blog.datazar.com/@ata_aman?source=footer_card", "https://blog.datazar.com/@ata_aman", "http://twitter.com/datazarhq", "https://blog.datazar.com?source=footer_card", "https://blog.datazar.com?source=footer_card", "https://blog.datazar.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "170": {"url": "https://news.ycombinator.com/item?id=14950255", "title": "", "text": "", "links": ["https://news.ycombinator.com", "https://edgylabs.com/machine-learning-to-enhance-smartphone-pictures/", "https://hn.algolia.com/?query=Machine%20Learning%20to%20Enhance%20Smartphone%20Pictures&sort=byDate&dateRange=all&type=story&storyText=false&prefix&page=0", "https://www.google.com/search?q=Machine%20Learning%20to%20Enhance%20Smartphone%20Pictures", "https://github.com/HackerNews/API", "http://www.ycombinator.com/apply/"]}, "171": {"url": "http://starmine.ai/datasets/ds02.html", "title": "", "text": "reddit Slack Twitter Instagram Facebook LinkedIn", "links": ["http://starmine.ai", "http://reddit.com/r/datasets", "https://join.slack.com/t/starmineai/shared_invite/MjIwNjg5OTUyODAzLTE1MDE2MTk1MzQtMmJhYTExNDViMA", "https://twitter.com/starmineAI", "https://www.instagram.com/starmine.ai/", "https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Fstarmine.ai/index.html&t=starmine%20ICO", "https://www.linkedin.com/cws/share?url=http%3A%2F%2Fstarmine.ai/index.html&token=&isFramed=true", "http://research.kraeutli.com/index.php/2013/11/the-tate-collection-on-github", "http://54.174.116.134/recommend/datasets/subscribers/free/supercolumns/supercolumns-elements-CMDB-nasdaq-nyse-otcbb-general-2017/supercolumns-elements-nasdaq-nyse-otcbb-general-UPDATE-2017-03-01.csv", "http://54.174.116.134/recommend/datasets/subscribers/free/supercolumns/supercolumns-elements-CMDB-nasdaq-nyse-otcbb-general-2017/supercolumns-elements-nasdaq-nyse-otcbb-general-UPDATE-2017-03-01.csv", "http://54.174.116.134/recommend/app/ai_connect-finance-historicaltrends_api?ccl1=playstation&ccl2=helium&ccl3=korea&ccl4=shampoo&ccl5=coffee&query=concept_column_labels&themesource=theme_CMDB-nasdaq-nyse-otcbb&month=01&year=all&db=CMDB-nasdaq-nyse-otcbb-general", "https://www0.gsb.columbia.edu/mygsb/faculty/research/pubfiles/1555/contagious.pdf", "https://trends.google.com/trends/explore?q=playstation,helium,korea,shampoo,coffee", "https://trends.google.com/trends/explore?q=playstation,helium,korea,shampoo,coffee"]}, "172": {"url": "https://github.com/maciejkula/spotlight/blob/master/examples/movielens_sequence/readme.rst", "title": "", "text": "Using sequences of user-item interactions as an input for recommender models has a number of attractive properties. Firstly, it recognizes that recommending the next item that a user may want to buy or see is precisely the goal we are trying to achieve. Secondly, it's plausible that the ordering of users' interactions carries additional information over and above just the identities of items they have interacted with. For example, a user is more likely to watch the next episode of a given TV series if they've just finished the previous episode. Finally, when the sequence of past interactions rather than the identity of the user is the input to a model, online systems can incorporate new users (and old users' new actions) in real time. They are fed to the existing model, and do not require a new model to be fit to incorporate new information (unlike factorization models). Recurrent neural networks are the most natural way of modelling such sequence problems. In recommendations, gated recurrent units (GRUs) have been used with success in the Session-based recommendations with recurrent neural networks paper. Spotlight implements a similar model using LSTM units as one of its sequence representations. But recurrent neural networks are not the only way of effectively representing sequences: convolutions can also do the job. In particular, we can use causal convolutions: convolutional filters applied to the sequence in a left-to-right fashion, emitting a representation at each step. They are causal in that the their output at time t is conditional on input up to t-1: this is necessary to ensure that they do not have access to the elements of the sequence we are trying to predict. Like LSTMs, causal convolutions can model sequences with long-term dependencies. This is achieved in two ways: stacking convolutional layers (with padding, every convolutional layer preserves the shape of the input), and dilation: insertion of gaps into the convolutional filters (otherwise known as atrous convolutions). Causal convolutions have been used in several recent high-profile papers: Using convolutional rather than recurrent networks for representing sequences has a couple of advantages, as described in this blog post: Spotlight implements causal convolution models as part of its sequence models package, alongside more traditional recurrent and pooling models. The Spotlight implementation has the following characteristics: The model is trained using one of Spotlight's implicit feedback losses, including pointwise (logistic and hinge) and pairwise (BPR as well as WARP-like adaptive hinge) losses. As with other Spotlight sequence models, the loss is computed for all the time steps of the sequence in one pass: for all timesteps t in the sequence, a prediction using elements up to t-1 is made, and the loss is averaged along both the time and the minibatch axis. This leads to siginficant training speed-ups relative to only computing the loss for the last element in the sequence. To see how causal CNNs compare to more traditional sequence models we can have a look at how they perform at predicting the next rated movie on the Movielens 1M dataset. With 1 million interactions spread among 6000 users and around 4000 movies it should be small enough to run quick experiments, but large enough to yield meaningful results. I chose to split the dataset into 80% train, and 10% test and validation sets. I construct 200-long sequences by splitting each user's item sequence into 200-long chunks; if a chunk is shorter than 200 elements, it's padded with zeros. I use mean reciprocal rank (MRR) as the evaluation metric. To choose hyperparameters, I run a quick, coarse grained hyperparameter search, using random sampling to draw 100 hyperparameter sets. With the data and hyperparameters ready, fitting and evaluating the model is relatively simple: Fitting the models is fairly quick, taking at most two or three minutes on a single K80 GPU. The code for the experiments is available in the experiments folder of the Spotlight repo. The results are as follows: It's difficult to draw clear-cut conclusions about the effect of each hyperparameter, but it looks like: To compare causal convolutions with more traditional sequence models I run similar hyperparameter searches for LSTM-based representations and pooling representations. The pooling representation is a simple averaging of item embedding across the sequence; the LSTM-based model runs an LSTM along a user's interactions, using the hidden state for prediction of the next element at each step. The results are as follows: A single layer LSTM seems to outperform causal convolutions, by an over 10% margin, helped by the adaptive hinge loss. Simple pooling performs quite badly. It looks like causal convolutions need some more work before beating recurrent networks. There are a couple of possible avenues for making them better: I'd love to get some input on these. If you have suggestions, let me know on Twitter or open an issue or PR in Spotlight.", "links": ["https://github.com/", "https://arxiv.org/abs/1511.06939", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.LSTMNet", "https://travis-ci.org/maciejkula/spotlight", "https://arxiv.org/pdf/1609.03499.pdf", "http://papers.nips.cc/paper/6527-conditional-image-generation-with-pixelcnn-decoders", "https://arxiv.org/abs/1610.10099", "https://medium.com/@TalPerry/convolutional-methods-for-text-d5260fd5675f", "https://arxiv.org/pdf/1610.10099.pdf", "https://maciejkula.github.io/spotlight/sequence/sequence.html", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.CNNNet", "https://maciejkula.github.io/spotlight/losses.html", "https://grouplens.org/datasets/movielens/1m/", "https://en.wikipedia.org/wiki/Mean_reciprocal_rank", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.LSTMNet", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.PoolNet", "https://maciejkula.github.io/spotlight/losses.html#spotlight.losses.adaptive_hinge_loss", "https://twitter.com/Maciej_Kula", "https://github.com/maciejkula/spotlight", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "173": {"url": "https://github.com/ynqa/word-embedding", "title": "", "text": "GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together.  Sign up             Use Git or checkout with SVN using the web URL.             This is an implementation of word embedding (also referred to as word representation) models in Golang. Word embedding makes words' meaning, structure, and concept mapping into vector space (and low dimension). For representative instance: Like this example, it could calculate word meaning by arithmetic operations between vectors. Listed models for word embedding, and checked it already implemented. Downloading text8 corpus, and training by Skip-Gram with negative sampling.", "links": ["https://github.com/", "https://github.com/ynqa/word-embedding/blob/master/LICENSE", "https://help.github.com/articles/which-remote-url-should-i-use", "https://travis-ci.org/ynqa/word-embedding", "https://godoc.org/github.com/ynqa/word-embedding", "https://goreportcard.com/report/github.com/ynqa/word-embedding", "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf", "http://nlp.stanford.edu/pubs/glove.pdf", "https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf", "http://mattmahoney.net/dc/textdata", "https://github.com/cjlin1/libsvm", "http://www.aclweb.org/anthology/Q15-1016", "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.648.8023&rep=rep1&type=pdf", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "174": {"url": "https://github.com/brannondorsey/keras_weight_animator", "title": "", "text": "GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together.  Sign up             Use Git or checkout with SVN using the web URL.          Save Keras weight matrices as short animated videos to better understand what and how your neural network models are learning. Below are examples of the first LSTM layer and the final output layer of a six-class RNN model trained over one epoch. Blue represents low values and red represents high values.   In order to render videos from the saved weight images you must also have the following packages installed on your machine: This module is named keras_weight_animator. It exposes a Keras callback function that you can include in any model fit(...) method. The two required parameters to image_saver_callback(...) are the Keras model and an output_directory to periodically save weight images to. By default, keras_weight_animator saves layer weights every 100 batches to output_directory as PNGs in folders named epoch_XXX-layer_NAME-weights_YY. Once training is complete, you can optionally create short animated video clips from the image sequences saved in output_directory using [bin/create_image_sequence.sh](bin/create_image_sequence.sh) path/to/output_directory. This will use parallel, mogrify, and ffmpeg to create a .mp4 from the image sequences located in each folder of output_directory. Video files will be named like epoch_XXX-layer_NAME-weights_YY.mp4. You can run this script automatically from your training script by passing the render_videos=True parameter to image_saver_callback(...). weight_image_sequences(...) takes a variety of optional keyword arguments. I've included an example usage of the module in examples/wisdm.py. This example uses smartphone accelerometer data from WISDM to classify human activity tasks like walking, standing, sitting, walking upstairs, etc... This example uses a one layer LSTM to classify a set of 60 data points (representing three seconds of data sampled at 20hz) as belonging to one of six classes. It outputs image sequences and videos to data/wisdm. Using a bash script to leverage parallel, ImageMagick, and FFMPEG isn't necessarily the most elegant solution, but its the one I had time for. The goal of this here lil' project was to write a quick tool that allows me to better understand how weights change over mini-batch updates in a variety of neural networks. Perhaps in the future I will come back and clean up some of the inelegancies. If you have interest in contributing or maintaining a cleaner version of this lib, please reach out at brannon@brannondorsey.com. This module is \u00a9 Brannon Dorsey 2017, released under an \u2665 MIT License \u2665. You are free to use, modify, distribute, sell, etc... this software under those terms. Example data is from the WIreless Sensor Datamining (WISDM) Actitracker dataset published by Fordham University: This idea is tangentially inspired by much of the work on Visualizing and Understanding Recurrent Networks by Andrej Karpathy, Justin Johnson, and Fei-Fei Li. GNU Parallel is adamant about citation to the point of excess IMHO, but for what its worth, here is their bibtex:", "links": ["https://github.com/", "https://github.com/brannondorsey/keras_weight_animator/blob/master/LICENSE", "https://help.github.com/articles/which-remote-url-should-i-use", "https://github.com/brannondorsey/keras_weight_animator/pull/1", "http://keras.io", "https://www.gnu.org/software/parallel/", "https://www.imagemagick.org/script/index.php", "https://ffmpeg.org/download.html", "https://keras.io/callbacks/", "http://matplotlib.org/users/colormaps.html", "http://www.cis.fordham.edu/wisdm/dataset.php", "https://arxiv.org/abs/1506.02078", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "175": {"url": "https://axibase.github.io/atsd-use-cases/Expatriation_Q2/", "title": "", "text": "Expatriation is the temporary or permanent relocation of a person by choice or by force from their native country to any other country in the world for work, pleasure, or purpose. History has seen expatriation from nearly every country in the world up to and including the current epoch. Famous American expatriates throughout history have included American founding father Benjamin Franklin, who worked as the first United States Minister to France for almost a decade after appointment by the Continental Congress in 1778. The  sixth President of the United States John Quincy Adams, who served as a U.S. Diplomat in four countries before winning the Presidency as a Democratic-Republican in 1825, most notably opening American diplomatic ties as the first United States Minister to the Russian Empire, a position he loved  so much he declined a subsequent offer to serve as an Associate Justice on the Supreme Court of the United States a few years later. American expatriates have come from all walks of life: authors, such as Ernest Hemingway who lived in China, Spain, Cuba, and was even present on the beach codenamed Omaha in Normandy during the allied landing. Musicians, like Jimi Hendrix who rose to international fame in London after leaving his native Seattle, directors like Stanley Kubrick, singers like Tina Turner, and even basketball players like  Allen Iverson, who played an incomplete season for a Turkish basketball team in 2010 after retiring from the NBA. For those who decide to make the change permanent, there comes a time when the inconvenience of living abroad as a resident alien outweighs the novelty and many decide to renounce their citizenship for one reason or another. Unsurprisingly, this is a long and complicated process, one of the main features of which is settling up with the Internal Revenue Service (IRS) via the eponymous Expatriation Tax. The Federal Register is a publishing outlet for the United States Government where the IRS officially releases quarterly information about American citizens who renounce their passport in lieu of a new one and other long-term residents who have decided to repatriate, view the raw data here and use the Axibase Data Crawler designed specifically for collecting, parsing, organzing and inserting historical expatriation data and new data as it becomes available.  Axibase covered last quarter\u2019s expatriation release here and is using this quarter\u2019s release to follow-up. Every election cycle countless celebrities and netizens issue their promise to leave if their candidate fails to get elected and the growing notoriety surrounding the 2016 Presidental Election has proven to  be no different. This type of social theater is almost as old as the country itself as there have been a number of similarly divisive elections throughout American history: Andrew Jackson\u2019s victory in 1828 was seen as a populace uprising against the corrupt political elite and a return to the authority of the common man, Abraham Lincoln\u2019s second term was viewed as the tightening of federal authority to excessive levels that ultimately led to the American Civil War, Franklin Roosevelt was called a dangerous communist whose infamous court-packing plan favorably increased the number of seats on the Supreme Court to his  advantage and was seen as abhorrent and traitorous by many, and of course who can forget recent elections that have featured scandal after scandal and two modern Presidents who served after losing the popular vote, but winning in the electorate. Figure 1   Query 1 Data queried in the SQL Console. Table 1 To give the issue further context, Figure 1 and Table 1 use the most current Department of Homeland Security (DHS) data  available to display current naturalization numbers in the United States. Naturalization is the sister process to expatriation, because as a person expatriates from one country they must be naturalized in another or end up a stateless person. Naturalization numbers in America have remained fairly constant throughout the last decade because there are a number of federal regulations that control the amount of long-term immigrants that America accepts each year and that quota is met without fail. As noted in the our Q1 article, the sister value and subject of this examination, expatriation numbers, has been steadily growing for several consecutive years. Figure 2.1   Query 2 The above query features robust syntax and calculated values. See the following tutorials  to understand more about SQL Console. Table 2 Figure 2.2   Expatriation has been increasing each year by roughly 30% since 2010, which featured abnormally high expatriation rates, most likely attributable to the economic turndown of the Great Recession which began in the United States as a result of the sub-prime mortgage crisis. American citizenship is often sought after for the economic opportunity that comes along with the passport, as the ability to work and do business in the country is heavily restricted or regulated, and with the value of that investment or  opportunity in question, it is unsurprising that the number of investors in the system, that is, new citizens, would fluctuate. It seems appropriate to call naturalization an investment because of the nature of the process, which is long, complicated, and often quite expensive similar to a long-position that will cost more at purchasing time but promises high returns after reaching maturity. The peak, or more appropriately, valley of the global recession occurred in 2009 when the global GDP contracted causing a decline in the median familty income of about five percent. Figure 3   Query 3 Table 3 Reducing the timespan to look at data over the last 5 years captures the repetition of this trend for this period of time. Of the eighteen quarters included in this query, only six of them, roughly 33% showed negative growth for this statistic. Growing expatriation numbers provide interesting insight into the perception of America both domestically and abroad, as these numbers not only include U.S. citizens who renounce their citizenship, but also long-term resident aliens who have returned home. Almost unbelievably, the IRS does not strip characteristic information about expatriates from their publications meaning that first and last names are included in the data. Using Social Security data concerning the most common first names chosen throughout the last century and United States Census data concerning the most common last names to compare, the following queries and visualizations show  the number of expatriates by common first and common last name throughout the entire observed period, ranked by their frequency in the IRS publication. The goal of these queries is purely demonstrative and lighthearted. There is no intention to invade privacy, defame people, or otherwise cause harm. Figure 4  Query 4 Table 4 All of these names appear on the list of most common American first names, linked here Figure 5  Query 5 Table 5 Only the name Smith appears on the list of most common American last names, linked here Use the Axibase Data Crawler to collect the data used in this artcle and examine it yourself in ChartLab.", "links": ["https://github.com/axibase/atsd-use-cases", "https://www.irs.gov/", "https://www.irs.gov/individuals/international-taxpayers/expatriation-tax", "https://www.federalregister.gov/", "https://www.federalregister.gov/documents/2017/05/10/2017-09475/quarterly-publication-of-individuals-who-have-chosen-to-expatriate-as-required-by-section-6039g", "https://github.com/axibase/atsd-data-crawlers/tree/irs-expatriation-data-crawler", "https://axibase.com", "https://apps.axibase.com/chartlab/654b9945#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql", "https://www.uscis.gov/news/fact-sheets/naturalization-fact-sheet", "http://www.unhcr.org/stateless-people.html", "https://apps.axibase.com/chartlab/7fa5b643#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql#examples", "https://apps.axibase.com/chartlab/7fa5b643/5/#fullscreen", "https://web.stanford.edu/group/recessiontrends/cgi-bin/web/sites/all/themes/barron/pdf/IncomeWealthDebt_fact_sheet.pdf", "https://apps.axibase.com/chartlab/7fa5b643/2/#fullscreen", "https://www.ssa.gov/oact/babynames/decades/century.html", "http://www.census.gov/main/www/cen2000.html", "https://www.ssa.gov/oact/babynames/decades/century.html", "http://www.census.gov/main/www/cen2000.html", "https://github.com/axibase/atsd-data-crawlers/tree/irs-expatriation-data-crawler", "https://apps.axibase.com", "https://github.com/axibase/atsd-use-cases", "https://github.com/axibase", "https://pages.github.com"]}, "176": {"url": "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/", "title": "", "text": "Machine learning is eating the world right now. Everyone and their mother are learning about machine learning models, classification, neural networks, and Andrew Ng. You\u2019ve decided you want to be a part of it, but where to start? In this article we\u2019ll cover some important characteristics of Python and why it\u2019s great for machine learning. We\u2019ll also cover some of the most important libraries it has for ML, and if it piques your interest, some places where you can learn more. Python is a great choice for machine learning for several reasons. First and foremost, it\u2019s a simple language\u00a0on the surface; even if you\u2019re not familiar with Python, getting up to speed is very quick if you\u2019ve ever used any other language with C-like syntax (i.e. every language out there). Second, Python has a great community, which results in good documentation and friendly, comprehensive answers in StackOverflow (fundamental!). Third, also stemming from the great community, there are plenty of useful libraries for Python (both as \u201cbatteries included\u201d and third party), which solve basically any problem that you can have (including machine learning). Yeah and it\u2019s true. Python isn\u2019t the fastest language out there: all those handy abstractions come at a cost. But here\u2019s the trick: libraries can and do offload the expensive calculations to the much more performant (but harder to use) C and C++. For instance, there\u2019s NumPy, which is a library for numerical computation. It\u2019s written in C, and it\u2019s fast. Practically every library out there that involves intensive calculations uses it \u2014 almost all the libraries listed next use it in some form. So if you read NumPy, think fast. Therefore, you can make your scripts run basically as fast as straight up writing them in a lower level language. So there\u2019s really nothing to worry about when it comes to speed. Are you starting out in machine learning? Want something that covers everything from feature engineering to training and testing a model? Look no further than scikit-learn! This fantastic piece of free software provides every tool necessary for machine learning and data mining. It\u2019s the de facto standard library for machine learning in Python, recommended for most of the \u2018old\u2019 ML algorithms. This library does both classification and regression, supporting basically every algorithm out there (support vector machines, random forest, naive bayes, and so on). It\u2019s built in such a way that allows easy switching of algorithms, so experimentation is easy. These \u2018older\u2019 algorithms are surprisingly resilient and work very well in a lot of cases. But that\u2019s not all! Scikit-learn also does dimensionality reduction, clustering, you name it. It\u2019s also blazingly fast since it runs on NumPy and SciPy (meaning that all the heavy number crunching is run on C instead of Python). Check out some examples to see everything this library is capable of, and the tutorials if you want to learn how it works. While not a machine learning library per se, NLTK is a must when working with natural language processing (NLP). It comes with a bundle of datasets and other lexical resources (useful for training models) in addition to libraries for working with text \u2014 for functions such as classification, tokenization, stemming, tagging, parsing and more. The usefulness of having all of this stuff neatly packaged can\u2019t be overstated. So if you are interested in NLP, check out some tutorials! Used widely in research and academia, Theano is the grandfather of all deep learning frameworks. Written in Python, it\u2019s tightly integrated with NumPy. Theano allows you to create neural networks, which are represented as mathematical expressions with multi-dimensional arrays. Theano handles this for you so you don\u2019t have to worry about the actual implementation of the math involved. It supports offloading calculations to the much faster GPU, which is a feature that everyone supports today, but back when they introduced it this wasn\u2019t the case. The library is very mature at this point and supports a very wide range of operations, which is a great plus when it comes to comparing it with other similar libraries. The biggest complaint out there is that the API may be unwieldy for some, making the library hard to use for beginners. However, there are wrappers that ease the pain and make working with Theano simple, such as Keras, Blocks and Lasagne. Interested in learning about Theano? Check out this Jupyter Notebook tutorial. The Google Brain team created TensorFlow for internal use in machine learning applications, and open sourced it in late 2015. They wanted something that could replace their older, closed source machine learning framework, DistBelief, which they said wasn\u2019t flexible enough and too tightly coupled to their infrastructure to be shared with other researchers around the world. And so TensorFlow was created. Learning from the mistakes of the past, many consider this library to be an improvement over Theano, claiming more flexibility and a more intuitive API. Not only can it be used for research but also for production environments, supporting huge clusters of GPUs for training. While it doesn\u2019t support as wide a range of operations as Theano, it has better computational graph visualizations. TensorFlow is very popular nowadays. In fact, if you\u2019ve heard about a single library on this list, it\u2019s probably this one: there isn\u2019t a day that goes by without a new blog post or paper mentioning TensorFlow gets published. This popularity translates into a lot of new users and a lot of tutorials, making it very welcoming to beginners. Keras is a fantastic library that provides a high-level API for neural networks and is capable of running on top of either Theano or TensorFlow. It makes harnessing the full power of these complex pieces of software much easier than using them directly. It\u2019s very user-friendly, putting user experience as a top priority. They manage this by using simple APIs and excellent feedback on errors. It\u2019s also modular, meaning that different models (neural layers, cost functions, and so on) can be plugged together with little restrictions. This also makes it very easy to extend, since it\u2019s simple to add new modules and connect them with the existing ones. Some people have called Keras so good that it is effectively cheating in machine learning. So if you\u2019re starting out with deep learning, go through the examples and documentation to get a feel for what you can do with it. And if you want to learn, start out with this tutorial and see where you can go from there. Two similar alternatives are Lasagne and Blocks, but they only run on Theano. So if you tried Keras and are unhappy with it, maybe try out one of these alternatives to see if they work out for you. Another popular deep learning framework is Torch, which is written in Lua. Facebook open-sourced a Python implementation of Torch called PyTorch, which allows you to conveniently use the same low-level libraries that Torch uses, but from Python instead of Lua. PyTorch is much better for debugging since one of the biggest differences between Theano/TensorFlow and PyTorch is that the former use symbolic computation while the latter doesn\u2019t. Symbolic computation means that coding an operation (say, \u2018x + y\u2019), it\u2019s not computed when that line is interpreted. Before getting executed it has to be compiled (translated to CUDA or C). This makes debugging harder in Theano/TensorFlow, since an error is much harder to associate with the line of code that caused it. Of course, doing things this way has its advantages, but debugging isn\u2019t one of them. If you want to start out with PyTorch the official tutorials are very friendly to beginners but get to advanced topics as well. Alright, you\u2019ve presented me with a lot of alternatives for machine learning libraries in Python. What should I choose? How do I compare these things? Where do I start? Our Ape Advice\u2122 for beginners is to try and not get bogged down by details. If you\u2019ve never done anything machine learning related, try out scikit-learn. You\u2019ll get an idea of how the cycle of tagging, training and testing work and how a model is developed. Now, if you want to try out deep learning, start out with Keras \u2014 which is widely agreed to be the easiest framework \u2014 and see where that takes you. After you have more experience, you will start to see what it is that you actually want from the framework: greater speed, a different API, or maybe something else, and you\u2019ll be able to make a more informed decision. And even then, there is an endless supply of articles out there comparing Theano, Torch, and TensorFlow. There\u2019s no real way to tell which one is the good one. It\u2019s important to take into account that all of them have wide support and are improving constantly, making comparisons harder to make. A six month old benchmark may be outdated, and year old claims of framework X doesn\u2019t support operation Y could no longer be valid. Finally, if you\u2019re interested in doing machine learning specifically applied to NLP, why not check out MonkeyLearn! Our platform provides a unique UX that makes it super easy to build, train and improve NLP models. You can either use pre-trained models for common use cases (like sentiment analysis, topic detection or keyword extraction) or train custom algorithms using your particular data. Also, you don\u2019t have to worry about the underlying infrastructure or deploying your models, our scalable cloud does this for you. You can start for free and integrate right away with our beautiful API. There are plenty of online resources out there to learn about machine learning ! Here are a few: So that was a brief intro to machine learning in Python and some of its libraries. The important part is not getting bogged down by details and just trying stuff out. Follow your curiosity, and don\u2019t be afraid to experiment. Know about a python library that was left out? Share it in the comments below!", "links": ["http://www.monkeylearn.com", "http://monkeylearn.com/use-cases/", "http://monkeylearn.com/docs/", "http://www.monkeylearn.com/pricing", "http://www.monkeylearn.com/blog/", "https://app.monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/?utm_source=ml-blog&utm_medium=header&utm_campaign=blog", "https://monkeylearn.com/blog", "http://monkeylearn.com/use-cases/", "http://monkeylearn.com/docs/", "http://www.monkeylearn.com/pricing", "http://www.monkeylearn.com/blog/", "https://app.monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/?utm_source=ml-blog&utm_medium=header&utm_campaign=blog", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/07/Post_2c.png", "http://scikit-learn.org/", "http://scikit-learn.org/stable/auto_examples/index.html#general-examples", "http://scikit-learn.org/stable/tutorial/index.html", "http://www.nltk.org/", "http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk", "http://deeplearning.net/software/theano/", "https://keras.io/", "https://github.com/mila-udem/blocks", "https://github.com/Lasagne/Lasagne", "http://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/Theano%20Tutorial.ipynb", "https://www.tensorflow.org/", "https://www.tensorflow.org/get_started/graph_viz", "https://www.tensorflow.org/tutorials/", "https://keras.io/", "https://news.ycombinator.com/item?id=13872764", "https://github.com/fchollet/keras#getting-started-30-seconds-to-keras", "https://keras.io/getting-started/functional-api-guide/", "https://elitedatascience.com/keras-tutorial-deep-learning-in-python", "https://github.com/Lasagne/Lasagne", "https://github.com/mila-udem/blocks", "http://torch.ch/", "http://pytorch.org/", "http://pytorch.org/tutorials/", "http://scikit-learn.org/stable/tutorial/basic/tutorial.html", "https://elitedatascience.com/keras-tutorial-deep-learning-in-python", "https://www.quora.com/Is-TensorFlow-better-than-other-leading-libraries-such-as-Torch-Theano", "http://www.ccri.com/2016/12/09/torch-vs-tensorflow-vs-theano/", "https://www.reddit.com/r/MachineLearning/comments/5w3q74/d_so_pytorch_vs_tensorflow_whats_the_verdict_on/", "https://github.com/zer0n/deepframeworks/blob/master/README.md", "https://monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/", "http://nbviewer.jupyter.org/github/rhiever/Data-Analysis-and-Machine-Learning-Projects/blob/master/example-data-science-notebook/Example%20Machine%20Learning%20Notebook.ipynb", "https://blog.monkeylearn.com/a-gentle-guide-to-machine-learning/", "https://www.coursera.org/learn/machine-learning", "http://course.fast.ai/", "https://monkeylearn.com/blog/author/bruno/", "https://monkeylearn.com/blog/category/news/", "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/#respond", "https://monkeylearn.com/blog/author/bruno/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/monkeylearn_zapier.png", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/wp-content/uploads/2017/06/Post_1e.png", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/naive-bayes-classifier.png", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/monkey-startup.png", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/category/applications/", "https://monkeylearn.com/blog/category/guides/", "https://monkeylearn.com/blog/category/howto/", "https://monkeylearn.com/blog/category/news/", "https://monkeylearn.com/blog/category/text-classification/", "https://www.monkeylearn.com", "https://www.monkeylearn.com", "http://slack.monkeylearn.com/", "https://twitter.com/monkeylearn", "https://github.com/monkeylearn/"]}, "177": {"url": "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "title": "Machine Learning Translation and the Google Translate Algorithm", "text": "Every day we use different technologies without even knowing how exactly they work. In fact, it\u2019s not very easy to understand engines powered by machine learning. The Statsbot team wants to make machine learning clear by telling data stories in this blog. Today, we\u2019ve decided to explore machine translators and explain how the Google Translate algorithm works. Years ago, it was very time consuming to translate the text from an unknown language. Using simple vocabularies with word-for-word translation was hard for two reasons: 1) the reader had to know the grammar rules and 2) needed to keep in mind all language versions while translating the whole sentence. Now, we don\u2019t need to struggle so much\u2013 we can translate phrases, sentences, and even large texts just by putting them in Google Translate. But most people don\u2019t actually care how the engine of machine learning translation works. This post is for those who do care. If the Google Translate engine tried to kept the translations for even short sentences, it wouldn\u2019t work because of the huge number of possible variations. The best idea can be to teach the computer sets of grammar rules and translate the sentences according to them. If only it were as easy as it sounds. If you have ever tried learning a foreign language, you know that there are always a lot of exceptions to rules. When we try to capture all these rules, exceptions and exceptions to the exceptions in the program, the quality of translation breaks down. Creating your own simple machine translator would be a great project for any data science resume. Let\u2019s try to investigate what hides in the \u201cblack boxes\u201d that we call machine translators. Deep neural networks can achieve excellent results in very complicated tasks (speech/visual object recognition), but despite their flexibility, they can be applied only for tasks where the input and target have fixed dimensionality. Here is where Long Short-Term Memory networks (LSTMs) come into play, helping us to work with sequences whose length we can\u2019t know a priori. LSTMs are a special kind of recurrent neural network (RNN), capable of learning long-term dependencies. All RNNs look like a chain of repeating modules. So the LSTM transmits data from module to module and, for example, for generating Ht we use not only Xt, but all previous input values X. To learn more about structure and mathematical models of LSTM, you can read the great article \u201cUnderstanding LSTM Networks.\u201d Our next step is bidirectional recurrent neural networks (BRNNs). What a BRNN does, is split the neurons of a regular RNN into two directions. One direction is for positive time, or forward states. The other direction is for negative time, or backward states. The output of these two states are not connected to inputs of the opposite direction states. To understand why BRNNs can work better than a simple RNN, imagine that we have a sentence of 9 words and we want to predict the 5th word. We can make it know either only the first 4 words, or the first 4 words and last 4 words. Of course, the quality in the second case would be better. Now we\u2019re ready to move to sequence to sequence models (also called seq2seq). The basic seq2seq model consist of two RNNs: an encoder network that processes the input and a decoder network that generates the output. Finally, we can make our first machine translator! However, let\u2019s think about one trick. Google Translate currently supports 103 languages, so we should have 103x102 different models for each pair of languages. Of course, the quality of these models varies according to the popularity of languages and the amount of documents needed for training this network. The best that we can do is to make one NN to take any language as input and translate into any language. That very idea was realized by Google engineers at the end of 2016. Architecture of NN was build on the seq2seq model, which we have already studied. The only exception is that between the encoder and decoder there are 8 layers of LSTM-RNN that have residual connections between layers with some tweaks for accuracy and speed. If you want to go deeper with that, take a look at the article Google\u2019s Neural Machine Translation System. The system requires a \u201ctoken\u201d at the beginning of the input sentence which specifies the language you\u2019re trying to translate the phrase into. This improves translation quality and enables translations even between two languages which the system hasn\u2019t seen yet, a method termed \u201cZero-Shot Translation.\u201d When we\u2019re talking about improvements and better results from Google Translate algorithms, how can we correctly evaluate that the first candidate for translation is better than the second? It\u2019s not a trivial problem, because for some commonly used sentences we have the sets of reference translations from the professional translators, that have, of course, some differences. There are a lot of approaches that partly solve this problem, but the most popular and effective metric is BLEU (bilingual evaluation understudy). Imagine, we have two candidates from machine translators: Although they have the same meaning they differ in quality and have different structure. Let\u2019s look at two human translations: Obviously, Candidate 1 is better, sharing more words and phrases compared to Candidate 2. This is a key idea of the simple BLEU approach. We can compare n-grams of the candidate with n-grams of the reference translation and count the number of matches (independent from their position). We use only n-gram precisions, because calculating recall is difficult with multiple refs and the result is the geometric average of n-gram scores. Now you can evaluate the complex engine of machine learning translation. Next time when you translate something with Google Translate, imagine how many millions of documents it analyzed before giving you the best language version.", "links": ["https://blog.statsbot.co?source=logo-lo_c1c71aa94a38---cfc9f21a543a", "https://twitter.com/statsbotco", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.statsbot.co%2Fmachine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co", "https://blog.statsbot.co/analytics/home", "https://blog.statsbot.co/datascience/home", "https://blog.statsbot.co/design/home", "https://blog.statsbot.co/startups/home", "https://blog.statsbot.co/bots/home", "https://blog.statsbot.co/news/home", "https://blog.statsbot.co/statsbot-digest-b0d7372f842a", "http://statsbot.co?utm_source=blog&utm_medium=navigation&utm_campaign=robot_face", "https://blog.statsbot.co/search", "https://blog.statsbot.co/@daniilkorbut?source=post_header_lockup", "https://blog.statsbot.co/@daniilkorbut?source=post_header_lockup", "https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html", "http://statsbot.co?utm_source=blog&utm_medium=article&utm_campaign=machine_translation", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6?utm_source=blog&utm_medium=article&utm_campaign=machine_translation", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6", "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "https://www.semanticscholar.org/paper/Hybrid-speech-recognition-with-Deep-Bidirectional-Graves-Jaitly/5807664af8e63d5207f59fb263c9e7bd3673be79", "https://research.googleblog.com/2016/09/a-neural-network-for-machine.html", "https://www.newscientist.com/article/2114748-google-translate-ai-invents-its-own-language-to-translate-with/", "https://research.googleblog.com/2016/09/a-neural-network-for-machine.html", "https://arxiv.org/abs/1609.08144", "https://en.wikipedia.org/wiki/BLEU", "https://en.wikipedia.org/wiki/N-gram", "https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3", "https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/tagged/machine-learning?source=post", "https://blog.statsbot.co/tagged/machine-translation?source=post", "https://blog.statsbot.co/tagged/google-translate?source=post", "https://blog.statsbot.co/tagged/data-science?source=post", "https://blog.statsbot.co/tagged/neural-networks?source=post", "https://blog.statsbot.co/@daniilkorbut?source=footer_card", "https://blog.statsbot.co/@daniilkorbut", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "178": {"url": "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "title": "Yanir SeroussiMy 10-step path to becoming a remote data scientist with\u00a0Automattic", "text": "About two years ago, I read the book The Year without Pants, which describes the author\u2019s experience leading a team at Automattic (the company behind WordPress.com, among other products). Automattic is a fully-distributed company, which means that all of its employees work remotely (hence pants are optional). While the book discusses some of the challenges of working remotely, the author\u2019s general experience was very positive. A few months after reading the book, I decided to look for a full-time position after a period of independent work. Ideally, I wanted a well-paid data science-y remote job with an established distributed tech company that offers a good life balance and makes products I care about. Automattic seemed to tick all my boxes, so I decided to apply for a job with them. This post describes my application steps, which ultimately led to me becoming a data scientist with Automattic. Before jumping in, it\u2019s worth noting that this post describes my personal experience. If you apply for a job with Automattic, your experience is likely to be different, as the process varies across teams, and evolves over time. I decided to apply for a data wrangler position with Automattic in October 2015. While data wrangler may sound less sexy than data scientist, reading the job ad led me to believe that the position may involve interesting data science work. This impression was strengthened by some LinkedIn stalking, which included finding current data wranglers and reading through their profiles and websites. I later found out that all the people on the data division start out as data wranglers, and then they may pick their own title. Some data wranglers do data science work, while others are more focused on data engineering, and there are some projects that require a broad range of skills. As the usefulness of the term data scientist is questionable, I\u2019m not too fussed about fancy job titles. It\u2019s more important to do interesting work in a supportive environment. Applying for the job was fairly straightforward. I simply followed the instructions from the ad:  Does this sound interesting? If yes, please send a short email to jobs @ this domain telling us about yourself and attach a resum\u00e9. Let us know what you can contribute to the team. Include the title of the position you\u2019re applying for and your name in the subject. Proofread! Make sure you spell and capitalize WordPress and Automattic correctly. We are lucky to receive hundreds of applications for every position, so try to make your application stand out. If you apply for multiple positions or send multiple emails there will be one reply.  Having been on the receiving side of job applications, I find it surprising that many people don\u2019t bother writing a cover letter, addressing the selection criteria in the ad, or even applying for a job they\u2019re qualified to do. Hence, my cover letter was fairly short, comprising of several bullet points that highlight the similarities between the job requirements and my experience. It was nothing fancy, but simple cover letters have worked well for me in the past. The initial application was followed by a long wait. From my research, this is the typical scenario. This is unsurprising, as Automattic is a fairly small company with a large footprint, which is both distributed and known as a great place to work (e.g., its Glassdoor rating is 4.9). Therefore, it attracts many applicants from all over the world, which take a while to process. In addition, Matt Mullenweg (Automattic\u2019s CEO) reviews job applications before passing them on to the team leads. As I didn\u2019t know that Matt reviewed job applications, I decided to try to shorten the wait by getting introduced to someone in the data division. My first attempt was via a second-degree LinkedIn connection who works for Automattic. He responded quickly when I reached out to him, saying that his experience working with the company is in line with the Glassdoor reviews \u2013 it\u2019s the best job he\u2019s had in his 15-year-long career. However, he couldn\u2019t help me with an intro, because there is no simple way around Automattic\u2019s internal processes. Nonetheless, he reassured me that it is worth waiting patiently, as the strict process means that you end up working with great people. I wasn\u2019t in a huge rush to find a job, but in December 2015 I decided to accept an offer to become the head of data science at Car Next Door. This was a good decision at the time, as I believe in the company\u2019s original vision of reducing the number of cars on the road through car sharing, and it seemed like there would be many interesting projects for me to work on. The position wasn\u2019t completely remote, but as the company was already spread across several cities, I was able to work from home for a day or two every week. In addition, it was a pleasant commute by bike from my Sydney home to the office, so putting the fully-remote job search on hold didn\u2019t seem like a major sacrifice. As I haven\u2019t heard anything from Automattic at that stage, it seemed unwise to reject a good offer, so I started working full-time with Car Next Door in January 2016. I successfully attracted Automattic\u2019s attention with a post I published on the misuse of the word insights by many tech companies, which included an example from WordPress.com. Greg Ichneumon Brown, one of the data wranglers, commented on the post, and invited me to apply to join Automattic and help them address the issues I raised. This happened after I accepted the offer from Car Next Door, and hasn\u2019t resulted in any speed up of the process, so I just gave up on Automattic and carried on with my life. I finally heard back from Automattic in February 2016 (four months after my initial application and a month into my employment with Car Next Door). Martin Remy, who leads the data division, emailed me to enquire if I\u2019m still interested in the position. I informed him that I was no longer looking for a job, but we agreed to have an informal chat, as I\u2019ve been waiting for such a long time. As is often the case with Automattic interviews, the chat with Martin was completely text-based. Working with a distributed team means that voice and video calls can be hard to schedule. Hence, Automattic relies heavily on textual channels, and text-based interviews allow the company to test the written communication skills of candidates. The chat revolved around my past work experience, and Martin also took the time to answer my questions about the company and the data division. At the conclusion of the chat, Martin suggested I contact him directly if I was ever interested in continuing the application process. While I was happy with my position at the time, the chat strengthened my positive impression of Automattic, and I decided that I would reapply if I were to look for a full-time position again. My next job search started earlier than I had anticipated. In October 2016, I decided to leave Car Next Door due to disagreements with the founders over the general direction of the company. In addition, I had more flexibility in choosing where to live, as my personal circumstances had changed. As I\u2019ve always been curious about life outside the capital cities of Australia, I wanted to move away from Sydney. While I could have probably continued working remotely with Car Next Door, I felt that it would be better to find a job with a fully-distributed team. Therefore, I messaged Martin and we scheduled another chat. The second chat with Martin took place in early November. Similarly to the first chat, it was conducted via Skype text messages, and revolved around my work in the time that has passed since the first chat. This time, as I was keen on continuing with the process, I asked more specific questions about what kind of work I\u2019m likely to end up doing and what the next steps would be. The answers were that I\u2019d be joining the data science team, and that the next steps are a pre-trial test, a paid trial, and a final interview with Matt. While this sounds straightforward, it took another six months until I finally became an Automattic employee (but I wasn\u2019t in a rush). The pre-trial test consisted of a data analysis task, where I was given a dataset and a set of questions to answer by Carly Stambaugh, the data science lead. The goal of the test is to evaluate the candidate\u2019s approach to a problem, and assess organisational and communication skills. As such, the focus isn\u2019t on obtaining a specific result, so candidates are given a choice of several potential avenues to explore. The open-ended nature of the task is reminiscent of many real-world data science projects, where you don\u2019t always have a clear idea of what you\u2019re going to discover. While some people may find this kind of uncertainty daunting, I find it interesting, as it is one of the things that makes data science a science. I spent a few days analysing the data and preparing a report, which was submitted as a Jupyter Notebook. After submitting my initial report, there were a few follow-up questions, which I answered by email. The report was reviewed by Carly and Martin, and as they were satisfied with my work, I was invited to proceed to the next stage: A paid trial project. The main part of the application process with Automattic is the paid trial project. The rationale behind doing paid trials was explained a few years ago by Matt in Hire by Auditions, Not Resumes:  Before we hire anyone, they go through a trial process first, on contract. They can do the work at night or over the weekend, so they don\u2019t have to leave their current job in the meantime. We pay a standard rate of $25 per hour, regardless of whether you\u2019re applying to be an engineer or the chief financial officer. During the trials, we give the applicants actual work. If you\u2019re applying to work in customer support, you\u2019ll answer tickets. If you\u2019re an engineer, you\u2019ll work on engineering problems. If you\u2019re a designer, you\u2019ll design. There\u2019s nothing like being in the trenches with someone, working with them day by day. It tells you something you can\u2019t learn from resumes, interviews, or reference checks. At the end of the trial, everyone involved has a great sense of whether they want to work together going forward. And, yes, that means everyone \u2014 it\u2019s a mutual tryout. Some people decide we\u2019re not the right fit for them.  The goal of my trial project was to improve the Elasticsearch language detection algorithm. This took about a month, and ultimately resulted in a pull request that got merged into the language detection plugin. I find this aspect of the process pretty exciting: While the plugin is used to classify millions of documents internally by Automattic, its impact extends beyond the company, as Elasticsearch is used by many other organisations and projects. This stands in contrast to many other technical job interviews, which consist of unpaid work on toy problems under stressful conditions, where the work performed is ultimately thrown away. While the monetary compensation for the trial work is lower than the market rate for data science consulting, I valued the opportunity to work on a real open source project, even if this hadn\u2019t led to me getting hired. There was much more to the trial project than what\u2019s shown in the final pull request. Most of the discussions were held on an internal project thread, primarly under the guidance of Carly (the data science lead), and Greg (the data wrangler who replied to my post a year earlier). The project was kicked off with a general problem statement: There was some evidence that the Elasticsearch language detection plugin doesn\u2019t perform well on short texts, and my mission was to improve it. As the plugin didn\u2019t include any tests for short texts, one of the main contributions of my work was the creation of datasets and tests to measure its accuracy on texts of different lengths. This was followed by some tweaks that improved the plugin\u2019s performance, as summarised in the pull request. Internally, this work consisted of several iterations where I came up with ideas, asked questions, implemented the ideas, shared the results, and discussed further steps. There are still many possible improvements to the work done in the trial. However, as trials generally last around a month, we decided to end it after a few iterations. I enjoyed the trial process, but it is definitely not for everyone. Most notably, there is a strong emphasis on asynchronous text-based communication, which is the main mode by which projects are coordinated at Automattic. People who don\u2019t enjoy written communication may find this aspect challenging, but I have always found that writing helps me organise my thoughts, and that I retain information better when reading than when listening to people speak. That being said, Automatticians do meet in person several times a year, and some teams have video chats for some discussions. While doing the trial, I had a video chat with Carly, which was the first (and last) time in the process that I got to see and hear a live human. However, this was not an essential part of the trial project, as our chat was mostly on the data scientist role and my job expectations. I finished working on the trial project just before Christmas. The feedback I received throughout the trial was positive, but Martin, Carly, and Greg had to go through the work and discuss it among themselves before making a final decision. This took about a month, due to the holiday period, various personal circumstances, and the data science team meetup that was scheduled for January 2017. Eventually, Martin got back to me with positive news: They were satisfied with my trial work, which meant there was only one stage left \u2013 the final interview with Matt Mullenweg, Automattic\u2019s CEO. Like other parts of the process, the interview with Matt is text-based. The way it works is fairly simple: I was instructed to message Matt on Slack and wait for a response, which may take days or weeks. I sent Matt a message on January 25, and was surprised to hear back from him the following morning. However, that day was Australia Day, which is a public holiday here. Therefore, I only got back to him two hours after he messaged me that morning, and by that time he was probably already busy with other things. This was the start of a pretty long wait. I left Car Next Door at the end of January, as I figured that I would be able to line up some other work even if things didn\u2019t work out with Automattic. My plan was to take some time off, and then move up to the Northern Rivers area of New South Wales. I had two Reef Life Survey trips planned, so I wasn\u2019t going to start working again before mid-April. I assumed that I would hear back from Matt before then, which would have allowed me to make an informed decision whether to look for another job or not. After two weeks of waiting, the time for my dive trips was nearing. As I was going to be without mobile reception for a while, I thought it\u2019d be worth letting Matt know my schedule. After discussing the matter with Martin, I messaged Matt. He responded, saying that we might as well do the interview at the beginning of April, as I won\u2019t be starting work before that time anyway. I would have preferred to be done with the interview earlier, but was happy to have some certainty and not worry about missing more chat messages before April. In early April, I returned from my second dive trip (which included a close encounter with Cyclone Debbie), and was hoping to sort out my remote work situation while completing the move up north. Unfortunately, while the move was successful, I was ready to give up on Automattic because I haven\u2019t heard back from Matt at all in April. However, Martin remained optimistic and encouraged me to wait patiently, which I did as I was pretty busy with the move and with some casual freelancing projects. The chat with Matt finally happened on May 2. As is often the case, it took a few hours and covered my background, the trial process, and some other general questions. I asked him about my long wait for the final chat, and he apologised for me being an outlier, as most chats happen within two weeks of a candidate being passed over to him. As the chat was about to conclude, we got to the topic of salary negotiation (which went well), and then the process was finally over! Within a few hours of the chat I was sent an offer letter and an employment contract. As Automattic has an entity in Australia (called Ausomattic), it\u2019s a fairly standard contract. I signed the contract and started work the following week \u2013 over a year and a half after my initial application. Even before I started working, I booked tickets to meet the data division in Montr\u00e9al \u2013 a fairly swift transition from the long wait for the final interview. As noted above, Automatticians get to choose their own job titles, so to become a data scientist with Automattic, I had to set my job title to Data Scientist. This is generally how many people become data scientists these days, even outside Automattic. However, job titles don\u2019t matter as much as job satisfaction. And after 2.5 months with Automattic, I\u2019m very satisfied with my decision to join the company. My first three weeks were spent doing customer support, like all new Automattic employees. Since then, I\u2019ve been involved in projects to make engagement measurement more consistent (harder than it sounds, as counting things is hard), and to improve the data science codebase (e.g., moving away from Legacy Python). Besides that, I also went to Montr\u00e9al for the data division meetup, and have started getting into chatbot work. I\u2019m looking forward to doing more work and sharing my experience here and on data.blog. Very enlightening post! It was very awesome to see that the insights you saw to Elasticsearch went to a PR. I bet that was worth the whole thing!  That\u2019s very exciting, I wanted to ask are you a self learner or do you have a degree,can you please share your background. Thank you  Thanks Mostafa. Yes, I have a BSc in computer science, and a PhD in what you would now call data science. See: https://www.linkedin.com/in/yanirseroussi/  Fill in your details below or click an icon to log in:     Connecting to %s         Enter your email address to follow this blog and receive notifications of new posts by email.", "links": ["https://yanirseroussi.com/", "https://yanirseroussi.com/", "https://yanirseroussi.com/about/", "https://yanirseroussi.com/presentations/", "https://yanirseroussi.com/phd-work/", "https://yanirseroussi.com/2014/04/05/kaggle-competition-summaries/", "http://scottberkun.com/yearwithoutpants/", "https://automattic.com/", "https://yanirseroussi.com/2015/03/22/the-long-road-to-a-lifestyle-business/", "http://web.archive.org/web/20150908140923/https://automattic.com/work-with-us/data-wrangler/", "https://yanirseroussi.com/2016/08/04/is-data-scientist-a-useless-job-title/", "https://automattic.com/about/", "https://www.glassdoor.com.au/Reviews/Automattic-Reviews-E751107.htm", "http://davemart.in/remote-hiring/", "https://www.carnextdoor.com.au/", "https://yanirseroussi.com/2015/12/08/this-holiday-season-give-me-real-insights/", "https://yanirseroussi.com/2015/12/08/this-holiday-season-give-me-real-insights/#comment-957", "http://jupyter.org/", "https://hbr.org/2014/01/hire-by-auditions-not-resumes", "https://www.elastic.co/products/elasticsearch", "https://github.com/jprante/elasticsearch-langdetect/pull/69", "https://github.com/jprante/elasticsearch-langdetect/pull/69", "https://yanirseroussi.com/2016/01/24/the-joys-of-offline-data-collection/", "https://www.whitsundaytimes.com.au/news/boat-caught-in-eye-of-cyclone-cruises-home/3164170/", "https://data.blog/2017/06/29/data-coalesce-automattic-data-division-meets-in-montreal/", "http://daynebatten.com/2016/06/counting-hard-data-science/", "http://powerfulpython.com/blog/magic-word-legacy-python/", "https://data.blog/2017/05/24/may-the-bot-be-with-you-how-algorithms-are-supporting-happiness-at-wordpress-com/", "https://data.blog/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=twitter", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=facebook", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=google-plus-1", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=linkedin", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=reddit", "https://yanirseroussi.com/category/data-science-2/", "https://yanirseroussi.com/tag/automattic/", "https://yanirseroussi.com/tag/career/", "https://yanirseroussi.com/tag/data-science/", "https://yanirseroussi.com/tag/elasticsearch/", "https://yanirseroussi.com/tag/personal/", "https://yanirseroussi.com/tag/wordpress/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "https://yanirseroussi.com/author/yanirseroussi/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comments", "https://yanirseroussi.com/2017/06/03/exploring-and-visualising-reef-life-survey-data/", "https://dotnetmeditations.com/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1698", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1698&_wpnonce=6c197b2684", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1698#respond", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1700", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1700&_wpnonce=350fa40ca9", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1700#respond", "http://yanirseroussi.com", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1705", "https://www.linkedin.com/in/yanirseroussi/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1705&_wpnonce=7f1e8d7061", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1705#respond", "https://gravatar.com/site/signup/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "https://yanirseroussi.com/2017/06/03/exploring-and-visualising-reef-life-survey-data/", "https://yanirseroussi.com/2017/01/08/customer-lifetime-value-and-the-proliferation-of-misinformation-on-the-internet/", "https://yanirseroussi.com/2016/09/19/ask-why-finding-motives-causes-and-purpose-in-data-science/", "https://yanirseroussi.com/2016/08/21/seven-ways-to-be-data-driven-off-a-cliff/", "https://yanirseroussi.com/category/bcrecommender/", "https://yanirseroussi.com/category/data-science-2/", "https://yanirseroussi.com/category/environment/", "https://yanirseroussi.com/category/general/", "https://yanirseroussi.com/category/kaggle-2/", "https://yanirseroussi.com/category/machine-intelligence/", "https://yanirseroussi.com/category/phd-work/", "https://wordpress.com/?ref=footer_blog"]}, "179": {"url": "https://medium.com/airbnb-engineering/using-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d", "title": "Using Machine Learning to Predict Value of Homes On\u00a0Airbnb", "text": "by Robert Chang Data products have always been an instrumental part of Airbnb\u2019s service. However, we have long recognized that it\u2019s costly to make data products. For example, personalized search ranking enables guests to more easily discover homes, and smart pricing allows hosts to set more competitive prices according to supply and demand. However, these projects each required a lot of dedicated data science and engineering time and effort. Recently, advances in Airbnb\u2019s machine learning infrastructure have lowered the cost significantly to deploy new machine learning models to production. For example, our ML Infra team built a general feature repository that allows users to leverage high quality, vetted, reusable features in their models. Data scientists have started to incorporate several AutoML tools into their workflows to speed up model selection and performance benchmarking. Additionally, ML infra created a new framework that will automatically translate Jupyter notebooks into Airflow pipelines. In this post, I will describe how these tools worked together to expedite the modeling process and hence lower the overall development costs for a specific use case of LTV modeling\u200a\u2014\u200apredicting the value of homes on Airbnb. Customer Lifetime Value (LTV), a popular concept among e-commerce and marketplace companies, captures the projected value of a user for a fixed time horizon, often measured in dollar terms. At e-commerce companies like Spotify or Netflix, LTV is often used to make pricing decisions like setting subscription fees. At marketplace companies like Airbnb, knowing users\u2019 LTVs enable us to allocate budget across different marketing channels more efficiently, calculate more precise bidding prices for online marketing based on keywords, and create better listing segments. While one can use past data to calculate the historical value of existing listings, we took one step further to predict LTV of new listings using machine learning. Data scientists are typically accustomed to machine learning related tasks such as feature engineering, prototyping, and model selection. However, taking a model prototype to production often requires an orthogonal set of data engineering skills that data scientists might not be familiar with. Luckily, At Airbnb we have machine learning tools that abstract away the engineering work behind productionizing ML models. In fact, we could not have put our model into production without these amazing tools. The remainder of this post is organized into four topics, along with the tools we used to tackle each task: One of the first steps of any supervised machine learning project is to define relevant features that are correlated with the chosen outcome variable, a process called feature engineering. For example, in predicting LTV, one might compute the percentage of the next 180 calendar dates that a listing is available or a listing\u2019s price relative to comparable listings in the same market. At Airbnb, feature engineering often means writing Hive queries to create features from scratch. However, this work is tedious and time consuming as it requires specific domain knowledge and business logic, which means the feature pipelines are often not easily sharable or even reusable. To make this work more scalable, we developed Zipline\u200a\u2014\u200aa training feature repository that provides features at different levels of granularity, such as at the host, guest, listing, or market level. The crowdsourced nature of this internal tool allows data scientists to use a wide variety of high quality, vetted features that others have prepared for past projects. If a desired feature is not available, a user can create her own feature with a feature configuration file like the following: When multiple features are required for the construction of a training set, Zipline will automatically perform intelligent key joins and backfill the training dataset behind the scenes. For the listing LTV model, we used existing Zipline features and also added a handful of our own. In sum, there were over 150 features in our model, including: With our features and outcome variable defined, we can now train a model to learn from our historical data. As in the example training dataset above, we often need to perform additional data processing before we can fit a model: In this step, we don\u2019t quite know what is the best set of features to use, so writing code that allows us to rapidly iterate is essential. The pipeline construct, commonly available in open-source tools like Scikit-Learn and Spark, is a very convenient tool for prototyping. Pipelines allow data scientists to specify high-level blueprints that describe how features should be transformed, and which models to train. To make it more concrete, below is a code snippet from our LTV model pipeline: At a high level, we use pipelines to specify data transformations for different types of features, depending on whether those features are of type binary, categorical, or numeric. FeatureUnion at the end simply combines the features column-wise to create the final training dataset. The advantage of writing prototypes with pipelines is that it abstracts away tedious data transformations using data transforms. Collectively, these transforms ensure that data will be transformed consistently across training and scoring, which solves a common problem of data transformation inconsistency when translating a prototype into production. Furthermore, pipelines also separates data transformations from model fitting. While not shown in the code above, data scientists can add a final step to specify an estimator for model fitting. By exploring different estimators, data scientists can perform model selection to pick the best model to improve the model\u2019s out of sample error. As mentioned in the previous section, we need to decide which candidate model is the best to put into production. To make such a decision, we need to weigh the tradeoffs between model interpretability and model complexity. For example, a sparse linear model might be very interpretable but not complex enough to generalize well. A tree based model might be flexible enough to capture non-linear patterns but not very interpretable. This is known as the Bias-Variance tradeoff. In applications such as insurance or credit screening, a model needs to be interpretable because it\u2019s important for the model to avoid inadvertently discriminating against certain customers. In applications such as image classification, however, it is much more important to have a performant classifier than an interpretable model. Given that model selection can be quite time consuming, we experimented with using various AutoML tools to speed up the process. By exploring a wide variety of models, we found which types of models tended to perform best. For example, we learned that eXtreme gradient boosted trees (XGBoost) significantly outperformed benchmark models such as mean response models, ridge regression models, and single decision trees. Given that our primary goal was to predict listing values, we felt comfortable productionizing our final model using XGBoost, which favors flexibility over interpretability. As we alluded to earlier, building a production pipeline is quite different from building a prototype on a local laptop. For example, how can we perform periodic re-training? How do we score a large number of examples efficiently? How do we build a pipeline to monitor model performance over time? At Airbnb, we built a framework called ML Automator that automagically translates a Jupyter notebook into an Airflow machine learning pipeline. This framework is designed specifically for data scientists who are already familiar with writing prototypes in Python, and want to take their model to production with limited experience in data engineering. Here is a code snippet demonstrating how the fit and transform functions are defined in our LTV model. The fit function tells the framework that a XGBoost model will be trained, and that data transformations will be carried out according to the pipeline we defined previously. Once the notebook is merged, ML Automator will wrap the trained model inside a Python UDF and create an Airflow pipeline like the one below. Data engineering tasks such as data serialization, scheduling of periodic re-training, and distributed scoring are all encapsulated as a part of this daily batch job. As a result, this framework significantly lowers the cost of model development for data scientists, as if there was a dedicated data engineer working alongside the data scientists to take the model into production! Note: Beyond productionization, there are other topics, such as tracking model performance over time or leveraging elastic compute environment for modeling, which we will not cover in this post. Rest assured, these are all active areas under development. In the past few months, data scientists have partnered very closely with ML Infra, and many great patterns and ideas arose out of this collaboration. In fact, we believe that these tools will unlock a new paradigm for how to develop machine learning models at Airbnb. We are very excited about the future of this framework and the new paradigm it brought along. By bridging the gap between prototyping and productionization, we can truly enable data scientists and engineers to pursue end-to-end machine learning projects and make our product better. Want to use or build these ML tools? We\u2019re always looking for talented people to join our Data Science and Analytics team! Special thanks to members of Data Science & ML Infra team who were involved in this work: Aaron Keys, Brad Hunter, Hamel Husain, Jiaying Shi, Krishna Puttaswamy, Michael Musson, Nick Handel, Varant Zanoyan, Vaughn Quoss et al. Additional thanks to Gary Tang, Jason Goodman, Jeff Feng, Lindsay Pettingill for reviewing this blog post.", "links": ["https://medium.com/", "https://medium.com/airbnb-engineering?source=logo-lo_ce1220fde668---53c7c27702d5", "https://twitter.com/AirbnbEng", "https://medium.com/m/signin?redirect=https%3A%2F%2Fmedium.com%2Fairbnb-engineering%2Fusing-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d", "https://medium.com/airbnb-engineering", "https://medium.com/airbnb-engineering/ai/home", "https://medium.com/airbnb-engineering/airbnb-engineering-backend/home", "https://medium.com/airbnb-engineering/data/home", "https://medium.com/airbnb-engineering/airbnb-engineering-infrastructure/home", "https://medium.com/airbnb-engineering/tagged/mobile", "https://medium.com/airbnb-engineering/web/home", "http://airbnb.io/projects/", "https://medium.com/airbnb-engineering/search", "https://medium.com/@rchang?source=post_header_lockup", "https://medium.com/@rchang?source=post_header_lockup", "https://twitter.com/_rchang", "https://medium.com/swlh/diligence-at-social-capital-part-3-cohorts-and-revenue-ltv-ab65a07464e1", "http://scikit-learn.org/stable/", "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html", "https://www.kaggle.com/general/16927", "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html", "https://spark.apache.org/docs/latest/ml-pipeline.html", "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html", "http://scikit-learn.org/stable/data_transforms.html", "http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html", "https://medium.com/airbnb-engineering/automated-machine-learning-a-paradigm-shift-that-accelerates-data-scientist-productivity-airbnb-f1f8a10d61f8", "http://scott.fortmann-roe.com/docs/BiasVariance.html", "https://medium.com/airbnb-engineering/automated-machine-learning-a-paradigm-shift-that-accelerates-data-scientist-productivity-airbnb-f1f8a10d61f8", "https://github.com/dmlc/xgboost", "https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8", "http://www.florianwilhelm.info/2016/10/python_udf_in_hive/", "https://airflow.incubator.apache.org/", "https://www.airbnb.com/careers/departments/data-science-analytics", "https://www.linkedin.com/in/aaronkeys/", "https://www.linkedin.com/in/brad-hunter-497621a/", "https://www.linkedin.com/in/hamelhusain/", "https://www.linkedin.com/in/jiaying-shi-a2142733/", "https://www.linkedin.com/in/krishnaputtaswamy/", "https://www.linkedin.com/in/michael-m-a37b1932/", "https://www.linkedin.com/in/nicholashandel/", "https://www.linkedin.com/in/vzanoyan/", "https://www.linkedin.com/in/vquoss/", "https://www.linkedin.com/in/thegarytang/", "https://medium.com/@jasonkgoodman", "https://twitter.com/jtfeng", "https://medium.com/@lpettingill", "https://medium.com/@jtfeng?source=post_page", "https://medium.com/@jasonkgoodman?source=post_page", "https://medium.com/@gary.tang_94319?source=post_page", "https://medium.com/@lpettingill?source=post_page", "https://medium.com/@vquoss?source=post_page", "https://medium.com/@eddie.santos.3?source=post_page", "https://medium.com/@NicholasHandel?source=post_page", "https://medium.com/tag/machine-learning?source=post", "https://medium.com/tag/data-science?source=post", "https://medium.com/tag/airbnb?source=post", "https://medium.com/tag/technology?source=post", "https://medium.com/@rchang?source=footer_card", "https://medium.com/@rchang", "http://twitter.com/Airbnb", "http://twitter.com/Twitter", "https://medium.com/airbnb-engineering?source=footer_card", "https://medium.com/airbnb-engineering?source=footer_card", "http://airbnb.io", "https://medium.com/airbnb-engineering", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "180": {"url": "https://www.ayasdi.com/blog/artificial-intelligence/why_ai_is_the_new_bigdata/", "title": "The \u201cBig Data-ization\u201d of Artificial Intelligence", "text": "\u2039 Back to Blog  Artificial Intelligence, Data, Machine Intelligence, Machine Learning It seems like it was only a few years ago that the term \u201cbig data\u201d went from a promising area of research and interest to something so ubiquitous that it lost all meaning, descending ultimately into the butt of jokes.  As everyone piled onto the big data bandwagon, it became impossible to separate truth from fiction. Every executive and entrepreneur that I ran into was doing a \u201cbig data\u201d thing. I recall meeting someone whose company shot videos for enterprise customers and was pitching it as a \u2018Big Data play\u2019 \u2013 because video files, you know, are huge \u2013 they take up lots of space.  Thankfully, the noise associated with \u201cbig data\u201d is abating as sophistication and common sense take hold. In fact, in many circles the term actually exposes the user as someone who doesn\u2019t really understand the space.  Unfortunately, the same malady has now afflicted AI. Everyone I meet is doing an \u2018AI play\u2019 \u2013 even if all they did was to build a simple linear regressor in Excel.  AI is, unfortunately, the new \u201cbig data.\u201d While not good, it is not all bad either.  After all, the data ecosystem benefited from all of the \u201cbig data\u201d attention and investment \u2013 creating some amazing software and producing some exceptional productivity gains.  The same will happen with AI \u2013 with the increased attention comes investment dollars which in turn will drive adoption \u2013 enhancing the ecosystem. Having said that we need to stop calling regression on excel AI \u2013 it\u2019s ridiculous and undermines some incredible work being done in the space.  Regression has been around for 200+ years. Gauss and Legendre didn\u2019t don the AI cloak when they discovered regression. Neither should the recent stats graduate trying to raise money for his/her startup.  I will tell you what I think qualifies as AI in a moment \u2013 but here are some thoughts that influence that discussion. First, the AI definition I use is focused on the narrow, application specific AI, not the more general problem of artificial general intelligence (AGI) where simulating a person using software is the equivalent of intelligence.  Second, the vast, vast majority of the data that exists in the world is unlabeled. It is not practical to label that data manually and doing so would likely create bias anyway. One can argue that the Internet as a whole is simply a mechanism for humans to entertain ourselves while providing label data for machines.  Unlabeled data presents a different challenge, one we will address shortly, but the key point here is that it is everywhere and represents the key to extracting business value (or any value).  Third, we are not producing data scientists at a rate that can keep pace with the growth of data. Even with the moniker as a the \u201csexiest job of the 21st century\u201d the pace at which data scientists are created doesn\u2019t begin to approach the growth rate we are seeing in data.  Fourth, data scientists, for the most part, are not UX designers or product managers or, in many cases even engineers. As a result, the subject matter experts, those that sit in the business, don\u2019t have effective interfaces to the data science outputs. The interfaces that they have \u2013 powerpoint, excel, or PDF reports have limited utility in transforming the behavior of a company. What is required is something to shape behavior is something more \u2013 applications.  So what does qualify as intelligence? In \u2018On Intelligence\u2019 Jeff Hawkins says that all AI boils down to memory and prediction. The argument is very persuasive, but a bit reductive. We have a slightly different take for what an AI should display and it encompasses a framework: Here are characteristics that I think any AI should display. While some of these elements may seem self-evident that is because they are taken as a single item. Intelligence has a broader context. All the elements must work in conjunction with each other to qualify as AI. \u00a0\u00a0\u00a0 The five elements are: Let\u2019s take each of these concepts in turn. Discovery is the ability of an intelligent system to learn from data without upfront human intervention. Often, this needs to be done without being presented with an explicit target. It relies on the use of unsupervised and semi-supervised machine learning techniques (such as segmentation, dimensionality reduction, anomaly detection, etc.), as well as more supervised techniques where there is an outcome or there are several outcomes of interest. Usually, in enterprise software, the term discovery refers to the ability of ETL/MDM solutions to discover the various schemas of tables in large databases and automatically find join keys etc. This is not what we mean by discovery. We use of the term very differently and has this has important implications. In complex datasets, it is nearly impossible to ask the \u201cright\u201d questions. To discover what value lies within the data one must \u00a0understand all the relationships that are inherent and important in the data. That requires a principled approach to hypothesis generation. \u00a0 One technique, topological data analysis (TDA), is exceptional at surfacing hidden relationships that exist in the data and identifying those relationships that are meaningful without having to ask specific questions of the data. The result is an output that is able to represent complex phenomena, and is therefore able to surface weaker signals as well as the stronger signals. \u00a0 This permits the detection of emergent phenomena. \u00a0 As a result, enterprises can now discover answers to questions they didn\u2019t even know to ask and do so with data that is unlabeled. Once the data set is understood through intelligent discovery, supervised approaches are applied to predict what will happen in the future. These types of problems include classification, regression and ranking.  For this pillar, most companies use a standard set of supervised machine learning algorithms including random forests, gradient boosting, linear/sparse learners. It should be noted, however, that the unsupervised work from the previous step is highly useful in many ways. For example, it can generate relevant features for use in prediction tasks or finding local patches of data where supervised algorithms may struggle (systematic errors). The predict phase is an important part of the business value associated with data science, however, generally, in predictive analytics, there exists a notion that this is the sum total of machine learning.  This is not the case by far.  Prediction, while important, is pretty well understood and does not, on its own qualify as \u201cintelligence.\u201d \u00a0It goes back to calling a maxed out Excel table and a linear regressor AI. It is just not the case. Further, Prediction can go wrong along a number of dimensions, particularly if the groups on which you are predicting are racked with some type of bias (algorithmic, sampling etc.)  Again, Prediction is key and provides tremendous business value done correctly, but in and of itself it is not AI. We need to stop calling it as such. Applications need to support interaction with humans in a way which makes outcomes recognizable and believable. For example, when one builds a predictive model, it is important to have an explanation of how the model is doing what it is doing, i.e. what the features in the model are doing in terms that are familiar to the users of the model. \u00a0This level of familiarity is important in generating trust and intuition.  Similarly, in the same way that automobiles have mechanisms not just for detecting the presence of a malfunction, but also for specifying the nature of the malfunction and suggesting a method for correcting it, so one needs to have a \u201cnuts and bolts\u201d understanding of how an application is working in order to \u201crepair\u201d it when it goes awry. \u00a0 Transparency AND Justification. There is a difference. Transparency tells you what algorithms and parameters were used, while, Justification tells you why. For intelligence to be meaningful, it must be able to justify and explain its assertions, as well as to be able to diagnose failures.  No business leader should deploy intelligent and autonomous applications against critical business problems without a thorough understanding of what variables power the model.  Enterprises cannot move to a model of intelligent applications without trust and transparency. \u00a0 AI without UX is of limited utility.  UX is what distributes that intelligence across the organization and pushes it to the edge \u2013 where it can consumed by practitioners and subject matter experts.  Ultimately, the process of operationalizing an intelligent application within the enterprise requires some change in the organization, an acceptance that the application will evolve over time and that will demand downstream changes \u2013 automated or otherwise.  \u00a0For this to happen, intelligent applications need to be \u201clive\u201d in the business process, seeing new data and automatically executing the loop of Discover, Predict, Justify on a frequency that makes sense for that business process. For some processes that may be quarterly, for others daily. That loop can even be measured in seconds. \u00a0 Intelligent systems are designed to detect and react as the data evolves. An intelligent system is one that is always learning, live in the workflow and constantly improving. \u00a0In the modern data world, an application that is not getting more intelligent is getting dumber.  Intelligent applications are designed to detect and react when data distributions evolve. As a result, they need to be \u201con the wire\u201d in order to detect that phenomena before it becomes a problem. Too many solutions provide an answer in a point of time, an intelligent system is one that is always learning through the framework outlined here. This is what defines intelligence \u2013 not a machine learning algorithm kicking out predictions or the results of a data scientist\u2019s work as a PDF. For the industry to continue to grow and evolve we need to start doing a better job of recognizing what is truly AI and what is imposter-ware.  Let\u2019s make it a point of celebrating the real AI work.", "links": ["https://www.ayasdi.com", "https://www.ayasdi.com/platform/", "https://www.ayasdi.com/industry-overview/", "https://www.ayasdi.com/financial-services/", "https://www.ayasdi.com/healthcare/", "https://www.ayasdi.com/public-sector/", "https://www.ayasdi.com/solutions/anti-money-laundering/", "https://www.ayasdi.com/solutions/clinical-variation-management/", "https://www.ayasdi.com/solutions/denials/", "https://www.ayasdi.com/solutions/regulatory-risk/", "https://www.ayasdi.com/solutions/population-health/", "https://www.ayasdi.com/company/", "https://www.ayasdi.com/company/leadership/", "https://www.ayasdi.com/company/careers/", "https://www.ayasdi.com/company/collaborators/", "https://www.ayasdi.com/company/customers/", "https://www.ayasdi.com/company/news-and-events/", "https://www.ayasdi.com/company/contact-us/", "https://www.ayasdi.com/blog/", "https://www.ayasdi.com/request-a-demo/", "https://www.ayasdi.com/blog/author/gurjeet-singh/", "https://www.ayasdi.com/blog/category/artificial-intelligence/", "https://www.ayasdi.com/blog/category/bigdata/", "https://www.ayasdi.com/blog/category/machine-intelligence/", "https://www.ayasdi.com/blog/category/machine-learning/", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/08/02144138/BG2.png", "https://priceonomics.com/the-discovery-of-statistical-regression/", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/05/12132340/Machine_Intelligence_Apps_WP_051617v01.pdf", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/08/02144141/DS1.png", "https://www.wired.com/brandlab/2015/05/jeff-hawkins-firing-silicon-brain/", "http://cdn.ayasdi.com/wp-content/uploads/2015/02/wp-tda-and-machine-learning.pdf", "https://www.ayasdi.com/blog/aml/intelligent-segmentation-as-the-attack-point-for-aml/", "http://feedburner.google.com/fb/a/mailverify?uri=AyasdiBlog", "https://ayasdicommunity.force.com/login", "https://www.ayasdi.com/terms-of-service/", "https://www.ayasdi.com/privacy-policy/", "https://www.ayasdi.com/pilot-terms/", "http://www.youtube.com/user/ayasdi", "http://feeds.feedburner.com/AyasdiBlog", "http://www.linkedin.com/company/ayasdi", "https://www.facebook.com/ayasdi", "https://twitter.com/ayasdi"]}, "181": {"url": "https://www.mapd.com/blog/2017/08/03/whos-got-the-best-mobile-network-in-the-us/", "title": "", "text": "Based on the deluge of commercials and ads in this competitive industry, it seems every major player claims that they do. The competition is apparently so fierce that even their spokesmen are being headhunted. Admittedly, it can be a bit confusing when you find these results in one city:  And these in another:  By the way, how do you even define \u2018best\u2019 network? It\u2019s a loaded question for sure. We\u2019ll get back to that in a minute. It\u2019s a question with relevance to nearly every American, considering the vast majority of us own a cellphone of some kind (over 95% according to Pew). And we don\u2019t leave them alone; we touch them thousands of times each day. In fact, a good mobile network can affect our very quality of life. Ever travelled any significant distance with children and poor mobile data speeds? The same question is also relevant to those very telecommunications providers themselves, of course, and particularly in light of the fact that obtaining new subscribers will increasingly mean winning them away from competitors. Ensuring you are outpacing your rivals with superior network metrics is a key ingredient to both marketing and market share success. Our friends and partners at Tutela have created the world\u2019s largest mobile information platform, collecting billions of crowdsourced data points anonymously. These span device, network, and application information to allow us to get from data to insights. They\u2019ve graciously given us (and now you) a slice of their treasure trove. Our demo contains a few months of US data (~112M rows), although Tutela has data for more metrics, for nearly every country, and for a much greater timeframe. There are obviously many variables to consider when it comes to a great network, and one carrier will almost certainly not take the top spot in each category. But we can identify areas of significance where we hope to find favorable results: Spoiler alert: we\u2019re not actually going to attempt to find a winner. Those are just some of the metrics you can see at a glance on our MapD Immerse demo dashboard:  This is immediately useful. Coverage, speed, signal strength, and latency all matter, so MapD allows you to visualize and correlate them for effective consumption. Not coincidentally, the MapD Immerse visualization tool is purpose-built to leverage the MapD Core backend GPU-powered, in-memory, columnar SQL database at scale. This allows  interactive analysis over multi-billion row data sets, with the ability to drill down to an individual row...in milliseconds. It\u2019s also why Tutela has selected to partner with MapD; no need for indexing, sampling, or pre-computation. This means you can zoom in and out, filter, and more in an ad hoc fashion. Let\u2019s take it for a spin. As a consumer, you might be interested in how well each service provider performs in a certain geographic region (i.e., where you live and work) to aid in making a purchasing decision.  For example, to zoom from country level to the Washington, DC area, just type the city name into the searchbox of the pointmap. MapD Immerse will automatically zoom in when you click enter. It looks like in this region, you might lean toward Verizon:  But it may not be as clear cut if you live in San Francisco:  In either case, Sprint better have some compelling discounts or incentives to entice new subscribers. Further, both Sprint and T-Mobile might leverage this information to prioritize buildout or upgrade of cell towers. If they\u2019re analyzing subscriber loss in these areas, this also may give them insight into why that could be occurring. Another valuable feature of the MapD platform is the ability to easily identify trends, anomalies, or outliers. Visualization can often be an invaluable enabler for these situations. Sometimes new revenue opportunities or cost savings can be had by capitalizing on correlations in data that you didn\u2019t even realize existed! See the spike in data downloads in the line chart? By highlighting the time frame with your mouse, MapD will cross filter all charts using the selected time slice. Doing the same with the line chart for data uploads further reveals that the largest spikes occur on May 28 and 30. Ah, that\u2019s Memorial Day weekend in the United States. So this makes sense.  We can get even more granular and note that the bulk of data transfer occurred between 12-1am local time (4-5am UTC) as the heatmap indicates. I wonder if Uber and Lyft also experienced a spike around that time, as celebrations came to an end?  When data exploration at scale is so easy, what other interesting insights will you find? Explore this demo using Tutela's dataset on MapD\u2019s platform on your own here.  If you\u2019d like to learn more about MapD\u2019s recently announced partnership with Tutela, watch our on-demand webinar, introducing and demonstrating Tutela Explorer, a new mobile data analytics solution that provides real-time, interactive and highly visual insight into the performance of mobile networks and device usage. Tutela Explorer is available now as-a-service with global data coverage from Tutela.", "links": ["http://www.mapd.com/blog", "https://www.nytimes.com/2016/10/15/business/sprint-verizon-hear-me-now-paul-marcarelli.html", "http://www.pewinternet.org/fact-sheet/mobile/", "http://www.networkworld.com/article/3092446/smartphones/we-touch-our-phones-2617-times-a-day-says-study.html", "https://www.tutela.com/explorer/", "https://www.mapd.com/demos/telecom", "https://www.mapd.com/demos/telecom", "https://www.mapd.com/platform/core/", "https://www.mapd.com/demos/telecom", "https://www.brighttalk.com/webcast/14525/267851", "https://www.tutela.com/explorer"]}, "182": {"url": "https://www.oreilly.com/learning/building-a-simple-graphql-server-with-neo4j", "title": "Building a simple GraphQL server with Neo4j", "text": "How to implement a GraphQL API that queries Neo4j for a simple movie app. If you're interested in making the move from REST to GraphQL, check out this OSCON video from the team at Github. GraphQL is a powerful new tool for building APIs that allows clients to ask for only the data they need. Originally designed at Facebook to minimize data sent over the wire and reduce round-trip API requests for rendering views in native mobile apps, GraphQL has since been open sourced to a healthy community that is building developer tools. There are also a number of large companies and startups such as GitHub, Yelp, Coursera, Shopify, and Mattermark building public and internal GraphQL APIs. Despite what the name seems to imply, GraphQL is not a query language for graph databases, it is instead an API query language and runtime for building APIs. The \u201cGraph\u201d component of the name comes from the graph data model that GraphQL uses in the frontend. GraphQL itself is simply a specification, and there are many great tools available for building GraphQL APIs in almost every language. In this post we'll make use of graphql-tools by Apollo to build a simple GraphQL API in JavaScript that queries a Neo4j graph database for movies and movie recommendations. We will follow a recipe approach: first, exploring the problem in more detail, then developing our solution, and finally we discuss our approach. Good resources for learning more about GraphQL are GraphQL.org and the Apollo Dev Blog. GraphQL by design can work with any database or backend system, however in this example we'll be using the Neo4j graph database as our persistence layer. Why use a graph database with GraphQL? The idea of application data as a graph is an underpinning design choice of GraphQL. For example, think of customers who have placed orders that contain products\u2014that's a graph! GraphQL enables developers to translate their backend data into the application data graph on the frontend, but if we use a graph database on the backend we can do away with this impedance mismatch and we have graphs all the way down. We'd like to build a simple GraphQL API for a movie app that can do two things: In a traditional REST-ish API approach we might create two separate endpoints, perhaps /movies/search and /movies/similar. One endpoint to search for movies by substring of the title, and another to return a list of similar movies. As we add additional features we might keep adding endpoints. With GraphQL, our API is served from a single endpoint /graphql that takes one or more GraphQL queries and returns JSON data in a shape that is specified by the GraphQL query - only the data requested by the client is returned. Let's see how we can build this simple API. We'll be building a simple node.js JavaScript web server using Express.js to serve our GraphQL endpoint1. First of all we\u2019ll need a Neo4j database (with data) for our GraphQL server to query. For this example we\u2019ll make use of a Neo4j Sandbox instance. Neo4j Sandbox allows us to quickly spin up a hosted Neo4j instance, optionally with existing datasets focused around specific use cases. We\u2019ll use the Recommendations Neo4j Sandbox which includes data about movies and movie reviews and is designed to be used for generating personalized recommendations (for example, by using collaborative filtering to recommend movies based on similar users\u2019 reviews). We'll be making use of a few dependencies for this GraphQL server. I won't list them all here (you can see the full list in the package.json for this project, but there a few worth noting: We\u2019ll follow the \u201cGraphQL First\u201d development paradigm. In this approach, we start by defining a GraphQL schema. This schema defines the types and queries available in our API and then becomes the specification for the API. If we were building a complete application, the frontend developers could use this schema to build out the frontend while the backend team builds the backend in parallel, speeding development. Once we\u2019ve defined our schema we\u2019ll need to create resolver functions that are responsible for fetching data from Neo4j. This schema will define the types and GraphQL queries that we'll be able to use in our API. You can think of the schema as the API blueprint. schema.js Each GraphQL field is resolved by a resolver function. The resolver function defines how data is fetched for that field. resolvers.js Now that we've defined the GraphQL schema and resolver functions we are ready to serve the GraphQL endpoint, using Express.js. server.js If you've ever used Express this should look familiar, you'll notice that we're creating two endpoints and serving them on localhost:8080 Let's take a look at what we just did: Now that we have our API running, let's use the GraphiQL in-browser IDE for GraphQL to search for movies whose titles contain \"Matrix\" and find similar movies for each matching movie. We can load GraphiQL by opening http://loalhost:8080/graphiql in our browser and querying with this GraphQL query:   Querying our GraphQL movies API using GraphiQL, the in-browser IDE for GraphQL GraphQL is still new to almost everyone with the exception of Facebook, where it has been used since 2012, so many conventions and best practices are still being developed. However, GraphQL First Development is a philosophy the community has adopted which gives structure to the process of building a GraphQL API. By defining a contract for the API, expressed as a GraphQL schema, the frontend and backend developer teams can independently implement their applications, using the schema as a guide. To make GraphQL First Development even easier with Neo4j, the Neo4j team has built neo4j-graphql-cli, a command line tool for easily spinning up a Neo4j backed GraphQL endpoint based on a user defined GraphQL schema that can optionally be annotated with Cypher queries - exposing the power of Cypher from within GraphQL. You can learn more about this here. 1 All code for this project is available on GitHub . This example is also available as an Apollo Launchpad Pad, which you can run live in the browser.\u21a9 William Lyon is an engineer on the Developer Relations team at Neo4j, the open source graph database, where he builds tools for integrating Neo4j with other technologies and helps users be successful with graphs. He also leads Neo4j's Data Journalism Accelerator Program. Prior to Neo4j, he worked as a software engineer for a variety of startups, building mobile apps for the real estate industry, trading tools for quantitative finance, and predictive APIs. William holds a masters degree in Computer Science from the University of Montana. You can... How to package up your own repositories for use via PHP's Composer. Will content-blocking change the Web? Learn how to use Node.js and Express.js to quickly bootstrap a web server, tie in MongoDB, and display it all using dynamic HTML templates Diversity and inclusion in the tech industry has experienced a severe failure. Erica Baker approaches the topic like we would for any other service failure: with a postmortem. \u00a9 2017 O'Reilly Media, Inc. All trademarks and registered trademarks appearing on oreilly.com are the property of their respective owners.", "links": ["https://www.safaribooksonline.com/home/?utm_medium=content&utm_source=oreilly.com&utm_campaign=lgen&utm_content=20170601+nav", "http://www.oreilly.com/conferences/", "http://shop.oreilly.com/", "https://www.safaribooksonline.com/home/?utm_medium=content&utm_source=oreilly.com&utm_campaign=lgen&utm_content=20170601+nav", "http://www.oreilly.com/conferences/", "http://shop.oreilly.com/", "https://pixabay.com/en/louvre-pyramid-mesh-perspective-2189967/", "https://www.safaribooksonline.com/library/view/oscon-2017-/9781491976227/video306664.html?utm_source=oreilly&utm_medium=newsite&utm_campaign=building-a-simple-graphql-server-with-neo4j", "https://github.com/apollographql/graphql-tools", "http://www.apollodata.com/", "http://graphql.org/", "https://dev-blog.apollodata.com/", "https://neo4j.com/sandbox-v2/", "https://github.com/johnymontana/movies-graphql-neo4j-server/blob/master/package.json", "http://dev.apollodata.com/tools/graphql-tools/index.html", "https://expressjs.com/", "https://github.com/neo4j/neo4j-javascript-driver", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://www.graph.cool/docs/faq/graphql-sdl-schema-definition-language-kr84dktnp0/", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "http://loalhost:8080/graphiql", "https://www.npmjs.com/package/neo4j-graphql-cli", "http://neo4j.com/developer/graphql", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://pixabay.com/en/louvre-pyramid-mesh-perspective-2189967/", "https://twitter.com/share", "http://oreilly.com/about/", "http://oreilly.com/work-with-us.html", "http://oreilly.com/careers/", "http://shop.oreilly.com/category/customer-service.do", "http://shop.oreilly.com/category/customer-service.do", "http://fb.co/OReilly", "http://twitter.com/oreillymedia", "https://www.youtube.com/user/OreillyMedia", "https://plus.google.com/+oreillymedia", "https://www.linkedin.com/company/oreilly-media", "http://oreilly.com/terms/", "http://oreilly.com/privacy.html", "http://www.oreilly.com/about/editorial_independence.html"]}, "183": {"url": "http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "title": "", "text": "", "links": ["http://www.getrevue.co/?ref=Revue+Profile", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profileimage&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profiledate&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profilename&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lXPZA?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lXPZA?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blogs.technet.microsoft.com/machinelearning/2017/07/25/lessons-learned-benchmarking-fast-machine-learning-algorithms/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/DM9WJ?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/DM9WJ?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://medium.com/@stathis/design-by-evolution-393e41863f98?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/XYWKo?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/XYWKo?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.datasciencelearner.com/complete-overview-learning-python-data-analysis/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/6nba0?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/6nba0?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://reinforce.io/blog/introduction-to-tensorforce/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xekd3?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xekd3?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.kdnuggets.com/2017/05/top-10-machine-learning-videos-on-youtube-updated.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Ka8JK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Ka8JK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://arxiv.org/abs/1704.01568?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/96KED?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/96KED?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://datascienceplus.com/random-forests-in-r/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Vd1wK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Vd1wK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blog.keras.io/the-future-of-deep-learning.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/eKr5e?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/eKr5e?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://medium.com/airbnb-engineering/using-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/odRNK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/odRNK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://machinelearning.apple.com/2017/07/07/GAN.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xekoa?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xekoa?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://arxiv.org/abs/1707.06642?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/mxRw2?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/mxRw2?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.techleer.com/articles/200-naive-bayes-machine-learning-algorithm-for-classification-problems/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/3K1nx?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/3K1nx?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463?gi=7fe02170b69e&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/OnXK7?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/OnXK7?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://aiukraine.com/en/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/nqRVq?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/nqRVq?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.bonaccorso.eu/2017/07/23/machine-learning-algorithms/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Z4bBj?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Z4bBj?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/jobs", "http://bit.ly/2uqHV4J", "https://flyelephant.net/jobs/Spinbackup-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/jobs/Spinbackup-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/yJRmd?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/yJRmd?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/jobs/Snap-Ukraine-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/jobs/Snap-Ukraine-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/jAR3B?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/jAR3B?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%238%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "https://www.getrevue.co/?utm_source=Data Science Digest&utm_medium=email&utm_content=footerlink&utm_campaign=Issue", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=forwarded&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%238%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "http://www.getrevue.co/?ref=Revue+Profile"]}, "184": {"url": "https://blog.insightdatascience.com/preparing-for-the-transition-to-applied-ai-d41e48403447", "title": "Transitioning from Software Engineering to Artificial Intelligence", "text": "Emmanuel Ameisen and Jeremy Karnowski, Insight AI A significant part of the Software Engineer role requires staying up-to-date with evolving frameworks, standards, and paradigms. Software Engineers strive to constantly learn, in order to always use the best tool for the job. As Machine Learning finds footholds in more applications every day, it has naturally become a topic that many Engineers want to master. Machine Learning, though, is harder to pick up than a new framework. To be an efficient practitioner, you require a solid understanding of the theory of the field, broad knowledge of the current state of the art, and an ability to frame problems in a non deterministic way. Many guides you can find online will simply teach you how to train an out-of-the-box model on a curated data set to achieve good accuracy and call it a day. The truth is that a much more extensive skillset is essential in becoming an effective Machine Learning Engineer. Below is a distillation of the many conversations we\u2019ve had with over 50 top Machine Learning teams all over The Bay Area and New York, who\u2019ve come to Insight to find AI Practitioners poised to tackle their problems and accelerate their expansion into Applied AI. Deploying a Machine Learning solution requires much more than just training an arbitrary model on your data. It requires an understanding of: In other words, in addition to engineering chops, you need to understand the fundamentals of statistics, linear algebra, and optimization theory in order to integrate, deploy, and debug models. Building a custom Machine Learning solution for a problem requires that you consider issues ranging from acquiring, labeling and pre-processing your data to building, updating, and serving an inference model, and everything in between. Finally, building a REST API for a standard web-app is a task that we can deem feasible ahead of time. Machine Learning models, on the other hand are not always guaranteed to converge, or produce usable outputs. The best way to learn how to scope and deliver impactful Machine Learning products, is to understand how their theoretical underpinnings relate to the taxonomy of your data. In order to understand Machine Learning, a solid knowledge of statistics fundamentals is essential. This involves understanding the following: When you are training a neural network, what is actually happening? What makes some tasks doable and others not? A good approach to this might be to first try to understand Machine Learning through graphics and examples, before diving deeper into the theory. Concepts to understand range from how different loss functions work, why back propagation is useful, or what a computational graph is. A deep understanding is crucial both for building a functional model, and to communicate about it efficiently to the rest of the organization. Following are a few resources, starting with high level overviews, and diving deeper. Another fundamental skill is the ability to read, understand and implement research papers. It can seem like a daunting task at first, so a good way to start is to look up a paper that already has code attached to it (on GitXiv for example) and try to understand the implementation in depth. Ask any Data Scientist and they\u2019ll tell you 90% of the work they do is data munging. This is just as important for Applied AI, as the success of your model correlates hugely with the quality (and quantity) of your data. Data work comes in many aspects, and falls within a few categories: The best way to get familiar with data wrangling is to grab a dataset in the wild and try to use it. There are many datasets online and many social media and news outlets sites have great APIs. Following the steps above, a good way to learn is to: Debugging Machine Learning algorithms that fail to converge or to give sensible results involves a very different process from debugging code. In the same vein, finding the right architecture and hyperparameters requires solid theoretical fundamentals, but also good infrastructure work to be able to test different configurations out. Because of the pace at which the fields evolve, the methods to debug models are constantly evolving. Here are a few \u201csanity checks\u201d from our discussions and experience deploying models that mirror in some ways the principles of KISS familiar to many Software Engineers. A lot of those steps can be accelerated significantly by your development skills, which brings us to our last skill. A lot of Applied Machine Learning will allow you to leverage Software Engineering skills, sometimes with a little twist. These skills include: For more details on some of the software skills we recommend acquiring to become a quality Machine Learning Engineer, check out our post dedicated to transitioning to Applied AI from Academia. The resources above will help you approach and tackle actual Machine Learning problems. But the field of Applied AI changes extremely quickly, and the best way to learn, is to get your hands dirty and actually try to build out an end-to-end solution to solve a real problem. Action Items: Find some inspiration, then dive in! Remember that while Machine Learning Engineering is about building products at heart, there is a research aspect to it. You will explore models and paradigms that will prove unsuccessful, and that is perfectly fine, as it will lead you to understand the intricacies of the problem better. AI is an exciting, ever-changing field. The demand for Machine Learning Engineers is strong, and it is easy to get overwhelmed with the amount of news surrounding the topic. We recommend following a few serious sources and newsletters, to be able to separate PR and abstract research from innovations that are immediately relevant to the field. Here are some sources to help out: Want to learn applied Artificial Intelligence from top professionals in Silicon Valley or New York? Learn more about the Artificial Intelligence program.", "links": ["https://blog.insightdatascience.com?source=logo-lo_3cd488c425bd---d02e65779d7b", "https://twitter.com/InsightDataSci", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.insightdatascience.com%2Fpreparing-for-the-transition-to-applied-ai-d41e48403447", "https://blog.insightdatascience.com", "https://blog.insightdatascience.com/tagged/about-insight", "https://blog.insightdatascience.com/tagged/insight-data-science", "https://blog.insightdatascience.com/tagged/insight-data-engineering", "https://blog.insightdatascience.com/tagged/insight-health-data", "https://blog.insightdatascience.com/tagged/insight-ai", "https://blog.insightdatascience.com/search", "https://blog.insightdatascience.com/@emmanuelameisen?source=post_header_lockup", "https://blog.insightdatascience.com/@emmanuelameisen?source=post_header_lockup", "https://www.udacity.com/course/deep-learning--ud730", "http://cs231n.stanford.edu/", "http://cs224d.stanford.edu/", "http://www.deeplearningbook.org/", "http://www.gitxiv.com/", "https://data.sfgov.org/", "https://www.data.gov/", "https://dev.twitter.com/streaming/public", "http://developer.nytimes.com/docs", "http://people.apache.org/~fhanik/kiss.html", "http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf", "https://github.com/JasperSnoek/spearmint", "https://blog.insightdatascience.com/preparing-for-the-transition-to-applied-ai-8eaf53624079", "https://news.ycombinator.com/", "https://jack-clark.net/import-ai/", "http://www.insightdatascience.com/blog/", "https://blog.insightdatascience.com/tagged/insight-ai", "https://blog.insightdatascience.com/using-deep-learning-to-reconstruct-high-resolution-audio-29deee8b7ccd", "https://blog.insightdatascience.com/separating-overlapping-chromosomes-with-deep-learning-based-image-segmentation-22f97afd3283", "http://insightdata.ai/notifications-list", "http:insightdata.ai", "https://medium.com/@jkarnows?source=post_page", "https://medium.com/@InsightData?source=post_page", "https://medium.com/@rossfadely?source=post_page", "https://blog.insightdatascience.com/tagged/machine-learning?source=post", "https://blog.insightdatascience.com/tagged/ai?source=post", "https://blog.insightdatascience.com/tagged/software-engineering?source=post", "https://blog.insightdatascience.com/tagged/insight-ai?source=post", "https://blog.insightdatascience.com/tagged/deep-learning?source=post", "https://blog.insightdatascience.com/@emmanuelameisen?source=footer_card", "https://blog.insightdatascience.com/@emmanuelameisen", "http://twitter.com/EmmanuelAmeisen", "https://blog.insightdatascience.com?source=footer_card", "https://blog.insightdatascience.com?source=footer_card", "https://blog.insightdatascience.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "185": {"url": "http://vooban.com/en/tips-articles-geek-stuff/hyperopt-tutorial-for-optimizing-neural-networks-hyperparameters/", "title": "", "text": "Hyperopt is a way to search through an hyperparameter space. For example, it can use the Tree-structured Parzen Estimator (TPE) algorithm, which explore intelligently the search space while narrowing down to the estimated best parameters. It is hence a good method for meta-optimizing a neural network which is itself an optimisation problem: tuning a neural network uses gradient descent methods, and tuning the hyperparameters needs to be done differently since gradient descent can\u2019t apply. Therefore, Hyperopt can be useful not only for tuning hyperparameters such as the learning rate, but also to tune more fancy parameters in a flexible way, such as changing the number of layers of certain types, or the number of neurons in a layer, or even the type of layer to use at a certain place in the network given an array of choices, each with nested tunable hyperparameters. This is an oriented random search, in contrast with a Grid Search where hyperparameters are pre-established with fixed steps increase. Random Search for Hyper-Parameter Optimization (such as what Hyperopt do) has proven to be an effective search technique. The paper about this technique sits among the most cited deep learning papers. To sum up, it is more efficient to search randomly through values and to intelligently narrow the search space rather than looping on fixed sets of values for the hyperparameters. Note that this blog post is also available as a Notebook on GitHub. It contains code that can be run with Jupyter. A parameter is defined with a certain uniformrange or else a probability distribution, such as: There is also a few quantized versions of those functions, which rounds the generated values at each step of \u201cq\u201d: It is also possible to use a \u201cchoice\u201d which can lead to hyperparameter nesting: Visualisations of the parameters for probability distributions can be found below. Then, more details on choices and parameter nesting will come. Note on the above charts (especially for the loguniform and uniform distributions): the blurred line averaging the values fades out toward the ends of the signal since it is zero-padded. The line ideally would not fade out by using techniques such as mirror-padding. Those are the best distributions for modeling the values a learning rate. That\u2019s because we want to observe changes in the learning rate according to changing it with multiplications rather than additions, e.g.: when adjusting the learning rate, we\u2019ll want to try to divide it or multiply it by 2 rather than adding and substracting a finite value. To proove this, let\u2019s generate a loguniform distribution for a multiplier of the learning rate, centered at 1.0. Dividing 1 by those values should yield the same distribution. Let\u2019s now define a simple search space and solve for f(x) = x^2 - x + 1, where x is an hyperparameter. Let\u2019s solve for minimizing f(x, y) = x^2 + y^2 using a space using a python dict as structure. Later, this will neable us to nest hyperparameters with choices in a clean way. Yet, we have defined spaces as a single parameter. But that is 1D. Normally, spaces contain many parameters. Let\u2019s define a more complex one and with one nested hyperparameter choice for an uniform float: This will require us to import a few more things, and return the results with a dict that has a \u201cstatus\u201d and \u201closs\u201d key at least. Let\u2019s keep in our return dict the evaluated space too as this may come in handy if we save results to disk. Note that the optimization could be parallelized by using MongoDB and storing the trials\u2019 state here. Althought this is a built-in feature of hyperopt, let\u2019s keep things simple for our examples here. Indeed, the TPE algorithm used by the fmin function has state which is stored in the trials and which is useful to narrow the search space dynamically once we have a few trials. It is then interesting to pause and resume a training, and to apply that to a real problem. This is what\u2019s done inside the hyperopt_optimize.py file of the GitHub repository for this project. There, as an example, we optimize a convolutional neural network for solving the CIFAR-100 problem. Star Fork You might as well like this other blog post of mine on how to use Git Large File Storage (Git LFS) to handle the versioning of huge files when working with machine learning projects. 510-1015 av. Wilfrid-Pelletier  Quebec, QC, Canada  G1W 0C4 418 800.0027 TOLL FREE: 1 844 800.0027 info@vooban.com", "links": ["http://vooban.com/en/", "http://vooban.com/en/", "http://vooban.com/en/", "http://vooban.com/en/products/", "http://vooban.com/en/products/#tpti", "http://vooban.com/en/products/#tdti", "http://vooban.com/en/approach/", "http://vooban.com/en/tactical-squad/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/", "http://vooban.com/en/tips-articles-geek-stuff/", "https://pdfs.semanticscholar.org/9f2a/efc3821853e963beda011ed770f740385b77.pdf", "http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a", "https://github.com/terryum/awesome-deep-learning-papers#optimization--training-techniques", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100/blob/master/IntroductionToHyperopt.ipynb", "http://jupyter.org/", "http://vooban.com/wp-content/uploads/2017/08/histogram_1_int.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_2_uniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_3_loguniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_4_normal.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_5_lognormal.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_6_inverse_loguniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_7_inverse_lognormal.png", "http://vooban.com/wp-content/uploads/2017/08/fx2.png", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100/fork", "http://vooban.com/en/tips-articles-geek-stuff/migrating-to-git-lfs-for-developing-deep-learning-applications-with-large-files/", "http://vooban.com/en/tips-articles-geek-stuff/discover-structure-behind-data-with-decision-trees/", "http://vooban.com/en/tips-articles-geek-stuff/", "https://www.google.ca/maps/place/Vooban/@46.7694849,-71.3055195,17z/data=!3m1!4b1!4m5!3m4!1s0x4cb890d721a9234b:0x979022270b4a9fa4!8m2!3d46.7694849!4d-71.3033308", "http://vooban.com/en/products/", "http://vooban.com/en/approach/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/", "https://twitter.com/vooban", "https://www.instagram.com/voobanvibe/", "https://www.facebook.com/vooban", "https://www.linkedin.com/company/vooban", "http://vooban.com/en/", "http://vooban.com/en/products/", "http://vooban.com/en/products/#tpti", "http://vooban.com/en/products/#tdti", "http://vooban.com/en/approach/", "http://vooban.com/en/tactical-squad/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/"]}, "186": {"url": "https://elitedatascience.com/learn-python-for-data-science", "title": "How to Learn Python for Data Science in 2017 (Updated)", "text": "In this guide, we\u2019ll cover how to learn Python for data science, including our favorite curriculum for self-study. You see, data science is about problem solving, exploration, and extracting valuable information from data. To do so effectively, you\u2019ll need to wrangle\u00a0datasets, train machine learning models, visualize results, and much more. Enter Python. This is the best time ever to learn Python. In fact, Forbes named it a top 10 technical skill in terms of job demand growth. Let\u2019s discuss why\u2026 Python is one of the most widespread languages in the world, and it has a passionate community of users: Python Popularity, TIOBE Index It has an even more loyal following within the data science profession. Some people judge the quality of a programming language by the simplicity of its \"hello, world!\" program. Python does pretty well by this standard: For comparison, here's the same output in Java: Great, case closed. See you back here\u00a0after you've mastered Python? Well, in all seriousness, simplicity is one of Python's greatest strengths. Thanks to its precise and efficient syntax, Python can accomplish the same tasks with less code than other languages. This makes implementing\u00a0solutions refreshingly fast. In addition, Python's vibrant data science community means you'll be able to find plenty of tutorials, code snippets, and people to commiserate with\u00a0fixes to common bugs. Stackoverflow will be one of your best friends. Finally,\u00a0Python\u00a0has an\u00a0all-star lineup of libraries (a.k.a. packages) for data analysis and machine learning, which drastically reduce the time it takes to produce results. More on these later. Before we go into what you'll need to learn, let's discuss what you won't need. You won't need a C.S. degree. Most data scientists will never deal with topics such as memory leaks, cryptography, or \"Big O\" notation. You'll be fine as long as you can write clean, logical code in a scripting language such as Python or R. You won't need a complete course on Python. Python and data science are\u00a0not synonymous. You won't need to memorize all the syntax. Instead, focus on grasping the intuition, such as when function is appropriate or how conditional statements work. You'll gradually remember the syntax after Googling, reading documentation, and good ol' fashioned practice. We recommend a top-down approach. We advocate\u00a0a top-down approach with the goal of getting results first and then solidifying concepts over time. In fact, we prefer to cut out \"classroom\" study in favor of real-world practice. This approach will allow you to build mastery over time while having more fun. There are many ways to install Python on your computer, but we recommend the Anaconda bundle, which comes with the libraries you'll need for data science. Effective programming is not about memorizing syntax, but rather mastering a new way of thinking. Therefore, take your time in building a solid foundation of core programming concepts. These will help you translate solutions in your head into instructions for a computer. If you are completely new to programming, we recommend the excellent Automate the Boring Stuff with Python book, which has been released for free online under a creative commons license. The book promises \"practical programming for total beginners,\" and it keeps each lesson down-to-earth. Read up to Chapter 6 - Manipulating Strings and complete the practice questions along the way. If you only need to brush up on Python syntax, then we recommend the following video, aptly named \"Learn Python in One Video:\" Again, the goal of this step is not to learn everything about Python and programming. Instead, focus on the intuition. You should be able to answer questions such as: If you'd like more practice with the core programming concepts, check out the following resources. Next, we're going to focus on the for data science part of \"how to learn Python for data science.\" As we mentioned earlier, Python has an all-star lineup of libraries for data science. Libraries are simply bundles of pre-existing functions and objects that you can import into your script to save time. These are the action steps we recommend for efficiently picking up a new library: We don't recommend diving much deeper into a library right now because you'll likely forget most of what you've learned by the time you jump into projects. Instead, aim to discover what each library is capable of. If you installed Python through the Anaconda bundle as we recommended above, it will also come with Jupyter Notebook. Jupyter Notebook is a lightweight IDE that's a favorite among data scientists. We recommend it for your projects. You can open a new notebook through Anaconda Navigator, which came with Anaconda. Check out this short video for instructions. These are the essential libraries you'll need: NumPy allows easy and efficient numeric computation, and many other data science libraries are built on top of it. Pandas is high-performance library for data structures and exploratory analysis. It's built on top of NumPy. Matplotlib is a flexible plotting and visualization library. It's powerful but somewhat cumbersome. You have the option of skipping Matplotlib for now and using Seaborn to get started (see our Seaborn recommendation below). Scikit-Learn is the premier general-purpose machine learning library in Python. It has many popular algorithms and modules for pre-processing, cross-validation, and much more. Seaborn makes it much easier to plot common data visualizations. It's built on top of Matplotlib and offers a more pleasant high-level wrapper. By now, you'll have a basic understanding of programming and a working knowledge of essential libraries. This actually covers most of the Python you'll need to get started with data science. At this point, some students will feel a bit overwhelmed. That's OK, and it's perfectly normal. If you were to take the slow and traditional bottom-up approach, you might feel less overwhelmed, but it would have taken you 10 times as long to get here. Now the key is to dive in immediately and start gluing everything together. Again, our goal up to here has been to just learn enough to get started. Next, it's time to solidify your knowledge through plenty of practice and projects. You have several options. The first option is to participate on Kaggle, a site that hosts data science competitions. The main advantage of Kaggle is that every project is self-contained. You're given the dataset, a goal, and tutorials to get you started. The major disadvantage of competitions is that they're usually not representative of real-world data science. The \"Getting Started\" competitions are way too basic while the standard competitions (i.e. those with prize pools) are usually too tough for beginners. If you're interested in this path, check out our Beginner's Guide to Kaggle. The next option is to structure your own projects and pick datasets that interest you. The main advantage of this approach is that the projects are more representative of real-world data science. You'll likely need to define your own goals, collect data, clean your dataset, engineer features, and so on. The disadvantage of DIY projects is that you'll need to already be familiar with a proper data science workflow. Without one, you could miss important steps or get stuck without knowing how to proceed. If you go with this path, check out our free 7-day crash course on applied machine learning, which covers the key steps in a data science workflow. We also have another article with several\u00a0DIY project ideas. Finally, there are guided end-to-end projects. Proper guided projects should combine the best of both words - they should be representative of real-world data science and allow you to solidify your skills through a carefully planned learning curve. Many data science bootcamps offer this as a main benefit. Bootcamps usually conclude with a \"capstone project\" that allows you to see all the moving pieces together, from start to finish. We've also crafted our own\u00a0Machine Learning Masterclass to solve this exact need. It will provide you over-the-shoulder mentorship for real-world projects while teaching you all of the key concepts in context. The masterclass also includes a comprehensive Python course that gets you up to speed ASAP. In fact, many successful students have enrolled without any prior programming experience. Learn more about it here. [\u2026] In terms of programming languages \u2013 Python is pretty powerful language with integration to all technologies in the CDH stack. For a basic course check out the link below \u2013 Python For Data Science [\u2026] * Denotes Required Field Copyright \u00a9 2017 \u00b7 EliteDataScience.com \u00b7 All Rights Reserved", "links": ["https://elitedatascience.com/", "https://elitedatascience.com/", "https://elitedatascience.com/blog", "https://elitedatascience.com/machine-learning-masterclass", "https://elitedatascience.com/about", "https://elitedatascience.com/about", "https://elitedatascience.com/guest-post-submissions", "https://elitedatascience.com/author/slice2o", "https://elitedatascience.com/learn-python-for-data-science#comments", "https://www.facebook.com/sharer.php?u=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://plus.google.com/share?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://www.linkedin.com/shareArticle?trk=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://twitter.com/intent/tweet?text=How%20to%20Learn%20Python%20for%20Data%20Science%20%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "https://www.forbes.com/sites/jeffkauflin/2017/01/08/the-10-technical-skills-with-explosive-growth-in-job-demand/#6fa6bf344f5c", "http://www.tiobe.com/tiobe-index/", "https://stackoverflow.com/questions/tagged/python?sort=votes&pageSize=15", "https://www.continuum.io/downloads", "https://automatetheboringstuff.com/", "https://automatetheboringstuff.com/", "https://codefights.com/", "http://www.pythonchallenge.com/index.php", "http://www.practicepython.org/", "http://interactivepython.org/runestone/static/thinkcspy/index.html", "http://jupyter.org/", "https://www.youtube.com/watch?v=-MyjG00la2k", "https://docs.scipy.org/doc/numpy/user/", "https://docs.scipy.org/doc/numpy-dev/user/quickstart.html", "https://pandas.pydata.org/pandas-docs/stable/", "https://pandas.pydata.org/pandas-docs/stable/10min.html", "https://matplotlib.org/contents.html", "https://matplotlib.org/users/pyplot_tutorial.html", "http://scikit-learn.org/stable/documentation.html", "http://elitedatascience.com/python-machine-learning-tutorial-scikit-learn", "https://seaborn.pydata.org/", "http://elitedatascience.com/python-seaborn-tutorial", "http://elitedatascience.com/beginner-kaggle", "http://elitedatascience.com/", "http://elitedatascience.com/machine-learning-projects-for-beginners", "http://elitedatascience.com/machine-learning-masterclass", "http://elitedatascience.com/machine-learning-masterclass", "https://www.facebook.com/sharer.php?u=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://plus.google.com/share?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://www.linkedin.com/shareArticle?trk=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://twitter.com/intent/tweet?text=How%20to%20Learn%20Python%20for%20Data%20Science%20%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "https://datascienceandmore.wordpress.com/2017/03/04/getting-the-basics-down/", "https://elitedatascience.com/learn-machine-learning", "https://elitedatascience.com/learn-python-for-data-science", "https://elitedatascience.com/feature-engineering-best-practices", "https://elitedatascience.com/beginner-kaggle", "https://elitedatascience.com/imbalanced-classes", "https://elitedatascience.com/beginner-mistakes", "https://elitedatascience.com/bias-variance-tradeoff", "https://elitedatascience.com/data-science-resources", "https://elitedatascience.com/guest-post-submissions", "https://elitedatascience.com/", "https://elitedatascience.com/terms-of-service", "https://elitedatascience.com/privacy-policy", "https://elitedatascience.com/about", "https://www.linkedin.com/shareArticle?trk=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science", "http://www.facebook.com/sharer.php?u=https://elitedatascience.com/learn-python-for-data-science", "https://plus.google.com/share?text=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science", "https://twitter.com/intent/tweet?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "http://service.weibo.com/share/share.php?url=https://elitedatascience.com/learn-python-for-data-science&title=How to Learn Python for Data Science (Updated)", "https://getpocket.com/save?title=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science"]}, "187": {"url": "https://stats.stackexchange.com/questions/297380/why-not-just-dump-the-neural-networks-and-deep-learning", "title": "", "text": "Fundamental problem with deep learning and neural networks in general. The solutions that fit training data are infinite. We don't have precise mathematical equation that is satisfied by only a single one and that we can say generalizes best. Simply speaking we don't know which generalizes best. Optimizing weights is not a convex problem, so we never know we end up with a global or a local minimum. So why not just dump the neural networks and instead search for a better ML model? Something that we understand, and something that is consistent with a set of mathematical equations? Linear and SVM do not have this mathematical drawbacks and are fully consistent with a a set of mathematical equations. Why not just think on same lines (need not be linear though) and come up with a new ML model better than Linear and SVM and neural networks and deep learning? Not being able to know what solution generalizes best is an issue, but it shouldn't deter us from otherwise using a good solution. Humans themselves often do not known what generalizes best (consider, for example, competing unifying theories of physics), but that doesn't cause us too many problems. It has been shown that it is extremely rare for training to fail because of local minimums. Most of the local minimums in a deep neural network are close in value to the global minimum, so this is not an issue. source But the broader answer is that you can talk all day about nonconvexity and model selection, and people will still use neural networks simply because they work better than anything else (at least on things like image classification).  Of course there are also people arguing that we shouldn't get too focused on CNNs like the community was focused on SVMs a few decades ago, and instead keep looking for the next big thing. In particular, I think I remember Hinton regretting the effectiveness of CNNs as something which might hinder research. related post As the comments to your question point out, there are a lot of people working on finding something better. I would though like to answer this question by expanding the comment left by @josh All models are wrong but some are useful (Wiki) The above statement is a general truth used to describe the nature of statistical models. Using data that we have available, we can create models that let us do useful things such as approximate a predicted value. Take for example Linear Regression  Using a number of observations, we can fit a model to give us an approximate value for a dependent variable given any value(s) for the independent variable(s).  Burnham, K. P.; Anderson, D. R. (2002), Model Selection and Multimodel > Inference: A Practical Information-Theoretic Approach (2nd ed.): \"A model is a simplification or approximation of reality and hence   will not reflect all of reality. ... Box noted that \u201call models are   wrong, but some are useful.\u201d While a model can never be \u201ctruth,\u201d a   model might be ranked from very useful, to useful, to somewhat useful   to, finally, essentially useless.\" Deviations from our model (as can be seen in the image above) appear random, some observations are below the line and some are above, but our regression line shows a general correlation. Whilst deviations in our model appear random, in realistic scenarios there will be other factors at play which cause this deviation. For example, imagine watching cars as they drove through a junction where they must turn either left or right to continue, the cars turn in no particular pattern. Whilst we could say that the direction the cars turn is completely random, does every driver reach the junction and at that point make a random decision of which way to turn? In reality they are probably heading somewhere specific for a specific reason, and without attempting to stop each car to ask them about their reasoning, we can only describe their actions as random.  Where we are able to fit a model with minimal deviation, how certain can we be that an unknown, unnoticed or immeasurable variable wont at some point throw our model? Does the flap of a butterfly\u2019s wings in Brazil set off a tornado in Texas? The problem with using the Linear and SVN models you mention alone is that we are somewhat required to manually observe our variables and how they each affect each other. We then need to decide what variables are important and write a task-specific algorithm. This can be straight forward if we only have a few variables, but what if we had thousands? What if we wanted to create a generalised image recognition model, could this realistically be achieved with this approach?  Deep Learning and Artificial Neural Networks (ANNs) can help us create useful models for huge data sets containing huge amounts of variables (e.g. image libraries). As you mention, there's an incomprehensible number of solutions which could fit the data using ANNs, but is this number really any different to the amount of solutions we would need to develop ourselves through trial and error? The application of ANNs do much of the work for us, we can specify our inputs and our desired outputs (and tweak them later to make improvements) and leave it up to the ANN to figure out the solution. This is why ANNs are often described as \"black boxes\". From a given input they output an approximation, however (in general terms) these approximations don't include details on how they were approximated. And so it really comes down to what problem you are trying to solve, as the problem will dictate what model approach is more useful. Models are not absolutely accurate and so there is always an element of being 'wrong', however the more accurate your results the more useful they are. Having more detail in the results on how the approximation was made may also be useful, depending on the problem it may even be more useful than increased accuracy. If for example you are calculating a persons credit score, using regression and SVMs provides calculations that can be better explored. Being able to both tweak the model directly and explain to customers the effect separate independent variables have on their overall score is very useful. An ANN may aid in processing larger amounts of variables to achieve a more accurate score, but would this accuracy be more useful? I guess for some problem we care less for the mathematical rigor and simplicity but more for its utility, current status is neural network is better in performing certain task like pattern recognition in image processing.  The global minimum may as well as be useless, so we don't really care if we find it or not. The reason is that, for deep networks, not only the time to find it becomes exponentially longer as the network size increases, but also the global minimum often corresponds to overfitting the training set. Thus the generalization ability of the DNN (which is what we really care about) would suffer. Also, often we prefer flatter minima corresponding to a higher value of the loss function, than sharper minima corresponding to a lower value of the loss function, because the second one will deal very badly with uncertainty in the inputs. This is becoming increasingly clear with the development of Bayesian Deep Learning. Robust Optimization beats Determinist Optimization very often, when applied to real world problems where uncertainty is important. Finally, it's a fact that DNNs just kick the ass of methods such as XGBoost at image classification and NLP. A company which must make a profit out of image classification will correctly select them as modelsto be deployed in production (and invest a significant amount of money on feature engineering, data pipeline, etc. but I digress). This doesn't mean that they dominate all the ML environment: for example, they do worse than XGBoost on structured data (see the last winners of Kaggle competitions) and they seem to not still do as well as particle filters on time series modelling. However, some very recent innovations on RNNs may modify this situation. There are excellent answers, mostly weighing in with the usefulness of DL and ANNs. But I would like to object the OP in a more fundamental way, since the question already takes for granted the mathematical inconsistency of neural networks. First of all, there is a mathematical theory behind (most models of) Neural Networks. You could likewise argue that linear regression does not generalize, unless the underlying model is... well, linear. In neural algorithms, a model is assumed (even if not explicitly) and the fitting error is computed. The fact that algorithms are modified with various heuristics does not void the original mathematical support. BTW, local optimization is also a mathematically consistent, let alone useful, theory. Along this line, if Neural Networks just constitute one class of methods within the whole toolbox of scientists, which is the line that separates Neural Networks from the rest of techniques? In fact, SVMs were once considered a class of NNs and they still appear in the same books. On the other hand, NNs could be regarded as a (nonlinear) regression technique, maybe with some simplification. I agree with the OP that we must search better, well founded, efficient algorithms, regardless you label them as NNs or not. Sign up using Google Sign up using Facebook Sign up using Email and Password   By posting your answer, you agree to the privacy policy and terms of service. asked 3 days ago viewed  7,704 times  active yesterday                                   site design / logo \u00a9 2017 Stack Exchange Inc; user contributions licensed under cc by-sa 3.0                                  with attribution required.                                  rev 2017.8.11.26777", "links": ["https://stats.stackexchange.com", "https://chat.stackexchange.com?tab=site&host=stats.stackexchange.com", "https://stats.stackexchange.com", "https://stats.meta.stackexchange.com", "https://stats.stackexchange.com/users/signup?ssrc=site_switcher&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.stackexchange.com/users/login?ssrc=site_switcher&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stackexchange.com/sites", "https://stackoverflow.blog", "https://stackexchange.com", "https://stats.stackexchange.com/users/signup?ssrc=head&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.stackexchange.com/users/login?ssrc=head&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.meta.stackexchange.com", "https://stackoverflow.com/company/about", "https://www.stackoverflowbusiness.com/?ref=topbar_help", "https://stats.stackexchange.com", "https://arxiv.org/pdf/1406.2572.pdf", "https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/clyohpf/?context=3", "https://arxiv.org/pdf/1406.2572.pdf", "https://arxiv.org/pdf/1412.0233.pdf", "https://en.wikipedia.org/wiki/All_models_are_wrong", "https://en.wikipedia.org/wiki/Butterfly_effect", "https://stats.stackexchange.com/questions/93705/meaning-of-a-neural-network-as-a-black-box", "https://stackexchange.com/legal/privacy-policy", "https://stackexchange.com/legal/terms-of-service", "http://stats.stackexchange.com/election", "http://stats.stackexchange.com/election", "https://stackexchange.com/questions?tab=hot", "https://ell.stackexchange.com/questions/138943/jokes-played-or-cracked", "https://security.stackexchange.com/questions/167412/isnt-the-ubuntus-system-prompt-for-my-password-a-bit-unsafe", "https://math.stackexchange.com/questions/2392309/closed-circle-as-a-metric-space", "https://worldbuilding.stackexchange.com/questions/88816/creature-that-swims-in-the-solid-ground", "https://interpersonal.stackexchange.com/questions/1539/is-it-rude-to-ask-tourists-where-they-are-from", "https://english.stackexchange.com/questions/405054/is-there-an-english-equivalent-for-the-persian-proverb-to-play-with-tail-of-lio", "https://codegolf.stackexchange.com/questions/138883/fractal-cathedral", "https://rpg.stackexchange.com/questions/105275/can-blessings-of-knowledge-be-applied-twice-to-different-skills", "https://superuser.com/questions/1239044/can-i-write-200-mb-150-min-music-to-a-700-mb-80-min-cd", "https://movies.stackexchange.com/questions/78712/do-penn-and-teller-learn-how-a-trick-works-after-the-show", "https://electronics.stackexchange.com/questions/323757/audible-difference-between-audio-grade-capacitors-in-the-same-range", "https://math.stackexchange.com/questions/2392723/how-important-are-the-role-of-asymptotes-in-a-hyperbola", "https://travel.stackexchange.com/questions/99713/in-germany-what-are-you-supposed-to-do-if-your-train-station-doesnt-have-any-w", "https://tex.stackexchange.com/questions/386053/how-to-check-software-generated-bibliographic-entries-for-errors-and-other-mista", "https://unix.stackexchange.com/questions/385339/how-does-curl-protect-a-password-from-appearing-in-ps-output", "https://workplace.stackexchange.com/questions/96824/how-to-get-a-raise-i-am-denied-to-because-of-sick-leaves", "https://academia.stackexchange.com/questions/94443/are-course-grade-distributions-supposed-to-be-bell-shaped", "https://travel.stackexchange.com/questions/100105/what-power-supply-is-needed-to-use-a-cpap-machine-in-the-eu", "https://tex.stackexchange.com/questions/386224/how-to-letter-space-german-abbreviations-automatically", "https://puzzling.stackexchange.com/questions/54195/measure-22-minutes-with-7-and-13-minute-hourglasses", "https://tex.stackexchange.com/questions/386239/tikz-fractal-uniform-cantor-set", "https://retrocomputing.stackexchange.com/questions/4481/what-is-the-relative-code-density-of-8-bit-microprocessors", "https://codegolf.stackexchange.com/questions/138887/implement-true-string-addition", "https://stackoverflow.com/questions/45629176/why-do-all-the-c-files-written-by-my-lecturer-start-with-a", "https://chat.stackexchange.com?tab=site&host=stats.stackexchange.com", "https://stats.meta.stackexchange.com", "https://stackoverflow.com/company/about", "https://stackoverflow.com", "https://www.stackoverflowbusiness.com/?utm_source=so-footer&utm_medium=referral&utm_campaign=brand-activation", "https://stackoverflow.com/jobs", "https://stackoverflow.com/company/about", "https://stackoverflow.com/company/press", "https://stackexchange.com/legal", "https://stackexchange.com/legal/privacy-policy", "https://stackexchange.com", "https://stackexchange.com/sites#technology", "https://stackexchange.com/sites#lifearts", "https://stackexchange.com/sites#culturerecreation", "https://stackexchange.com/sites#science", "https://api.stackexchange.com", "https://data.stackexchange.com", "https://stackoverflow.blog?blb=1", "https://www.facebook.com/officialstackoverflow/", "https://twitter.com/stackoverflow", "https://linkedin.com/company/stack-overflow", "https://creativecommons.org/licenses/by-sa/3.0/", "https://stackoverflow.blog/2009/06/25/attribution-required/"]}, "188": {"url": "https://blog.godatadriven.com/practical-airflow-tutorial", "title": "", "text": "Airflow is a scheduler for workflows such as data pipelines, similar to Luigi and Oozie. It's written in Python and we at GoDataDriven have been contributing to it in the last few months. This tutorial is loosely based on the Airflow tutorial in the official documentation. It will walk you through the basics of setting up Airflow and creating an Airflow workflow, and it will give you some practical tips. A (possibly) more up-to-date version of this blog can be found in my git repo. Setting up a basic configuration of Airflow is pretty straightforward. After installing the Python package, we'll need a database to store some data and start the core Airflow services. You can skip this section if Airflow is already set up. Make sure that you can run airflow commands, know where to put your DAGs and have access to the web UI. Airflow is installable with pip via a simple pip install apache-airflow. Either use a separate Python virtual environment or install it in your default python environment. To use the conda virtual environment as defined in environment.yml from my git repo: You should now have an (almost) working Airflow installation. Alternatively, install Airflow yourself by running: Airflow used to be packaged as airflow but is packaged as apache-airflow since version 1.8.1. Make sure that you install any extra packages with the right Python package: e.g. use pip install apache-airflow[dask] if you've installed apache-airflow and do not use pip install airflow[dask]. Leaving out the prefix apache- will install an old version of Airflow next to your current version, leading to a world of hurt. You may run into problems if you don't have the right binaries or Python packages installed for certain backends or operators. When specifying support for e.g. PostgreSQL when installing extra Airflow packages, make sure the database is installed; do a brew install postgresql or apt-get install postgresql before the pip install apache-airflow[postgres]. Similarly, when running into HiveOperator errors, do a pip install apache-airflow[hive] and make sure you can use Hive. Before you can use Airflow you have to initialize its database. The database contains information about historical & running workflows, connections to external data sources,  user management, etc. Once the database is set up, Airflow's UI can be accessed by running a web server and workflows can be started. The default database is a SQLite database, which is fine for this tutorial. In a production setting you'll probably be using something like MySQL or PostgreSQL. You'll probably want to back it up as this database stores the state of everything related to Airflow. Airflow will use the directory set in the environment variable AIRFLOW_HOME to store its configuration and our SQlite database. This directory will be used after your first Airflow command. If you don't set the environment variable AIRFLOW_HOME, Airflow will create the directory ~/airflow/ to put its files in. Set environment variable AIRFLOW_HOME to e.g. your current directory $(pwd): or any other suitable directory. Next, initialize the database: Now start the web server and go to localhost:8080 to check out the UI: It should look something like this:  With the web server running workflows can be started from a new terminal window. Open a new terminal, activate the virtual environment and set the environment variable AIRFLOW_HOME for this terminal as well: Make sure that you're an in the same directory as before when using $(pwd). Run a supplied example: And check in the web UI that it has run by going to Browse -> Task Instances. This concludes all the setting up that you need for this tutorial. We'll create a workflow by specifying actions as a Directed Acyclic Graph (DAG) in Python. The tasks of a workflow make up a Graph; the graph is Directed because the tasks are ordered; and we don't want to get stuck in an eternal loop so the graph also has to be Acyclic. The figure below shows an example of a DAG:  The DAG of this tutorial is a bit easier. It will consist of the following tasks: and we'll plan daily execution of this workflow. Go to the folder that you've designated to be your AIRFLOW_HOME and find the DAGs folder located in subfolder dags/ (if you cannot find, check the setting dags_folder in $AIRFLOW_HOME/airflow.cfg). Create a Python file with the name airflow_tutorial.py that will contain your DAG. Your workflow will automatically be picked up and scheduled to run. First we'll configure settings that are shared by all our tasks. Settings for tasks can be passed as arguments when creating them, but we can also pass a dictionary with default values to the DAG. This allows us to share default arguments for all the tasks in our DAG is the best place to set e.g. the owner and start date of our DAG. Add the following import and dictionary to airflow_tutorial.py to specify the owner, start time, and retry settings that are shared by our tasks: These settings tell Airflow that this workflow is owned by 'me', that the workflow is valid since June 1st of 2017, it should not send emails and it is allowed to retry the workflow once if it fails with a delay of 5 minutes. Other common default arguments are email settings on failure and the end time. We'll now create a DAG object that will contain our tasks. Name it airflow_tutorial_v01 and pass default_args: With schedule_interval='0 * * * *' we've specified a run at every hour 0; the DAG will run each day at 00:00. See crontab.guru for help deciphering cron schedule expressions. Alternatively, you can use strings like '@daily' and '@hourly'. We've used a context manager to create a DAG (new since 1.8). All the tasks for the DAG should be indented to indicate that they are part of this DAG. Without this context manager you'd have to set the dag parameter for each of your tasks. Airflow will generate DAG runs from the start_date with the specified schedule_interval. Once a DAG is active, Airflow continuously checks in the database if all the DAG runs have successfully ran since the start_date. Any missing DAG runs are automatically scheduled. When you initialize on 2016-01-04 a DAG with a start_date at 2016-01-01 and a daily schedule_interval, Airflow will schedule DAG runs for all the days between 2016-01-01 and 2016-01-04. A run starts after the time for the run has passed. The time for which the workflow runs is called the execution_date. The daily workflow for 2016-06-02 runs after 2016-06-02 23:59 and the hourly workflow for 2016-07-03 01:00 starts after 2016-07-03 01:59. From the ETL viewpoint this makes sense: you can only process the daily data for a day after it has passed. This can, however, ask for some juggling with date for other workflows. For Machine Learning models you may want to use all the data up to a given date, you'll have to add the schedule_interval to your execution_date somewhere in the workflow logic. Because Airflow saves all the (scheduled) DAG runs in its database, you should not change the start_date and schedule_interval of a DAG. Instead, up the version number of the DAG (e.g. airflow_tutorial_v02) and avoid running unnecessary tasks by using the web interface or command line tools Timezones and especially daylight savings can mean trouble when scheduling things, so keep your Airflow machine in UTC. You don't want to skip an hour because daylight savings kicks in (or out). Tasks are represented by operators that either perform an action, transfer data, or sense if something has been done. Examples of actions are running a bash script or calling a Python function; of transfers are copying tables between databases or uploading a file; and of sensors are checking if a file exists or data has been added to a database. We'll create a workflow consisting of three tasks: we'll print 'hello', wait for 10 seconds and finally print 'world'.  The first two are done with the BashOperator and the latter with the PythonOperator. Give each operator an unique task ID and something to do: Note how we can pass bash commands in the BashOperator and that the PythonOperator asks for a Python function that can be called. Dependencies in tasks are added by setting other actions as upstream (or downstream).  Link the operations in a chain so that sleep will be run after print_hello and is followed by print_world; print_hello -> sleep -> print_world: After rearranging the code your final DAG should look something like: First check that DAG file contains valid Python code by executing the file with Python: You can manually test a single task for a given execution_date with airflow test: This runs the task locally as if it was for 2017-07-01, ignoring other tasks and without communicating to the database. Now that you're confident that your dag works, turn on the DAG in the web UI and sit back while Airflow starts backfilling the dag runs! You now know the basics of setting up Airflow, creating a DAG and turning it on; time to go deeper!  Address: Wibautstraat 202, 1091 GS Amsterdam, The Netherlands           Phone: +31 (0)35 672 9069           Email: signal@godatadriven.com", "links": ["http://godatadriven.com/contact", "http://godatadriven.com/careers", "http://blog.godatadriven.com", "http://godatadriven.com/business-consultancy", "http://godatadriven.com/big-data-infrastructure", "http://godatadriven.com/predictive-modelling", "http://godatadriven.com/products-overview", "http://godatadriven.com/data-discovery", "http://godatadriven.com/data-science-audit", "http://godatadriven.com/data-science-suite", "http://godatadriven.com/data-science-box", "http://godatadriven.com/divolte", "http://godatadriven.com/customers", "http://godatadriven.com/casestudy-airfranceklm", "http://godatadriven.com/casestudy-bakkersland", "http://godatadriven.com/casestudy-bol", "http://godatadriven.com/casestudy-eneco", "http://godatadriven.com/casestudy-eretail", "http://godatadriven.com/casestudy-ing", "http://godatadriven.com/casestudy-nuon", "http://godatadriven.com/casestudy-npo", "http://godatadriven.com/casestudy-schipholgroup", "http://godatadriven.com/casestudy-transavia", "http://godatadriven.com/technology-overview", "http://godatadriven.com/databricks", "http://godatadriven.com/hadoop", "http://godatadriven.com/python", "http://godatadriven.com/r", "http://godatadriven.com/spark", "http://godatadriven.com/training-overview", "http://godatadriven.com/training-schedule", "http://godatadriven.com/bigdata-training", "http://godatadriven.com/datascience-training", "http://godatadriven.com/data-science-accelerator-program", "http://godatadriven.com/meetups", "http://godatadriven.com/events", "http://blog.godatadriven.com", "http://godatadriven.com/our-story", "http://godatadriven.com/team", "http://godatadriven.com/careers", "http://godatadriven.com/media-resources", "https://blog.godatadriven.com", "http://godatadriven.com/contact", "http://godatadriven.com", "http://blog.godatadriven.com", "https://airflow.incubator.apache.org/", "https://github.com/spotify/luigi", "https://oozie.apache.org/", "https://blog.godatadriven.com/open-source-201702", "https://blog.godatadriven.com/open-source-201703", "https://blog.godatadriven.com/open-source-201704", "https://blog.godatadriven.com/open-source-201705", "https://blog.godatadriven.com/open-source-201707", "https://pythonhosted.org/airflow/tutorial.html", "https://github.com/hgrif/airflow-tutorial", "https://github.com/hgrif/airflow-tutorial", "http://conda.pydata.org/miniconda.html", "http://localhost:8080/", "https://airflow.incubator.apache.org/configuration.html", "https://airflow.incubator.apache.org/security.html", "https://github.com/apache/incubator-airflow/tree/master/scripts", "https://crontab.guru/#0_*_*_*_*", "https://jeffknupp.com/blog/2016/03/07/python-with-context-managers/", "https://airflow.incubator.apache.org/configuration.html#connections", "https://airflow.incubator.apache.org/ui.html#variable-view", "https://airflow.incubator.apache.org/tutorial.html#templating-with-jinja", "https://github.com/apache/incubator-airflow/blob/master/airflow/example_dags/example_bash_operator.py", "https://airflow.incubator.apache.org/code.html#airflow.operators.PythonOperator", "https://pythonhosted.org/airflow/tutorial.html", "https://gtoonstra.github.io/etl-with-airflow/", "https://medium.com/handy-tech/airflow-tips-tricks-and-pitfalls-9ba53fba14eb", "http://godatadriven.com/players/henk-griffioen", "http://blog.godatadriven.com/practical-airflow-tutorial", "http://blog.godatadriven.com/open-source-201707", "http://blog.godatadriven.com/cd-python-eggs-vsts", "http://blog.godatadriven.com/ldappy", "http://blog.godatadriven.com/open-source-201706", "http://blog.godatadriven.com/vendor-free-ds", "https://twitter.com/GoDataDriven", "http://godatadriven.com/business-consultancy", "http://godatadriven.com/big-data-infrastructure", "http://godatadriven.com/predictive-modelling", "http://godatadriven.com/products-overview", "http://godatadriven.com/data-discovery", "http://godatadriven.com/data-science-audit", "http://godatadriven.com/data-science-suite", "http://godatadriven.com/data-science-box", "http://godatadriven.com/divolte", "http://godatadriven.com/customers", "http://godatadriven.com/casestudy-airfranceklm", "http://godatadriven.com/casestudy-bakkersland", "http://godatadriven.com/casestudy-bol", "http://godatadriven.com/casestudy-eneco", "http://godatadriven.com/casestudy-eretail", "http://godatadriven.com/casestudy-ing", "http://godatadriven.com/casestudy-nuon", "http://godatadriven.com/casestudy-npo", "http://godatadriven.com/casestudy-schipholgroup", "http://godatadriven.com/casestudy-transavia", "http://godatadriven.com/technology-overview", "http://godatadriven.com/hadoop", "http://godatadriven.com/python", "http://godatadriven.com/r", "http://godatadriven.com/spark", "http://godatadriven.com/training-overview", "http://godatadriven.com/training-schedule", "http://godatadriven.com/bigdata-training", "http://godatadriven.com/datascience-training", "http://godatadriven.com/data-science-accelerator-program", "http://godatadriven.com/events", "http://godatadriven.com/meetups", "http://blog.godatadriven.com", "http://godatadriven.com/our-story", "http://godatadriven.com/team", "http://godatadriven.com/careers", "http://godatadriven.com/media-resources", "http://godatadriven.com/news", "http://godatadriven.com/contact", "https://cdn.xebia.com/documents/Cookies+Policy+Xebia+20170130.pdf"]}, "189": {"url": "https://concepttoclinic.drivendata.org/", "title": "", "text": "Lung Cancer Early Detection Challenge  We\u2019re calling on a global community of data scientists, engineers, designers, and researchers to build an open source software application that brings advances from machine learning into the clinic. We\u2019re not just optimizing an algorithm for a single metric\u2014we\u2019re collaborating to build tools which put AI in the hands of clinicians. In addition to pushing forward the cutting-edge of open clinical software, top contributors will be eligible for a share of $100,000 in monetary prizes generously provided by the Bonnie J. Addario Lung Cancer Foundation.          Contribute now by grabbing an issue from the project's GitHub repository and submitting a PR!        Lung cancer causes more deaths each year than any other cancer in the US, and early detection makes a big difference. Recent advances in machine learning \u2013 including open source algorithms from the 2017 Data Science Bowl \u2013 help find and interpret early signs of cancer. Using imagery from CT scans, this application will build on cutting-edge algorithms in machine learning to help clinicians identify and interpret lung nodules. Your guide to contributing throughout the challenge. All you need to start contributing is a passion for building great software and a github account.  Contributions of all sizes can earn points as they add value to the project, by building on pre-designated issues, building out new content, and building up the community. $100,000 in cash, as well as other in-kind prizes, will be available for top contributors filling key roles throughout the challenge. A live leaderboard keeps track of points earned throughout the challenge by top contributors filling key roles. Meet the amazing group of of technical, clinical, and domain experts that are helping to guide the end-to-end design of this challenge. The Addario Lung Cancer Foundation (ALCF) has set the audacious goal of making lung cancer a chronically managed disease by 2023.  Ultimately, the goal of this project is to produce a tool that draws on machine intelligence, is useful to clinicians, and makes a lasting difference in the lives of patients. We hope you will find a way to contribute!  Lung Cancer Early Detection Challenge", "links": ["https://www.drivendata.org/termsofuse/", "https://www.drivendata.org/copyrightpolicy/", "https://www.drivendata.org/privacypolicy/", "https://www.drivendata.org/", "http://www.drivendata.co/"]}, "190": {"url": "http://appsilondatascience.com/blog/rstats/2017/08/09/shiny-collections.html", "title": "shiny.collections,  Google Docs-like live collaboration in ShinyAppsilon Data Science Blog", "text": "A few weeks ago, our CTO Marek Rogala gave an engaging talk at the User Conference in Brussels. Despite being one of the last talks, he drew a crowd that filled the room and had significant viewership online. Marek\u2019s talk was entitled shiny.collections: Google Docs-like live collaboration in Shiny. In short, he went over a package we built that allows for persistence and collaboration without losing reactivity in Shiny. Users can enter information and have continuity, knowing that if they exit and return to the app, their inputs will still be there. Let\u2019s go over his talk. Recently, Shiny users have demanded more from their applications. The advent of cloud applications, especially ones like Google Docs, have accustomed users to collaboration. They expect their Shiny apps to be more than just an interactive tool for data visualisation, but rather, a production ready application that works like any other tool. Their data has to be saved automatically. Interactive collaboration is a must in some use cases. And the application must be delivered as fast as possible. You need to use a reactive database such as rethinkDB, Firebase, or mongoDB to achieve this. mongoDB isn\u2019t really a reactive database, but there are ways where it can be used as one. We are going to use rethinkDB in our case. Also, we are going to use rethinker, which is a rethinkDB driver for R. But using rethinker is not the most ideal solution, as callback do not work very well with Shiny and it is quite painful to configure to our needs. To get around this, we created our own package built upon rethinker called shiny.collections.  It lets you easily connect to your shiny app and takes care of all of the trickiness involved. During his talk, Marek showed a live demo that involved creating a chat application. Check out how to do this yourself. I reccomend watching Marek first, as he has some valuable commentary you won\u2019t find in the blog post. This example is just a simple use case of what you can achieve with shiny.collections. We\u2019ve made it a priority to make it convenient and easily integrable with other powerful tools, including DT, leaflet or rhandsontable. Our goals for the future of this package include getting it on cran and diversifying the functionality of the API. Take a look at the our package and contribute. Get in touch with us for all of you other data science needs as well.  Published 09 Aug 2017", "links": ["https://channel9.msdn.com/Events/useR-international-R-User-conferences/useR-International-R-User-2017-Conference/shinycollections-Google-Docs-like-live-collaboration-in-Shiny#comments", "https://cran.r-project.org/web/packages/rethinker/index.html", "https://appsilon.github.io/shiny.collections/", "http://appsilondatascience.com/blog/rstats/2017/07/02/shiny-chat.html", "http://appsilondatascience.com", "http://disqus.com/?ref_noscript"]}, "191": {"url": "https://blog.statsbot.co/introduction-to-imitation-learning-32334c3b1e7a", "title": "Introduction to Imitation Learning", "text": "This post is the first in the series where we will describe what Imitation Learning is. For today\u2019s article, the Statsbot team asked computer scientist Vitaly Kurin to briefly introduce Imitation Learning and outline the basics of Reinforcement Learning. Living organisms are extremely complex. Even the relatively simple ones such as flies or worms. They are able not only to operate successfully in the real world, but are extremely resilient to changes in conditions. And that\u2019s if we\u2019re not even talking about humans. We can plan ahead, we can alter our plans given new information, and we can collaborate with others and execute our plans more effectively. Science and such projects as CERN or a huge Radio Telescope in Arecibo are perfect examples of human collaboration and the art of planning. Yes, we do a lot of stupid stuff, but let\u2019s not talk about that here and instead focus on the good. Yes, we can now beat the best human in chess or Go, we can get a crazy score in Video Pinball for Atari 2600, we can even already make a pure human broke by challenging him in poker. Can we open a bottle of champagne and celebrate the triumph? I\u2019m afraid not. Yes, machine learning has recently made a significant leap forward. The combination of new Deep Learning ideas with old ones has enabled us to advance in many domains, such as computer vision, speech recognition, and text translation. Reinforcement Learning has also benefited greatly from its marriage with Deep Learning. You\u2019ve definitely heard of Deep Reinforcement Learning success such as achieving superhuman score in Atari 2600 games, solving Go, and making robots learn parkour. Though, we must admit, that operating successfully in the real world is much harder than playing Go or Space Invaders. Many of the tasks are much harder than this. Imagine a kid riding a bicycle in the middle of a crowded city center or a man driving a Porsche 911 on an autobahn at 200 miles per hour. Let\u2019s all admit that we are not there yet. The typical machine learning approach is to train a model from scratch. Give it a million images and some time to figure it out. Give it a week and let it play Space Invaders until it reaches some acceptable score. We, as humans, beg to differ. When a typical human starts to play some game he has never seen, he already has a huge amount of prior information. If he sees a door in Montezuma\u2019s Revenge, he realizes that somewhere there should lie a key and he needs to find it. When he finds the key, he remembers that the closed door is back through the two previous rooms and he returns to open it. When he sees a ladder, he realizes that he can climb it because he has done this hundreds of time already. What if we could somehow transfer human knowledge about the world to an agent? How can we extract all this information? How can we create a model out of it? There is such a way. It\u2019s called Imitation Learning. Imitation Learning is not the only name for leveraging human data for good. Some researchers also call it apprenticeship learning, others refer to it as Learning from Demonstration. From our point of view, there is no substantial difference between all of these titles and we will use Imitation Learning from now on. In order to introduce Imitation Learning, we will need to understand the basics of Reinforcement Learning first. Let\u2019s move on. It\u2019s not hard to get the general idea of the Reinforcement Learning setup. There is some agent and we want this agent to learn some task. Let\u2019s say, we have an Atari 2600 console, the agent has access to the joystick and can see what\u2019s happening on the screen. Let\u2019s say, 60 times per second we give our agent a screenshot of the game and ask him what button he wants to press. If our agent does well, he can see that his score is increasing (positive reinforcement), otherwise we can give him a penalty as a negative reward (negative reinforcement). Gradually, by trial and error, the agent starts to understand that it\u2019s better to avoid some of the actions and do those which bring him a reward. Let\u2019s make it more formal and describe the process stated above mathematically. We can describe the RL framework mentioned above (observe -> act -> get the reward and the next state) as a Markov Decision Process (MDP): where: We also need a definition of a policy function for the next section. Policy is a function, that returns an action given the state: And, actually, our final goal when solving an MDP is to learn such a policy in order to maximize the reward for our agent. Let\u2019s take an example of an MDP. The circles represent the states, arrows with green labels are actions, red labels are the rewards for actions, and the square is the terminal state. The green numeric labels are the transition probabilities. Our student starts in a state with the blue circle. He studies, but this is hard and sometimes boring. He decides to open a Facebook app and once he is there, he can either quit or continue scrolling. He then studies more and more, and finally decides to go to the pub. The state is a smaller filled circle, since now there is an element of randomness, based on the amount of knowledge the student forgets after visiting the pub. He can then either study more and pass the exam (+10 in reward), or he can go to sleep and finish the MDP right now. Since we will use DQN and related ideas in the future, let\u2019s briefly understand what is going on here. The whole approach is built upon approximating the so-called Q function and building the agent\u2019s behavior based on it. The idea of the Q function is the following: it returns you the entire expected discounted reward flow for the particular action and the particular state, given that starting from the next state we will be following our policy \ud835\udfb9. It answers the question: \u201cHow good is to press this button in this state?\u201d The Q function obeys the Bellman equation: And, finally, the Bellman principle of optimality is the following: notwithstanding what happened before, we should always take the action with the highest Q to maximize the reward flow: But how do we get such a Q function, you might ask. Let\u2019s look at an example. Imagine, you want to grab a coffee (+20 in reward) and a chocolate in a vending machine (+10 in reward). Your total reward cannot exceed thirty. Moreover, if you have taken the coffee already, it cannot be higher than 10 (the reward for chocolate) from now on. This is the idea: the Q value for the current step and action is equal to the maximum Q value for the next state (since we behave optimally) + the reward we get for the transition. The value of the quadratic objective function becomes: Q-learning itself is not new. Q-learning which uses neural networks as a function approximators is also not new (e.g. neural fitted-q iteration). A DQN paper was the first to use deep convolutional networks to solve this type of problem and introduced a couple of novelties that make the training process much more stable. First of all, experience replay. The vanilla Q-learning point is to make a step, get the reward and the next state, then update the approximation function parameters based on this transition. The DQN idea is to make the transition and save it in a \u201creplay memory\u201d\u200a\u2014\u200aan array that stores the last 10\u2076 (<insert any large number here>) transitions with the information about the reward, states before and after the transition, and if the event is terminal (game over) or not. Having this experience replay we can randomly sample mini-batches from it and learn more effectively. Another thing that makes the algorithm more stable is that DQN uses two neural networks: the first to compute the Q value for the current state and the second to compute the Q value for the next state. You can see that from the equation with the objective: two different Q functions use \ud835\udfb1 and \ud835\udfb1\u2019, respectively. Each 10,000 steps, the parameters \ud835\udfb1\u2019 are copied from the learned parameters \ud835\udfb1 and this helps a lot in increasing the stability. The problem here with using one function is that when we update the weights, both Q(s,a) and Q(s\u2019,a\u2019) increase and this might lead to oscillations or policy divergence. Using two separate networks adds a delay between an update and computation of the target Q value and reduces such cases. If you have further interest in the phenomena, read the Method section in the DQN Nature paper. Okay, everything described above sounds quite simple. If there is still something you do not understand, please, have a look at David Silver\u2019s lecture where he explains everything perfectly! Knowing all of these, can we build an True AI now? I\u2019m sorry, but we can\u2019t. There are several problems that hinder us from building an agent that will beat ByuN at StarCraft II, bring an autonomous car to the market, or give you an opportunity to buy your grandma a robot that will do the dishes for her after lunch. One of these problems is that the rewards our agent gets might be very sparse in time. Let\u2019s say, you play chess. If you lose, how do you know when you made a catastrophic move? Moreover, it\u2019s highly possible that there was not a catastrophic move, but several average ones. Another problem that is closely connected to the previous one, is the sample-efficiency problem. Or, more honestly, sample-inefficiency. Even to master a simple game such as Space Invaders might take a couple of days in-game time. It\u2019s easy to speed up learning in games since we have access to the simulators, but what if we want to learn something in real life? Unfortunately, physics is not there yet and we cannot speed up time. There is an approach that could potentially solve these problems and a bunch of others\u200a\u2014\u200aImitation Learning, as we mentioned at the beginning of this post. As we said, we, humans, rarely learn something without any prior information. Let\u2019s use this data! What should we do? The idea of Imitation Learning is implicitly giving an agent prior information about the world by mimicking human behavior in some sense. Imitation Learning will not only help us solve the sample-inefficiency or computational feasibility problems, it might potentially make the training process safer. We cannot just put an autonomous car in the middle of the street and let it do whatever it wants. We do not want it to kill humans that are around, destroy someone\u2019s property, or the equipment itself. Pretraining it on a human demonstrator\u2019s data might make the training process faster and avoid undesirable situations. Training a model requires some data. Training a Deep Learning model requires even more data. Training a Deep Reinforcement Learning model requires\u2026 Okay, you get the idea. So, this series is only partly describes what we can do with demonstration data. The main point of all of this is to call for human demonstration datasets, because we do not have many, unfortunately, up to this moment. Okay, we should stop here. The key points of this post are: In the next chapter we will write more about Behavior Cloning\u200a\u2014\u200athe simplest approach to an RL problem that can leverage the human demonstration data. Stay tuned!", "links": ["https://blog.statsbot.co?source=logo-lo_2d4c132dda11---cfc9f21a543a", "https://twitter.com/statsbotco", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.statsbot.co%2Fintroduction-to-imitation-learning-32334c3b1e7a", "https://blog.statsbot.co", "https://blog.statsbot.co/analytics/home", "https://blog.statsbot.co/datascience/home", "https://blog.statsbot.co/design/home", "https://blog.statsbot.co/startups/home", "https://blog.statsbot.co/bots/home", "https://blog.statsbot.co/news/home", "https://blog.statsbot.co/statsbot-digest-b0d7372f842a", "http://statsbot.co?utm_source=blog&utm_medium=navigation&utm_campaign=robot_face", "https://blog.statsbot.co/search", "https://blog.statsbot.co/@yobibyte?source=post_header_lockup", "https://blog.statsbot.co/@yobibyte?source=post_header_lockup", "https://atlas.cern/discover/about", "http://statsbot.co?utm_source=blog&utm_medium=article&utm_campaign=imitation_learning", "https://en.wikipedia.org/wiki/CERN", "https://en.wikipedia.org/wiki/Arecibo_Observatory", "https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning/", "https://deepmind.com/research/alphago/", "https://www.theverge.com/tldr/2017/7/10/15946542/deepmind-parkour-agent-reinforcement-learning", "http://www.ceva-dsp.com/ourblog/wp-content/uploads/sites/3/2016/04/AlphaGo-Lee-Se-dol.png", "https://simple.wikipedia.org/wiki/File:Rl_agent.png", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf", "https://link.springer.com/article/10.1007/BF00992698", "https://pdfs.semanticscholar.org/2820/01869bd502c7917db8b32b75593addfbbc68.pdf", "https://link.springer.com/article/10.1007/BF00992699", "http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html?foxtrotcallback=true", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "http://wiki.teamliquid.net/starcraft2/ByuN", "http://toyoutheartist.co.uk/technique/imitation/", "https://mitpress.mit.edu/books/reinforcement-learning", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html", "https://arxiv.org/abs/1312.5602", "https://www.nature.com/nature/journal/v518/n7540/full/nature14236.html", "https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner", "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/google-analytics-audit-checklist-and-tools-fca7df2f2e7a", "https://blog.statsbot.co/google-analytics-audit-checklist-and-tools-fca7df2f2e7a", "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co/tagged/imitation-learning?source=post", "https://blog.statsbot.co/tagged/data-science?source=post", "https://blog.statsbot.co/tagged/machine-learning?source=post", "https://blog.statsbot.co/tagged/ai?source=post", "https://blog.statsbot.co/tagged/reinforcement-learning?source=post", "https://blog.statsbot.co/@yobibyte?source=footer_card", "https://blog.statsbot.co/@yobibyte", "https://github.com/yobibyte/yobiblog", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "192": {"url": "https://axibase.github.io/atsd-use-cases/DataShorts/Dollar-EX/", "title": "", "text": "Data Source: Federal Reserve Economic Data (FRED) Visualizations: ChartLab Structured Query Language: SQL Console from Axibase Download the Community Edition of Axibase Time Series Database here Data is aggregated daily in visualizations, and annually in SQL queries. Favorable dollar exhange rates are shown in green while less favorable exchange rates are shown in red, based on the overall 5-year average exchange rate. Each section is accompanied by a brief analysis to give the data context. Using computed value settings thresholds have been calculated using the underlying data, the upper 25th percentile of a given exchange rate is considered favorable while the lower 25th percentile is considered less than favorable. The data is coded with an ID which is provided in the index as well. Open any ChartLab display below to navigate through time, or change the metric to reflect the desired exchange rate. Europe Asia North America As the amount of debt carried by various EU member countries grew to increasingly worrisome heights, the Euro plunged in 2015  losing ground against the dollar. Further, as oil prices slumped the European Central Bank began to compensate by launching a program of government bond purchases. The combination of these factors and the growth of the dollar post-recession have all contributed to the currently favorable exchange rate enjoyed by the dollar. Query 1.1 Table 1.1 Figure 1.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index During the worst of the European Debt Crisis which saw European Union per capita debt grow at unprecedented rates across the continent, EU investors sought refuge in the stability of the Swiss Franc, backed by the world-renowned Swiss banking system. Further, the Swiss National Bank removed the peg to the Euro unexpectably in 2015 causing a huge surge in 2015, clearly visible in the visualization below as a sudden change from a favorable exchange rate for dollar holders to an all-time low for the observed time period. Query 1.2 Table 1.2 Figure 1.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Largely influenced by the divisive Brexit vote, the value of the Pound has remained relatively low since the referendum in late June of 2016, which aligns perfectly with the Pound\u2019s slight depreciation in value against the dollar on the chart below. Query 1.3 Table 1.3 Average Exchange Rate: 1.50 Figure 1.3   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index After purchasing trillions of Yen worth of Japanese national debt from the private sector three years ago, the Central Bank of Japan has been watching the gradual decrease in value of the Yen against the dollar. Most analysts, see this fall as controlled and predict a bright future for the Yen as the Central Bank can no longer afford to collect more debt than they already have. However, the Yen\u2019s rise much also be managed as the sudden appreciation of the currency could result in an increase in loan defaults because industries which have taken loans at current Yen prices would be paying them back at a much higher rate. Query 2.1 Table 2.1 Figure 2.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index To prevent dramatic internal inflation, the People\u2019s Bank of China closely regulates local, or onshore, trading of the Yuan by fixing the price each day. Although independent officially, the international, or offshore, price of the Yuan usually remains fairly close to its onshore value. Additionally, the PBoC carefully controls the outflow of capital and often hedges their own currency by maintaining positions in international currencies. Query 2.2 Table 2.2 Figure 2.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index The Hong Kong Dollar is officially pegged to the value of the United States Dollar so any change in relative  value is planned. The \u201cMiracle of the Orient\u201d continues to be one of the most dominant financial markets in the world. Query 2.3 Table 2.3 Figure 2.3   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index One of the financial centers of the world, Singapore is home to the second-busiest port in the world and one of the largest oil refining industries worldwide as well. With one of the highest per capita GDP values, Singapore has cemented its place of international importance despite inexplicably loose financial policy whereby the Monetary Authority of Singapore does not regulate their currency\u2019s value by adjusting interest rates, as is common with most central banking systems. Query 2.4 Table 2.4 Figure 2.4   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Relative to the USD, the Canadian dollar has been making a bit of a resurgence in 2017 as United States GDP numbers missed its targets in quarter two. Although dependant on the price of oil due to their large natural reserves, the diverse Canadian economy is one of the few globally-active nations with a significant trade surplus, although that too have been reduced in recent years. Query 3.1 Table 3.1 Figure 3.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Boasting the highest per capita income of Latin America, the Mexican economy has continued to demonstrate healthy growth over the last several decades. North America\u2019s number one producer of automobiles also enjoys a $46 billion trade surplus with the United States, and is the 9th ranked owner of United States government debt. This dependence on America affords Mexico the ability to spur dramatic internal growth that has led to the creation of a booming electronics sector, but at the cost of a  certain amount of reliance on the United States as roughly half of the country\u2019s imports and exports originate or end up  on U.S. soil. Query 3.2 Table 3.2 Figure 3.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index", "links": ["https://github.com/axibase/atsd-use-cases", "https://fred.stlouisfed.org/categories/94", "https://apps.axibase.com", "https://github.com/axibase/atsd/tree/master/api/sql", "https://axibase.com", "https://axibase.com/products/axibase-time-series-database/", "https://github.com/axibase/atsd/blob/master/installation/README.md#installation", "https://axibase.com/products/axibase-time-series-database/visualization/widgets/configuring-the-widgets/", "https://apps.axibse.com", "https://apps.axibase.com/chartlab/424eb6b2/5/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/9/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/6/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/4/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/2/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/3/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/8/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/7/#fullscreen", "https://github.com/axibase/atsd-use-cases", "https://github.com/axibase", "https://pages.github.com"]}, "193": {"url": "http://byteacademy.co/blog/overview-NLG", "title": "", "text": "NLG (Natural Language Generation), a subfield of Artificial Intelligence, is a hot topic in the technology news today. We hear a lot about AI that can soon replace writers and journalists beginning the era of machine creativity. But, what\u2019s all this fuss about? In this article, we unveil what NLG really is and show that it can bring a lot of benefits to businesses and consumers. In a nutshell, NLG is a sub-field of NLP (Natural Language Processing) that studies methods of automatic transformation of structured data into a human-readable text. In practice, there are two major types of NLG applications: template-based NLG and advanced NLG. \u00a0 Template-based NLG is the simplest solution that uses templates with canned text and placeholders to insert data into them. Such systems heavily rely on hard-coded rules, which makes them less flexible than advanced NLG. Since template-based NLG tools have a limited number of templates and require special data representations, they can not be easily reused across different projects and business use cases. \u00a0 Advanced NLG tools are more flexible thanks to the use of supervised and unsupervised Machine Learning (ML). Rather than tying down structured data to the Procrustean bed of templates, advanced NLG uses neural networks that learn morphological, lexical, and grammar patterns from large corpora of written language. Soft probabilistic methods used in the advanced NLG algorithms allow predicting the likelihood of one word appearing after another, and correcting language errors, such as misspellings. ML algorithms used in the advanced NLG are also better in dealing with new words and expressions not included in the original training samples. \u00a0 Modern NLG service providers such as Narrative Science and Automated Insights prefer advanced NLG methods because they allow creating rich data-driven models that produce intelligent insights from data. These algorithms are much more skillful in making right word choices and writing narratives that reflect intentions and business needs of the NLG users. As an added bonus, advanced NLG models can preprocess and analyze data which makes them not just translators of structured data into text, but automatic analysts able to provide actionable insights. \u00a0 Despite the fact that NLG methods have been used since the 1970s, they got a powerful momentum only recently and thanks to the AI/ML revolution. Today, many startups offer cloud-based NLG services to businesses. NLG is also gaining traction in mass media and journalism. Major American newspapers are already experimenting with the automatic storytelling. For example, in 2016 the Washington Post unveiled its automatic storytelling AI named Heliograph AI. Heliograph was used in the coverage of Rio Olympics and the US Presidential election in 2016. \u00a0 Leveraging data mining techniques and ML models the machine reporter can convert structured statistical data, diagrams, graphs, weather forecasts and other data-rich content into excellent descriptive reports that sound if though they were written by the professional reporters. But, isn\u2019t this dangerous for journalism as a profession? Proponents of automatic storytellers say that they actually free up much time for reporters to add analysis and real insights to stories rather than spending countless hours publishing news and descriptive reports[i]. NLG tools may be used in other innovative ways as well: Benefits of NLG, however, go beyond journalism. There is a growing demand for NLG services among major companies. For example, Quill, an NLG system developed by Narrative Science, is used by such companies as Deloitte, Groupon, and Credit Suisse[iv]. These companies opt for NLG solutions for a reason. Growing acceptance of NLG among businesses makes it a promising field to study. If you want to learn more about NLG, \u00a0Byte Academy offers a Natural Language course that covers Natural Language Processing and Natural Language Generation. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0  Your Name (required)    Your Email (required)    \u00a0 \u00a0 References: [i] \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 WashPost PR Blog (August 5, 2016). The Washington Post Experiments With Automated Storytelling to Help Power 2016 Rio Olympics Coverage. WashPost PR Blog. Retrieved from https://www.washingtonpost.com/pr/wp/2016/08/05/the-washington-post-experiments-with-automated-storytelling-to-help-power-2016-rio-olympics-coverage/?utm_term=.bf63b03c4aeb [ii] \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dayan, Zohar (2015). Hearst, USA Today Sports, & Viralnova Partner With Wibbitz For Video Strategy. Wibbitz Blog. Retrieved from http://blog.wibbitz.com/wibbitz-partners-hearst-usa-today-sports-group-and-viralnova-to-expand-video-strategy [iii] \u00a0\u00a0\u00a0\u00a0\u00a0 Keohane, Joe (2017). What News-Writing Bots Mean for the Future of Journalism. Wired. Retrieved from https://www.wired.com/2017/02/robots-wrote-this-story/ [iv] \u00a0\u00a0\u00a0\u00a0 Narrative Science. Turn Your Data Into Better Decisions With Quill. Retrieved from https://narrativescience.com/Platform info.in@byteacademy.co Byte Dev Licensed by the State of New York, New York State Education Department \u00a9 2017 Byte Academy LLC.  All rights reserved.", "links": ["http://byteacademy.co", "http://byteacademy.co/program/", "http://byteacademy.co/courses/", "http://byteacademy.co/data-science-mini-courses/", "http://byteacademy.co/upcoming-classes/", "http://byteacademy.co/students/", "http://byteacademy.co/hiring/", "http://byteacademy.co/events/", "http://byteacademy.co/blog/", "http://byteacademy.co/india/", "https://byteacademy.fluidreview.com", "http://feeds.feedburner.com/ByteAcademy", "http://byteacademy.co/category/data-science/", "http://byteacademy.co/category/career/", "http://byteacademy.co/category/fintech/", "http://byteacademy.co/category/medtech/", "http://byteacademy.co/category/programming-tips/", "http://byteacademy.co/category/startup/", "http://byteacademy.co/category/student-stories/", "http://byteacademy.co/category/blockchain/", "http://byteacademy.co/category/current-events/", "http://byteacademy.co/category/diversity/", "http://byteacademy.co/category/events/", "http://byteacademy.co/category/finovate/", "http://byteacademy.co/category/light-reads/", "http://byteacademy.co/category/millennials/", "http://byteacademy.co/category/quant/", "http://byteacademy.co/2017/", "http://byteacademy.co/2016/", "http://byteacademy.co/2015/", "http://byteacademy.co/category/data-science/", "http://byteacademy.co/blog/artificial-intelligence-future", "https://www.washingtonpost.com/pr/wp/2016/08/05/the-washington-post-experiments-with-automated-storytelling-to-help-power-2016-rio-olympics-coverage/?utm_term=.bf63b03c4aeb", "http://www.reddit.com/submit?url=http://byteacademy.co/blog/overview-NLG", "http://www.tumblr.com/share?v=3&u=http://byteacademy.co/blog/overview-NLG&t=Overview of Natural Language Generation (NLG)", "http://byteacademy.co/blog/artificial-intelligence-future", "http://byteacademy.co/blog/artificial-intelligence-future", "http://byteacademy.co/blog/data-science-podcasts", "http://byteacademy.co/blog/data-science-podcasts", "http://byteacademy.co/blog/data-science-fintech/", "http://byteacademy.co/blog/data-science-fintech/", "http://byteacademy.co/financial-aid/", "http://byteacademy.co/upcoming-classes/", "http://byteacademy.co/contact-us/", "http://byteacademy.co/corporate-training/", "http://byteacademy.co/faq/", "http://byteacademy.co/press-page/", "http://byteacademy.co/careers-at-byte/", "http://byteacademy.co/brand-ambassador/", "https://www.meetup.com/Byte-Academy-Finance-and-Technology-community/", "http://www.bytedev.co", "https://www.facebook.com/byteacademy/", "https://twitter.com/ByteAcademyCo", "https://www.linkedin.com/edu/school?id=171001", "https://github.com/ByteAcademyCo", "https://www.instagram.com/byteacademy/", "https://www.meetup.com/Byte-Academy-Finance-and-Technology-community/", "https://www.quora.com/topic/Byte-Academy", "https://www.youtube.com/channel/UCrMcJALnO748TSK27bZQzSg", "http://byteacademy.co/privacy-policy/"]}, "194": {"url": "http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "title": "", "text": "", "links": ["http://www.getrevue.co/?ref=Revue+Profile", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profileimage&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profiledate&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profilename&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Mm6Kn?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Mm6Kn?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.datasciencecentral.com/profiles/blogs/automated-machine-learning-for-professionals?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lXBy8?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lXBy8?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.nature.com/articles/s41467-017-00181-8?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/d0nAm?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/d0nAm?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/4mO6D?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/4mO6D?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/eK78N?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/eK78N?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://sourcedexter.com/amazing-tensorflow-github-projects/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xejEQ?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xejEQ?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://techblog.appnexus.com/a-keras-multithreaded-dataframe-generator-for-millions-of-image-files-84d3027f6f43?gi=79cece23ee37&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/6nA72?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/6nA72?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://blog.revolutionanalytics.com/2017/08/a-modern-database-interface-for-r.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/5kyK3?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/5kyK3?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://ruder.io/deep-learning-nlp-best-practices/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/7yxNV?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/7yxNV?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://hyperparameter.space/blog/when-not-to-use-deep-learning/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/8A1NP?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/8A1NP?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278?gi=8e0386afa61e&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/1bKwn?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/1bKwn?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://adeshpande3.github.io/How-I-Used-Deep-Learning-to-Train-a-Chatbot-to-Talk-Like-Me?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lX0x4?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lX0x4?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.youtube.com/channel/UC0n76gicaarsN_Y9YShWwhw/playlists?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/qYVyK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/qYVyK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.youtube.com/watch?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest&v=0fhUJT21-bs", "http://rev.vu/Nkeov?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Nkeov?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%239%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "https://www.getrevue.co/?utm_source=Data Science Digest&utm_medium=email&utm_content=footerlink&utm_campaign=Issue", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=forwarded&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%239%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "http://www.getrevue.co/?ref=Revue+Profile"]}, "195": {"url": "http://rpsychologist.com/d3/bayes/", "title": "", "text": "Created by Kristoffer Magnusson The visualization shows a Bayesian two-sample t test, for simplicity the variance is assumed to be known. It illustrates both Bayesian estimation via the posterior distribution for the effect, and Bayesian hypothesis testing via Bayes factor. The frequentist p-value is also shown. The null hypothesis, H0 is that the effect \u03b4 = 0, and the alternative H1: \u03b4 \u2260 0, just like a two-tailed t test. You can use the sliders to vary the observed effect (Cohen's d), sample size (n per group) and the prior on \u03b4. The prior on the effect is a scaled unit-information prior. The black, and red circle on the curves represents the likelihood of 0 under the prior and posterior. Their likelihood ratio is the Savage-Dickey density ratio, which I use here as to compute Bayes factor. The p-value is the traditional p-value for a two-sample t test with known variance (i.e. a Z test).        HDI is the posterior highest density interval, which in this case is analogous a credible interval. And CI is the traditional frequentist confidence interval.  Check out Alexander Etz's blog series \"Understanding Bayes\" for a really good introduction to Bayes factor. Fabian Dablander also wrote a really good post, \"Bayesian statistics: why and how\", which introduces Bayesian inference in general. If you're interesting in an easy way to perform a Bayesian t test check out JASP, or BayesFactor if you use R. Interactive visualization of Cohen's d effect size Interactive visualization of statistical power and significance testing Interactive visualization of Confidence Intervals Have any suggestion? Or found any bugs? Send them to me, my contact info can be found here. Designed and built by Kristoffer Magnusson. Built with D3.js, jStat and Bootstrap.", "links": ["http://rpsychologist.com", "http://rpsychologist.com", "https://twitter.com/krstoffr", "https://se.linkedin.com/pub/kristoffer-magnusson/b5/133/1b6", "https://twitter.com/share", "http://alexanderetz.com/understanding-bayes/", "http://blog.efpsa.org/2015/08/03/bayesian-statistics-why-and-how/", "https://jasp-stats.org", "http://bayesfactorpcl.r-forge.r-project.org", "http://rpsychologist.com/d3/cohend/", "http://rpsychologist.com/d3/NHST/", "http://rpsychologist.com/d3/CI/", "http://rpsychologist.com/tag/d3js.html", "http://rpsychologist.com/about", "http://rpsychologist.com", "http://d3js.org/", "http://jstat.org/", "http://getbootstrap.com/", "http://twitter.com/krstoffr", "https://github.com/rpsychologist"]}, "196": {"url": "https://blog.datasyndrome.com/generalists-dominate-data-science-f01882f25347", "title": "Generalists Dominate Data\u00a0Science", "text": "Analytics products and systems are best built by small teams of generalists. Large teams of specialists become dominated by communication overhead, and the effect of \u201cChinese whispers\u201d distorts the flow of tasks and stagnates creativity. Data scientists should develop generalist skills to become more efficient members of a data science team. Building data products takes a team covering a broad and diverse skillset. From the customer representative at one end, to the operations engineer at the other, the spectrum of roles in a product analytics team looks like this: Large companies often fill each role with a pair of shoes, resulting in a twelve person team like the one below. The problem with this setup is that it becomes much more difficult to achieve consensus and to perform any task that spans roles. And in data science, most tasks span roles. To look at a particular example, lets focus on the creation of a chart as part of a data product. To begin, a product manager creates a specification, then an interaction designer mocks up the chart, handing it off to a data scientist to fill with data (and hopefully to explore the data and find a chart worth producing), then a back-end engineer to setup an API to grab that data, a front-end web developer to create a web page using the data that matches the mock, and an experience designer to ensure the entire thing feels right and makes sense. Charts take iteration, so this cycle of communication could happen repeatedly for each chart. You can see how communication overhead starts to predominate. A meeting of six people is a full-blown, formal meeting. It is hard to get things done in formal meetings. In the next figure, we see how a data product team might be composed of four generalists: a data engineer, a data scientist/back-end developer, a designer who can build front ends and a product manager that can write marketing copy and cut deals. This is how a startup team would span the skill spectrum, and you can probably see how this makes them more efficient. Revisiting the chart example, creating a chart becomes a collaboration between the product manager, a designer who codes, and a data scientist. This is the kind of ad hoc meeting of 2\u20133 people where \u201cshit gets done\u201d efficiently. This group will be more efficient than the six man group. Put another way: this small team will kick the large team\u2019s ass. In the big company system, sometimes the only way to get anything done efficiently is to go \u201cguerilla generalist\u201d and work with other generalists to cut people out of the chain. This is bad politically, and is part of what drives effective people from big companies. We\u2019ve shown that small teams generalists outperform large teams of specialists. In fact, generalist skills are something every data scientists should work to develop. That doesn\u2019t mean you can\u2019t specialize, but should combine specialization with generalization in order to develop \u201cT-shaped skills.\u201d The T-shaped employee is one that can lend deep expertise across projects while fulfilling multiple roles in his own. It takes time to develop general skills, and that is why the path to becoming a data scientist is not a six month bootcamp, but a ten year journey. Along this path, remember to try to be T-Shaped! Need help building an analytics product or platform? The Data Syndrome team of data scientists and data engineers is available to build your data products and systems as a service. We also offer training in Agile Data Science for all members of data science teams. Clapping shows how much you appreciated Russell Jurney\u2019s story.", "links": ["https://blog.datasyndrome.com?source=logo-lo_3b1b106e36b---500653fb51a1", "https://twitter.com/datasyndrome", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.datasyndrome.com%2Fgeneralists-dominate-data-science-f01882f25347", "https://blog.datasyndrome.com/@rjurney?source=post_header_lockup", "https://blog.datasyndrome.com/@rjurney?source=post_header_lockup", "https://personalmba.com/communication-overhead/", "https://en.wikipedia.org/wiki/Chinese_whispers", "https://en.wikipedia.org/wiki/T-shaped_skills", "http://datasyndrome.com", "http://datasyndrome.com/training", "https://blog.datasyndrome.com/tagged/data-science?source=post", "https://blog.datasyndrome.com/tagged/agile?source=post", "https://blog.datasyndrome.com/tagged/generalists?source=post", "https://blog.datasyndrome.com/tagged/analytics?source=post", "https://blog.datasyndrome.com/tagged/software-engineering?source=post", "https://blog.datasyndrome.com/@rjurney?source=footer_card", "https://blog.datasyndrome.com/@rjurney", "https://blog.datasyndrome.com?source=footer_card", "https://blog.datasyndrome.com?source=footer_card", "https://blog.datasyndrome.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "197": {"url": "https://github.com/axibase/atsd-use-cases/tree/master/FED_FORDSR#the-average-american-debt-profile", "title": "", "text": "Debt is a complicated concept. After the sub-prime mortgage crisis of the late 2000s, modern Americans are all too familiar with the problems of irresponsible spending on credit. Student loan recipients who queue up to drop off another application for a job in a field they did not study are quick to point to the trappings of deficit spending as a means of wealth creation. Politicians and voters on both sides of the aisle point to the ever-growing United States Government debt with anxiety for the future. And yet despite all the doom and gloom, the American financial system is one of the most stable and robust in the world, in no small part thanks to ingenious monetary policy and hegemonic economic position organized over the entire course of the country's history, modern American consumers are among the wealthiest on the planet. The United States Federal Reserve is the central banking system of the United States, responsible for monitoring the global financial climate and enacting policy that supports the American economy and American consumers. They maintain a number of statistics about these consumers and their monetary practices to better inform their decisions and practices. Provided by the Federal Reserve, this dataset must be correctly parsed during import. The quarterly date format needs to be converted into a monthly format that ATSD can interprete (Q/q letter is not supported). We also need to discard metadata lines contained in the multi-line header. This can be accompilshed with a schema-based parser that provides granular control over the document's rows and columns using RFC 7111 selectors and Javascript: Script 1.1 For step-by-step instructions on data customization with schema based parsing, see this support tutorial. The Financial Obligation Ratio (FOR) is an estimate of the ratio of required debt payments to disposable income. This is a broad calculation and includes all kinds of debt:  mortgage payments, credit cards, property tax and lease payments. Each of these metrics can be expanded further to include associated costs, such as homeowner's insurance for example. The Federal Reserve releases this number each quarter. Figure 1.1   Use the dropdown menus at the top of the visualization screen to navigate through time, selecting the starttime and endtime values to observe a desired period. The data can also be queried with a structured query language in the SQL Console. The data will be aggregated annually, derived from the average value of each quarter within a given year: Query 1.1 Table 1.1 All values are shown as a percent of one hundred, where the whole is representative of the total income of the average person. The Debt Service Ratio (DSR) is more specific than the Financial Obligation Ratio in that it typically does not include non-essential debt payments. Here, it has been parsed into two categories, mortgage debt and consumer debt. These numbers represent the average percent of a person's earned salary each month which much be used to make the required payments associated with consumer credit and mortgage. Typically the DSR is an initial calculation performed to determine a person's eligibility to receive a mortgage. A DSR value of less than 48% is generally preferred, meaning that with a particular mortgage plus other credit obligations at least 52% of a person's gross monthly earning would still be available to them after making the required payments. Figure 2.1   Query 2.1 Table 2.1 Because the FOR value includes the DSR value plus additional non-essential credit values, and the DSR value is parsed into both consumer and mortgage related debt, these three values can be shown in a new visualization that creates a typical consumer profile of the average American. By using the calculated value setting shown below, additional data not specifically included in the set can be displayed: Script 2.1 Shown below is the debt profile of the average American consumer from 1980 to 2017, navigate through time using the dropdown menus at the top of the screen to select a desired span of time and compare how bearing debt has changed over the course of the last three decades. Figure 3.1   The visualization can also be organized to show the amount of each type of debt as it relates to the others: Figure 3.2   Additionally, these values can be compared on an annual basis as shown in the visualization below: Figure 3.3   To view the distribution of these values across time, a histogram is shown below: Figure 3.4   In the following box diagram, explore time with the dropdown menus at the top of the visualization screen. The visualization shows the distribution of debt values as a percentage of total income, with the initial time period set to include the entire data set: Figure 3.5   The following SQL query will detail the above visualizations in one table, displaying averaged annual values of each component described above: non-essential credit payments, mortgage credit payments, and consumer credit payments, as well as the Financial Obligation Ratio (FOR), or total debt obligations. Query 3.1 Table 3.1 The above dataset can illuminate a number of features of the American economy and a number of characteristics of the average American consumer. While modern Americans are quick to denounce the zeitgeist of living outside of one's means, the data shows that in fact, the amount of debt carried by the average American is on par with or even lower in some cases than that of his 1980's counterpart. In fact, the only metric which has demonstrated a legitimate increase in value over the last several decades has been the roughly one percent increase in non-essential credit holdings by the average consumer. According to data from the Economic Research Department of the Saint Louis Branch of the Federal Reserve, the 2015 US median household income was $56,516 per year in 2015 USD. This number can be applied to the above table and visualized in ChartLab to create more comprehensive data. Figure 3.6   The above visualization aggregates the values from Table 3.1 based on a time period of the user's selection. Use the dropdown menu at the top of the screen to select the aggregation period. The initial visualization shows the average values for each metric over the entire period of time in 2015 USD by obligation amount per quarter. The following query summons the same data shown above, but further parses it to show annual average monthly payments instead of quarterly values in 2015 USD for a person making the 2015 median United States income of $56,516 a year. Query 4.3 Table 3.2 As it turns out, the idea that your parents paid less for their house than you will is only true in absolute terms. When compared with current numbers and controlled for inflation, the average 2017 consumer will pay roughly the same portion of their income towards a place to hang their hat up as the average 1980 consumer. The Federal Reserve is able to pull certain levers of power from the Eccles Building in Washington, D.C. such as printing more money, or raising and lowering interest rates to cope with inflation. However, all of these are reactionary measures meant to create small changes that have a butterfly effect over time. Ultimately, the machinations of the Board of Governers have always be something opaque and esoteric to the average man, leading to many people denouncing the Federal Reserve System entirely, occasionally opting for a return of the gold standard or leveling accusations of wrong-doing. However, after reviewing the data above, it seems that at least on a consumer level, the average American actually has more today than they would have had thirty years ago, or even just five years ago. Of course, the Federal Reserve isn't completely responsible for the wise consumer choices made in the current decades, but monetary policy enacted by the various branches of the Federal Reserve are responsible for maintaining the economic conditions that Americans, and consumers the world over, have come to expect from the United States economy.", "links": ["https://github.com/", "https://github.com/axibase/atsd-use-cases/pull/86", "http://www.usdebtclock.org/", "https://www.federalreserve.gov/", "https://www.federalreserve.gov/datadownload/Download.aspx?rel=FOR&series=91e0f9a6b8e6a4b1ef334ce2eaf22860&filetype=csv&label=include&layout=seriescolumn&from=01/01/1980&to=12/31/2017", "https://axibase.com/products/axibase-time-series-database/writing-data/csv/", "https://apps.axibase.com/chartlab/842f1dd9/#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql", "https://apps.axibase.com/chartlab/85522dd3/#fullscreen", "https://apps.axibase.com/chartlab/f25de723/#fullscreen", "https://apps.axibase.com/chartlab/f25de723/3/#fullscreen", "https://apps.axibase.com/chartlab/81ea0ea0/#fullscreen", "https://apps.axibase.com/chartlab/9f74c179/#fullscreen", "https://apps.axibase.com/chartlab/20ff0ade/#fullscreen", "https://fred.stlouisfed.org/series/MEHOINUSA646N", "https://research.stlouisfed.org/", "https://www.stlouisfed.org/", "https://apps.axibase.com", "https://apps.axibase.com/chartlab/da132e01/11/#fullscreen", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "198": {"url": "http://tech.marksblogg.com/billion-nyc-taxi-rides-aws-ec2-p2-16xlarge-brytlytdb.html", "title": "", "text": "I have 14 years of hands-on build and consulting experience with clients in the UK, Ireland & Germany. I've done both back- and frontend work for Bank of America Merrill Lynch, Blackberry, Bloomberg, British Telecom, Ford, Google, ITV, News UK, Pizza Hut, Royal Bank of Scotland, Royal Mail, T-Mobile, Vertu (Nokia subsidiary at the time), Williams Formula 1 Team and Xerox. I hold both a Canadian and a British passport. My CV & my LinkedIn profile.         Home           | Benchmarks           | Archives            | Atom Feed  Posted on Fri 28 July 2017 BrytlytDB is an in-GPU-memory database built on top of PostgreSQL. It's operated using many of PostgreSQL's command line utilities, it's wire protocol compatible so third-party PostgreSQL clients can connect to BrytlytDB and queries are even parsed, planned and optimised by PostgreSQL's regular codebase before the execution plan is passed off to GPU-optimised portions of code BrytlytDB offer. Clustering works right out of the box, GPU-powered JOINs are supported, Stored Procedures are fully functional, Deep- and Machine Learning workloads via Torch are supported and BI visualisation software in the form of SpotLyt is included with BrytlytDB as well. The feature list is too long for one blog to cover so in this post I'll just be taking a look at how performant BrytlytDB's OLAP functionality is. In this benchmark I'll see how well 32 Tesla K80 GPUs spread across two EC2 instances perform when querying 1.1 billion taxi trips. I'll be using the same dataset I've used to benchmark Amazon Athena, BigQuery, ClickHouse, Elasticsearch, EMR, kdb+/q, MapD, PostgreSQL, Redshift and Vertica. I have a single-page summary of all these benchmarks for comparison. For this benchmark I'll be using two p2.16xlarge EC2 instances running Ubuntu 16.04.2 LTS in Amazon Web Services' eu-west-1a region in Ireland. Each machine has 8 Nvidia K80  cards which have 2 GPUs each, 64 virtual CPUs and 732 GB of memory. There's also 20 Gbit/s of networking capacity available to each instance. Below are the specifications of the compute capabilities available each one of the EC2 instances. This is the layout of the GPUs available on one of the two EC2 instances. The first machine has an IP address of 52.214.237.134 and has three roles: Global Transaction Manager, Coordinator and host to the first 16 data nodes. The second machine has an IP address of 34.250.232.38 and has two roles: it is also a coordinator and hosts the second set of 16 data nodes. TCP port 5432 is open between the two machines for communicating via PostgreSQL's wire protocol, TCP port 7777 is open for global transaction manager communication and ports 20,000 through to 20,0031 are open for data node communication. In addition to the default 20 GB EBS volumes on each EC2 instance there are six 500 GB General Purpose SSDs attached which offer a baseline of 100 IOPS each and can burst to 3,000 IOPS if need be. These drives are setup in a RAID 0 configuration on each instance. Before setting up the RAID array I'll install a few dependencies. This was run on both EC2 instances. Below are the commands used to setup the RAID array on each instance. Here's what the RAID layout looked like after it was setup. This RAID 0 setup offers a partition with a capacity of 3.2 TB on each instance: I've run the following to download BrytlytDB's install script. BrytlytDB is commercial software so I cannot divulge the URL I pulled this from at this time. The following was run on both EC2 instances. I've then edited the install script on the first instance with the following instance-specific values: And the install script on the second instance was edited with the following instance-specific values: With those changes in place I then ran the install script on both EC2 instances. The install script conducts the following: Once that's done I can form the cluster with the following registration script. The following was run on the first EC2 instance. The second registration script run on the other EC2 instance is identical with the first with the exception of the coordinator line pointing to 52.214.237.134 instead. For this benchmark I've downloaded and decompressed one half the 500 GB of CSV data I created in my Billion Taxi Rides in Redshift blog post onto each EC2 instance. The data sits across 56 files across both machines but for the quickest load time I want to create 32 reasonably equally-sized CSV files and load 16 files on each EC2 instance simultaneously. The 32 files will pair up with the 32 GPUs available across the cluster and should allow for the best utilisation of the GPU resources when running queries. Here is half of the original 500 GB data set on the first EC2 instance: And this is the other half on the second instance: On each instance I'll decompress the GZIP files. Below took about four minutes to complete on each instance. Then I'll concatenate the ~560 million lines of CSV data on each instance and break it up into files of 35 million lines each. The above completed in 8 minutes on each EC2 instance. I'm now left with 16 CSV files on the first EC2 instance: And 16 files on the other EC2 instance: I'll then connect to the coordinator node on each EC2 instance and setup BrytlytDB's gpu_manager_fdw extension and foreign data wrapper. I can then create the trips table. The above sets the maximum row count per GPU device at 38 million and creates an index on the cab_type column. The cab_type column has very low cardinality so the indexing will be less beneficial versus a column with a higher cardinality of values. Some of the data types used in the above table aren't the types I normally use in my benchmarks. The reason for this is that there is still limited data type support available in this early version of BrytlytDB. I've had to substitute DATETIME with DATE which will truncate the timestamps in the dataset to just the date rather than the full date and time. There is no SMALLINT support yet so I've had to use the larger INT type as an replacement for those fields. DECIMAL types aren't yet supported so I'll be using DOUBLE PRECISION as a replacement there. The above replacements could speed up or slow down the queries I benchmark with so I'm hoping to do another benchmark when data type support is widened in the future. With the table created I'll launch 32 simultaneous load jobs across the two EC2 instances to load the data into BrytlytDB's trips table. This is the import script I ran on the first EC2 instance: And this is the load script for the second instance. The first EC2 instance loaded its half of the dataset in 1 hour, 12 minutes and 21 seconds. The second instance loaded its half in 1 hour, 9 minutes and 57 seconds. After the data was loaded in I could see the PostgreSQL data directory filled up with reasonably evenly-sized data folders. Here's what they look like on the first EC2 instance: I was then able to use PostgreSQL's CLI tool to make sure I can see the table and all 1.1 billion records. The times quoted below are the lowest query times seen during a series of runs. As with all my benchmarks, I use the lowest query time as a way of indicating \"top speed\". The following completed in 0.762 seconds. The following completed in 2.472 seconds. The following completed in 4.131 seconds. The following completed in 6.041 seconds. Given all the features of PostgreSQL are still available I'm blown away at how fast BrytlytDB is able to aggregate data. The cluster I used cost around $30 / hour which means this system out performed other Cloud-based and PostgreSQL-based data warehousing solutions both in terms of wall clock time and in terms of cost per hour of running the cluster. That being said I know Nvidia's K80 chips use extremely fast memory and have thousands of compute cores so I expect further optimisations from BrytlytDB to drive down these already fast query times. \u2190 Back to Index Copyright \u00a9 2014 - 2017 Mark Litwintschik. This site's template is based off a template by Giulio Fidente.", "links": ["https://twitter.com/marklit82", "http://tech.marksblogg.com/theme/cv.pdf", "https://uk.linkedin.com/in/marklitwintschik/", "http://tech.marksblogg.com/feeds/all.atom.xml", "http://www.brytlyt.com/", "https://uk.linkedin.com/in/marklitwintschik/", "https://github.com/giulivo/pelican-svbhack"]}, "199": {"url": "https://blog.datazar.com/why-we-focus-on-design-just-as-much-as-functionality-f6ccb3b2c926", "title": "Why We Focus on Design Just as Much as Functionality", "text": "It\u2019s no secret that scientific software isn\u2019t the most beautiful software in terms of design/aesthetics (generally speaking here). It is very effective though; it gets the job done. It does exactly what you expect it to do. Being a huge movie buff, I\u2019ve always wondered why the software we use in the laboratories or classrooms looks nothing like what we see in the movies. Movies do this thing where tech looks exciting and futuristic and super fun to use. And then I go back to reality and it\u2019s nothing like what I just watched. It\u2019s worth mentioning that not everything in the movies looks practical, especially when it comes to software or even more ridiculous: computer hacking. But that\u2019s another subject. The fact there\u2019s some art missing is something I noticed not only in the software we use in the lab, but also the instruments we build and the papers we write. It seems as if the art has been completely taken out of the sciences. Before you say it, we obviously can\u2019t incorporate art into somethings like highly sensitive instruments as unnecessary additions area big no because then you\u2019re interfering. Another thing to note might be the fact that hundreds or even some thousands years ago, the scientists were also artists. Art came naturally to them as they were also discovering mother nature. At that time, science was also only available to the rich which meant a couple of things: they could afford to spend a little bit of time incorporating art into their scientific work as they weren\u2019t rushing to meet grant deadlines, but it also meant they were already educated in the arts from an early age. These days anyone can be whatever they want to be. To the very least, it\u2019s easier now than it was in that era by an order of magnitude. Today we\u2019re taught that art almost has no place in the sciences and we must be efficient and to the point. Anything extra is generally referred to as fluff. If you like to romanticize science, you\u2019re not really a scientist. But I strongly believe that it\u2019s the artistic and I might add\u200a\u2014\u200aphilosophical\u200a\u2014\u200afluff that gets people interested in science. The hard math and core scientific values obviously being there regardless. All of this results in the absence of art in the sciences which makes art seem dry and boring. Science in its purest form is absolutely beautiful if communicated right. That\u2019s why we love to do our part at Datazar. We\u2019re far, far from perfect or even close to what we want to be. But putting just as much thought in design and UX as we put in functionality is a core principle. There\u2019s a misconception that incorporating art means adding more stuff. Incorporating art can also mean doing less, using less and making the most of what you already have in a beautiful and simple way. An example would be removing that obscene amount of jargon from your paper. So I ask everyone designing software for scientists, design for humans not robots. Scientists are humans too. So go ahead add that CSS animation.", "links": ["https://blog.datazar.com?source=logo-lo_75a4b9fd97e4---e2c7e6e1c75", "https://twitter.com/DatazarHQ", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.datazar.com%2Fwhy-we-focus-on-design-just-as-much-as-functionality-f6ccb3b2c926", "https://blog.datazar.com", "https://blog.datazar.com/tagged/r-language", "https://blog.datazar.com/tagged/open-data", "https://blog.datazar.com/tagged/how-to", "https://www.r-bloggers.com/", "https://blog.datazar.com/search", "https://blog.datazar.com/@ata_aman?source=post_header_lockup", "https://blog.datazar.com/@ata_aman?source=post_header_lockup", "https://blog.datazar.com/tagged/science?source=post", "https://blog.datazar.com/tagged/art?source=post", "https://blog.datazar.com/tagged/research?source=post", "https://blog.datazar.com/tagged/software-development?source=post", "https://blog.datazar.com/tagged/design?source=post", "https://blog.datazar.com/@ata_aman?source=footer_card", "https://blog.datazar.com/@ata_aman", "http://twitter.com/datazarhq", "https://blog.datazar.com?source=footer_card", "https://blog.datazar.com?source=footer_card", "https://blog.datazar.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "200": {"url": "https://news.ycombinator.com/item?id=14950255", "title": "", "text": "", "links": ["https://news.ycombinator.com", "https://edgylabs.com/machine-learning-to-enhance-smartphone-pictures/", "https://hn.algolia.com/?query=Machine%20Learning%20to%20Enhance%20Smartphone%20Pictures&sort=byDate&dateRange=all&type=story&storyText=false&prefix&page=0", "https://www.google.com/search?q=Machine%20Learning%20to%20Enhance%20Smartphone%20Pictures", "https://github.com/HackerNews/API", "http://www.ycombinator.com/apply/"]}, "201": {"url": "http://starmine.ai/datasets/ds02.html", "title": "", "text": "reddit Slack Twitter Instagram Facebook LinkedIn", "links": ["http://starmine.ai", "http://reddit.com/r/datasets", "https://join.slack.com/t/starmineai/shared_invite/MjIwNjg5OTUyODAzLTE1MDE2MTk1MzQtMmJhYTExNDViMA", "https://twitter.com/starmineAI", "https://www.instagram.com/starmine.ai/", "https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Fstarmine.ai/index.html&t=starmine%20ICO", "https://www.linkedin.com/cws/share?url=http%3A%2F%2Fstarmine.ai/index.html&token=&isFramed=true", "http://research.kraeutli.com/index.php/2013/11/the-tate-collection-on-github", "http://54.174.116.134/recommend/datasets/subscribers/free/supercolumns/supercolumns-elements-CMDB-nasdaq-nyse-otcbb-general-2017/supercolumns-elements-nasdaq-nyse-otcbb-general-UPDATE-2017-03-01.csv", "http://54.174.116.134/recommend/datasets/subscribers/free/supercolumns/supercolumns-elements-CMDB-nasdaq-nyse-otcbb-general-2017/supercolumns-elements-nasdaq-nyse-otcbb-general-UPDATE-2017-03-01.csv", "http://54.174.116.134/recommend/app/ai_connect-finance-historicaltrends_api?ccl1=playstation&ccl2=helium&ccl3=korea&ccl4=shampoo&ccl5=coffee&query=concept_column_labels&themesource=theme_CMDB-nasdaq-nyse-otcbb&month=01&year=all&db=CMDB-nasdaq-nyse-otcbb-general", "https://www0.gsb.columbia.edu/mygsb/faculty/research/pubfiles/1555/contagious.pdf", "https://trends.google.com/trends/explore?q=playstation,helium,korea,shampoo,coffee", "https://trends.google.com/trends/explore?q=playstation,helium,korea,shampoo,coffee"]}, "202": {"url": "https://github.com/maciejkula/spotlight/blob/master/examples/movielens_sequence/readme.rst", "title": "", "text": "Using sequences of user-item interactions as an input for recommender models has a number of attractive properties. Firstly, it recognizes that recommending the next item that a user may want to buy or see is precisely the goal we are trying to achieve. Secondly, it's plausible that the ordering of users' interactions carries additional information over and above just the identities of items they have interacted with. For example, a user is more likely to watch the next episode of a given TV series if they've just finished the previous episode. Finally, when the sequence of past interactions rather than the identity of the user is the input to a model, online systems can incorporate new users (and old users' new actions) in real time. They are fed to the existing model, and do not require a new model to be fit to incorporate new information (unlike factorization models). Recurrent neural networks are the most natural way of modelling such sequence problems. In recommendations, gated recurrent units (GRUs) have been used with success in the Session-based recommendations with recurrent neural networks paper. Spotlight implements a similar model using LSTM units as one of its sequence representations. But recurrent neural networks are not the only way of effectively representing sequences: convolutions can also do the job. In particular, we can use causal convolutions: convolutional filters applied to the sequence in a left-to-right fashion, emitting a representation at each step. They are causal in that the their output at time t is conditional on input up to t-1: this is necessary to ensure that they do not have access to the elements of the sequence we are trying to predict. Like LSTMs, causal convolutions can model sequences with long-term dependencies. This is achieved in two ways: stacking convolutional layers (with padding, every convolutional layer preserves the shape of the input), and dilation: insertion of gaps into the convolutional filters (otherwise known as atrous convolutions). Causal convolutions have been used in several recent high-profile papers: Using convolutional rather than recurrent networks for representing sequences has a couple of advantages, as described in this blog post: Spotlight implements causal convolution models as part of its sequence models package, alongside more traditional recurrent and pooling models. The Spotlight implementation has the following characteristics: The model is trained using one of Spotlight's implicit feedback losses, including pointwise (logistic and hinge) and pairwise (BPR as well as WARP-like adaptive hinge) losses. As with other Spotlight sequence models, the loss is computed for all the time steps of the sequence in one pass: for all timesteps t in the sequence, a prediction using elements up to t-1 is made, and the loss is averaged along both the time and the minibatch axis. This leads to siginficant training speed-ups relative to only computing the loss for the last element in the sequence. To see how causal CNNs compare to more traditional sequence models we can have a look at how they perform at predicting the next rated movie on the Movielens 1M dataset. With 1 million interactions spread among 6000 users and around 4000 movies it should be small enough to run quick experiments, but large enough to yield meaningful results. I chose to split the dataset into 80% train, and 10% test and validation sets. I construct 200-long sequences by splitting each user's item sequence into 200-long chunks; if a chunk is shorter than 200 elements, it's padded with zeros. I use mean reciprocal rank (MRR) as the evaluation metric. To choose hyperparameters, I run a quick, coarse grained hyperparameter search, using random sampling to draw 100 hyperparameter sets. With the data and hyperparameters ready, fitting and evaluating the model is relatively simple: Fitting the models is fairly quick, taking at most two or three minutes on a single K80 GPU. The code for the experiments is available in the experiments folder of the Spotlight repo. The results are as follows: It's difficult to draw clear-cut conclusions about the effect of each hyperparameter, but it looks like: To compare causal convolutions with more traditional sequence models I run similar hyperparameter searches for LSTM-based representations and pooling representations. The pooling representation is a simple averaging of item embedding across the sequence; the LSTM-based model runs an LSTM along a user's interactions, using the hidden state for prediction of the next element at each step. The results are as follows: A single layer LSTM seems to outperform causal convolutions, by an over 10% margin, helped by the adaptive hinge loss. Simple pooling performs quite badly. It looks like causal convolutions need some more work before beating recurrent networks. There are a couple of possible avenues for making them better: I'd love to get some input on these. If you have suggestions, let me know on Twitter or open an issue or PR in Spotlight.", "links": ["https://github.com/", "https://arxiv.org/abs/1511.06939", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.LSTMNet", "https://travis-ci.org/maciejkula/spotlight", "https://arxiv.org/pdf/1609.03499.pdf", "http://papers.nips.cc/paper/6527-conditional-image-generation-with-pixelcnn-decoders", "https://arxiv.org/abs/1610.10099", "https://medium.com/@TalPerry/convolutional-methods-for-text-d5260fd5675f", "https://arxiv.org/pdf/1610.10099.pdf", "https://maciejkula.github.io/spotlight/sequence/sequence.html", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.CNNNet", "https://maciejkula.github.io/spotlight/losses.html", "https://grouplens.org/datasets/movielens/1m/", "https://en.wikipedia.org/wiki/Mean_reciprocal_rank", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.LSTMNet", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.PoolNet", "https://maciejkula.github.io/spotlight/losses.html#spotlight.losses.adaptive_hinge_loss", "https://twitter.com/Maciej_Kula", "https://github.com/maciejkula/spotlight", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "203": {"url": "https://github.com/ynqa/word-embedding", "title": "", "text": "GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together.  Sign up             Use Git or checkout with SVN using the web URL.             This is an implementation of word embedding (also referred to as word representation) models in Golang. Word embedding makes words' meaning, structure, and concept mapping into vector space (and low dimension). For representative instance: Like this example, it could calculate word meaning by arithmetic operations between vectors. Listed models for word embedding, and checked it already implemented. Downloading text8 corpus, and training by Skip-Gram with negative sampling.", "links": ["https://github.com/", "https://github.com/ynqa/word-embedding/blob/master/LICENSE", "https://help.github.com/articles/which-remote-url-should-i-use", "https://travis-ci.org/ynqa/word-embedding", "https://godoc.org/github.com/ynqa/word-embedding", "https://goreportcard.com/report/github.com/ynqa/word-embedding", "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf", "http://nlp.stanford.edu/pubs/glove.pdf", "https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf", "http://mattmahoney.net/dc/textdata", "https://github.com/cjlin1/libsvm", "http://www.aclweb.org/anthology/Q15-1016", "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.648.8023&rep=rep1&type=pdf", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "204": {"url": "https://github.com/brannondorsey/keras_weight_animator", "title": "", "text": "GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together.  Sign up             Use Git or checkout with SVN using the web URL.          Save Keras weight matrices as short animated videos to better understand what and how your neural network models are learning. Below are examples of the first LSTM layer and the final output layer of a six-class RNN model trained over one epoch. Blue represents low values and red represents high values.   In order to render videos from the saved weight images you must also have the following packages installed on your machine: This module is named keras_weight_animator. It exposes a Keras callback function that you can include in any model fit(...) method. The two required parameters to image_saver_callback(...) are the Keras model and an output_directory to periodically save weight images to. By default, keras_weight_animator saves layer weights every 100 batches to output_directory as PNGs in folders named epoch_XXX-layer_NAME-weights_YY. Once training is complete, you can optionally create short animated video clips from the image sequences saved in output_directory using [bin/create_image_sequence.sh](bin/create_image_sequence.sh) path/to/output_directory. This will use parallel, mogrify, and ffmpeg to create a .mp4 from the image sequences located in each folder of output_directory. Video files will be named like epoch_XXX-layer_NAME-weights_YY.mp4. You can run this script automatically from your training script by passing the render_videos=True parameter to image_saver_callback(...). weight_image_sequences(...) takes a variety of optional keyword arguments. I've included an example usage of the module in examples/wisdm.py. This example uses smartphone accelerometer data from WISDM to classify human activity tasks like walking, standing, sitting, walking upstairs, etc... This example uses a one layer LSTM to classify a set of 60 data points (representing three seconds of data sampled at 20hz) as belonging to one of six classes. It outputs image sequences and videos to data/wisdm. Using a bash script to leverage parallel, ImageMagick, and FFMPEG isn't necessarily the most elegant solution, but its the one I had time for. The goal of this here lil' project was to write a quick tool that allows me to better understand how weights change over mini-batch updates in a variety of neural networks. Perhaps in the future I will come back and clean up some of the inelegancies. If you have interest in contributing or maintaining a cleaner version of this lib, please reach out at brannon@brannondorsey.com. This module is \u00a9 Brannon Dorsey 2017, released under an \u2665 MIT License \u2665. You are free to use, modify, distribute, sell, etc... this software under those terms. Example data is from the WIreless Sensor Datamining (WISDM) Actitracker dataset published by Fordham University: This idea is tangentially inspired by much of the work on Visualizing and Understanding Recurrent Networks by Andrej Karpathy, Justin Johnson, and Fei-Fei Li. GNU Parallel is adamant about citation to the point of excess IMHO, but for what its worth, here is their bibtex:", "links": ["https://github.com/", "https://github.com/brannondorsey/keras_weight_animator/blob/master/LICENSE", "https://help.github.com/articles/which-remote-url-should-i-use", "https://github.com/brannondorsey/keras_weight_animator/pull/1", "http://keras.io", "https://www.gnu.org/software/parallel/", "https://www.imagemagick.org/script/index.php", "https://ffmpeg.org/download.html", "https://keras.io/callbacks/", "http://matplotlib.org/users/colormaps.html", "http://www.cis.fordham.edu/wisdm/dataset.php", "https://arxiv.org/abs/1506.02078", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "205": {"url": "https://axibase.github.io/atsd-use-cases/Expatriation_Q2/", "title": "", "text": "Expatriation is the temporary or permanent relocation of a person by choice or by force from their native country to any other country in the world for work, pleasure, or purpose. History has seen expatriation from nearly every country in the world up to and including the current epoch. Famous American expatriates throughout history have included American founding father Benjamin Franklin, who worked as the first United States Minister to France for almost a decade after appointment by the Continental Congress in 1778. The  sixth President of the United States John Quincy Adams, who served as a U.S. Diplomat in four countries before winning the Presidency as a Democratic-Republican in 1825, most notably opening American diplomatic ties as the first United States Minister to the Russian Empire, a position he loved  so much he declined a subsequent offer to serve as an Associate Justice on the Supreme Court of the United States a few years later. American expatriates have come from all walks of life: authors, such as Ernest Hemingway who lived in China, Spain, Cuba, and was even present on the beach codenamed Omaha in Normandy during the allied landing. Musicians, like Jimi Hendrix who rose to international fame in London after leaving his native Seattle, directors like Stanley Kubrick, singers like Tina Turner, and even basketball players like  Allen Iverson, who played an incomplete season for a Turkish basketball team in 2010 after retiring from the NBA. For those who decide to make the change permanent, there comes a time when the inconvenience of living abroad as a resident alien outweighs the novelty and many decide to renounce their citizenship for one reason or another. Unsurprisingly, this is a long and complicated process, one of the main features of which is settling up with the Internal Revenue Service (IRS) via the eponymous Expatriation Tax. The Federal Register is a publishing outlet for the United States Government where the IRS officially releases quarterly information about American citizens who renounce their passport in lieu of a new one and other long-term residents who have decided to repatriate, view the raw data here and use the Axibase Data Crawler designed specifically for collecting, parsing, organzing and inserting historical expatriation data and new data as it becomes available.  Axibase covered last quarter\u2019s expatriation release here and is using this quarter\u2019s release to follow-up. Every election cycle countless celebrities and netizens issue their promise to leave if their candidate fails to get elected and the growing notoriety surrounding the 2016 Presidental Election has proven to  be no different. This type of social theater is almost as old as the country itself as there have been a number of similarly divisive elections throughout American history: Andrew Jackson\u2019s victory in 1828 was seen as a populace uprising against the corrupt political elite and a return to the authority of the common man, Abraham Lincoln\u2019s second term was viewed as the tightening of federal authority to excessive levels that ultimately led to the American Civil War, Franklin Roosevelt was called a dangerous communist whose infamous court-packing plan favorably increased the number of seats on the Supreme Court to his  advantage and was seen as abhorrent and traitorous by many, and of course who can forget recent elections that have featured scandal after scandal and two modern Presidents who served after losing the popular vote, but winning in the electorate. Figure 1   Query 1 Data queried in the SQL Console. Table 1 To give the issue further context, Figure 1 and Table 1 use the most current Department of Homeland Security (DHS) data  available to display current naturalization numbers in the United States. Naturalization is the sister process to expatriation, because as a person expatriates from one country they must be naturalized in another or end up a stateless person. Naturalization numbers in America have remained fairly constant throughout the last decade because there are a number of federal regulations that control the amount of long-term immigrants that America accepts each year and that quota is met without fail. As noted in the our Q1 article, the sister value and subject of this examination, expatriation numbers, has been steadily growing for several consecutive years. Figure 2.1   Query 2 The above query features robust syntax and calculated values. See the following tutorials  to understand more about SQL Console. Table 2 Figure 2.2   Expatriation has been increasing each year by roughly 30% since 2010, which featured abnormally high expatriation rates, most likely attributable to the economic turndown of the Great Recession which began in the United States as a result of the sub-prime mortgage crisis. American citizenship is often sought after for the economic opportunity that comes along with the passport, as the ability to work and do business in the country is heavily restricted or regulated, and with the value of that investment or  opportunity in question, it is unsurprising that the number of investors in the system, that is, new citizens, would fluctuate. It seems appropriate to call naturalization an investment because of the nature of the process, which is long, complicated, and often quite expensive similar to a long-position that will cost more at purchasing time but promises high returns after reaching maturity. The peak, or more appropriately, valley of the global recession occurred in 2009 when the global GDP contracted causing a decline in the median familty income of about five percent. Figure 3   Query 3 Table 3 Reducing the timespan to look at data over the last 5 years captures the repetition of this trend for this period of time. Of the eighteen quarters included in this query, only six of them, roughly 33% showed negative growth for this statistic. Growing expatriation numbers provide interesting insight into the perception of America both domestically and abroad, as these numbers not only include U.S. citizens who renounce their citizenship, but also long-term resident aliens who have returned home. Almost unbelievably, the IRS does not strip characteristic information about expatriates from their publications meaning that first and last names are included in the data. Using Social Security data concerning the most common first names chosen throughout the last century and United States Census data concerning the most common last names to compare, the following queries and visualizations show  the number of expatriates by common first and common last name throughout the entire observed period, ranked by their frequency in the IRS publication. The goal of these queries is purely demonstrative and lighthearted. There is no intention to invade privacy, defame people, or otherwise cause harm. Figure 4  Query 4 Table 4 All of these names appear on the list of most common American first names, linked here Figure 5  Query 5 Table 5 Only the name Smith appears on the list of most common American last names, linked here Use the Axibase Data Crawler to collect the data used in this artcle and examine it yourself in ChartLab.", "links": ["https://github.com/axibase/atsd-use-cases", "https://www.irs.gov/", "https://www.irs.gov/individuals/international-taxpayers/expatriation-tax", "https://www.federalregister.gov/", "https://www.federalregister.gov/documents/2017/05/10/2017-09475/quarterly-publication-of-individuals-who-have-chosen-to-expatriate-as-required-by-section-6039g", "https://github.com/axibase/atsd-data-crawlers/tree/irs-expatriation-data-crawler", "https://axibase.com", "https://apps.axibase.com/chartlab/654b9945#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql", "https://www.uscis.gov/news/fact-sheets/naturalization-fact-sheet", "http://www.unhcr.org/stateless-people.html", "https://apps.axibase.com/chartlab/7fa5b643#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql#examples", "https://apps.axibase.com/chartlab/7fa5b643/5/#fullscreen", "https://web.stanford.edu/group/recessiontrends/cgi-bin/web/sites/all/themes/barron/pdf/IncomeWealthDebt_fact_sheet.pdf", "https://apps.axibase.com/chartlab/7fa5b643/2/#fullscreen", "https://www.ssa.gov/oact/babynames/decades/century.html", "http://www.census.gov/main/www/cen2000.html", "https://www.ssa.gov/oact/babynames/decades/century.html", "http://www.census.gov/main/www/cen2000.html", "https://github.com/axibase/atsd-data-crawlers/tree/irs-expatriation-data-crawler", "https://apps.axibase.com", "https://github.com/axibase/atsd-use-cases", "https://github.com/axibase", "https://pages.github.com"]}, "206": {"url": "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/", "title": "", "text": "Machine learning is eating the world right now. Everyone and their mother are learning about machine learning models, classification, neural networks, and Andrew Ng. You\u2019ve decided you want to be a part of it, but where to start? In this article we\u2019ll cover some important characteristics of Python and why it\u2019s great for machine learning. We\u2019ll also cover some of the most important libraries it has for ML, and if it piques your interest, some places where you can learn more. Python is a great choice for machine learning for several reasons. First and foremost, it\u2019s a simple language\u00a0on the surface; even if you\u2019re not familiar with Python, getting up to speed is very quick if you\u2019ve ever used any other language with C-like syntax (i.e. every language out there). Second, Python has a great community, which results in good documentation and friendly, comprehensive answers in StackOverflow (fundamental!). Third, also stemming from the great community, there are plenty of useful libraries for Python (both as \u201cbatteries included\u201d and third party), which solve basically any problem that you can have (including machine learning). Yeah and it\u2019s true. Python isn\u2019t the fastest language out there: all those handy abstractions come at a cost. But here\u2019s the trick: libraries can and do offload the expensive calculations to the much more performant (but harder to use) C and C++. For instance, there\u2019s NumPy, which is a library for numerical computation. It\u2019s written in C, and it\u2019s fast. Practically every library out there that involves intensive calculations uses it \u2014 almost all the libraries listed next use it in some form. So if you read NumPy, think fast. Therefore, you can make your scripts run basically as fast as straight up writing them in a lower level language. So there\u2019s really nothing to worry about when it comes to speed. Are you starting out in machine learning? Want something that covers everything from feature engineering to training and testing a model? Look no further than scikit-learn! This fantastic piece of free software provides every tool necessary for machine learning and data mining. It\u2019s the de facto standard library for machine learning in Python, recommended for most of the \u2018old\u2019 ML algorithms. This library does both classification and regression, supporting basically every algorithm out there (support vector machines, random forest, naive bayes, and so on). It\u2019s built in such a way that allows easy switching of algorithms, so experimentation is easy. These \u2018older\u2019 algorithms are surprisingly resilient and work very well in a lot of cases. But that\u2019s not all! Scikit-learn also does dimensionality reduction, clustering, you name it. It\u2019s also blazingly fast since it runs on NumPy and SciPy (meaning that all the heavy number crunching is run on C instead of Python). Check out some examples to see everything this library is capable of, and the tutorials if you want to learn how it works. While not a machine learning library per se, NLTK is a must when working with natural language processing (NLP). It comes with a bundle of datasets and other lexical resources (useful for training models) in addition to libraries for working with text \u2014 for functions such as classification, tokenization, stemming, tagging, parsing and more. The usefulness of having all of this stuff neatly packaged can\u2019t be overstated. So if you are interested in NLP, check out some tutorials! Used widely in research and academia, Theano is the grandfather of all deep learning frameworks. Written in Python, it\u2019s tightly integrated with NumPy. Theano allows you to create neural networks, which are represented as mathematical expressions with multi-dimensional arrays. Theano handles this for you so you don\u2019t have to worry about the actual implementation of the math involved. It supports offloading calculations to the much faster GPU, which is a feature that everyone supports today, but back when they introduced it this wasn\u2019t the case. The library is very mature at this point and supports a very wide range of operations, which is a great plus when it comes to comparing it with other similar libraries. The biggest complaint out there is that the API may be unwieldy for some, making the library hard to use for beginners. However, there are wrappers that ease the pain and make working with Theano simple, such as Keras, Blocks and Lasagne. Interested in learning about Theano? Check out this Jupyter Notebook tutorial. The Google Brain team created TensorFlow for internal use in machine learning applications, and open sourced it in late 2015. They wanted something that could replace their older, closed source machine learning framework, DistBelief, which they said wasn\u2019t flexible enough and too tightly coupled to their infrastructure to be shared with other researchers around the world. And so TensorFlow was created. Learning from the mistakes of the past, many consider this library to be an improvement over Theano, claiming more flexibility and a more intuitive API. Not only can it be used for research but also for production environments, supporting huge clusters of GPUs for training. While it doesn\u2019t support as wide a range of operations as Theano, it has better computational graph visualizations. TensorFlow is very popular nowadays. In fact, if you\u2019ve heard about a single library on this list, it\u2019s probably this one: there isn\u2019t a day that goes by without a new blog post or paper mentioning TensorFlow gets published. This popularity translates into a lot of new users and a lot of tutorials, making it very welcoming to beginners. Keras is a fantastic library that provides a high-level API for neural networks and is capable of running on top of either Theano or TensorFlow. It makes harnessing the full power of these complex pieces of software much easier than using them directly. It\u2019s very user-friendly, putting user experience as a top priority. They manage this by using simple APIs and excellent feedback on errors. It\u2019s also modular, meaning that different models (neural layers, cost functions, and so on) can be plugged together with little restrictions. This also makes it very easy to extend, since it\u2019s simple to add new modules and connect them with the existing ones. Some people have called Keras so good that it is effectively cheating in machine learning. So if you\u2019re starting out with deep learning, go through the examples and documentation to get a feel for what you can do with it. And if you want to learn, start out with this tutorial and see where you can go from there. Two similar alternatives are Lasagne and Blocks, but they only run on Theano. So if you tried Keras and are unhappy with it, maybe try out one of these alternatives to see if they work out for you. Another popular deep learning framework is Torch, which is written in Lua. Facebook open-sourced a Python implementation of Torch called PyTorch, which allows you to conveniently use the same low-level libraries that Torch uses, but from Python instead of Lua. PyTorch is much better for debugging since one of the biggest differences between Theano/TensorFlow and PyTorch is that the former use symbolic computation while the latter doesn\u2019t. Symbolic computation means that coding an operation (say, \u2018x + y\u2019), it\u2019s not computed when that line is interpreted. Before getting executed it has to be compiled (translated to CUDA or C). This makes debugging harder in Theano/TensorFlow, since an error is much harder to associate with the line of code that caused it. Of course, doing things this way has its advantages, but debugging isn\u2019t one of them. If you want to start out with PyTorch the official tutorials are very friendly to beginners but get to advanced topics as well. Alright, you\u2019ve presented me with a lot of alternatives for machine learning libraries in Python. What should I choose? How do I compare these things? Where do I start? Our Ape Advice\u2122 for beginners is to try and not get bogged down by details. If you\u2019ve never done anything machine learning related, try out scikit-learn. You\u2019ll get an idea of how the cycle of tagging, training and testing work and how a model is developed. Now, if you want to try out deep learning, start out with Keras \u2014 which is widely agreed to be the easiest framework \u2014 and see where that takes you. After you have more experience, you will start to see what it is that you actually want from the framework: greater speed, a different API, or maybe something else, and you\u2019ll be able to make a more informed decision. And even then, there is an endless supply of articles out there comparing Theano, Torch, and TensorFlow. There\u2019s no real way to tell which one is the good one. It\u2019s important to take into account that all of them have wide support and are improving constantly, making comparisons harder to make. A six month old benchmark may be outdated, and year old claims of framework X doesn\u2019t support operation Y could no longer be valid. Finally, if you\u2019re interested in doing machine learning specifically applied to NLP, why not check out MonkeyLearn! Our platform provides a unique UX that makes it super easy to build, train and improve NLP models. You can either use pre-trained models for common use cases (like sentiment analysis, topic detection or keyword extraction) or train custom algorithms using your particular data. Also, you don\u2019t have to worry about the underlying infrastructure or deploying your models, our scalable cloud does this for you. You can start for free and integrate right away with our beautiful API. There are plenty of online resources out there to learn about machine learning ! Here are a few: So that was a brief intro to machine learning in Python and some of its libraries. The important part is not getting bogged down by details and just trying stuff out. Follow your curiosity, and don\u2019t be afraid to experiment. Know about a python library that was left out? Share it in the comments below!", "links": ["http://www.monkeylearn.com", "http://monkeylearn.com/use-cases/", "http://monkeylearn.com/docs/", "http://www.monkeylearn.com/pricing", "http://www.monkeylearn.com/blog/", "https://app.monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/?utm_source=ml-blog&utm_medium=header&utm_campaign=blog", "https://monkeylearn.com/blog", "http://monkeylearn.com/use-cases/", "http://monkeylearn.com/docs/", "http://www.monkeylearn.com/pricing", "http://www.monkeylearn.com/blog/", "https://app.monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/?utm_source=ml-blog&utm_medium=header&utm_campaign=blog", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/07/Post_2c.png", "http://scikit-learn.org/", "http://scikit-learn.org/stable/auto_examples/index.html#general-examples", "http://scikit-learn.org/stable/tutorial/index.html", "http://www.nltk.org/", "http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk", "http://deeplearning.net/software/theano/", "https://keras.io/", "https://github.com/mila-udem/blocks", "https://github.com/Lasagne/Lasagne", "http://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/Theano%20Tutorial.ipynb", "https://www.tensorflow.org/", "https://www.tensorflow.org/get_started/graph_viz", "https://www.tensorflow.org/tutorials/", "https://keras.io/", "https://news.ycombinator.com/item?id=13872764", "https://github.com/fchollet/keras#getting-started-30-seconds-to-keras", "https://keras.io/getting-started/functional-api-guide/", "https://elitedatascience.com/keras-tutorial-deep-learning-in-python", "https://github.com/Lasagne/Lasagne", "https://github.com/mila-udem/blocks", "http://torch.ch/", "http://pytorch.org/", "http://pytorch.org/tutorials/", "http://scikit-learn.org/stable/tutorial/basic/tutorial.html", "https://elitedatascience.com/keras-tutorial-deep-learning-in-python", "https://www.quora.com/Is-TensorFlow-better-than-other-leading-libraries-such-as-Torch-Theano", "http://www.ccri.com/2016/12/09/torch-vs-tensorflow-vs-theano/", "https://www.reddit.com/r/MachineLearning/comments/5w3q74/d_so_pytorch_vs_tensorflow_whats_the_verdict_on/", "https://github.com/zer0n/deepframeworks/blob/master/README.md", "https://monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/", "http://nbviewer.jupyter.org/github/rhiever/Data-Analysis-and-Machine-Learning-Projects/blob/master/example-data-science-notebook/Example%20Machine%20Learning%20Notebook.ipynb", "https://blog.monkeylearn.com/a-gentle-guide-to-machine-learning/", "https://www.coursera.org/learn/machine-learning", "http://course.fast.ai/", "https://monkeylearn.com/blog/author/bruno/", "https://monkeylearn.com/blog/category/news/", "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/#respond", "https://monkeylearn.com/blog/author/bruno/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/monkeylearn_zapier.png", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/wp-content/uploads/2017/06/Post_1e.png", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/naive-bayes-classifier.png", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/monkey-startup.png", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/category/applications/", "https://monkeylearn.com/blog/category/guides/", "https://monkeylearn.com/blog/category/howto/", "https://monkeylearn.com/blog/category/news/", "https://monkeylearn.com/blog/category/text-classification/", "https://www.monkeylearn.com", "https://www.monkeylearn.com", "http://slack.monkeylearn.com/", "https://twitter.com/monkeylearn", "https://github.com/monkeylearn/"]}, "207": {"url": "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "title": "Machine Learning Translation and the Google Translate Algorithm", "text": "Every day we use different technologies without even knowing how exactly they work. In fact, it\u2019s not very easy to understand engines powered by machine learning. The Statsbot team wants to make machine learning clear by telling data stories in this blog. Today, we\u2019ve decided to explore machine translators and explain how the Google Translate algorithm works. Years ago, it was very time consuming to translate the text from an unknown language. Using simple vocabularies with word-for-word translation was hard for two reasons: 1) the reader had to know the grammar rules and 2) needed to keep in mind all language versions while translating the whole sentence. Now, we don\u2019t need to struggle so much\u2013 we can translate phrases, sentences, and even large texts just by putting them in Google Translate. But most people don\u2019t actually care how the engine of machine learning translation works. This post is for those who do care. If the Google Translate engine tried to kept the translations for even short sentences, it wouldn\u2019t work because of the huge number of possible variations. The best idea can be to teach the computer sets of grammar rules and translate the sentences according to them. If only it were as easy as it sounds. If you have ever tried learning a foreign language, you know that there are always a lot of exceptions to rules. When we try to capture all these rules, exceptions and exceptions to the exceptions in the program, the quality of translation breaks down. Creating your own simple machine translator would be a great project for any data science resume. Let\u2019s try to investigate what hides in the \u201cblack boxes\u201d that we call machine translators. Deep neural networks can achieve excellent results in very complicated tasks (speech/visual object recognition), but despite their flexibility, they can be applied only for tasks where the input and target have fixed dimensionality. Here is where Long Short-Term Memory networks (LSTMs) come into play, helping us to work with sequences whose length we can\u2019t know a priori. LSTMs are a special kind of recurrent neural network (RNN), capable of learning long-term dependencies. All RNNs look like a chain of repeating modules. So the LSTM transmits data from module to module and, for example, for generating Ht we use not only Xt, but all previous input values X. To learn more about structure and mathematical models of LSTM, you can read the great article \u201cUnderstanding LSTM Networks.\u201d Our next step is bidirectional recurrent neural networks (BRNNs). What a BRNN does, is split the neurons of a regular RNN into two directions. One direction is for positive time, or forward states. The other direction is for negative time, or backward states. The output of these two states are not connected to inputs of the opposite direction states. To understand why BRNNs can work better than a simple RNN, imagine that we have a sentence of 9 words and we want to predict the 5th word. We can make it know either only the first 4 words, or the first 4 words and last 4 words. Of course, the quality in the second case would be better. Now we\u2019re ready to move to sequence to sequence models (also called seq2seq). The basic seq2seq model consist of two RNNs: an encoder network that processes the input and a decoder network that generates the output. Finally, we can make our first machine translator! However, let\u2019s think about one trick. Google Translate currently supports 103 languages, so we should have 103x102 different models for each pair of languages. Of course, the quality of these models varies according to the popularity of languages and the amount of documents needed for training this network. The best that we can do is to make one NN to take any language as input and translate into any language. That very idea was realized by Google engineers at the end of 2016. Architecture of NN was build on the seq2seq model, which we have already studied. The only exception is that between the encoder and decoder there are 8 layers of LSTM-RNN that have residual connections between layers with some tweaks for accuracy and speed. If you want to go deeper with that, take a look at the article Google\u2019s Neural Machine Translation System. The system requires a \u201ctoken\u201d at the beginning of the input sentence which specifies the language you\u2019re trying to translate the phrase into. This improves translation quality and enables translations even between two languages which the system hasn\u2019t seen yet, a method termed \u201cZero-Shot Translation.\u201d When we\u2019re talking about improvements and better results from Google Translate algorithms, how can we correctly evaluate that the first candidate for translation is better than the second? It\u2019s not a trivial problem, because for some commonly used sentences we have the sets of reference translations from the professional translators, that have, of course, some differences. There are a lot of approaches that partly solve this problem, but the most popular and effective metric is BLEU (bilingual evaluation understudy). Imagine, we have two candidates from machine translators: Although they have the same meaning they differ in quality and have different structure. Let\u2019s look at two human translations: Obviously, Candidate 1 is better, sharing more words and phrases compared to Candidate 2. This is a key idea of the simple BLEU approach. We can compare n-grams of the candidate with n-grams of the reference translation and count the number of matches (independent from their position). We use only n-gram precisions, because calculating recall is difficult with multiple refs and the result is the geometric average of n-gram scores. Now you can evaluate the complex engine of machine learning translation. Next time when you translate something with Google Translate, imagine how many millions of documents it analyzed before giving you the best language version.", "links": ["https://blog.statsbot.co?source=logo-lo_fe8c89fd4179---cfc9f21a543a", "https://twitter.com/statsbotco", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.statsbot.co%2Fmachine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co", "https://blog.statsbot.co/analytics/home", "https://blog.statsbot.co/datascience/home", "https://blog.statsbot.co/design/home", "https://blog.statsbot.co/startups/home", "https://blog.statsbot.co/bots/home", "https://blog.statsbot.co/news/home", "https://blog.statsbot.co/statsbot-digest-b0d7372f842a", "http://statsbot.co?utm_source=blog&utm_medium=navigation&utm_campaign=robot_face", "https://blog.statsbot.co/search", "https://blog.statsbot.co/@daniilkorbut?source=post_header_lockup", "https://blog.statsbot.co/@daniilkorbut?source=post_header_lockup", "https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html", "http://statsbot.co?utm_source=blog&utm_medium=article&utm_campaign=machine_translation", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6?utm_source=blog&utm_medium=article&utm_campaign=machine_translation", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6", "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "https://www.semanticscholar.org/paper/Hybrid-speech-recognition-with-Deep-Bidirectional-Graves-Jaitly/5807664af8e63d5207f59fb263c9e7bd3673be79", "https://research.googleblog.com/2016/09/a-neural-network-for-machine.html", "https://www.newscientist.com/article/2114748-google-translate-ai-invents-its-own-language-to-translate-with/", "https://research.googleblog.com/2016/09/a-neural-network-for-machine.html", "https://arxiv.org/abs/1609.08144", "https://en.wikipedia.org/wiki/BLEU", "https://en.wikipedia.org/wiki/N-gram", "https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3", "https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/tagged/machine-learning?source=post", "https://blog.statsbot.co/tagged/machine-translation?source=post", "https://blog.statsbot.co/tagged/google-translate?source=post", "https://blog.statsbot.co/tagged/data-science?source=post", "https://blog.statsbot.co/tagged/neural-networks?source=post", "https://blog.statsbot.co/@daniilkorbut?source=footer_card", "https://blog.statsbot.co/@daniilkorbut", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "208": {"url": "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "title": "Yanir SeroussiMy 10-step path to becoming a remote data scientist with\u00a0Automattic", "text": "About two years ago, I read the book The Year without Pants, which describes the author\u2019s experience leading a team at Automattic (the company behind WordPress.com, among other products). Automattic is a fully-distributed company, which means that all of its employees work remotely (hence pants are optional). While the book discusses some of the challenges of working remotely, the author\u2019s general experience was very positive. A few months after reading the book, I decided to look for a full-time position after a period of independent work. Ideally, I wanted a well-paid data science-y remote job with an established distributed tech company that offers a good life balance and makes products I care about. Automattic seemed to tick all my boxes, so I decided to apply for a job with them. This post describes my application steps, which ultimately led to me becoming a data scientist with Automattic. Before jumping in, it\u2019s worth noting that this post describes my personal experience. If you apply for a job with Automattic, your experience is likely to be different, as the process varies across teams, and evolves over time. I decided to apply for a data wrangler position with Automattic in October 2015. While data wrangler may sound less sexy than data scientist, reading the job ad led me to believe that the position may involve interesting data science work. This impression was strengthened by some LinkedIn stalking, which included finding current data wranglers and reading through their profiles and websites. I later found out that all the people on the data division start out as data wranglers, and then they may pick their own title. Some data wranglers do data science work, while others are more focused on data engineering, and there are some projects that require a broad range of skills. As the usefulness of the term data scientist is questionable, I\u2019m not too fussed about fancy job titles. It\u2019s more important to do interesting work in a supportive environment. Applying for the job was fairly straightforward. I simply followed the instructions from the ad:  Does this sound interesting? If yes, please send a short email to jobs @ this domain telling us about yourself and attach a resum\u00e9. Let us know what you can contribute to the team. Include the title of the position you\u2019re applying for and your name in the subject. Proofread! Make sure you spell and capitalize WordPress and Automattic correctly. We are lucky to receive hundreds of applications for every position, so try to make your application stand out. If you apply for multiple positions or send multiple emails there will be one reply.  Having been on the receiving side of job applications, I find it surprising that many people don\u2019t bother writing a cover letter, addressing the selection criteria in the ad, or even applying for a job they\u2019re qualified to do. Hence, my cover letter was fairly short, comprising of several bullet points that highlight the similarities between the job requirements and my experience. It was nothing fancy, but simple cover letters have worked well for me in the past. The initial application was followed by a long wait. From my research, this is the typical scenario. This is unsurprising, as Automattic is a fairly small company with a large footprint, which is both distributed and known as a great place to work (e.g., its Glassdoor rating is 4.9). Therefore, it attracts many applicants from all over the world, which take a while to process. In addition, Matt Mullenweg (Automattic\u2019s CEO) reviews job applications before passing them on to the team leads. As I didn\u2019t know that Matt reviewed job applications, I decided to try to shorten the wait by getting introduced to someone in the data division. My first attempt was via a second-degree LinkedIn connection who works for Automattic. He responded quickly when I reached out to him, saying that his experience working with the company is in line with the Glassdoor reviews \u2013 it\u2019s the best job he\u2019s had in his 15-year-long career. However, he couldn\u2019t help me with an intro, because there is no simple way around Automattic\u2019s internal processes. Nonetheless, he reassured me that it is worth waiting patiently, as the strict process means that you end up working with great people. I wasn\u2019t in a huge rush to find a job, but in December 2015 I decided to accept an offer to become the head of data science at Car Next Door. This was a good decision at the time, as I believe in the company\u2019s original vision of reducing the number of cars on the road through car sharing, and it seemed like there would be many interesting projects for me to work on. The position wasn\u2019t completely remote, but as the company was already spread across several cities, I was able to work from home for a day or two every week. In addition, it was a pleasant commute by bike from my Sydney home to the office, so putting the fully-remote job search on hold didn\u2019t seem like a major sacrifice. As I haven\u2019t heard anything from Automattic at that stage, it seemed unwise to reject a good offer, so I started working full-time with Car Next Door in January 2016. I successfully attracted Automattic\u2019s attention with a post I published on the misuse of the word insights by many tech companies, which included an example from WordPress.com. Greg Ichneumon Brown, one of the data wranglers, commented on the post, and invited me to apply to join Automattic and help them address the issues I raised. This happened after I accepted the offer from Car Next Door, and hasn\u2019t resulted in any speed up of the process, so I just gave up on Automattic and carried on with my life. I finally heard back from Automattic in February 2016 (four months after my initial application and a month into my employment with Car Next Door). Martin Remy, who leads the data division, emailed me to enquire if I\u2019m still interested in the position. I informed him that I was no longer looking for a job, but we agreed to have an informal chat, as I\u2019ve been waiting for such a long time. As is often the case with Automattic interviews, the chat with Martin was completely text-based. Working with a distributed team means that voice and video calls can be hard to schedule. Hence, Automattic relies heavily on textual channels, and text-based interviews allow the company to test the written communication skills of candidates. The chat revolved around my past work experience, and Martin also took the time to answer my questions about the company and the data division. At the conclusion of the chat, Martin suggested I contact him directly if I was ever interested in continuing the application process. While I was happy with my position at the time, the chat strengthened my positive impression of Automattic, and I decided that I would reapply if I were to look for a full-time position again. My next job search started earlier than I had anticipated. In October 2016, I decided to leave Car Next Door due to disagreements with the founders over the general direction of the company. In addition, I had more flexibility in choosing where to live, as my personal circumstances had changed. As I\u2019ve always been curious about life outside the capital cities of Australia, I wanted to move away from Sydney. While I could have probably continued working remotely with Car Next Door, I felt that it would be better to find a job with a fully-distributed team. Therefore, I messaged Martin and we scheduled another chat. The second chat with Martin took place in early November. Similarly to the first chat, it was conducted via Skype text messages, and revolved around my work in the time that has passed since the first chat. This time, as I was keen on continuing with the process, I asked more specific questions about what kind of work I\u2019m likely to end up doing and what the next steps would be. The answers were that I\u2019d be joining the data science team, and that the next steps are a pre-trial test, a paid trial, and a final interview with Matt. While this sounds straightforward, it took another six months until I finally became an Automattic employee (but I wasn\u2019t in a rush). The pre-trial test consisted of a data analysis task, where I was given a dataset and a set of questions to answer by Carly Stambaugh, the data science lead. The goal of the test is to evaluate the candidate\u2019s approach to a problem, and assess organisational and communication skills. As such, the focus isn\u2019t on obtaining a specific result, so candidates are given a choice of several potential avenues to explore. The open-ended nature of the task is reminiscent of many real-world data science projects, where you don\u2019t always have a clear idea of what you\u2019re going to discover. While some people may find this kind of uncertainty daunting, I find it interesting, as it is one of the things that makes data science a science. I spent a few days analysing the data and preparing a report, which was submitted as a Jupyter Notebook. After submitting my initial report, there were a few follow-up questions, which I answered by email. The report was reviewed by Carly and Martin, and as they were satisfied with my work, I was invited to proceed to the next stage: A paid trial project. The main part of the application process with Automattic is the paid trial project. The rationale behind doing paid trials was explained a few years ago by Matt in Hire by Auditions, Not Resumes:  Before we hire anyone, they go through a trial process first, on contract. They can do the work at night or over the weekend, so they don\u2019t have to leave their current job in the meantime. We pay a standard rate of $25 per hour, regardless of whether you\u2019re applying to be an engineer or the chief financial officer. During the trials, we give the applicants actual work. If you\u2019re applying to work in customer support, you\u2019ll answer tickets. If you\u2019re an engineer, you\u2019ll work on engineering problems. If you\u2019re a designer, you\u2019ll design. There\u2019s nothing like being in the trenches with someone, working with them day by day. It tells you something you can\u2019t learn from resumes, interviews, or reference checks. At the end of the trial, everyone involved has a great sense of whether they want to work together going forward. And, yes, that means everyone \u2014 it\u2019s a mutual tryout. Some people decide we\u2019re not the right fit for them.  The goal of my trial project was to improve the Elasticsearch language detection algorithm. This took about a month, and ultimately resulted in a pull request that got merged into the language detection plugin. I find this aspect of the process pretty exciting: While the plugin is used to classify millions of documents internally by Automattic, its impact extends beyond the company, as Elasticsearch is used by many other organisations and projects. This stands in contrast to many other technical job interviews, which consist of unpaid work on toy problems under stressful conditions, where the work performed is ultimately thrown away. While the monetary compensation for the trial work is lower than the market rate for data science consulting, I valued the opportunity to work on a real open source project, even if this hadn\u2019t led to me getting hired. There was much more to the trial project than what\u2019s shown in the final pull request. Most of the discussions were held on an internal project thread, primarly under the guidance of Carly (the data science lead), and Greg (the data wrangler who replied to my post a year earlier). The project was kicked off with a general problem statement: There was some evidence that the Elasticsearch language detection plugin doesn\u2019t perform well on short texts, and my mission was to improve it. As the plugin didn\u2019t include any tests for short texts, one of the main contributions of my work was the creation of datasets and tests to measure its accuracy on texts of different lengths. This was followed by some tweaks that improved the plugin\u2019s performance, as summarised in the pull request. Internally, this work consisted of several iterations where I came up with ideas, asked questions, implemented the ideas, shared the results, and discussed further steps. There are still many possible improvements to the work done in the trial. However, as trials generally last around a month, we decided to end it after a few iterations. I enjoyed the trial process, but it is definitely not for everyone. Most notably, there is a strong emphasis on asynchronous text-based communication, which is the main mode by which projects are coordinated at Automattic. People who don\u2019t enjoy written communication may find this aspect challenging, but I have always found that writing helps me organise my thoughts, and that I retain information better when reading than when listening to people speak. That being said, Automatticians do meet in person several times a year, and some teams have video chats for some discussions. While doing the trial, I had a video chat with Carly, which was the first (and last) time in the process that I got to see and hear a live human. However, this was not an essential part of the trial project, as our chat was mostly on the data scientist role and my job expectations. I finished working on the trial project just before Christmas. The feedback I received throughout the trial was positive, but Martin, Carly, and Greg had to go through the work and discuss it among themselves before making a final decision. This took about a month, due to the holiday period, various personal circumstances, and the data science team meetup that was scheduled for January 2017. Eventually, Martin got back to me with positive news: They were satisfied with my trial work, which meant there was only one stage left \u2013 the final interview with Matt Mullenweg, Automattic\u2019s CEO. Like other parts of the process, the interview with Matt is text-based. The way it works is fairly simple: I was instructed to message Matt on Slack and wait for a response, which may take days or weeks. I sent Matt a message on January 25, and was surprised to hear back from him the following morning. However, that day was Australia Day, which is a public holiday here. Therefore, I only got back to him two hours after he messaged me that morning, and by that time he was probably already busy with other things. This was the start of a pretty long wait. I left Car Next Door at the end of January, as I figured that I would be able to line up some other work even if things didn\u2019t work out with Automattic. My plan was to take some time off, and then move up to the Northern Rivers area of New South Wales. I had two Reef Life Survey trips planned, so I wasn\u2019t going to start working again before mid-April. I assumed that I would hear back from Matt before then, which would have allowed me to make an informed decision whether to look for another job or not. After two weeks of waiting, the time for my dive trips was nearing. As I was going to be without mobile reception for a while, I thought it\u2019d be worth letting Matt know my schedule. After discussing the matter with Martin, I messaged Matt. He responded, saying that we might as well do the interview at the beginning of April, as I won\u2019t be starting work before that time anyway. I would have preferred to be done with the interview earlier, but was happy to have some certainty and not worry about missing more chat messages before April. In early April, I returned from my second dive trip (which included a close encounter with Cyclone Debbie), and was hoping to sort out my remote work situation while completing the move up north. Unfortunately, while the move was successful, I was ready to give up on Automattic because I haven\u2019t heard back from Matt at all in April. However, Martin remained optimistic and encouraged me to wait patiently, which I did as I was pretty busy with the move and with some casual freelancing projects. The chat with Matt finally happened on May 2. As is often the case, it took a few hours and covered my background, the trial process, and some other general questions. I asked him about my long wait for the final chat, and he apologised for me being an outlier, as most chats happen within two weeks of a candidate being passed over to him. As the chat was about to conclude, we got to the topic of salary negotiation (which went well), and then the process was finally over! Within a few hours of the chat I was sent an offer letter and an employment contract. As Automattic has an entity in Australia (called Ausomattic), it\u2019s a fairly standard contract. I signed the contract and started work the following week \u2013 over a year and a half after my initial application. Even before I started working, I booked tickets to meet the data division in Montr\u00e9al \u2013 a fairly swift transition from the long wait for the final interview. As noted above, Automatticians get to choose their own job titles, so to become a data scientist with Automattic, I had to set my job title to Data Scientist. This is generally how many people become data scientists these days, even outside Automattic. However, job titles don\u2019t matter as much as job satisfaction. And after 2.5 months with Automattic, I\u2019m very satisfied with my decision to join the company. My first three weeks were spent doing customer support, like all new Automattic employees. Since then, I\u2019ve been involved in projects to make engagement measurement more consistent (harder than it sounds, as counting things is hard), and to improve the data science codebase (e.g., moving away from Legacy Python). Besides that, I also went to Montr\u00e9al for the data division meetup, and have started getting into chatbot work. I\u2019m looking forward to doing more work and sharing my experience here and on data.blog. Very enlightening post! It was very awesome to see that the insights you saw to Elasticsearch went to a PR. I bet that was worth the whole thing!  That\u2019s very exciting, I wanted to ask are you a self learner or do you have a degree,can you please share your background. Thank you  Thanks Mostafa. Yes, I have a BSc in computer science, and a PhD in what you would now call data science. See: https://www.linkedin.com/in/yanirseroussi/  Fill in your details below or click an icon to log in:     Connecting to %s         Enter your email address to follow this blog and receive notifications of new posts by email.", "links": ["https://yanirseroussi.com/", "https://yanirseroussi.com/", "https://yanirseroussi.com/about/", "https://yanirseroussi.com/presentations/", "https://yanirseroussi.com/phd-work/", "https://yanirseroussi.com/2014/04/05/kaggle-competition-summaries/", "http://scottberkun.com/yearwithoutpants/", "https://automattic.com/", "https://yanirseroussi.com/2015/03/22/the-long-road-to-a-lifestyle-business/", "http://web.archive.org/web/20150908140923/https://automattic.com/work-with-us/data-wrangler/", "https://yanirseroussi.com/2016/08/04/is-data-scientist-a-useless-job-title/", "https://automattic.com/about/", "https://www.glassdoor.com.au/Reviews/Automattic-Reviews-E751107.htm", "http://davemart.in/remote-hiring/", "https://www.carnextdoor.com.au/", "https://yanirseroussi.com/2015/12/08/this-holiday-season-give-me-real-insights/", "https://yanirseroussi.com/2015/12/08/this-holiday-season-give-me-real-insights/#comment-957", "http://jupyter.org/", "https://hbr.org/2014/01/hire-by-auditions-not-resumes", "https://www.elastic.co/products/elasticsearch", "https://github.com/jprante/elasticsearch-langdetect/pull/69", "https://github.com/jprante/elasticsearch-langdetect/pull/69", "https://yanirseroussi.com/2016/01/24/the-joys-of-offline-data-collection/", "https://www.whitsundaytimes.com.au/news/boat-caught-in-eye-of-cyclone-cruises-home/3164170/", "https://data.blog/2017/06/29/data-coalesce-automattic-data-division-meets-in-montreal/", "http://daynebatten.com/2016/06/counting-hard-data-science/", "http://powerfulpython.com/blog/magic-word-legacy-python/", "https://data.blog/2017/05/24/may-the-bot-be-with-you-how-algorithms-are-supporting-happiness-at-wordpress-com/", "https://data.blog/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=twitter", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=facebook", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=google-plus-1", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=linkedin", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=reddit", "https://yanirseroussi.com/category/data-science-2/", "https://yanirseroussi.com/tag/automattic/", "https://yanirseroussi.com/tag/career/", "https://yanirseroussi.com/tag/data-science/", "https://yanirseroussi.com/tag/elasticsearch/", "https://yanirseroussi.com/tag/personal/", "https://yanirseroussi.com/tag/wordpress/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "https://yanirseroussi.com/author/yanirseroussi/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comments", "https://yanirseroussi.com/2017/06/03/exploring-and-visualising-reef-life-survey-data/", "https://dotnetmeditations.com/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1698", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1698&_wpnonce=6c197b2684", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1698#respond", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1700", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1700&_wpnonce=350fa40ca9", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1700#respond", "http://yanirseroussi.com", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1705", "https://www.linkedin.com/in/yanirseroussi/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1705&_wpnonce=7f1e8d7061", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1705#respond", "https://gravatar.com/site/signup/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "https://yanirseroussi.com/2017/06/03/exploring-and-visualising-reef-life-survey-data/", "https://yanirseroussi.com/2017/01/08/customer-lifetime-value-and-the-proliferation-of-misinformation-on-the-internet/", "https://yanirseroussi.com/2016/09/19/ask-why-finding-motives-causes-and-purpose-in-data-science/", "https://yanirseroussi.com/2016/08/21/seven-ways-to-be-data-driven-off-a-cliff/", "https://yanirseroussi.com/category/bcrecommender/", "https://yanirseroussi.com/category/data-science-2/", "https://yanirseroussi.com/category/environment/", "https://yanirseroussi.com/category/general/", "https://yanirseroussi.com/category/kaggle-2/", "https://yanirseroussi.com/category/machine-intelligence/", "https://yanirseroussi.com/category/phd-work/", "https://wordpress.com/?ref=footer_blog"]}, "209": {"url": "https://medium.com/airbnb-engineering/using-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d", "title": "Using Machine Learning to Predict Value of Homes On\u00a0Airbnb", "text": "by Robert Chang Data products have always been an instrumental part of Airbnb\u2019s service. However, we have long recognized that it\u2019s costly to make data products. For example, personalized search ranking enables guests to more easily discover homes, and smart pricing allows hosts to set more competitive prices according to supply and demand. However, these projects each required a lot of dedicated data science and engineering time and effort. Recently, advances in Airbnb\u2019s machine learning infrastructure have lowered the cost significantly to deploy new machine learning models to production. For example, our ML Infra team built a general feature repository that allows users to leverage high quality, vetted, reusable features in their models. Data scientists have started to incorporate several AutoML tools into their workflows to speed up model selection and performance benchmarking. Additionally, ML infra created a new framework that will automatically translate Jupyter notebooks into Airflow pipelines. In this post, I will describe how these tools worked together to expedite the modeling process and hence lower the overall development costs for a specific use case of LTV modeling\u200a\u2014\u200apredicting the value of homes on Airbnb. Customer Lifetime Value (LTV), a popular concept among e-commerce and marketplace companies, captures the projected value of a user for a fixed time horizon, often measured in dollar terms. At e-commerce companies like Spotify or Netflix, LTV is often used to make pricing decisions like setting subscription fees. At marketplace companies like Airbnb, knowing users\u2019 LTVs enable us to allocate budget across different marketing channels more efficiently, calculate more precise bidding prices for online marketing based on keywords, and create better listing segments. While one can use past data to calculate the historical value of existing listings, we took one step further to predict LTV of new listings using machine learning. Data scientists are typically accustomed to machine learning related tasks such as feature engineering, prototyping, and model selection. However, taking a model prototype to production often requires an orthogonal set of data engineering skills that data scientists might not be familiar with. Luckily, At Airbnb we have machine learning tools that abstract away the engineering work behind productionizing ML models. In fact, we could not have put our model into production without these amazing tools. The remainder of this post is organized into four topics, along with the tools we used to tackle each task: One of the first steps of any supervised machine learning project is to define relevant features that are correlated with the chosen outcome variable, a process called feature engineering. For example, in predicting LTV, one might compute the percentage of the next 180 calendar dates that a listing is available or a listing\u2019s price relative to comparable listings in the same market. At Airbnb, feature engineering often means writing Hive queries to create features from scratch. However, this work is tedious and time consuming as it requires specific domain knowledge and business logic, which means the feature pipelines are often not easily sharable or even reusable. To make this work more scalable, we developed Zipline\u200a\u2014\u200aa training feature repository that provides features at different levels of granularity, such as at the host, guest, listing, or market level. The crowdsourced nature of this internal tool allows data scientists to use a wide variety of high quality, vetted features that others have prepared for past projects. If a desired feature is not available, a user can create her own feature with a feature configuration file like the following: When multiple features are required for the construction of a training set, Zipline will automatically perform intelligent key joins and backfill the training dataset behind the scenes. For the listing LTV model, we used existing Zipline features and also added a handful of our own. In sum, there were over 150 features in our model, including: With our features and outcome variable defined, we can now train a model to learn from our historical data. As in the example training dataset above, we often need to perform additional data processing before we can fit a model: In this step, we don\u2019t quite know what is the best set of features to use, so writing code that allows us to rapidly iterate is essential. The pipeline construct, commonly available in open-source tools like Scikit-Learn and Spark, is a very convenient tool for prototyping. Pipelines allow data scientists to specify high-level blueprints that describe how features should be transformed, and which models to train. To make it more concrete, below is a code snippet from our LTV model pipeline: At a high level, we use pipelines to specify data transformations for different types of features, depending on whether those features are of type binary, categorical, or numeric. FeatureUnion at the end simply combines the features column-wise to create the final training dataset. The advantage of writing prototypes with pipelines is that it abstracts away tedious data transformations using data transforms. Collectively, these transforms ensure that data will be transformed consistently across training and scoring, which solves a common problem of data transformation inconsistency when translating a prototype into production. Furthermore, pipelines also separates data transformations from model fitting. While not shown in the code above, data scientists can add a final step to specify an estimator for model fitting. By exploring different estimators, data scientists can perform model selection to pick the best model to improve the model\u2019s out of sample error. As mentioned in the previous section, we need to decide which candidate model is the best to put into production. To make such a decision, we need to weigh the tradeoffs between model interpretability and model complexity. For example, a sparse linear model might be very interpretable but not complex enough to generalize well. A tree based model might be flexible enough to capture non-linear patterns but not very interpretable. This is known as the Bias-Variance tradeoff. In applications such as insurance or credit screening, a model needs to be interpretable because it\u2019s important for the model to avoid inadvertently discriminating against certain customers. In applications such as image classification, however, it is much more important to have a performant classifier than an interpretable model. Given that model selection can be quite time consuming, we experimented with using various AutoML tools to speed up the process. By exploring a wide variety of models, we found which types of models tended to perform best. For example, we learned that eXtreme gradient boosted trees (XGBoost) significantly outperformed benchmark models such as mean response models, ridge regression models, and single decision trees. Given that our primary goal was to predict listing values, we felt comfortable productionizing our final model using XGBoost, which favors flexibility over interpretability. As we alluded to earlier, building a production pipeline is quite different from building a prototype on a local laptop. For example, how can we perform periodic re-training? How do we score a large number of examples efficiently? How do we build a pipeline to monitor model performance over time? At Airbnb, we built a framework called ML Automator that automagically translates a Jupyter notebook into an Airflow machine learning pipeline. This framework is designed specifically for data scientists who are already familiar with writing prototypes in Python, and want to take their model to production with limited experience in data engineering. Here is a code snippet demonstrating how the fit and transform functions are defined in our LTV model. The fit function tells the framework that a XGBoost model will be trained, and that data transformations will be carried out according to the pipeline we defined previously. Once the notebook is merged, ML Automator will wrap the trained model inside a Python UDF and create an Airflow pipeline like the one below. Data engineering tasks such as data serialization, scheduling of periodic re-training, and distributed scoring are all encapsulated as a part of this daily batch job. As a result, this framework significantly lowers the cost of model development for data scientists, as if there was a dedicated data engineer working alongside the data scientists to take the model into production! Note: Beyond productionization, there are other topics, such as tracking model performance over time or leveraging elastic compute environment for modeling, which we will not cover in this post. Rest assured, these are all active areas under development. In the past few months, data scientists have partnered very closely with ML Infra, and many great patterns and ideas arose out of this collaboration. In fact, we believe that these tools will unlock a new paradigm for how to develop machine learning models at Airbnb. We are very excited about the future of this framework and the new paradigm it brought along. By bridging the gap between prototyping and productionization, we can truly enable data scientists and engineers to pursue end-to-end machine learning projects and make our product better. Want to use or build these ML tools? We\u2019re always looking for talented people to join our Data Science and Analytics team! Special thanks to members of Data Science & ML Infra team who were involved in this work: Aaron Keys, Brad Hunter, Hamel Husain, Jiaying Shi, Krishna Puttaswamy, Michael Musson, Nick Handel, Varant Zanoyan, Vaughn Quoss et al. Additional thanks to Gary Tang, Jason Goodman, Jeff Feng, Lindsay Pettingill for reviewing this blog post.", "links": ["https://medium.com/", "https://medium.com/airbnb-engineering?source=logo-lo_44bf5ce712fe---53c7c27702d5", "https://twitter.com/AirbnbEng", "https://medium.com/m/signin?redirect=https%3A%2F%2Fmedium.com%2Fairbnb-engineering%2Fusing-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d", "https://medium.com/airbnb-engineering", "https://medium.com/airbnb-engineering/ai/home", "https://medium.com/airbnb-engineering/airbnb-engineering-backend/home", "https://medium.com/airbnb-engineering/data/home", "https://medium.com/airbnb-engineering/airbnb-engineering-infrastructure/home", "https://medium.com/airbnb-engineering/tagged/mobile", "https://medium.com/airbnb-engineering/web/home", "http://airbnb.io/projects/", "https://medium.com/airbnb-engineering/search", "https://medium.com/@rchang?source=post_header_lockup", "https://medium.com/@rchang?source=post_header_lockup", "https://twitter.com/_rchang", "https://medium.com/swlh/diligence-at-social-capital-part-3-cohorts-and-revenue-ltv-ab65a07464e1", "http://scikit-learn.org/stable/", "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html", "https://www.kaggle.com/general/16927", "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html", "https://spark.apache.org/docs/latest/ml-pipeline.html", "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html", "http://scikit-learn.org/stable/data_transforms.html", "http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html", "https://medium.com/airbnb-engineering/automated-machine-learning-a-paradigm-shift-that-accelerates-data-scientist-productivity-airbnb-f1f8a10d61f8", "http://scott.fortmann-roe.com/docs/BiasVariance.html", "https://medium.com/airbnb-engineering/automated-machine-learning-a-paradigm-shift-that-accelerates-data-scientist-productivity-airbnb-f1f8a10d61f8", "https://github.com/dmlc/xgboost", "https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8", "http://www.florianwilhelm.info/2016/10/python_udf_in_hive/", "https://airflow.incubator.apache.org/", "https://www.airbnb.com/careers/departments/data-science-analytics", "https://www.linkedin.com/in/aaronkeys/", "https://www.linkedin.com/in/brad-hunter-497621a/", "https://www.linkedin.com/in/hamelhusain/", "https://www.linkedin.com/in/jiaying-shi-a2142733/", "https://www.linkedin.com/in/krishnaputtaswamy/", "https://www.linkedin.com/in/michael-m-a37b1932/", "https://www.linkedin.com/in/nicholashandel/", "https://www.linkedin.com/in/vzanoyan/", "https://www.linkedin.com/in/vquoss/", "https://www.linkedin.com/in/thegarytang/", "https://medium.com/@jasonkgoodman", "https://twitter.com/jtfeng", "https://medium.com/@lpettingill", "https://medium.com/@jtfeng?source=post_page", "https://medium.com/@jasonkgoodman?source=post_page", "https://medium.com/@gary.tang_94319?source=post_page", "https://medium.com/@lpettingill?source=post_page", "https://medium.com/@vquoss?source=post_page", "https://medium.com/@eddie.santos.3?source=post_page", "https://medium.com/@NicholasHandel?source=post_page", "https://medium.com/tag/machine-learning?source=post", "https://medium.com/tag/data-science?source=post", "https://medium.com/tag/airbnb?source=post", "https://medium.com/tag/technology?source=post", "https://medium.com/@rchang?source=footer_card", "https://medium.com/@rchang", "http://twitter.com/Airbnb", "http://twitter.com/Twitter", "https://medium.com/airbnb-engineering?source=footer_card", "https://medium.com/airbnb-engineering?source=footer_card", "http://airbnb.io", "https://medium.com/airbnb-engineering", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "210": {"url": "https://www.ayasdi.com/blog/artificial-intelligence/why_ai_is_the_new_bigdata/", "title": "The \u201cBig Data-ization\u201d of Artificial Intelligence", "text": "\u2039 Back to Blog  Artificial Intelligence, Data, Machine Intelligence, Machine Learning It seems like it was only a few years ago that the term \u201cbig data\u201d went from a promising area of research and interest to something so ubiquitous that it lost all meaning, descending ultimately into the butt of jokes.  As everyone piled onto the big data bandwagon, it became impossible to separate truth from fiction. Every executive and entrepreneur that I ran into was doing a \u201cbig data\u201d thing. I recall meeting someone whose company shot videos for enterprise customers and was pitching it as a \u2018Big Data play\u2019 \u2013 because video files, you know, are huge \u2013 they take up lots of space.  Thankfully, the noise associated with \u201cbig data\u201d is abating as sophistication and common sense take hold. In fact, in many circles the term actually exposes the user as someone who doesn\u2019t really understand the space.  Unfortunately, the same malady has now afflicted AI. Everyone I meet is doing an \u2018AI play\u2019 \u2013 even if all they did was to build a simple linear regressor in Excel.  AI is, unfortunately, the new \u201cbig data.\u201d While not good, it is not all bad either.  After all, the data ecosystem benefited from all of the \u201cbig data\u201d attention and investment \u2013 creating some amazing software and producing some exceptional productivity gains.  The same will happen with AI \u2013 with the increased attention comes investment dollars which in turn will drive adoption \u2013 enhancing the ecosystem. Having said that we need to stop calling regression on excel AI \u2013 it\u2019s ridiculous and undermines some incredible work being done in the space.  Regression has been around for 200+ years. Gauss and Legendre didn\u2019t don the AI cloak when they discovered regression. Neither should the recent stats graduate trying to raise money for his/her startup.  I will tell you what I think qualifies as AI in a moment \u2013 but here are some thoughts that influence that discussion. First, the AI definition I use is focused on the narrow, application specific AI, not the more general problem of artificial general intelligence (AGI) where simulating a person using software is the equivalent of intelligence.  Second, the vast, vast majority of the data that exists in the world is unlabeled. It is not practical to label that data manually and doing so would likely create bias anyway. One can argue that the Internet as a whole is simply a mechanism for humans to entertain ourselves while providing label data for machines.  Unlabeled data presents a different challenge, one we will address shortly, but the key point here is that it is everywhere and represents the key to extracting business value (or any value).  Third, we are not producing data scientists at a rate that can keep pace with the growth of data. Even with the moniker as a the \u201csexiest job of the 21st century\u201d the pace at which data scientists are created doesn\u2019t begin to approach the growth rate we are seeing in data.  Fourth, data scientists, for the most part, are not UX designers or product managers or, in many cases even engineers. As a result, the subject matter experts, those that sit in the business, don\u2019t have effective interfaces to the data science outputs. The interfaces that they have \u2013 powerpoint, excel, or PDF reports have limited utility in transforming the behavior of a company. What is required is something to shape behavior is something more \u2013 applications.  So what does qualify as intelligence? In \u2018On Intelligence\u2019 Jeff Hawkins says that all AI boils down to memory and prediction. The argument is very persuasive, but a bit reductive. We have a slightly different take for what an AI should display and it encompasses a framework: Here are characteristics that I think any AI should display. While some of these elements may seem self-evident that is because they are taken as a single item. Intelligence has a broader context. All the elements must work in conjunction with each other to qualify as AI. \u00a0\u00a0\u00a0 The five elements are: Let\u2019s take each of these concepts in turn. Discovery is the ability of an intelligent system to learn from data without upfront human intervention. Often, this needs to be done without being presented with an explicit target. It relies on the use of unsupervised and semi-supervised machine learning techniques (such as segmentation, dimensionality reduction, anomaly detection, etc.), as well as more supervised techniques where there is an outcome or there are several outcomes of interest. Usually, in enterprise software, the term discovery refers to the ability of ETL/MDM solutions to discover the various schemas of tables in large databases and automatically find join keys etc. This is not what we mean by discovery. We use of the term very differently and has this has important implications. In complex datasets, it is nearly impossible to ask the \u201cright\u201d questions. To discover what value lies within the data one must \u00a0understand all the relationships that are inherent and important in the data. That requires a principled approach to hypothesis generation. \u00a0 One technique, topological data analysis (TDA), is exceptional at surfacing hidden relationships that exist in the data and identifying those relationships that are meaningful without having to ask specific questions of the data. The result is an output that is able to represent complex phenomena, and is therefore able to surface weaker signals as well as the stronger signals. \u00a0 This permits the detection of emergent phenomena. \u00a0 As a result, enterprises can now discover answers to questions they didn\u2019t even know to ask and do so with data that is unlabeled. Once the data set is understood through intelligent discovery, supervised approaches are applied to predict what will happen in the future. These types of problems include classification, regression and ranking.  For this pillar, most companies use a standard set of supervised machine learning algorithms including random forests, gradient boosting, linear/sparse learners. It should be noted, however, that the unsupervised work from the previous step is highly useful in many ways. For example, it can generate relevant features for use in prediction tasks or finding local patches of data where supervised algorithms may struggle (systematic errors). The predict phase is an important part of the business value associated with data science, however, generally, in predictive analytics, there exists a notion that this is the sum total of machine learning.  This is not the case by far.  Prediction, while important, is pretty well understood and does not, on its own qualify as \u201cintelligence.\u201d \u00a0It goes back to calling a maxed out Excel table and a linear regressor AI. It is just not the case. Further, Prediction can go wrong along a number of dimensions, particularly if the groups on which you are predicting are racked with some type of bias (algorithmic, sampling etc.)  Again, Prediction is key and provides tremendous business value done correctly, but in and of itself it is not AI. We need to stop calling it as such. Applications need to support interaction with humans in a way which makes outcomes recognizable and believable. For example, when one builds a predictive model, it is important to have an explanation of how the model is doing what it is doing, i.e. what the features in the model are doing in terms that are familiar to the users of the model. \u00a0This level of familiarity is important in generating trust and intuition.  Similarly, in the same way that automobiles have mechanisms not just for detecting the presence of a malfunction, but also for specifying the nature of the malfunction and suggesting a method for correcting it, so one needs to have a \u201cnuts and bolts\u201d understanding of how an application is working in order to \u201crepair\u201d it when it goes awry. \u00a0 Transparency AND Justification. There is a difference. Transparency tells you what algorithms and parameters were used, while, Justification tells you why. For intelligence to be meaningful, it must be able to justify and explain its assertions, as well as to be able to diagnose failures.  No business leader should deploy intelligent and autonomous applications against critical business problems without a thorough understanding of what variables power the model.  Enterprises cannot move to a model of intelligent applications without trust and transparency. \u00a0 AI without UX is of limited utility.  UX is what distributes that intelligence across the organization and pushes it to the edge \u2013 where it can consumed by practitioners and subject matter experts.  Ultimately, the process of operationalizing an intelligent application within the enterprise requires some change in the organization, an acceptance that the application will evolve over time and that will demand downstream changes \u2013 automated or otherwise.  \u00a0For this to happen, intelligent applications need to be \u201clive\u201d in the business process, seeing new data and automatically executing the loop of Discover, Predict, Justify on a frequency that makes sense for that business process. For some processes that may be quarterly, for others daily. That loop can even be measured in seconds. \u00a0 Intelligent systems are designed to detect and react as the data evolves. An intelligent system is one that is always learning, live in the workflow and constantly improving. \u00a0In the modern data world, an application that is not getting more intelligent is getting dumber.  Intelligent applications are designed to detect and react when data distributions evolve. As a result, they need to be \u201con the wire\u201d in order to detect that phenomena before it becomes a problem. Too many solutions provide an answer in a point of time, an intelligent system is one that is always learning through the framework outlined here. This is what defines intelligence \u2013 not a machine learning algorithm kicking out predictions or the results of a data scientist\u2019s work as a PDF. For the industry to continue to grow and evolve we need to start doing a better job of recognizing what is truly AI and what is imposter-ware.  Let\u2019s make it a point of celebrating the real AI work.", "links": ["https://www.ayasdi.com", "https://www.ayasdi.com/platform/", "https://www.ayasdi.com/industry-overview/", "https://www.ayasdi.com/financial-services/", "https://www.ayasdi.com/healthcare/", "https://www.ayasdi.com/public-sector/", "https://www.ayasdi.com/solutions/anti-money-laundering/", "https://www.ayasdi.com/solutions/clinical-variation-management/", "https://www.ayasdi.com/solutions/denials/", "https://www.ayasdi.com/solutions/regulatory-risk/", "https://www.ayasdi.com/solutions/population-health/", "https://www.ayasdi.com/company/", "https://www.ayasdi.com/company/leadership/", "https://www.ayasdi.com/company/careers/", "https://www.ayasdi.com/company/collaborators/", "https://www.ayasdi.com/company/customers/", "https://www.ayasdi.com/company/news-and-events/", "https://www.ayasdi.com/company/contact-us/", "https://www.ayasdi.com/blog/", "https://www.ayasdi.com/request-a-demo/", "https://www.ayasdi.com/blog/author/gurjeet-singh/", "https://www.ayasdi.com/blog/category/artificial-intelligence/", "https://www.ayasdi.com/blog/category/bigdata/", "https://www.ayasdi.com/blog/category/machine-intelligence/", "https://www.ayasdi.com/blog/category/machine-learning/", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/08/02144138/BG2.png", "https://priceonomics.com/the-discovery-of-statistical-regression/", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/05/12132340/Machine_Intelligence_Apps_WP_051617v01.pdf", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/08/02144141/DS1.png", "https://www.wired.com/brandlab/2015/05/jeff-hawkins-firing-silicon-brain/", "http://cdn.ayasdi.com/wp-content/uploads/2015/02/wp-tda-and-machine-learning.pdf", "https://www.ayasdi.com/blog/aml/intelligent-segmentation-as-the-attack-point-for-aml/", "http://feedburner.google.com/fb/a/mailverify?uri=AyasdiBlog", "https://ayasdicommunity.force.com/login", "https://www.ayasdi.com/terms-of-service/", "https://www.ayasdi.com/privacy-policy/", "https://www.ayasdi.com/pilot-terms/", "http://www.youtube.com/user/ayasdi", "http://feeds.feedburner.com/AyasdiBlog", "http://www.linkedin.com/company/ayasdi", "https://www.facebook.com/ayasdi", "https://twitter.com/ayasdi"]}, "211": {"url": "https://www.mapd.com/blog/2017/08/03/whos-got-the-best-mobile-network-in-the-us/", "title": "", "text": "Based on the deluge of commercials and ads in this competitive industry, it seems every major player claims that they do. The competition is apparently so fierce that even their spokesmen are being headhunted. Admittedly, it can be a bit confusing when you find these results in one city:  And these in another:  By the way, how do you even define \u2018best\u2019 network? It\u2019s a loaded question for sure. We\u2019ll get back to that in a minute. It\u2019s a question with relevance to nearly every American, considering the vast majority of us own a cellphone of some kind (over 95% according to Pew). And we don\u2019t leave them alone; we touch them thousands of times each day. In fact, a good mobile network can affect our very quality of life. Ever travelled any significant distance with children and poor mobile data speeds? The same question is also relevant to those very telecommunications providers themselves, of course, and particularly in light of the fact that obtaining new subscribers will increasingly mean winning them away from competitors. Ensuring you are outpacing your rivals with superior network metrics is a key ingredient to both marketing and market share success. Our friends and partners at Tutela have created the world\u2019s largest mobile information platform, collecting billions of crowdsourced data points anonymously. These span device, network, and application information to allow us to get from data to insights. They\u2019ve graciously given us (and now you) a slice of their treasure trove. Our demo contains a few months of US data (~112M rows), although Tutela has data for more metrics, for nearly every country, and for a much greater timeframe. There are obviously many variables to consider when it comes to a great network, and one carrier will almost certainly not take the top spot in each category. But we can identify areas of significance where we hope to find favorable results: Spoiler alert: we\u2019re not actually going to attempt to find a winner. Those are just some of the metrics you can see at a glance on our MapD Immerse demo dashboard:  This is immediately useful. Coverage, speed, signal strength, and latency all matter, so MapD allows you to visualize and correlate them for effective consumption. Not coincidentally, the MapD Immerse visualization tool is purpose-built to leverage the MapD Core backend GPU-powered, in-memory, columnar SQL database at scale. This allows  interactive analysis over multi-billion row data sets, with the ability to drill down to an individual row...in milliseconds. It\u2019s also why Tutela has selected to partner with MapD; no need for indexing, sampling, or pre-computation. This means you can zoom in and out, filter, and more in an ad hoc fashion. Let\u2019s take it for a spin. As a consumer, you might be interested in how well each service provider performs in a certain geographic region (i.e., where you live and work) to aid in making a purchasing decision.  For example, to zoom from country level to the Washington, DC area, just type the city name into the searchbox of the pointmap. MapD Immerse will automatically zoom in when you click enter. It looks like in this region, you might lean toward Verizon:  But it may not be as clear cut if you live in San Francisco:  In either case, Sprint better have some compelling discounts or incentives to entice new subscribers. Further, both Sprint and T-Mobile might leverage this information to prioritize buildout or upgrade of cell towers. If they\u2019re analyzing subscriber loss in these areas, this also may give them insight into why that could be occurring. Another valuable feature of the MapD platform is the ability to easily identify trends, anomalies, or outliers. Visualization can often be an invaluable enabler for these situations. Sometimes new revenue opportunities or cost savings can be had by capitalizing on correlations in data that you didn\u2019t even realize existed! See the spike in data downloads in the line chart? By highlighting the time frame with your mouse, MapD will cross filter all charts using the selected time slice. Doing the same with the line chart for data uploads further reveals that the largest spikes occur on May 28 and 30. Ah, that\u2019s Memorial Day weekend in the United States. So this makes sense.  We can get even more granular and note that the bulk of data transfer occurred between 12-1am local time (4-5am UTC) as the heatmap indicates. I wonder if Uber and Lyft also experienced a spike around that time, as celebrations came to an end?  When data exploration at scale is so easy, what other interesting insights will you find? Explore this demo using Tutela's dataset on MapD\u2019s platform on your own here.  If you\u2019d like to learn more about MapD\u2019s recently announced partnership with Tutela, watch our on-demand webinar, introducing and demonstrating Tutela Explorer, a new mobile data analytics solution that provides real-time, interactive and highly visual insight into the performance of mobile networks and device usage. Tutela Explorer is available now as-a-service with global data coverage from Tutela.", "links": ["http://www.mapd.com/blog", "https://www.nytimes.com/2016/10/15/business/sprint-verizon-hear-me-now-paul-marcarelli.html", "http://www.pewinternet.org/fact-sheet/mobile/", "http://www.networkworld.com/article/3092446/smartphones/we-touch-our-phones-2617-times-a-day-says-study.html", "https://www.tutela.com/explorer/", "https://www.mapd.com/demos/telecom", "https://www.mapd.com/demos/telecom", "https://www.mapd.com/platform/core/", "https://www.mapd.com/demos/telecom", "https://www.brighttalk.com/webcast/14525/267851", "https://www.tutela.com/explorer"]}, "212": {"url": "https://www.oreilly.com/learning/building-a-simple-graphql-server-with-neo4j", "title": "Building a simple GraphQL server with Neo4j", "text": "How to implement a GraphQL API that queries Neo4j for a simple movie app. If you're interested in making the move from REST to GraphQL, check out this OSCON video from the team at Github. GraphQL is a powerful new tool for building APIs that allows clients to ask for only the data they need. Originally designed at Facebook to minimize data sent over the wire and reduce round-trip API requests for rendering views in native mobile apps, GraphQL has since been open sourced to a healthy community that is building developer tools. There are also a number of large companies and startups such as GitHub, Yelp, Coursera, Shopify, and Mattermark building public and internal GraphQL APIs. Despite what the name seems to imply, GraphQL is not a query language for graph databases, it is instead an API query language and runtime for building APIs. The \u201cGraph\u201d component of the name comes from the graph data model that GraphQL uses in the frontend. GraphQL itself is simply a specification, and there are many great tools available for building GraphQL APIs in almost every language. In this post we'll make use of graphql-tools by Apollo to build a simple GraphQL API in JavaScript that queries a Neo4j graph database for movies and movie recommendations. We will follow a recipe approach: first, exploring the problem in more detail, then developing our solution, and finally we discuss our approach. Good resources for learning more about GraphQL are GraphQL.org and the Apollo Dev Blog. GraphQL by design can work with any database or backend system, however in this example we'll be using the Neo4j graph database as our persistence layer. Why use a graph database with GraphQL? The idea of application data as a graph is an underpinning design choice of GraphQL. For example, think of customers who have placed orders that contain products\u2014that's a graph! GraphQL enables developers to translate their backend data into the application data graph on the frontend, but if we use a graph database on the backend we can do away with this impedance mismatch and we have graphs all the way down. We'd like to build a simple GraphQL API for a movie app that can do two things: In a traditional REST-ish API approach we might create two separate endpoints, perhaps /movies/search and /movies/similar. One endpoint to search for movies by substring of the title, and another to return a list of similar movies. As we add additional features we might keep adding endpoints. With GraphQL, our API is served from a single endpoint /graphql that takes one or more GraphQL queries and returns JSON data in a shape that is specified by the GraphQL query - only the data requested by the client is returned. Let's see how we can build this simple API. We'll be building a simple node.js JavaScript web server using Express.js to serve our GraphQL endpoint1. First of all we\u2019ll need a Neo4j database (with data) for our GraphQL server to query. For this example we\u2019ll make use of a Neo4j Sandbox instance. Neo4j Sandbox allows us to quickly spin up a hosted Neo4j instance, optionally with existing datasets focused around specific use cases. We\u2019ll use the Recommendations Neo4j Sandbox which includes data about movies and movie reviews and is designed to be used for generating personalized recommendations (for example, by using collaborative filtering to recommend movies based on similar users\u2019 reviews). We'll be making use of a few dependencies for this GraphQL server. I won't list them all here (you can see the full list in the package.json for this project, but there a few worth noting: We\u2019ll follow the \u201cGraphQL First\u201d development paradigm. In this approach, we start by defining a GraphQL schema. This schema defines the types and queries available in our API and then becomes the specification for the API. If we were building a complete application, the frontend developers could use this schema to build out the frontend while the backend team builds the backend in parallel, speeding development. Once we\u2019ve defined our schema we\u2019ll need to create resolver functions that are responsible for fetching data from Neo4j. This schema will define the types and GraphQL queries that we'll be able to use in our API. You can think of the schema as the API blueprint. schema.js Each GraphQL field is resolved by a resolver function. The resolver function defines how data is fetched for that field. resolvers.js Now that we've defined the GraphQL schema and resolver functions we are ready to serve the GraphQL endpoint, using Express.js. server.js If you've ever used Express this should look familiar, you'll notice that we're creating two endpoints and serving them on localhost:8080 Let's take a look at what we just did: Now that we have our API running, let's use the GraphiQL in-browser IDE for GraphQL to search for movies whose titles contain \"Matrix\" and find similar movies for each matching movie. We can load GraphiQL by opening http://loalhost:8080/graphiql in our browser and querying with this GraphQL query:   Querying our GraphQL movies API using GraphiQL, the in-browser IDE for GraphQL GraphQL is still new to almost everyone with the exception of Facebook, where it has been used since 2012, so many conventions and best practices are still being developed. However, GraphQL First Development is a philosophy the community has adopted which gives structure to the process of building a GraphQL API. By defining a contract for the API, expressed as a GraphQL schema, the frontend and backend developer teams can independently implement their applications, using the schema as a guide. To make GraphQL First Development even easier with Neo4j, the Neo4j team has built neo4j-graphql-cli, a command line tool for easily spinning up a Neo4j backed GraphQL endpoint based on a user defined GraphQL schema that can optionally be annotated with Cypher queries - exposing the power of Cypher from within GraphQL. You can learn more about this here. 1 All code for this project is available on GitHub . This example is also available as an Apollo Launchpad Pad, which you can run live in the browser.\u21a9 William Lyon is an engineer on the Developer Relations team at Neo4j, the open source graph database, where he builds tools for integrating Neo4j with other technologies and helps users be successful with graphs. He also leads Neo4j's Data Journalism Accelerator Program. Prior to Neo4j, he worked as a software engineer for a variety of startups, building mobile apps for the real estate industry, trading tools for quantitative finance, and predictive APIs. William holds a masters degree in Computer Science from the University of Montana. You can... How to package up your own repositories for use via PHP's Composer. Will content-blocking change the Web? Learn how to use Node.js and Express.js to quickly bootstrap a web server, tie in MongoDB, and display it all using dynamic HTML templates Diversity and inclusion in the tech industry has experienced a severe failure. Erica Baker approaches the topic like we would for any other service failure: with a postmortem. \u00a9 2017 O'Reilly Media, Inc. All trademarks and registered trademarks appearing on oreilly.com are the property of their respective owners.", "links": ["https://www.safaribooksonline.com/home/?utm_medium=content&utm_source=oreilly.com&utm_campaign=lgen&utm_content=20170601+nav", "http://www.oreilly.com/conferences/", "http://shop.oreilly.com/", "https://www.safaribooksonline.com/home/?utm_medium=content&utm_source=oreilly.com&utm_campaign=lgen&utm_content=20170601+nav", "http://www.oreilly.com/conferences/", "http://shop.oreilly.com/", "https://pixabay.com/en/louvre-pyramid-mesh-perspective-2189967/", "https://www.safaribooksonline.com/library/view/oscon-2017-/9781491976227/video306664.html?utm_source=oreilly&utm_medium=newsite&utm_campaign=building-a-simple-graphql-server-with-neo4j", "https://github.com/apollographql/graphql-tools", "http://www.apollodata.com/", "http://graphql.org/", "https://dev-blog.apollodata.com/", "https://neo4j.com/sandbox-v2/", "https://github.com/johnymontana/movies-graphql-neo4j-server/blob/master/package.json", "http://dev.apollodata.com/tools/graphql-tools/index.html", "https://expressjs.com/", "https://github.com/neo4j/neo4j-javascript-driver", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://www.graph.cool/docs/faq/graphql-sdl-schema-definition-language-kr84dktnp0/", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "http://loalhost:8080/graphiql", "https://www.npmjs.com/package/neo4j-graphql-cli", "http://neo4j.com/developer/graphql", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://pixabay.com/en/louvre-pyramid-mesh-perspective-2189967/", "https://twitter.com/share", "http://oreilly.com/about/", "http://oreilly.com/work-with-us.html", "http://oreilly.com/careers/", "http://shop.oreilly.com/category/customer-service.do", "http://shop.oreilly.com/category/customer-service.do", "http://fb.co/OReilly", "http://twitter.com/oreillymedia", "https://www.youtube.com/user/OreillyMedia", "https://plus.google.com/+oreillymedia", "https://www.linkedin.com/company/oreilly-media", "http://oreilly.com/terms/", "http://oreilly.com/privacy.html", "http://www.oreilly.com/about/editorial_independence.html"]}, "213": {"url": "http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "title": "", "text": "", "links": ["http://www.getrevue.co/?ref=Revue+Profile", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profileimage&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profiledate&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profilename&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lXPZA?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lXPZA?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blogs.technet.microsoft.com/machinelearning/2017/07/25/lessons-learned-benchmarking-fast-machine-learning-algorithms/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/DM9WJ?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/DM9WJ?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://medium.com/@stathis/design-by-evolution-393e41863f98?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/XYWKo?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/XYWKo?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.datasciencelearner.com/complete-overview-learning-python-data-analysis/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/6nba0?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/6nba0?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://reinforce.io/blog/introduction-to-tensorforce/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xekd3?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xekd3?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.kdnuggets.com/2017/05/top-10-machine-learning-videos-on-youtube-updated.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Ka8JK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Ka8JK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://arxiv.org/abs/1704.01568?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/96KED?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/96KED?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://datascienceplus.com/random-forests-in-r/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Vd1wK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Vd1wK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blog.keras.io/the-future-of-deep-learning.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/eKr5e?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/eKr5e?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://medium.com/airbnb-engineering/using-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/odRNK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/odRNK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://machinelearning.apple.com/2017/07/07/GAN.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xekoa?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xekoa?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://arxiv.org/abs/1707.06642?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/mxRw2?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/mxRw2?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.techleer.com/articles/200-naive-bayes-machine-learning-algorithm-for-classification-problems/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/3K1nx?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/3K1nx?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463?gi=7fe02170b69e&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/OnXK7?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/OnXK7?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://aiukraine.com/en/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/nqRVq?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/nqRVq?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.bonaccorso.eu/2017/07/23/machine-learning-algorithms/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Z4bBj?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Z4bBj?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/jobs", "http://bit.ly/2uqHV4J", "https://flyelephant.net/jobs/Spinbackup-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/jobs/Spinbackup-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/yJRmd?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/yJRmd?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/jobs/Snap-Ukraine-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/jobs/Snap-Ukraine-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/jAR3B?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/jAR3B?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%238%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "https://www.getrevue.co/?utm_source=Data Science Digest&utm_medium=email&utm_content=footerlink&utm_campaign=Issue", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=forwarded&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%238%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "http://www.getrevue.co/?ref=Revue+Profile"]}, "214": {"url": "https://blog.insightdatascience.com/preparing-for-the-transition-to-applied-ai-d41e48403447", "title": "Transitioning from Software Engineering to Artificial Intelligence", "text": "Emmanuel Ameisen and Jeremy Karnowski, Insight AI A significant part of the Software Engineer role requires staying up-to-date with evolving frameworks, standards, and paradigms. Software Engineers strive to constantly learn, in order to always use the best tool for the job. As Machine Learning finds footholds in more applications every day, it has naturally become a topic that many Engineers want to master. Machine Learning, though, is harder to pick up than a new framework. To be an efficient practitioner, you require a solid understanding of the theory of the field, broad knowledge of the current state of the art, and an ability to frame problems in a non deterministic way. Many guides you can find online will simply teach you how to train an out-of-the-box model on a curated data set to achieve good accuracy and call it a day. The truth is that a much more extensive skillset is essential in becoming an effective Machine Learning Engineer. Below is a distillation of the many conversations we\u2019ve had with over 50 top Machine Learning teams all over The Bay Area and New York, who\u2019ve come to Insight to find AI Practitioners poised to tackle their problems and accelerate their expansion into Applied AI. Deploying a Machine Learning solution requires much more than just training an arbitrary model on your data. It requires an understanding of: In other words, in addition to engineering chops, you need to understand the fundamentals of statistics, linear algebra, and optimization theory in order to integrate, deploy, and debug models. Building a custom Machine Learning solution for a problem requires that you consider issues ranging from acquiring, labeling and pre-processing your data to building, updating, and serving an inference model, and everything in between. Finally, building a REST API for a standard web-app is a task that we can deem feasible ahead of time. Machine Learning models, on the other hand are not always guaranteed to converge, or produce usable outputs. The best way to learn how to scope and deliver impactful Machine Learning products, is to understand how their theoretical underpinnings relate to the taxonomy of your data. In order to understand Machine Learning, a solid knowledge of statistics fundamentals is essential. This involves understanding the following: When you are training a neural network, what is actually happening? What makes some tasks doable and others not? A good approach to this might be to first try to understand Machine Learning through graphics and examples, before diving deeper into the theory. Concepts to understand range from how different loss functions work, why back propagation is useful, or what a computational graph is. A deep understanding is crucial both for building a functional model, and to communicate about it efficiently to the rest of the organization. Following are a few resources, starting with high level overviews, and diving deeper. Another fundamental skill is the ability to read, understand and implement research papers. It can seem like a daunting task at first, so a good way to start is to look up a paper that already has code attached to it (on GitXiv for example) and try to understand the implementation in depth. Ask any Data Scientist and they\u2019ll tell you 90% of the work they do is data munging. This is just as important for Applied AI, as the success of your model correlates hugely with the quality (and quantity) of your data. Data work comes in many aspects, and falls within a few categories: The best way to get familiar with data wrangling is to grab a dataset in the wild and try to use it. There are many datasets online and many social media and news outlets sites have great APIs. Following the steps above, a good way to learn is to: Debugging Machine Learning algorithms that fail to converge or to give sensible results involves a very different process from debugging code. In the same vein, finding the right architecture and hyperparameters requires solid theoretical fundamentals, but also good infrastructure work to be able to test different configurations out. Because of the pace at which the fields evolve, the methods to debug models are constantly evolving. Here are a few \u201csanity checks\u201d from our discussions and experience deploying models that mirror in some ways the principles of KISS familiar to many Software Engineers. A lot of those steps can be accelerated significantly by your development skills, which brings us to our last skill. A lot of Applied Machine Learning will allow you to leverage Software Engineering skills, sometimes with a little twist. These skills include: For more details on some of the software skills we recommend acquiring to become a quality Machine Learning Engineer, check out our post dedicated to transitioning to Applied AI from Academia. The resources above will help you approach and tackle actual Machine Learning problems. But the field of Applied AI changes extremely quickly, and the best way to learn, is to get your hands dirty and actually try to build out an end-to-end solution to solve a real problem. Action Items: Find some inspiration, then dive in! Remember that while Machine Learning Engineering is about building products at heart, there is a research aspect to it. You will explore models and paradigms that will prove unsuccessful, and that is perfectly fine, as it will lead you to understand the intricacies of the problem better. AI is an exciting, ever-changing field. The demand for Machine Learning Engineers is strong, and it is easy to get overwhelmed with the amount of news surrounding the topic. We recommend following a few serious sources and newsletters, to be able to separate PR and abstract research from innovations that are immediately relevant to the field. Here are some sources to help out: Want to learn applied Artificial Intelligence from top professionals in Silicon Valley or New York? Learn more about the Artificial Intelligence program. Clapping shows how much you appreciated Emmanuel Ameisen\u2019s story.", "links": ["https://blog.insightdatascience.com?source=logo-lo_a3f738834672---d02e65779d7b", "https://twitter.com/InsightDataSci", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.insightdatascience.com%2Fpreparing-for-the-transition-to-applied-ai-d41e48403447", "https://blog.insightdatascience.com", "https://blog.insightdatascience.com/tagged/about-insight", "https://blog.insightdatascience.com/tagged/insight-data-science", "https://blog.insightdatascience.com/tagged/insight-data-engineering", "https://blog.insightdatascience.com/tagged/insight-health-data", "https://blog.insightdatascience.com/tagged/insight-ai", "https://blog.insightdatascience.com/search", "https://blog.insightdatascience.com/@emmanuelameisen?source=post_header_lockup", "https://blog.insightdatascience.com/@emmanuelameisen?source=post_header_lockup", "https://www.udacity.com/course/deep-learning--ud730", "http://cs231n.stanford.edu/", "http://cs224d.stanford.edu/", "http://www.deeplearningbook.org/", "http://www.gitxiv.com/", "https://data.sfgov.org/", "https://www.data.gov/", "https://dev.twitter.com/streaming/public", "http://developer.nytimes.com/docs", "http://people.apache.org/~fhanik/kiss.html", "http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf", "https://github.com/JasperSnoek/spearmint", "https://blog.insightdatascience.com/preparing-for-the-transition-to-applied-ai-8eaf53624079", "https://news.ycombinator.com/", "https://jack-clark.net/import-ai/", "http://www.insightdatascience.com/blog/", "https://blog.insightdatascience.com/tagged/insight-ai", "https://blog.insightdatascience.com/using-deep-learning-to-reconstruct-high-resolution-audio-29deee8b7ccd", "https://blog.insightdatascience.com/separating-overlapping-chromosomes-with-deep-learning-based-image-segmentation-22f97afd3283", "http://insightdata.ai/notifications-list", "http:insightdata.ai", "https://medium.com/@jkarnows?source=post_page", "https://medium.com/@InsightData?source=post_page", "https://medium.com/@rossfadely?source=post_page", "https://blog.insightdatascience.com/tagged/machine-learning?source=post", "https://blog.insightdatascience.com/tagged/ai?source=post", "https://blog.insightdatascience.com/tagged/software-engineering?source=post", "https://blog.insightdatascience.com/tagged/insight-ai?source=post", "https://blog.insightdatascience.com/tagged/deep-learning?source=post", "https://blog.insightdatascience.com/@emmanuelameisen?source=footer_card", "https://blog.insightdatascience.com/@emmanuelameisen", "http://twitter.com/EmmanuelAmeisen", "https://blog.insightdatascience.com?source=footer_card", "https://blog.insightdatascience.com?source=footer_card", "https://blog.insightdatascience.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "215": {"url": "http://vooban.com/en/tips-articles-geek-stuff/hyperopt-tutorial-for-optimizing-neural-networks-hyperparameters/", "title": "", "text": "Hyperopt is a way to search through an hyperparameter space. For example, it can use the Tree-structured Parzen Estimator (TPE) algorithm, which explore intelligently the search space while narrowing down to the estimated best parameters. It is hence a good method for meta-optimizing a neural network which is itself an optimisation problem: tuning a neural network uses gradient descent methods, and tuning the hyperparameters needs to be done differently since gradient descent can\u2019t apply. Therefore, Hyperopt can be useful not only for tuning hyperparameters such as the learning rate, but also to tune more fancy parameters in a flexible way, such as changing the number of layers of certain types, or the number of neurons in a layer, or even the type of layer to use at a certain place in the network given an array of choices, each with nested tunable hyperparameters. This is an oriented random search, in contrast with a Grid Search where hyperparameters are pre-established with fixed steps increase. Random Search for Hyper-Parameter Optimization (such as what Hyperopt do) has proven to be an effective search technique. The paper about this technique sits among the most cited deep learning papers. To sum up, it is more efficient to search randomly through values and to intelligently narrow the search space rather than looping on fixed sets of values for the hyperparameters. Note that this blog post is also available as a Notebook on GitHub. It contains code that can be run with Jupyter. A parameter is defined with a certain uniformrange or else a probability distribution, such as: There is also a few quantized versions of those functions, which rounds the generated values at each step of \u201cq\u201d: It is also possible to use a \u201cchoice\u201d which can lead to hyperparameter nesting: Visualisations of the parameters for probability distributions can be found below. Then, more details on choices and parameter nesting will come. Note on the above charts (especially for the loguniform and uniform distributions): the blurred line averaging the values fades out toward the ends of the signal since it is zero-padded. The line ideally would not fade out by using techniques such as mirror-padding. Those are the best distributions for modeling the values a learning rate. That\u2019s because we want to observe changes in the learning rate according to changing it with multiplications rather than additions, e.g.: when adjusting the learning rate, we\u2019ll want to try to divide it or multiply it by 2 rather than adding and substracting a finite value. To proove this, let\u2019s generate a loguniform distribution for a multiplier of the learning rate, centered at 1.0. Dividing 1 by those values should yield the same distribution. Let\u2019s now define a simple search space and solve for f(x) = x^2 - x + 1, where x is an hyperparameter. Let\u2019s solve for minimizing f(x, y) = x^2 + y^2 using a space using a python dict as structure. Later, this will neable us to nest hyperparameters with choices in a clean way. Yet, we have defined spaces as a single parameter. But that is 1D. Normally, spaces contain many parameters. Let\u2019s define a more complex one and with one nested hyperparameter choice for an uniform float: This will require us to import a few more things, and return the results with a dict that has a \u201cstatus\u201d and \u201closs\u201d key at least. Let\u2019s keep in our return dict the evaluated space too as this may come in handy if we save results to disk. Note that the optimization could be parallelized by using MongoDB and storing the trials\u2019 state here. Althought this is a built-in feature of hyperopt, let\u2019s keep things simple for our examples here. Indeed, the TPE algorithm used by the fmin function has state which is stored in the trials and which is useful to narrow the search space dynamically once we have a few trials. It is then interesting to pause and resume a training, and to apply that to a real problem. This is what\u2019s done inside the hyperopt_optimize.py file of the GitHub repository for this project. There, as an example, we optimize a convolutional neural network for solving the CIFAR-100 problem. Star Fork You might as well like this other blog post of mine on how to use Git Large File Storage (Git LFS) to handle the versioning of huge files when working with machine learning projects. 510-1015 av. Wilfrid-Pelletier  Quebec, QC, Canada  G1W 0C4 418 800.0027 TOLL FREE: 1 844 800.0027 info@vooban.com", "links": ["http://vooban.com/en/", "http://vooban.com/en/", "http://vooban.com/en/", "http://vooban.com/en/products/", "http://vooban.com/en/products/#tpti", "http://vooban.com/en/products/#tdti", "http://vooban.com/en/approach/", "http://vooban.com/en/tactical-squad/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/", "http://vooban.com/en/tips-articles-geek-stuff/", "https://pdfs.semanticscholar.org/9f2a/efc3821853e963beda011ed770f740385b77.pdf", "http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a", "https://github.com/terryum/awesome-deep-learning-papers#optimization--training-techniques", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100/blob/master/IntroductionToHyperopt.ipynb", "http://jupyter.org/", "http://vooban.com/wp-content/uploads/2017/08/histogram_1_int.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_2_uniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_3_loguniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_4_normal.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_5_lognormal.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_6_inverse_loguniform.png", "http://vooban.com/wp-content/uploads/2017/08/histogram_7_inverse_lognormal.png", "http://vooban.com/wp-content/uploads/2017/08/fx2.png", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100", "https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100/fork", "http://vooban.com/en/tips-articles-geek-stuff/migrating-to-git-lfs-for-developing-deep-learning-applications-with-large-files/", "http://vooban.com/en/tips-articles-geek-stuff/discover-structure-behind-data-with-decision-trees/", "http://vooban.com/en/tips-articles-geek-stuff/", "https://www.google.ca/maps/place/Vooban/@46.7694849,-71.3055195,17z/data=!3m1!4b1!4m5!3m4!1s0x4cb890d721a9234b:0x979022270b4a9fa4!8m2!3d46.7694849!4d-71.3033308", "http://vooban.com/en/products/", "http://vooban.com/en/approach/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/", "https://twitter.com/vooban", "https://www.instagram.com/voobanvibe/", "https://www.facebook.com/vooban", "https://www.linkedin.com/company/vooban", "http://vooban.com/en/", "http://vooban.com/en/products/", "http://vooban.com/en/products/#tpti", "http://vooban.com/en/products/#tdti", "http://vooban.com/en/approach/", "http://vooban.com/en/tactical-squad/", "http://vooban.com/en/company-services/", "http://vooban.com/en/content/", "http://vooban.com/en/vibe/", "http://vooban.com/en/contact-us/"]}, "216": {"url": "https://elitedatascience.com/learn-python-for-data-science", "title": "How to Learn Python for Data Science in 2017 (Updated)", "text": "In this guide, we\u2019ll cover how to learn Python for data science, including our favorite curriculum for self-study. You see, data science is about problem solving, exploration, and extracting valuable information from data. To do so effectively, you\u2019ll need to wrangle\u00a0datasets, train machine learning models, visualize results, and much more. Enter Python. This is the best time ever to learn Python. In fact, Forbes named it a top 10 technical skill in terms of job demand growth. Let\u2019s discuss why\u2026 Python is one of the most widespread languages in the world, and it has a passionate community of users: Python Popularity, TIOBE Index It has an even more loyal following within the data science profession. Some people judge the quality of a programming language by the simplicity of its \"hello, world!\" program. Python does pretty well by this standard: For comparison, here's the same output in Java: Great, case closed. See you back here\u00a0after you've mastered Python? Well, in all seriousness, simplicity is one of Python's greatest strengths. Thanks to its precise and efficient syntax, Python can accomplish the same tasks with less code than other languages. This makes implementing\u00a0solutions refreshingly fast. In addition, Python's vibrant data science community means you'll be able to find plenty of tutorials, code snippets, and people to commiserate with\u00a0fixes to common bugs. Stackoverflow will be one of your best friends. Finally,\u00a0Python\u00a0has an\u00a0all-star lineup of libraries (a.k.a. packages) for data analysis and machine learning, which drastically reduce the time it takes to produce results. More on these later. Before we go into what you'll need to learn, let's discuss what you won't need. You won't need a C.S. degree. Most data scientists will never deal with topics such as memory leaks, cryptography, or \"Big O\" notation. You'll be fine as long as you can write clean, logical code in a scripting language such as Python or R. You won't need a complete course on Python. Python and data science are\u00a0not synonymous. You won't need to memorize all the syntax. Instead, focus on grasping the intuition, such as when function is appropriate or how conditional statements work. You'll gradually remember the syntax after Googling, reading documentation, and good ol' fashioned practice. We recommend a top-down approach. We advocate\u00a0a top-down approach with the goal of getting results first and then solidifying concepts over time. In fact, we prefer to cut out \"classroom\" study in favor of real-world practice. This approach will allow you to build mastery over time while having more fun. There are many ways to install Python on your computer, but we recommend the Anaconda bundle, which comes with the libraries you'll need for data science. Effective programming is not about memorizing syntax, but rather mastering a new way of thinking. Therefore, take your time in building a solid foundation of core programming concepts. These will help you translate solutions in your head into instructions for a computer. If you are completely new to programming, we recommend the excellent Automate the Boring Stuff with Python book, which has been released for free online under a creative commons license. The book promises \"practical programming for total beginners,\" and it keeps each lesson down-to-earth. Read up to Chapter 6 - Manipulating Strings and complete the practice questions along the way. If you only need to brush up on Python syntax, then we recommend the following video, aptly named \"Learn Python in One Video:\" Again, the goal of this step is not to learn everything about Python and programming. Instead, focus on the intuition. You should be able to answer questions such as: If you'd like more practice with the core programming concepts, check out the following resources. Next, we're going to focus on the for data science part of \"how to learn Python for data science.\" As we mentioned earlier, Python has an all-star lineup of libraries for data science. Libraries are simply bundles of pre-existing functions and objects that you can import into your script to save time. These are the action steps we recommend for efficiently picking up a new library: We don't recommend diving much deeper into a library right now because you'll likely forget most of what you've learned by the time you jump into projects. Instead, aim to discover what each library is capable of. If you installed Python through the Anaconda bundle as we recommended above, it will also come with Jupyter Notebook. Jupyter Notebook is a lightweight IDE that's a favorite among data scientists. We recommend it for your projects. You can open a new notebook through Anaconda Navigator, which came with Anaconda. Check out this short video for instructions. These are the essential libraries you'll need: NumPy allows easy and efficient numeric computation, and many other data science libraries are built on top of it. Pandas is high-performance library for data structures and exploratory analysis. It's built on top of NumPy. Matplotlib is a flexible plotting and visualization library. It's powerful but somewhat cumbersome. You have the option of skipping Matplotlib for now and using Seaborn to get started (see our Seaborn recommendation below). Scikit-Learn is the premier general-purpose machine learning library in Python. It has many popular algorithms and modules for pre-processing, cross-validation, and much more. Seaborn makes it much easier to plot common data visualizations. It's built on top of Matplotlib and offers a more pleasant high-level wrapper. By now, you'll have a basic understanding of programming and a working knowledge of essential libraries. This actually covers most of the Python you'll need to get started with data science. At this point, some students will feel a bit overwhelmed. That's OK, and it's perfectly normal. If you were to take the slow and traditional bottom-up approach, you might feel less overwhelmed, but it would have taken you 10 times as long to get here. Now the key is to dive in immediately and start gluing everything together. Again, our goal up to here has been to just learn enough to get started. Next, it's time to solidify your knowledge through plenty of practice and projects. You have several options. The first option is to participate on Kaggle, a site that hosts data science competitions. The main advantage of Kaggle is that every project is self-contained. You're given the dataset, a goal, and tutorials to get you started. The major disadvantage of competitions is that they're usually not representative of real-world data science. The \"Getting Started\" competitions are way too basic while the standard competitions (i.e. those with prize pools) are usually too tough for beginners. If you're interested in this path, check out our Beginner's Guide to Kaggle. The next option is to structure your own projects and pick datasets that interest you. The main advantage of this approach is that the projects are more representative of real-world data science. You'll likely need to define your own goals, collect data, clean your dataset, engineer features, and so on. The disadvantage of DIY projects is that you'll need to already be familiar with a proper data science workflow. Without one, you could miss important steps or get stuck without knowing how to proceed. If you go with this path, check out our free 7-day crash course on applied machine learning, which covers the key steps in a data science workflow. We also have another article with several\u00a0DIY project ideas. Finally, there are guided end-to-end projects. Proper guided projects should combine the best of both words - they should be representative of real-world data science and allow you to solidify your skills through a carefully planned learning curve. Many data science bootcamps offer this as a main benefit. Bootcamps usually conclude with a \"capstone project\" that allows you to see all the moving pieces together, from start to finish. We've also crafted our own\u00a0Machine Learning Masterclass to solve this exact need. It will provide you over-the-shoulder mentorship for real-world projects while teaching you all of the key concepts in context. The masterclass also includes a comprehensive Python course that gets you up to speed ASAP. In fact, many successful students have enrolled without any prior programming experience. Learn more about it here. [\u2026] In terms of programming languages \u2013 Python is pretty powerful language with integration to all technologies in the CDH stack. For a basic course check out the link below \u2013 Python For Data Science [\u2026] * Denotes Required Field Copyright \u00a9 2017 \u00b7 EliteDataScience.com \u00b7 All Rights Reserved", "links": ["https://elitedatascience.com/", "https://elitedatascience.com/", "https://elitedatascience.com/blog", "https://elitedatascience.com/machine-learning-masterclass", "https://elitedatascience.com/about", "https://elitedatascience.com/about", "https://elitedatascience.com/guest-post-submissions", "https://elitedatascience.com/author/slice2o", "https://elitedatascience.com/learn-python-for-data-science#comments", "https://www.facebook.com/sharer.php?u=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://plus.google.com/share?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://www.linkedin.com/shareArticle?trk=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://twitter.com/intent/tweet?text=How%20to%20Learn%20Python%20for%20Data%20Science%20%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "https://www.forbes.com/sites/jeffkauflin/2017/01/08/the-10-technical-skills-with-explosive-growth-in-job-demand/#6fa6bf344f5c", "http://www.tiobe.com/tiobe-index/", "https://stackoverflow.com/questions/tagged/python?sort=votes&pageSize=15", "https://www.continuum.io/downloads", "https://automatetheboringstuff.com/", "https://automatetheboringstuff.com/", "https://codefights.com/", "http://www.pythonchallenge.com/index.php", "http://www.practicepython.org/", "http://interactivepython.org/runestone/static/thinkcspy/index.html", "http://jupyter.org/", "https://www.youtube.com/watch?v=-MyjG00la2k", "https://docs.scipy.org/doc/numpy/user/", "https://docs.scipy.org/doc/numpy-dev/user/quickstart.html", "https://pandas.pydata.org/pandas-docs/stable/", "https://pandas.pydata.org/pandas-docs/stable/10min.html", "https://matplotlib.org/contents.html", "https://matplotlib.org/users/pyplot_tutorial.html", "http://scikit-learn.org/stable/documentation.html", "http://elitedatascience.com/python-machine-learning-tutorial-scikit-learn", "https://seaborn.pydata.org/", "http://elitedatascience.com/python-seaborn-tutorial", "http://elitedatascience.com/beginner-kaggle", "http://elitedatascience.com/", "http://elitedatascience.com/machine-learning-projects-for-beginners", "http://elitedatascience.com/machine-learning-masterclass", "http://elitedatascience.com/machine-learning-masterclass", "https://www.facebook.com/sharer.php?u=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://plus.google.com/share?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://www.linkedin.com/shareArticle?trk=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https%3A%2F%2Felitedatascience.com%2Flearn-python-for-data-science", "https://twitter.com/intent/tweet?text=How%20to%20Learn%20Python%20for%20Data%20Science%20%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "https://datascienceandmore.wordpress.com/2017/03/04/getting-the-basics-down/", "https://elitedatascience.com/learn-machine-learning", "https://elitedatascience.com/learn-python-for-data-science", "https://elitedatascience.com/feature-engineering-best-practices", "https://elitedatascience.com/beginner-kaggle", "https://elitedatascience.com/imbalanced-classes", "https://elitedatascience.com/beginner-mistakes", "https://elitedatascience.com/bias-variance-tradeoff", "https://elitedatascience.com/data-science-resources", "https://elitedatascience.com/guest-post-submissions", "https://elitedatascience.com/", "https://elitedatascience.com/terms-of-service", "https://elitedatascience.com/privacy-policy", "https://elitedatascience.com/about", "https://www.linkedin.com/shareArticle?trk=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science", "http://www.facebook.com/sharer.php?u=https://elitedatascience.com/learn-python-for-data-science", "https://plus.google.com/share?text=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science", "https://twitter.com/intent/tweet?text=How+to+Learn+Python+for+Data+Science+%28Updated%29&url=https://elitedatascience.com/learn-python-for-data-science", "http://service.weibo.com/share/share.php?url=https://elitedatascience.com/learn-python-for-data-science&title=How to Learn Python for Data Science (Updated)", "https://getpocket.com/save?title=How to Learn Python for Data Science (Updated)&url=https://elitedatascience.com/learn-python-for-data-science"]}, "217": {"url": "https://stats.stackexchange.com/questions/297380/why-not-just-dump-the-neural-networks-and-deep-learning", "title": "", "text": "Fundamental problem with deep learning and neural networks in general. The solutions that fit training data are infinite. We don't have precise mathematical equation that is satisfied by only a single one and that we can say generalizes best. Simply speaking we don't know which generalizes best. Optimizing weights is not a convex problem, so we never know we end up with a global or a local minimum. So why not just dump the neural networks and instead search for a better ML model? Something that we understand, and something that is consistent with a set of mathematical equations? Linear and SVM do not have this mathematical drawbacks and are fully consistent with a a set of mathematical equations. Why not just think on same lines (need not be linear though) and come up with a new ML model better than Linear and SVM and neural networks and deep learning? Not being able to know what solution generalizes best is an issue, but it shouldn't deter us from otherwise using a good solution. Humans themselves often do not known what generalizes best (consider, for example, competing unifying theories of physics), but that doesn't cause us too many problems. It has been shown that it is extremely rare for training to fail because of local minimums. Most of the local minimums in a deep neural network are close in value to the global minimum, so this is not an issue. source But the broader answer is that you can talk all day about nonconvexity and model selection, and people will still use neural networks simply because they work better than anything else (at least on things like image classification).  Of course there are also people arguing that we shouldn't get too focused on CNNs like the community was focused on SVMs a few decades ago, and instead keep looking for the next big thing. In particular, I think I remember Hinton regretting the effectiveness of CNNs as something which might hinder research. related post As the comments to your question point out, there are a lot of people working on finding something better. I would though like to answer this question by expanding the comment left by @josh All models are wrong but some are useful (Wiki) The above statement is a general truth used to describe the nature of statistical models. Using data that we have available, we can create models that let us do useful things such as approximate a predicted value. Take for example Linear Regression  Using a number of observations, we can fit a model to give us an approximate value for a dependent variable given any value(s) for the independent variable(s).  Burnham, K. P.; Anderson, D. R. (2002), Model Selection and Multimodel > Inference: A Practical Information-Theoretic Approach (2nd ed.): \"A model is a simplification or approximation of reality and hence   will not reflect all of reality. ... Box noted that \u201call models are   wrong, but some are useful.\u201d While a model can never be \u201ctruth,\u201d a   model might be ranked from very useful, to useful, to somewhat useful   to, finally, essentially useless.\" Deviations from our model (as can be seen in the image above) appear random, some observations are below the line and some are above, but our regression line shows a general correlation. Whilst deviations in our model appear random, in realistic scenarios there will be other factors at play which cause this deviation. For example, imagine watching cars as they drove through a junction where they must turn either left or right to continue, the cars turn in no particular pattern. Whilst we could say that the direction the cars turn is completely random, does every driver reach the junction and at that point make a random decision of which way to turn? In reality they are probably heading somewhere specific for a specific reason, and without attempting to stop each car to ask them about their reasoning, we can only describe their actions as random.  Where we are able to fit a model with minimal deviation, how certain can we be that an unknown, unnoticed or immeasurable variable wont at some point throw our model? Does the flap of a butterfly\u2019s wings in Brazil set off a tornado in Texas? The problem with using the Linear and SVN models you mention alone is that we are somewhat required to manually observe our variables and how they each affect each other. We then need to decide what variables are important and write a task-specific algorithm. This can be straight forward if we only have a few variables, but what if we had thousands? What if we wanted to create a generalised image recognition model, could this realistically be achieved with this approach?  Deep Learning and Artificial Neural Networks (ANNs) can help us create useful models for huge data sets containing huge amounts of variables (e.g. image libraries). As you mention, there's an incomprehensible number of solutions which could fit the data using ANNs, but is this number really any different to the amount of solutions we would need to develop ourselves through trial and error? The application of ANNs do much of the work for us, we can specify our inputs and our desired outputs (and tweak them later to make improvements) and leave it up to the ANN to figure out the solution. This is why ANNs are often described as \"black boxes\". From a given input they output an approximation, however (in general terms) these approximations don't include details on how they were approximated. And so it really comes down to what problem you are trying to solve, as the problem will dictate what model approach is more useful. Models are not absolutely accurate and so there is always an element of being 'wrong', however the more accurate your results the more useful they are. Having more detail in the results on how the approximation was made may also be useful, depending on the problem it may even be more useful than increased accuracy. If for example you are calculating a persons credit score, using regression and SVMs provides calculations that can be better explored. Being able to both tweak the model directly and explain to customers the effect separate independent variables have on their overall score is very useful. An ANN may aid in processing larger amounts of variables to achieve a more accurate score, but would this accuracy be more useful? I guess for some problem we care less for the mathematical rigor and simplicity but more for its utility, current status is neural network is better in performing certain task like pattern recognition in image processing.  The global minimum may as well as be useless, so we don't really care if we find it or not. The reason is that, for deep networks, not only the time to find it becomes exponentially longer as the network size increases, but also the global minimum often corresponds to overfitting the training set. Thus the generalization ability of the DNN (which is what we really care about) would suffer. Also, often we prefer flatter minima corresponding to a higher value of the loss function, than sharper minima corresponding to a lower value of the loss function, because the second one will deal very badly with uncertainty in the inputs. This is becoming increasingly clear with the development of Bayesian Deep Learning. Robust Optimization beats Determinist Optimization very often, when applied to real world problems where uncertainty is important. Finally, it's a fact that DNNs just kick the ass of methods such as XGBoost at image classification and NLP. A company which must make a profit out of image classification will correctly select them as modelsto be deployed in production (and invest a significant amount of money on feature engineering, data pipeline, etc. but I digress). This doesn't mean that they dominate all the ML environment: for example, they do worse than XGBoost on structured data (see the last winners of Kaggle competitions) and they seem to not still do as well as particle filters on time series modelling. However, some very recent innovations on RNNs may modify this situation. There are excellent answers, mostly weighing in with the usefulness of DL and ANNs. But I would like to object the OP in a more fundamental way, since the question already takes for granted the mathematical inconsistency of neural networks. First of all, there is a mathematical theory behind (most models of) Neural Networks. You could likewise argue that linear regression does not generalize, unless the underlying model is... well, linear. In neural algorithms, a model is assumed (even if not explicitly) and the fitting error is computed. The fact that algorithms are modified with various heuristics does not void the original mathematical support. BTW, local optimization is also a mathematically consistent, let alone useful, theory. Along this line, if Neural Networks just constitute one class of methods within the whole toolbox of scientists, which is the line that separates Neural Networks from the rest of techniques? In fact, SVMs were once considered a class of NNs and they still appear in the same books. On the other hand, NNs could be regarded as a (nonlinear) regression technique, maybe with some simplification. I agree with the OP that we must search better, well founded, efficient algorithms, regardless you label them as NNs or not. Sign up using Google Sign up using Facebook Sign up using Email and Password   By posting your answer, you agree to the privacy policy and terms of service. asked 3 days ago viewed  7,704 times  active yesterday                                   site design / logo \u00a9 2017 Stack Exchange Inc; user contributions licensed under cc by-sa 3.0                                  with attribution required.                                  rev 2017.8.11.26777", "links": ["https://stats.stackexchange.com", "https://chat.stackexchange.com?tab=site&host=stats.stackexchange.com", "https://stats.stackexchange.com", "https://stats.meta.stackexchange.com", "https://stats.stackexchange.com/users/signup?ssrc=site_switcher&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.stackexchange.com/users/login?ssrc=site_switcher&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stackexchange.com/sites", "https://stackoverflow.blog", "https://stackexchange.com", "https://stats.stackexchange.com/users/signup?ssrc=head&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.stackexchange.com/users/login?ssrc=head&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f297380%2fwhy-not-just-dump-the-neural-networks-and-deep-learning", "https://stats.meta.stackexchange.com", "https://stackoverflow.com/company/about", "https://www.stackoverflowbusiness.com/?ref=topbar_help", "https://stats.stackexchange.com", "https://arxiv.org/pdf/1406.2572.pdf", "https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/clyohpf/?context=3", "https://arxiv.org/pdf/1406.2572.pdf", "https://arxiv.org/pdf/1412.0233.pdf", "https://en.wikipedia.org/wiki/All_models_are_wrong", "https://en.wikipedia.org/wiki/Butterfly_effect", "https://stats.stackexchange.com/questions/93705/meaning-of-a-neural-network-as-a-black-box", "https://stackexchange.com/legal/privacy-policy", "https://stackexchange.com/legal/terms-of-service", "http://stats.stackexchange.com/election", "http://stats.stackexchange.com/election", "https://stackexchange.com/questions?tab=hot", "https://travel.stackexchange.com/questions/99713/in-germany-what-are-you-supposed-to-do-if-your-train-station-doesnt-have-any-w", "https://stackoverflow.com/questions/45629176/why-do-all-the-c-files-written-by-my-lecturer-start-with-a", "https://math.stackexchange.com/questions/2392309/closed-circle-as-a-metric-space", "https://security.stackexchange.com/questions/167422/is-receiving-fake-torrent-data-possible", "https://interpersonal.stackexchange.com/questions/1737/how-do-i-tell-a-co-worker-that-just-because-something-is-a-hobby-doesnt-mean-i", "https://academia.stackexchange.com/questions/94474/will-one-with-record-of-unfinished-phd-on-his-her-cv-be-doubted-of-his-her-abili", "https://english.stackexchange.com/questions/405590/does-een-denote-endearment", "https://math.stackexchange.com/questions/2392728/proving-that-the-limit-of-an-abstract-function-with-certain-properties-is-zero", "https://tex.stackexchange.com/questions/386263/suppress-globally-set-before-end-axis-code-in-pgfplots", "https://interpersonal.stackexchange.com/questions/1539/is-it-rude-to-ask-tourists-where-they-are-from", "https://scifi.stackexchange.com/questions/167116/tv-series-episode-where-a-stand-up-comedian-is-forced-to-perform-in-hell-for-all", "https://puzzling.stackexchange.com/questions/54195/measure-22-minutes-with-7-and-13-minute-hourglasses", "https://movies.stackexchange.com/questions/78712/do-penn-and-teller-learn-how-a-trick-works-after-the-show", "https://aviation.stackexchange.com/questions/42927/why-does-atc-ask-a-crew-who-has-declared-an-emergency-if-their-aircraft-will-be", "https://workplace.stackexchange.com/questions/96824/how-to-get-a-raise-i-am-denied-to-because-of-sick-leaves", "https://history.stackexchange.com/questions/39549/was-russia-the-only-country-in-wwii-that-succeeded-in-expansionist-war-aims", "https://space.stackexchange.com/questions/22590/how-did-people-know-how-to-build-the-first-space-ship", "https://electronics.stackexchange.com/questions/323757/audible-difference-between-audio-grade-capacitors-in-the-same-range", "https://tex.stackexchange.com/questions/386239/tikz-fractal-uniform-cantor-set", "https://worldbuilding.stackexchange.com/questions/88816/creature-that-swims-in-the-solid-ground", "https://money.stackexchange.com/questions/83755/do-people-tend-to-spend-less-when-using-cash-than-credit-cards", "https://german.stackexchange.com/questions/38460/why-are-some-words-spelled-with-tz-if-z-already-has-the-ts-sound", "https://blender.stackexchange.com/questions/87969/how-to-scale-two-objects-to-fit-together", "https://math.stackexchange.com/questions/2392029/why-does-rudin-say-the-rational-number-system-is-inadequate-as-a-field", "https://chat.stackexchange.com?tab=site&host=stats.stackexchange.com", "https://stats.meta.stackexchange.com", "https://stackoverflow.com/company/about", "https://stackoverflow.com", "https://www.stackoverflowbusiness.com/?utm_source=so-footer&utm_medium=referral&utm_campaign=brand-activation", "https://stackoverflow.com/jobs", "https://stackoverflow.com/company/about", "https://stackoverflow.com/company/press", "https://stackexchange.com/legal", "https://stackexchange.com/legal/privacy-policy", "https://stackexchange.com", "https://stackexchange.com/sites#technology", "https://stackexchange.com/sites#lifearts", "https://stackexchange.com/sites#culturerecreation", "https://stackexchange.com/sites#science", "https://api.stackexchange.com", "https://data.stackexchange.com", "https://stackoverflow.blog?blb=1", "https://www.facebook.com/officialstackoverflow/", "https://twitter.com/stackoverflow", "https://linkedin.com/company/stack-overflow", "https://creativecommons.org/licenses/by-sa/3.0/", "https://stackoverflow.blog/2009/06/25/attribution-required/"]}, "218": {"url": "https://blog.godatadriven.com/practical-airflow-tutorial", "title": "", "text": "Airflow is a scheduler for workflows such as data pipelines, similar to Luigi and Oozie. It's written in Python and we at GoDataDriven have been contributing to it in the last few months. This tutorial is loosely based on the Airflow tutorial in the official documentation. It will walk you through the basics of setting up Airflow and creating an Airflow workflow, and it will give you some practical tips. A (possibly) more up-to-date version of this blog can be found in my git repo. Setting up a basic configuration of Airflow is pretty straightforward. After installing the Python package, we'll need a database to store some data and start the core Airflow services. You can skip this section if Airflow is already set up. Make sure that you can run airflow commands, know where to put your DAGs and have access to the web UI. Airflow is installable with pip via a simple pip install apache-airflow. Either use a separate Python virtual environment or install it in your default python environment. To use the conda virtual environment as defined in environment.yml from my git repo: You should now have an (almost) working Airflow installation. Alternatively, install Airflow yourself by running: Airflow used to be packaged as airflow but is packaged as apache-airflow since version 1.8.1. Make sure that you install any extra packages with the right Python package: e.g. use pip install apache-airflow[dask] if you've installed apache-airflow and do not use pip install airflow[dask]. Leaving out the prefix apache- will install an old version of Airflow next to your current version, leading to a world of hurt. You may run into problems if you don't have the right binaries or Python packages installed for certain backends or operators. When specifying support for e.g. PostgreSQL when installing extra Airflow packages, make sure the database is installed; do a brew install postgresql or apt-get install postgresql before the pip install apache-airflow[postgres]. Similarly, when running into HiveOperator errors, do a pip install apache-airflow[hive] and make sure you can use Hive. Before you can use Airflow you have to initialize its database. The database contains information about historical & running workflows, connections to external data sources,  user management, etc. Once the database is set up, Airflow's UI can be accessed by running a web server and workflows can be started. The default database is a SQLite database, which is fine for this tutorial. In a production setting you'll probably be using something like MySQL or PostgreSQL. You'll probably want to back it up as this database stores the state of everything related to Airflow. Airflow will use the directory set in the environment variable AIRFLOW_HOME to store its configuration and our SQlite database. This directory will be used after your first Airflow command. If you don't set the environment variable AIRFLOW_HOME, Airflow will create the directory ~/airflow/ to put its files in. Set environment variable AIRFLOW_HOME to e.g. your current directory $(pwd): or any other suitable directory. Next, initialize the database: Now start the web server and go to localhost:8080 to check out the UI: It should look something like this:  With the web server running workflows can be started from a new terminal window. Open a new terminal, activate the virtual environment and set the environment variable AIRFLOW_HOME for this terminal as well: Make sure that you're an in the same directory as before when using $(pwd). Run a supplied example: And check in the web UI that it has run by going to Browse -> Task Instances. This concludes all the setting up that you need for this tutorial. We'll create a workflow by specifying actions as a Directed Acyclic Graph (DAG) in Python. The tasks of a workflow make up a Graph; the graph is Directed because the tasks are ordered; and we don't want to get stuck in an eternal loop so the graph also has to be Acyclic. The figure below shows an example of a DAG:  The DAG of this tutorial is a bit easier. It will consist of the following tasks: and we'll plan daily execution of this workflow. Go to the folder that you've designated to be your AIRFLOW_HOME and find the DAGs folder located in subfolder dags/ (if you cannot find, check the setting dags_folder in $AIRFLOW_HOME/airflow.cfg). Create a Python file with the name airflow_tutorial.py that will contain your DAG. Your workflow will automatically be picked up and scheduled to run. First we'll configure settings that are shared by all our tasks. Settings for tasks can be passed as arguments when creating them, but we can also pass a dictionary with default values to the DAG. This allows us to share default arguments for all the tasks in our DAG is the best place to set e.g. the owner and start date of our DAG. Add the following import and dictionary to airflow_tutorial.py to specify the owner, start time, and retry settings that are shared by our tasks: These settings tell Airflow that this workflow is owned by 'me', that the workflow is valid since June 1st of 2017, it should not send emails and it is allowed to retry the workflow once if it fails with a delay of 5 minutes. Other common default arguments are email settings on failure and the end time. We'll now create a DAG object that will contain our tasks. Name it airflow_tutorial_v01 and pass default_args: With schedule_interval='0 * * * *' we've specified a run at every hour 0; the DAG will run each day at 00:00. See crontab.guru for help deciphering cron schedule expressions. Alternatively, you can use strings like '@daily' and '@hourly'. We've used a context manager to create a DAG (new since 1.8). All the tasks for the DAG should be indented to indicate that they are part of this DAG. Without this context manager you'd have to set the dag parameter for each of your tasks. Airflow will generate DAG runs from the start_date with the specified schedule_interval. Once a DAG is active, Airflow continuously checks in the database if all the DAG runs have successfully ran since the start_date. Any missing DAG runs are automatically scheduled. When you initialize on 2016-01-04 a DAG with a start_date at 2016-01-01 and a daily schedule_interval, Airflow will schedule DAG runs for all the days between 2016-01-01 and 2016-01-04. A run starts after the time for the run has passed. The time for which the workflow runs is called the execution_date. The daily workflow for 2016-06-02 runs after 2016-06-02 23:59 and the hourly workflow for 2016-07-03 01:00 starts after 2016-07-03 01:59. From the ETL viewpoint this makes sense: you can only process the daily data for a day after it has passed. This can, however, ask for some juggling with date for other workflows. For Machine Learning models you may want to use all the data up to a given date, you'll have to add the schedule_interval to your execution_date somewhere in the workflow logic. Because Airflow saves all the (scheduled) DAG runs in its database, you should not change the start_date and schedule_interval of a DAG. Instead, up the version number of the DAG (e.g. airflow_tutorial_v02) and avoid running unnecessary tasks by using the web interface or command line tools Timezones and especially daylight savings can mean trouble when scheduling things, so keep your Airflow machine in UTC. You don't want to skip an hour because daylight savings kicks in (or out). Tasks are represented by operators that either perform an action, transfer data, or sense if something has been done. Examples of actions are running a bash script or calling a Python function; of transfers are copying tables between databases or uploading a file; and of sensors are checking if a file exists or data has been added to a database. We'll create a workflow consisting of three tasks: we'll print 'hello', wait for 10 seconds and finally print 'world'.  The first two are done with the BashOperator and the latter with the PythonOperator. Give each operator an unique task ID and something to do: Note how we can pass bash commands in the BashOperator and that the PythonOperator asks for a Python function that can be called. Dependencies in tasks are added by setting other actions as upstream (or downstream).  Link the operations in a chain so that sleep will be run after print_hello and is followed by print_world; print_hello -> sleep -> print_world: After rearranging the code your final DAG should look something like: First check that DAG file contains valid Python code by executing the file with Python: You can manually test a single task for a given execution_date with airflow test: This runs the task locally as if it was for 2017-07-01, ignoring other tasks and without communicating to the database. Now that you're confident that your dag works, turn on the DAG in the web UI and sit back while Airflow starts backfilling the dag runs! You now know the basics of setting up Airflow, creating a DAG and turning it on; time to go deeper!  Address: Wibautstraat 202, 1091 GS Amsterdam, The Netherlands           Phone: +31 (0)35 672 9069           Email: signal@godatadriven.com", "links": ["http://godatadriven.com/contact", "http://godatadriven.com/careers", "http://blog.godatadriven.com", "http://godatadriven.com/business-consultancy", "http://godatadriven.com/big-data-infrastructure", "http://godatadriven.com/predictive-modelling", "http://godatadriven.com/products-overview", "http://godatadriven.com/data-discovery", "http://godatadriven.com/data-science-audit", "http://godatadriven.com/data-science-suite", "http://godatadriven.com/data-science-box", "http://godatadriven.com/divolte", "http://godatadriven.com/customers", "http://godatadriven.com/casestudy-airfranceklm", "http://godatadriven.com/casestudy-bakkersland", "http://godatadriven.com/casestudy-bol", "http://godatadriven.com/casestudy-eneco", "http://godatadriven.com/casestudy-eretail", "http://godatadriven.com/casestudy-ing", "http://godatadriven.com/casestudy-nuon", "http://godatadriven.com/casestudy-npo", "http://godatadriven.com/casestudy-schipholgroup", "http://godatadriven.com/casestudy-transavia", "http://godatadriven.com/technology-overview", "http://godatadriven.com/databricks", "http://godatadriven.com/hadoop", "http://godatadriven.com/python", "http://godatadriven.com/r", "http://godatadriven.com/spark", "http://godatadriven.com/training-overview", "http://godatadriven.com/training-schedule", "http://godatadriven.com/bigdata-training", "http://godatadriven.com/datascience-training", "http://godatadriven.com/data-science-accelerator-program", "http://godatadriven.com/meetups", "http://godatadriven.com/events", "http://blog.godatadriven.com", "http://godatadriven.com/our-story", "http://godatadriven.com/team", "http://godatadriven.com/careers", "http://godatadriven.com/media-resources", "https://blog.godatadriven.com", "http://godatadriven.com/contact", "http://godatadriven.com", "http://blog.godatadriven.com", "https://airflow.incubator.apache.org/", "https://github.com/spotify/luigi", "https://oozie.apache.org/", "https://blog.godatadriven.com/open-source-201702", "https://blog.godatadriven.com/open-source-201703", "https://blog.godatadriven.com/open-source-201704", "https://blog.godatadriven.com/open-source-201705", "https://blog.godatadriven.com/open-source-201707", "https://pythonhosted.org/airflow/tutorial.html", "https://github.com/hgrif/airflow-tutorial", "https://github.com/hgrif/airflow-tutorial", "http://conda.pydata.org/miniconda.html", "http://localhost:8080/", "https://airflow.incubator.apache.org/configuration.html", "https://airflow.incubator.apache.org/security.html", "https://github.com/apache/incubator-airflow/tree/master/scripts", "https://crontab.guru/#0_*_*_*_*", "https://jeffknupp.com/blog/2016/03/07/python-with-context-managers/", "https://airflow.incubator.apache.org/configuration.html#connections", "https://airflow.incubator.apache.org/ui.html#variable-view", "https://airflow.incubator.apache.org/tutorial.html#templating-with-jinja", "https://github.com/apache/incubator-airflow/blob/master/airflow/example_dags/example_bash_operator.py", "https://airflow.incubator.apache.org/code.html#airflow.operators.PythonOperator", "https://pythonhosted.org/airflow/tutorial.html", "https://gtoonstra.github.io/etl-with-airflow/", "https://medium.com/handy-tech/airflow-tips-tricks-and-pitfalls-9ba53fba14eb", "http://godatadriven.com/players/henk-griffioen", "http://blog.godatadriven.com/practical-airflow-tutorial", "http://blog.godatadriven.com/open-source-201707", "http://blog.godatadriven.com/cd-python-eggs-vsts", "http://blog.godatadriven.com/ldappy", "http://blog.godatadriven.com/open-source-201706", "http://blog.godatadriven.com/vendor-free-ds", "https://twitter.com/GoDataDriven", "http://godatadriven.com/business-consultancy", "http://godatadriven.com/big-data-infrastructure", "http://godatadriven.com/predictive-modelling", "http://godatadriven.com/products-overview", "http://godatadriven.com/data-discovery", "http://godatadriven.com/data-science-audit", "http://godatadriven.com/data-science-suite", "http://godatadriven.com/data-science-box", "http://godatadriven.com/divolte", "http://godatadriven.com/customers", "http://godatadriven.com/casestudy-airfranceklm", "http://godatadriven.com/casestudy-bakkersland", "http://godatadriven.com/casestudy-bol", "http://godatadriven.com/casestudy-eneco", "http://godatadriven.com/casestudy-eretail", "http://godatadriven.com/casestudy-ing", "http://godatadriven.com/casestudy-nuon", "http://godatadriven.com/casestudy-npo", "http://godatadriven.com/casestudy-schipholgroup", "http://godatadriven.com/casestudy-transavia", "http://godatadriven.com/technology-overview", "http://godatadriven.com/hadoop", "http://godatadriven.com/python", "http://godatadriven.com/r", "http://godatadriven.com/spark", "http://godatadriven.com/training-overview", "http://godatadriven.com/training-schedule", "http://godatadriven.com/bigdata-training", "http://godatadriven.com/datascience-training", "http://godatadriven.com/data-science-accelerator-program", "http://godatadriven.com/events", "http://godatadriven.com/meetups", "http://blog.godatadriven.com", "http://godatadriven.com/our-story", "http://godatadriven.com/team", "http://godatadriven.com/careers", "http://godatadriven.com/media-resources", "http://godatadriven.com/news", "http://godatadriven.com/contact", "https://cdn.xebia.com/documents/Cookies+Policy+Xebia+20170130.pdf"]}, "219": {"url": "https://concepttoclinic.drivendata.org/", "title": "", "text": "Lung Cancer Early Detection Challenge  We\u2019re calling on a global community of data scientists, engineers, designers, and researchers to build an open source software application that brings advances from machine learning into the clinic. We\u2019re not just optimizing an algorithm for a single metric\u2014we\u2019re collaborating to build tools which put AI in the hands of clinicians. In addition to pushing forward the cutting-edge of open clinical software, top contributors will be eligible for a share of $100,000 in monetary prizes generously provided by the Bonnie J. Addario Lung Cancer Foundation.          Contribute now by grabbing an issue from the project's GitHub repository and submitting a PR!        Lung cancer causes more deaths each year than any other cancer in the US, and early detection makes a big difference. Recent advances in machine learning \u2013 including open source algorithms from the 2017 Data Science Bowl \u2013 help find and interpret early signs of cancer. Using imagery from CT scans, this application will build on cutting-edge algorithms in machine learning to help clinicians identify and interpret lung nodules. Your guide to contributing throughout the challenge. All you need to start contributing is a passion for building great software and a github account.  Contributions of all sizes can earn points as they add value to the project, by building on pre-designated issues, building out new content, and building up the community. $100,000 in cash, as well as other in-kind prizes, will be available for top contributors filling key roles throughout the challenge. A live leaderboard keeps track of points earned throughout the challenge by top contributors filling key roles. Meet the amazing group of of technical, clinical, and domain experts that are helping to guide the end-to-end design of this challenge. The Addario Lung Cancer Foundation (ALCF) has set the audacious goal of making lung cancer a chronically managed disease by 2023.  Ultimately, the goal of this project is to produce a tool that draws on machine intelligence, is useful to clinicians, and makes a lasting difference in the lives of patients. We hope you will find a way to contribute!  Lung Cancer Early Detection Challenge", "links": ["https://www.drivendata.org/termsofuse/", "https://www.drivendata.org/copyrightpolicy/", "https://www.drivendata.org/privacypolicy/", "https://www.drivendata.org/", "http://www.drivendata.co/"]}, "220": {"url": "http://appsilondatascience.com/blog/rstats/2017/08/09/shiny-collections.html", "title": "shiny.collections,  Google Docs-like live collaboration in ShinyAppsilon Data Science Blog", "text": "A few weeks ago, our CTO Marek Rogala gave an engaging talk at the User Conference in Brussels. Despite being one of the last talks, he drew a crowd that filled the room and had significant viewership online. Marek\u2019s talk was entitled shiny.collections: Google Docs-like live collaboration in Shiny. In short, he went over a package we built that allows for persistence and collaboration without losing reactivity in Shiny. Users can enter information and have continuity, knowing that if they exit and return to the app, their inputs will still be there. Let\u2019s go over his talk. Recently, Shiny users have demanded more from their applications. The advent of cloud applications, especially ones like Google Docs, have accustomed users to collaboration. They expect their Shiny apps to be more than just an interactive tool for data visualisation, but rather, a production ready application that works like any other tool. Their data has to be saved automatically. Interactive collaboration is a must in some use cases. And the application must be delivered as fast as possible. You need to use a reactive database such as rethinkDB, Firebase, or mongoDB to achieve this. mongoDB isn\u2019t really a reactive database, but there are ways where it can be used as one. We are going to use rethinkDB in our case. Also, we are going to use rethinker, which is a rethinkDB driver for R. But using rethinker is not the most ideal solution, as callback do not work very well with Shiny and it is quite painful to configure to our needs. To get around this, we created our own package built upon rethinker called shiny.collections.  It lets you easily connect to your shiny app and takes care of all of the trickiness involved. During his talk, Marek showed a live demo that involved creating a chat application. Check out how to do this yourself. I reccomend watching Marek first, as he has some valuable commentary you won\u2019t find in the blog post. This example is just a simple use case of what you can achieve with shiny.collections. We\u2019ve made it a priority to make it convenient and easily integrable with other powerful tools, including DT, leaflet or rhandsontable. Our goals for the future of this package include getting it on cran and diversifying the functionality of the API. Take a look at the our package and contribute. Get in touch with us for all of you other data science needs as well.  Published 09 Aug 2017", "links": ["https://channel9.msdn.com/Events/useR-international-R-User-conferences/useR-International-R-User-2017-Conference/shinycollections-Google-Docs-like-live-collaboration-in-Shiny#comments", "https://cran.r-project.org/web/packages/rethinker/index.html", "https://appsilon.github.io/shiny.collections/", "http://appsilondatascience.com/blog/rstats/2017/07/02/shiny-chat.html", "http://appsilondatascience.com", "http://disqus.com/?ref_noscript"]}, "221": {"url": "https://blog.statsbot.co/introduction-to-imitation-learning-32334c3b1e7a", "title": "Introduction to Imitation Learning", "text": "This post is the first in the series where we will describe what Imitation Learning is. For today\u2019s article, the Statsbot team asked computer scientist Vitaly Kurin to briefly introduce Imitation Learning and outline the basics of Reinforcement Learning. Living organisms are extremely complex. Even the relatively simple ones such as flies or worms. They are able not only to operate successfully in the real world, but are extremely resilient to changes in conditions. And that\u2019s if we\u2019re not even talking about humans. We can plan ahead, we can alter our plans given new information, and we can collaborate with others and execute our plans more effectively. Science and such projects as CERN or a huge Radio Telescope in Arecibo are perfect examples of human collaboration and the art of planning. Yes, we do a lot of stupid stuff, but let\u2019s not talk about that here and instead focus on the good. Yes, we can now beat the best human in chess or Go, we can get a crazy score in Video Pinball for Atari 2600, we can even already make a pure human broke by challenging him in poker. Can we open a bottle of champagne and celebrate the triumph? I\u2019m afraid not. Yes, machine learning has recently made a significant leap forward. The combination of new Deep Learning ideas with old ones has enabled us to advance in many domains, such as computer vision, speech recognition, and text translation. Reinforcement Learning has also benefited greatly from its marriage with Deep Learning. You\u2019ve definitely heard of Deep Reinforcement Learning success such as achieving superhuman score in Atari 2600 games, solving Go, and making robots learn parkour. Though, we must admit, that operating successfully in the real world is much harder than playing Go or Space Invaders. Many of the tasks are much harder than this. Imagine a kid riding a bicycle in the middle of a crowded city center or a man driving a Porsche 911 on an autobahn at 200 miles per hour. Let\u2019s all admit that we are not there yet. The typical machine learning approach is to train a model from scratch. Give it a million images and some time to figure it out. Give it a week and let it play Space Invaders until it reaches some acceptable score. We, as humans, beg to differ. When a typical human starts to play some game he has never seen, he already has a huge amount of prior information. If he sees a door in Montezuma\u2019s Revenge, he realizes that somewhere there should lie a key and he needs to find it. When he finds the key, he remembers that the closed door is back through the two previous rooms and he returns to open it. When he sees a ladder, he realizes that he can climb it because he has done this hundreds of time already. What if we could somehow transfer human knowledge about the world to an agent? How can we extract all this information? How can we create a model out of it? There is such a way. It\u2019s called Imitation Learning. Imitation Learning is not the only name for leveraging human data for good. Some researchers also call it apprenticeship learning, others refer to it as Learning from Demonstration. From our point of view, there is no substantial difference between all of these titles and we will use Imitation Learning from now on. In order to introduce Imitation Learning, we will need to understand the basics of Reinforcement Learning first. Let\u2019s move on. It\u2019s not hard to get the general idea of the Reinforcement Learning setup. There is some agent and we want this agent to learn some task. Let\u2019s say, we have an Atari 2600 console, the agent has access to the joystick and can see what\u2019s happening on the screen. Let\u2019s say, 60 times per second we give our agent a screenshot of the game and ask him what button he wants to press. If our agent does well, he can see that his score is increasing (positive reinforcement), otherwise we can give him a penalty as a negative reward (negative reinforcement). Gradually, by trial and error, the agent starts to understand that it\u2019s better to avoid some of the actions and do those which bring him a reward. Let\u2019s make it more formal and describe the process stated above mathematically. We can describe the RL framework mentioned above (observe -> act -> get the reward and the next state) as a Markov Decision Process (MDP): where: We also need a definition of a policy function for the next section. Policy is a function, that returns an action given the state: And, actually, our final goal when solving an MDP is to learn such a policy in order to maximize the reward for our agent. Let\u2019s take an example of an MDP. The circles represent the states, arrows with green labels are actions, red labels are the rewards for actions, and the square is the terminal state. The green numeric labels are the transition probabilities. Our student starts in a state with the blue circle. He studies, but this is hard and sometimes boring. He decides to open a Facebook app and once he is there, he can either quit or continue scrolling. He then studies more and more, and finally decides to go to the pub. The state is a smaller filled circle, since now there is an element of randomness, based on the amount of knowledge the student forgets after visiting the pub. He can then either study more and pass the exam (+10 in reward), or he can go to sleep and finish the MDP right now. Since we will use DQN and related ideas in the future, let\u2019s briefly understand what is going on here. The whole approach is built upon approximating the so-called Q function and building the agent\u2019s behavior based on it. The idea of the Q function is the following: it returns you the entire expected discounted reward flow for the particular action and the particular state, given that starting from the next state we will be following our policy \ud835\udfb9. It answers the question: \u201cHow good is to press this button in this state?\u201d The Q function obeys the Bellman equation: And, finally, the Bellman principle of optimality is the following: notwithstanding what happened before, we should always take the action with the highest Q to maximize the reward flow: But how do we get such a Q function, you might ask. Let\u2019s look at an example. Imagine, you want to grab a coffee (+20 in reward) and a chocolate in a vending machine (+10 in reward). Your total reward cannot exceed thirty. Moreover, if you have taken the coffee already, it cannot be higher than 10 (the reward for chocolate) from now on. This is the idea: the Q value for the current step and action is equal to the maximum Q value for the next state (since we behave optimally) + the reward we get for the transition. The value of the quadratic objective function becomes: Q-learning itself is not new. Q-learning which uses neural networks as a function approximators is also not new (e.g. neural fitted-q iteration). A DQN paper was the first to use deep convolutional networks to solve this type of problem and introduced a couple of novelties that make the training process much more stable. First of all, experience replay. The vanilla Q-learning point is to make a step, get the reward and the next state, then update the approximation function parameters based on this transition. The DQN idea is to make the transition and save it in a \u201creplay memory\u201d\u200a\u2014\u200aan array that stores the last 10\u2076 (<insert any large number here>) transitions with the information about the reward, states before and after the transition, and if the event is terminal (game over) or not. Having this experience replay we can randomly sample mini-batches from it and learn more effectively. Another thing that makes the algorithm more stable is that DQN uses two neural networks: the first to compute the Q value for the current state and the second to compute the Q value for the next state. You can see that from the equation with the objective: two different Q functions use \ud835\udfb1 and \ud835\udfb1\u2019, respectively. Each 10,000 steps, the parameters \ud835\udfb1\u2019 are copied from the learned parameters \ud835\udfb1 and this helps a lot in increasing the stability. The problem here with using one function is that when we update the weights, both Q(s,a) and Q(s\u2019,a\u2019) increase and this might lead to oscillations or policy divergence. Using two separate networks adds a delay between an update and computation of the target Q value and reduces such cases. If you have further interest in the phenomena, read the Method section in the DQN Nature paper. Okay, everything described above sounds quite simple. If there is still something you do not understand, please, have a look at David Silver\u2019s lecture where he explains everything perfectly! Knowing all of these, can we build an True AI now? I\u2019m sorry, but we can\u2019t. There are several problems that hinder us from building an agent that will beat ByuN at StarCraft II, bring an autonomous car to the market, or give you an opportunity to buy your grandma a robot that will do the dishes for her after lunch. One of these problems is that the rewards our agent gets might be very sparse in time. Let\u2019s say, you play chess. If you lose, how do you know when you made a catastrophic move? Moreover, it\u2019s highly possible that there was not a catastrophic move, but several average ones. Another problem that is closely connected to the previous one, is the sample-efficiency problem. Or, more honestly, sample-inefficiency. Even to master a simple game such as Space Invaders might take a couple of days in-game time. It\u2019s easy to speed up learning in games since we have access to the simulators, but what if we want to learn something in real life? Unfortunately, physics is not there yet and we cannot speed up time. There is an approach that could potentially solve these problems and a bunch of others\u200a\u2014\u200aImitation Learning, as we mentioned at the beginning of this post. As we said, we, humans, rarely learn something without any prior information. Let\u2019s use this data! What should we do? The idea of Imitation Learning is implicitly giving an agent prior information about the world by mimicking human behavior in some sense. Imitation Learning will not only help us solve the sample-inefficiency or computational feasibility problems, it might potentially make the training process safer. We cannot just put an autonomous car in the middle of the street and let it do whatever it wants. We do not want it to kill humans that are around, destroy someone\u2019s property, or the equipment itself. Pretraining it on a human demonstrator\u2019s data might make the training process faster and avoid undesirable situations. Training a model requires some data. Training a Deep Learning model requires even more data. Training a Deep Reinforcement Learning model requires\u2026 Okay, you get the idea. So, this series is only partly describes what we can do with demonstration data. The main point of all of this is to call for human demonstration datasets, because we do not have many, unfortunately, up to this moment. Okay, we should stop here. The key points of this post are: In the next chapter we will write more about Behavior Cloning\u200a\u2014\u200athe simplest approach to an RL problem that can leverage the human demonstration data. Stay tuned! Clapping shows how much you appreciated Vitaly Kurin\u2019s story.", "links": ["https://blog.statsbot.co?source=logo-lo_820228a4ace3---cfc9f21a543a", "https://twitter.com/statsbotco", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.statsbot.co%2Fintroduction-to-imitation-learning-32334c3b1e7a", "https://blog.statsbot.co", "https://blog.statsbot.co/analytics/home", "https://blog.statsbot.co/datascience/home", "https://blog.statsbot.co/design/home", "https://blog.statsbot.co/startups/home", "https://blog.statsbot.co/bots/home", "https://blog.statsbot.co/news/home", "https://blog.statsbot.co/statsbot-digest-b0d7372f842a", "http://statsbot.co?utm_source=blog&utm_medium=navigation&utm_campaign=robot_face", "https://blog.statsbot.co/search", "https://blog.statsbot.co/@yobibyte?source=post_header_lockup", "https://blog.statsbot.co/@yobibyte?source=post_header_lockup", "https://atlas.cern/discover/about", "http://statsbot.co?utm_source=blog&utm_medium=article&utm_campaign=imitation_learning", "https://en.wikipedia.org/wiki/CERN", "https://en.wikipedia.org/wiki/Arecibo_Observatory", "https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning/", "https://deepmind.com/research/alphago/", "https://www.theverge.com/tldr/2017/7/10/15946542/deepmind-parkour-agent-reinforcement-learning", "http://www.ceva-dsp.com/ourblog/wp-content/uploads/sites/3/2016/04/AlphaGo-Lee-Se-dol.png", "https://simple.wikipedia.org/wiki/File:Rl_agent.png", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf", "https://link.springer.com/article/10.1007/BF00992698", "https://pdfs.semanticscholar.org/2820/01869bd502c7917db8b32b75593addfbbc68.pdf", "https://link.springer.com/article/10.1007/BF00992699", "http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html?foxtrotcallback=true", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "https://blog.statsbot.co/3-types-of-artificial-intelligence-4fb7df20fdd8", "http://wiki.teamliquid.net/starcraft2/ByuN", "http://toyoutheartist.co.uk/technique/imitation/", "https://mitpress.mit.edu/books/reinforcement-learning", "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html", "https://arxiv.org/abs/1312.5602", "https://www.nature.com/nature/journal/v518/n7540/full/nature14236.html", "https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner", "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/google-analytics-audit-checklist-and-tools-fca7df2f2e7a", "https://blog.statsbot.co/google-analytics-audit-checklist-and-tools-fca7df2f2e7a", "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co/tagged/imitation-learning?source=post", "https://blog.statsbot.co/tagged/data-science?source=post", "https://blog.statsbot.co/tagged/machine-learning?source=post", "https://blog.statsbot.co/tagged/ai?source=post", "https://blog.statsbot.co/tagged/reinforcement-learning?source=post", "https://blog.statsbot.co/@yobibyte?source=footer_card", "https://blog.statsbot.co/@yobibyte", "https://github.com/yobibyte/yobiblog", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "222": {"url": "https://axibase.github.io/atsd-use-cases/DataShorts/Dollar-EX/", "title": "", "text": "Data Source: Federal Reserve Economic Data (FRED) Visualizations: ChartLab Structured Query Language: SQL Console from Axibase Download the Community Edition of Axibase Time Series Database here Data is aggregated daily in visualizations, and annually in SQL queries. Favorable dollar exhange rates are shown in green while less favorable exchange rates are shown in red, based on the overall 5-year average exchange rate. Each section is accompanied by a brief analysis to give the data context. Using computed value settings thresholds have been calculated using the underlying data, the upper 25th percentile of a given exchange rate is considered favorable while the lower 25th percentile is considered less than favorable. The data is coded with an ID which is provided in the index as well. Open any ChartLab display below to navigate through time, or change the metric to reflect the desired exchange rate. Europe Asia North America As the amount of debt carried by various EU member countries grew to increasingly worrisome heights, the Euro plunged in 2015  losing ground against the dollar. Further, as oil prices slumped the European Central Bank began to compensate by launching a program of government bond purchases. The combination of these factors and the growth of the dollar post-recession have all contributed to the currently favorable exchange rate enjoyed by the dollar. Query 1.1 Table 1.1 Figure 1.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index During the worst of the European Debt Crisis which saw European Union per capita debt grow at unprecedented rates across the continent, EU investors sought refuge in the stability of the Swiss Franc, backed by the world-renowned Swiss banking system. Further, the Swiss National Bank removed the peg to the Euro unexpectably in 2015 causing a huge surge in 2015, clearly visible in the visualization below as a sudden change from a favorable exchange rate for dollar holders to an all-time low for the observed time period. Query 1.2 Table 1.2 Figure 1.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Largely influenced by the divisive Brexit vote, the value of the Pound has remained relatively low since the referendum in late June of 2016, which aligns perfectly with the Pound\u2019s slight depreciation in value against the dollar on the chart below. Query 1.3 Table 1.3 Average Exchange Rate: 1.50 Figure 1.3   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index After purchasing trillions of Yen worth of Japanese national debt from the private sector three years ago, the Central Bank of Japan has been watching the gradual decrease in value of the Yen against the dollar. Most analysts, see this fall as controlled and predict a bright future for the Yen as the Central Bank can no longer afford to collect more debt than they already have. However, the Yen\u2019s rise much also be managed as the sudden appreciation of the currency could result in an increase in loan defaults because industries which have taken loans at current Yen prices would be paying them back at a much higher rate. Query 2.1 Table 2.1 Figure 2.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index To prevent dramatic internal inflation, the People\u2019s Bank of China closely regulates local, or onshore, trading of the Yuan by fixing the price each day. Although independent officially, the international, or offshore, price of the Yuan usually remains fairly close to its onshore value. Additionally, the PBoC carefully controls the outflow of capital and often hedges their own currency by maintaining positions in international currencies. Query 2.2 Table 2.2 Figure 2.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index The Hong Kong Dollar is officially pegged to the value of the United States Dollar so any change in relative  value is planned. The \u201cMiracle of the Orient\u201d continues to be one of the most dominant financial markets in the world. Query 2.3 Table 2.3 Figure 2.3   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index One of the financial centers of the world, Singapore is home to the second-busiest port in the world and one of the largest oil refining industries worldwide as well. With one of the highest per capita GDP values, Singapore has cemented its place of international importance despite inexplicably loose financial policy whereby the Monetary Authority of Singapore does not regulate their currency\u2019s value by adjusting interest rates, as is common with most central banking systems. Query 2.4 Table 2.4 Figure 2.4   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Relative to the USD, the Canadian dollar has been making a bit of a resurgence in 2017 as United States GDP numbers missed its targets in quarter two. Although dependant on the price of oil due to their large natural reserves, the diverse Canadian economy is one of the few globally-active nations with a significant trade surplus, although that too have been reduced in recent years. Query 3.1 Table 3.1 Figure 3.1   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index Boasting the highest per capita income of Latin America, the Mexican economy has continued to demonstrate healthy growth over the last several decades. North America\u2019s number one producer of automobiles also enjoys a $46 billion trade surplus with the United States, and is the 9th ranked owner of United States government debt. This dependence on America affords Mexico the ability to spur dramatic internal growth that has led to the creation of a booming electronics sector, but at the cost of a  certain amount of reliance on the United States as roughly half of the country\u2019s imports and exports originate or end up  on U.S. soil. Query 3.2 Table 3.2 Figure 3.2   Open the ChartLab visualization above to navigate through time or select a different country\u2019s currency. Return to the Index", "links": ["https://github.com/axibase/atsd-use-cases", "https://fred.stlouisfed.org/categories/94", "https://apps.axibase.com", "https://github.com/axibase/atsd/tree/master/api/sql", "https://axibase.com", "https://axibase.com/products/axibase-time-series-database/", "https://github.com/axibase/atsd/blob/master/installation/README.md#installation", "https://axibase.com/products/axibase-time-series-database/visualization/widgets/configuring-the-widgets/", "https://apps.axibse.com", "https://apps.axibase.com/chartlab/424eb6b2/5/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/9/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/6/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/4/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/2/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/3/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/8/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/#fullscreen", "https://apps.axibase.com/chartlab/424eb6b2/7/#fullscreen", "https://github.com/axibase/atsd-use-cases", "https://github.com/axibase", "https://pages.github.com"]}, "223": {"url": "http://byteacademy.co/blog/overview-NLG", "title": "", "text": "NLG (Natural Language Generation), a subfield of Artificial Intelligence, is a hot topic in the technology news today. We hear a lot about AI that can soon replace writers and journalists beginning the era of machine creativity. But, what\u2019s all this fuss about? In this article, we unveil what NLG really is and show that it can bring a lot of benefits to businesses and consumers. In a nutshell, NLG is a sub-field of NLP (Natural Language Processing) that studies methods of automatic transformation of structured data into a human-readable text. In practice, there are two major types of NLG applications: template-based NLG and advanced NLG. \u00a0 Template-based NLG is the simplest solution that uses templates with canned text and placeholders to insert data into them. Such systems heavily rely on hard-coded rules, which makes them less flexible than advanced NLG. Since template-based NLG tools have a limited number of templates and require special data representations, they can not be easily reused across different projects and business use cases. \u00a0 Advanced NLG tools are more flexible thanks to the use of supervised and unsupervised Machine Learning (ML). Rather than tying down structured data to the Procrustean bed of templates, advanced NLG uses neural networks that learn morphological, lexical, and grammar patterns from large corpora of written language. Soft probabilistic methods used in the advanced NLG algorithms allow predicting the likelihood of one word appearing after another, and correcting language errors, such as misspellings. ML algorithms used in the advanced NLG are also better in dealing with new words and expressions not included in the original training samples. \u00a0 Modern NLG service providers such as Narrative Science and Automated Insights prefer advanced NLG methods because they allow creating rich data-driven models that produce intelligent insights from data. These algorithms are much more skillful in making right word choices and writing narratives that reflect intentions and business needs of the NLG users. As an added bonus, advanced NLG models can preprocess and analyze data which makes them not just translators of structured data into text, but automatic analysts able to provide actionable insights. \u00a0 Despite the fact that NLG methods have been used since the 1970s, they got a powerful momentum only recently and thanks to the AI/ML revolution. Today, many startups offer cloud-based NLG services to businesses. NLG is also gaining traction in mass media and journalism. Major American newspapers are already experimenting with the automatic storytelling. For example, in 2016 the Washington Post unveiled its automatic storytelling AI named Heliograph AI. Heliograph was used in the coverage of Rio Olympics and the US Presidential election in 2016. \u00a0 Leveraging data mining techniques and ML models the machine reporter can convert structured statistical data, diagrams, graphs, weather forecasts and other data-rich content into excellent descriptive reports that sound if though they were written by the professional reporters. But, isn\u2019t this dangerous for journalism as a profession? Proponents of automatic storytellers say that they actually free up much time for reporters to add analysis and real insights to stories rather than spending countless hours publishing news and descriptive reports[i]. NLG tools may be used in other innovative ways as well: Benefits of NLG, however, go beyond journalism. There is a growing demand for NLG services among major companies. For example, Quill, an NLG system developed by Narrative Science, is used by such companies as Deloitte, Groupon, and Credit Suisse[iv]. These companies opt for NLG solutions for a reason. Growing acceptance of NLG among businesses makes it a promising field to study. If you want to learn more about NLG, \u00a0Byte Academy offers a Natural Language course that covers Natural Language Processing and Natural Language Generation. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0  Your Name (required)    Your Email (required)    \u00a0 \u00a0 References: [i] \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 WashPost PR Blog (August 5, 2016). The Washington Post Experiments With Automated Storytelling to Help Power 2016 Rio Olympics Coverage. WashPost PR Blog. Retrieved from https://www.washingtonpost.com/pr/wp/2016/08/05/the-washington-post-experiments-with-automated-storytelling-to-help-power-2016-rio-olympics-coverage/?utm_term=.bf63b03c4aeb [ii] \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Dayan, Zohar (2015). Hearst, USA Today Sports, & Viralnova Partner With Wibbitz For Video Strategy. Wibbitz Blog. Retrieved from http://blog.wibbitz.com/wibbitz-partners-hearst-usa-today-sports-group-and-viralnova-to-expand-video-strategy [iii] \u00a0\u00a0\u00a0\u00a0\u00a0 Keohane, Joe (2017). What News-Writing Bots Mean for the Future of Journalism. Wired. Retrieved from https://www.wired.com/2017/02/robots-wrote-this-story/ [iv] \u00a0\u00a0\u00a0\u00a0 Narrative Science. Turn Your Data Into Better Decisions With Quill. Retrieved from https://narrativescience.com/Platform info.in@byteacademy.co Byte Dev Licensed by the State of New York, New York State Education Department \u00a9 2017 Byte Academy LLC.  All rights reserved.", "links": ["http://byteacademy.co", "http://byteacademy.co/program/", "http://byteacademy.co/courses/", "http://byteacademy.co/data-science-mini-courses/", "http://byteacademy.co/upcoming-classes/", "http://byteacademy.co/students/", "http://byteacademy.co/hiring/", "http://byteacademy.co/events/", "http://byteacademy.co/blog/", "http://byteacademy.co/india/", "https://byteacademy.fluidreview.com", "http://feeds.feedburner.com/ByteAcademy", "http://byteacademy.co/category/data-science/", "http://byteacademy.co/category/career/", "http://byteacademy.co/category/fintech/", "http://byteacademy.co/category/medtech/", "http://byteacademy.co/category/programming-tips/", "http://byteacademy.co/category/startup/", "http://byteacademy.co/category/student-stories/", "http://byteacademy.co/category/blockchain/", "http://byteacademy.co/category/current-events/", "http://byteacademy.co/category/diversity/", "http://byteacademy.co/category/events/", "http://byteacademy.co/category/finovate/", "http://byteacademy.co/category/light-reads/", "http://byteacademy.co/category/millennials/", "http://byteacademy.co/category/quant/", "http://byteacademy.co/2017/", "http://byteacademy.co/2016/", "http://byteacademy.co/2015/", "http://byteacademy.co/category/data-science/", "http://byteacademy.co/blog/artificial-intelligence-future", "https://www.washingtonpost.com/pr/wp/2016/08/05/the-washington-post-experiments-with-automated-storytelling-to-help-power-2016-rio-olympics-coverage/?utm_term=.bf63b03c4aeb", "http://www.reddit.com/submit?url=http://byteacademy.co/blog/overview-NLG", "http://www.tumblr.com/share?v=3&u=http://byteacademy.co/blog/overview-NLG&t=Overview of Natural Language Generation (NLG)", "http://byteacademy.co/blog/artificial-intelligence-future", "http://byteacademy.co/blog/artificial-intelligence-future", "http://byteacademy.co/blog/data-science-podcasts", "http://byteacademy.co/blog/data-science-podcasts", "http://byteacademy.co/blog/data-science-fintech/", "http://byteacademy.co/blog/data-science-fintech/", "http://byteacademy.co/financial-aid/", "http://byteacademy.co/upcoming-classes/", "http://byteacademy.co/contact-us/", "http://byteacademy.co/corporate-training/", "http://byteacademy.co/faq/", "http://byteacademy.co/press-page/", "http://byteacademy.co/careers-at-byte/", "http://byteacademy.co/brand-ambassador/", "https://www.meetup.com/Byte-Academy-Finance-and-Technology-community/", "http://www.bytedev.co", "https://www.facebook.com/byteacademy/", "https://twitter.com/ByteAcademyCo", "https://www.linkedin.com/edu/school?id=171001", "https://github.com/ByteAcademyCo", "https://www.instagram.com/byteacademy/", "https://www.meetup.com/Byte-Academy-Finance-and-Technology-community/", "https://www.quora.com/topic/Byte-Academy", "https://www.youtube.com/channel/UCrMcJALnO748TSK27bZQzSg", "http://byteacademy.co/privacy-policy/"]}, "224": {"url": "http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "title": "", "text": "", "links": ["http://www.getrevue.co/?ref=Revue+Profile", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profileimage&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profiledate&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profilename&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Mm6Kn?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Mm6Kn?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.datasciencecentral.com/profiles/blogs/automated-machine-learning-for-professionals?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lXBy8?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lXBy8?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.nature.com/articles/s41467-017-00181-8?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/d0nAm?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/d0nAm?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/4mO6D?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/4mO6D?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/eK78N?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/eK78N?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://sourcedexter.com/amazing-tensorflow-github-projects/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xejEQ?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xejEQ?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://techblog.appnexus.com/a-keras-multithreaded-dataframe-generator-for-millions-of-image-files-84d3027f6f43?gi=79cece23ee37&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/6nA72?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/6nA72?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://blog.revolutionanalytics.com/2017/08/a-modern-database-interface-for-r.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/5kyK3?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/5kyK3?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://ruder.io/deep-learning-nlp-best-practices/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/7yxNV?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/7yxNV?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://hyperparameter.space/blog/when-not-to-use-deep-learning/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/8A1NP?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/8A1NP?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278?gi=8e0386afa61e&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/1bKwn?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/1bKwn?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://adeshpande3.github.io/How-I-Used-Deep-Learning-to-Train-a-Chatbot-to-Talk-Like-Me?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lX0x4?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lX0x4?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.youtube.com/channel/UC0n76gicaarsN_Y9YShWwhw/playlists?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/qYVyK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/qYVyK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.youtube.com/watch?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest&v=0fhUJT21-bs", "http://rev.vu/Nkeov?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Nkeov?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%239%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "https://www.getrevue.co/?utm_source=Data Science Digest&utm_medium=email&utm_content=footerlink&utm_campaign=Issue", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=forwarded&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%239%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-9-67190", "http://www.getrevue.co/?ref=Revue+Profile"]}, "225": {"url": "http://rpsychologist.com/d3/bayes/", "title": "", "text": "Created by Kristoffer Magnusson The visualization shows a Bayesian two-sample t test, for simplicity the variance is assumed to be known. It illustrates both Bayesian estimation via the posterior distribution for the effect, and Bayesian hypothesis testing via Bayes factor. The frequentist p-value is also shown. The null hypothesis, H0 is that the effect \u03b4 = 0, and the alternative H1: \u03b4 \u2260 0, just like a two-tailed t test. You can use the sliders to vary the observed effect (Cohen's d), sample size (n per group) and the prior on \u03b4. The prior on the effect is a scaled unit-information prior. The black, and red circle on the curves represents the likelihood of 0 under the prior and posterior. Their likelihood ratio is the Savage-Dickey density ratio, which I use here as to compute Bayes factor. The p-value is the traditional p-value for a two-sample t test with known variance (i.e. a Z test).        HDI is the posterior highest density interval, which in this case is analogous a credible interval. And CI is the traditional frequentist confidence interval.  Check out Alexander Etz's blog series \"Understanding Bayes\" for a really good introduction to Bayes factor. Fabian Dablander also wrote a really good post, \"Bayesian statistics: why and how\", which introduces Bayesian inference in general. If you're interesting in an easy way to perform a Bayesian t test check out JASP, or BayesFactor if you use R. Interactive visualization of Cohen's d effect size Interactive visualization of statistical power and significance testing Interactive visualization of Confidence Intervals Have any suggestion? Or found any bugs? Send them to me, my contact info can be found here. Designed and built by Kristoffer Magnusson. Built with D3.js, jStat and Bootstrap.", "links": ["http://rpsychologist.com", "http://rpsychologist.com", "https://twitter.com/krstoffr", "https://se.linkedin.com/pub/kristoffer-magnusson/b5/133/1b6", "https://twitter.com/share", "http://alexanderetz.com/understanding-bayes/", "http://blog.efpsa.org/2015/08/03/bayesian-statistics-why-and-how/", "https://jasp-stats.org", "http://bayesfactorpcl.r-forge.r-project.org", "http://rpsychologist.com/d3/cohend/", "http://rpsychologist.com/d3/NHST/", "http://rpsychologist.com/d3/CI/", "http://rpsychologist.com/tag/d3js.html", "http://rpsychologist.com/about", "http://rpsychologist.com", "http://d3js.org/", "http://jstat.org/", "http://getbootstrap.com/", "http://twitter.com/krstoffr", "https://github.com/rpsychologist"]}, "226": {"url": "https://blog.datasyndrome.com/generalists-dominate-data-science-f01882f25347", "title": "Generalists Dominate Data\u00a0Science", "text": "Analytics products and systems are best built by small teams of generalists. Large teams of specialists become dominated by communication overhead, and the effect of \u201cChinese whispers\u201d distorts the flow of tasks and stagnates creativity. Data scientists should develop generalist skills to become more efficient members of a data science team. Building data products takes a team covering a broad and diverse skillset. From the customer representative at one end, to the operations engineer at the other, the spectrum of roles in a product analytics team looks like this: Large companies often fill each role with a pair of shoes, resulting in a twelve person team like the one below. The problem with this setup is that it becomes much more difficult to achieve consensus and to perform any task that spans roles. And in data science, most tasks span roles. To look at a particular example, lets focus on the creation of a chart as part of a data product. To begin, a product manager creates a specification, then an interaction designer mocks up the chart, handing it off to a data scientist to fill with data (and hopefully to explore the data and find a chart worth producing), then a back-end engineer to setup an API to grab that data, a front-end web developer to create a web page using the data that matches the mock, and an experience designer to ensure the entire thing feels right and makes sense. Charts take iteration, so this cycle of communication could happen repeatedly for each chart. You can see how communication overhead starts to predominate. A meeting of six people is a full-blown, formal meeting. It is hard to get things done in formal meetings. In the next figure, we see how a data product team might be composed of four generalists: a data engineer, a data scientist/back-end developer, a designer who can build front ends and a product manager that can write marketing copy and cut deals. This is how a startup team would span the skill spectrum, and you can probably see how this makes them more efficient. Revisiting the chart example, creating a chart becomes a collaboration between the product manager, a designer who codes, and a data scientist. This is the kind of ad hoc meeting of 2\u20133 people where \u201cshit gets done\u201d efficiently. This group will be more efficient than the six man group. Put another way: this small team will kick the large team\u2019s ass. In the big company system, sometimes the only way to get anything done efficiently is to go \u201cguerilla generalist\u201d and work with other generalists to cut people out of the chain. This is bad politically, and is part of what drives effective people from big companies. We\u2019ve shown that small teams generalists outperform large teams of specialists. In fact, generalist skills are something every data scientists should work to develop. That doesn\u2019t mean you can\u2019t specialize, but should combine specialization with generalization in order to develop \u201cT-shaped skills.\u201d The T-shaped employee is one that can lend deep expertise across projects while fulfilling multiple roles in his own. It takes time to develop general skills, and that is why the path to becoming a data scientist is not a six month bootcamp, but a ten year journey. Along this path, remember to try to be T-Shaped! Need help building an analytics product or platform? The Data Syndrome team of data scientists and data engineers is available to build your data products and systems as a service. We also offer training in Agile Data Science for all members of data science teams. Clapping shows how much you appreciated Russell Jurney\u2019s story.", "links": ["https://blog.datasyndrome.com?source=logo-lo_57d953cee571---500653fb51a1", "https://twitter.com/datasyndrome", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.datasyndrome.com%2Fgeneralists-dominate-data-science-f01882f25347", "https://blog.datasyndrome.com/@rjurney?source=post_header_lockup", "https://blog.datasyndrome.com/@rjurney?source=post_header_lockup", "https://personalmba.com/communication-overhead/", "https://en.wikipedia.org/wiki/Chinese_whispers", "https://en.wikipedia.org/wiki/T-shaped_skills", "http://datasyndrome.com", "http://datasyndrome.com/training", "https://blog.datasyndrome.com/tagged/data-science?source=post", "https://blog.datasyndrome.com/tagged/agile?source=post", "https://blog.datasyndrome.com/tagged/generalists?source=post", "https://blog.datasyndrome.com/tagged/analytics?source=post", "https://blog.datasyndrome.com/tagged/software-engineering?source=post", "https://blog.datasyndrome.com/@rjurney?source=footer_card", "https://blog.datasyndrome.com/@rjurney", "https://blog.datasyndrome.com?source=footer_card", "https://blog.datasyndrome.com?source=footer_card", "https://blog.datasyndrome.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "227": {"url": "https://github.com/axibase/atsd-use-cases/tree/master/FED_FORDSR#the-average-american-debt-profile", "title": "", "text": "Debt is a complicated concept. After the sub-prime mortgage crisis of the late 2000s, modern Americans are all too familiar with the problems of irresponsible spending on credit. Student loan recipients who queue up to drop off another application for a job in a field they did not study are quick to point to the trappings of deficit spending as a means of wealth creation. Politicians and voters on both sides of the aisle point to the ever-growing United States Government debt with anxiety for the future. And yet despite all the doom and gloom, the American financial system is one of the most stable and robust in the world, in no small part thanks to ingenious monetary policy and hegemonic economic position organized over the entire course of the country's history, modern American consumers are among the wealthiest on the planet. The United States Federal Reserve is the central banking system of the United States, responsible for monitoring the global financial climate and enacting policy that supports the American economy and American consumers. They maintain a number of statistics about these consumers and their monetary practices to better inform their decisions and practices. Provided by the Federal Reserve, this dataset must be correctly parsed during import. The quarterly date format needs to be converted into a monthly format that ATSD can interprete (Q/q letter is not supported). We also need to discard metadata lines contained in the multi-line header. This can be accompilshed with a schema-based parser that provides granular control over the document's rows and columns using RFC 7111 selectors and Javascript: Script 1.1 For step-by-step instructions on data customization with schema based parsing, see this support tutorial. The Financial Obligation Ratio (FOR) is an estimate of the ratio of required debt payments to disposable income. This is a broad calculation and includes all kinds of debt:  mortgage payments, credit cards, property tax and lease payments. Each of these metrics can be expanded further to include associated costs, such as homeowner's insurance for example. The Federal Reserve releases this number each quarter. Figure 1.1   Use the dropdown menus at the top of the visualization screen to navigate through time, selecting the starttime and endtime values to observe a desired period. The data can also be queried with a structured query language in the SQL Console. The data will be aggregated annually, derived from the average value of each quarter within a given year: Query 1.1 Table 1.1 All values are shown as a percent of one hundred, where the whole is representative of the total income of the average person. The Debt Service Ratio (DSR) is more specific than the Financial Obligation Ratio in that it typically does not include non-essential debt payments. Here, it has been parsed into two categories, mortgage debt and consumer debt. These numbers represent the average percent of a person's earned salary each month which much be used to make the required payments associated with consumer credit and mortgage. Typically the DSR is an initial calculation performed to determine a person's eligibility to receive a mortgage. A DSR value of less than 48% is generally preferred, meaning that with a particular mortgage plus other credit obligations at least 52% of a person's gross monthly earning would still be available to them after making the required payments. Figure 2.1   Query 2.1 Table 2.1 Because the FOR value includes the DSR value plus additional non-essential credit values, and the DSR value is parsed into both consumer and mortgage related debt, these three values can be shown in a new visualization that creates a typical consumer profile of the average American. By using the calculated value setting shown below, additional data not specifically included in the set can be displayed: Script 2.1 Shown below is the debt profile of the average American consumer from 1980 to 2017, navigate through time using the dropdown menus at the top of the screen to select a desired span of time and compare how bearing debt has changed over the course of the last three decades. Figure 3.1   The visualization can also be organized to show the amount of each type of debt as it relates to the others: Figure 3.2   Additionally, these values can be compared on an annual basis as shown in the visualization below: Figure 3.3   To view the distribution of these values across time, a histogram is shown below: Figure 3.4   In the following box diagram, explore time with the dropdown menus at the top of the visualization screen. The visualization shows the distribution of debt values as a percentage of total income, with the initial time period set to include the entire data set: Figure 3.5   The following SQL query will detail the above visualizations in one table, displaying averaged annual values of each component described above: non-essential credit payments, mortgage credit payments, and consumer credit payments, as well as the Financial Obligation Ratio (FOR), or total debt obligations. Query 3.1 Table 3.1 The above dataset can illuminate a number of features of the American economy and a number of characteristics of the average American consumer. While modern Americans are quick to denounce the zeitgeist of living outside of one's means, the data shows that in fact, the amount of debt carried by the average American is on par with or even lower in some cases than that of his 1980's counterpart. In fact, the only metric which has demonstrated a legitimate increase in value over the last several decades has been the roughly one percent increase in non-essential credit holdings by the average consumer. According to data from the Economic Research Department of the Saint Louis Branch of the Federal Reserve, the 2015 US median household income was $56,516 per year in 2015 USD. This number can be applied to the above table and visualized in ChartLab to create more comprehensive data. Figure 3.6   The above visualization aggregates the values from Table 3.1 based on a time period of the user's selection. Use the dropdown menu at the top of the screen to select the aggregation period. The initial visualization shows the average values for each metric over the entire period of time in 2015 USD by obligation amount per quarter. The following query summons the same data shown above, but further parses it to show annual average monthly payments instead of quarterly values in 2015 USD for a person making the 2015 median United States income of $56,516 a year. Query 4.3 Table 3.2 As it turns out, the idea that your parents paid less for their house than you will is only true in absolute terms. When compared with current numbers and controlled for inflation, the average 2017 consumer will pay roughly the same portion of their income towards a place to hang their hat up as the average 1980 consumer. The Federal Reserve is able to pull certain levers of power from the Eccles Building in Washington, D.C. such as printing more money, or raising and lowering interest rates to cope with inflation. However, all of these are reactionary measures meant to create small changes that have a butterfly effect over time. Ultimately, the machinations of the Board of Governers have always be something opaque and esoteric to the average man, leading to many people denouncing the Federal Reserve System entirely, occasionally opting for a return of the gold standard or leveling accusations of wrong-doing. However, after reviewing the data above, it seems that at least on a consumer level, the average American actually has more today than they would have had thirty years ago, or even just five years ago. Of course, the Federal Reserve isn't completely responsible for the wise consumer choices made in the current decades, but monetary policy enacted by the various branches of the Federal Reserve are responsible for maintaining the economic conditions that Americans, and consumers the world over, have come to expect from the United States economy.", "links": ["https://github.com/", "https://github.com/axibase/atsd-use-cases/pull/86", "http://www.usdebtclock.org/", "https://www.federalreserve.gov/", "https://www.federalreserve.gov/datadownload/Download.aspx?rel=FOR&series=91e0f9a6b8e6a4b1ef334ce2eaf22860&filetype=csv&label=include&layout=seriescolumn&from=01/01/1980&to=12/31/2017", "https://axibase.com/products/axibase-time-series-database/writing-data/csv/", "https://apps.axibase.com/chartlab/842f1dd9/#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql", "https://apps.axibase.com/chartlab/85522dd3/#fullscreen", "https://apps.axibase.com/chartlab/f25de723/#fullscreen", "https://apps.axibase.com/chartlab/f25de723/3/#fullscreen", "https://apps.axibase.com/chartlab/81ea0ea0/#fullscreen", "https://apps.axibase.com/chartlab/9f74c179/#fullscreen", "https://apps.axibase.com/chartlab/20ff0ade/#fullscreen", "https://fred.stlouisfed.org/series/MEHOINUSA646N", "https://research.stlouisfed.org/", "https://www.stlouisfed.org/", "https://apps.axibase.com", "https://apps.axibase.com/chartlab/da132e01/11/#fullscreen", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "228": {"url": "http://tech.marksblogg.com/billion-nyc-taxi-rides-aws-ec2-p2-16xlarge-brytlytdb.html", "title": "", "text": "I have 14 years of hands-on build and consulting experience with clients in the UK, Ireland & Germany. I've done both back- and frontend work for Bank of America Merrill Lynch, Blackberry, Bloomberg, British Telecom, Ford, Google, ITV, News UK, Pizza Hut, Royal Bank of Scotland, Royal Mail, T-Mobile, Vertu (Nokia subsidiary at the time), Williams Formula 1 Team and Xerox. I hold both a Canadian and a British passport. My CV & my LinkedIn profile.         Home           | Benchmarks           | Archives            | Atom Feed  Posted on Fri 28 July 2017 BrytlytDB is an in-GPU-memory database built on top of PostgreSQL. It's operated using many of PostgreSQL's command line utilities, it's wire protocol compatible so third-party PostgreSQL clients can connect to BrytlytDB and queries are even parsed, planned and optimised by PostgreSQL's regular codebase before the execution plan is passed off to GPU-optimised portions of code BrytlytDB offer. Clustering works right out of the box, GPU-powered JOINs are supported, Stored Procedures are fully functional, Deep- and Machine Learning workloads via Torch are supported and BI visualisation software in the form of SpotLyt is included with BrytlytDB as well. The feature list is too long for one blog to cover so in this post I'll just be taking a look at how performant BrytlytDB's OLAP functionality is. In this benchmark I'll see how well 32 Tesla K80 GPUs spread across two EC2 instances perform when querying 1.1 billion taxi trips. I'll be using the same dataset I've used to benchmark Amazon Athena, BigQuery, ClickHouse, Elasticsearch, EMR, kdb+/q, MapD, PostgreSQL, Redshift and Vertica. I have a single-page summary of all these benchmarks for comparison. For this benchmark I'll be using two p2.16xlarge EC2 instances running Ubuntu 16.04.2 LTS in Amazon Web Services' eu-west-1a region in Ireland. Each machine has 8 Nvidia K80  cards which have 2 GPUs each, 64 virtual CPUs and 732 GB of memory. There's also 20 Gbit/s of networking capacity available to each instance. Below are the specifications of the compute capabilities available each one of the EC2 instances. This is the layout of the GPUs available on one of the two EC2 instances. The first machine has an IP address of 52.214.237.134 and has three roles: Global Transaction Manager, Coordinator and host to the first 16 data nodes. The second machine has an IP address of 34.250.232.38 and has two roles: it is also a coordinator and hosts the second set of 16 data nodes. TCP port 5432 is open between the two machines for communicating via PostgreSQL's wire protocol, TCP port 7777 is open for global transaction manager communication and ports 20,000 through to 20,0031 are open for data node communication. In addition to the default 20 GB EBS volumes on each EC2 instance there are six 500 GB General Purpose SSDs attached which offer a baseline of 100 IOPS each and can burst to 3,000 IOPS if need be. These drives are setup in a RAID 0 configuration on each instance. Before setting up the RAID array I'll install a few dependencies. This was run on both EC2 instances. Below are the commands used to setup the RAID array on each instance. Here's what the RAID layout looked like after it was setup. This RAID 0 setup offers a partition with a capacity of 3.2 TB on each instance: I've run the following to download BrytlytDB's install script. BrytlytDB is commercial software so I cannot divulge the URL I pulled this from at this time. The following was run on both EC2 instances. I've then edited the install script on the first instance with the following instance-specific values: And the install script on the second instance was edited with the following instance-specific values: With those changes in place I then ran the install script on both EC2 instances. The install script conducts the following: Once that's done I can form the cluster with the following registration script. The following was run on the first EC2 instance. The second registration script run on the other EC2 instance is identical with the first with the exception of the coordinator line pointing to 52.214.237.134 instead. For this benchmark I've downloaded and decompressed one half the 500 GB of CSV data I created in my Billion Taxi Rides in Redshift blog post onto each EC2 instance. The data sits across 56 files across both machines but for the quickest load time I want to create 32 reasonably equally-sized CSV files and load 16 files on each EC2 instance simultaneously. The 32 files will pair up with the 32 GPUs available across the cluster and should allow for the best utilisation of the GPU resources when running queries. Here is half of the original 500 GB data set on the first EC2 instance: And this is the other half on the second instance: On each instance I'll decompress the GZIP files. Below took about four minutes to complete on each instance. Then I'll concatenate the ~560 million lines of CSV data on each instance and break it up into files of 35 million lines each. The above completed in 8 minutes on each EC2 instance. I'm now left with 16 CSV files on the first EC2 instance: And 16 files on the other EC2 instance: I'll then connect to the coordinator node on each EC2 instance and setup BrytlytDB's gpu_manager_fdw extension and foreign data wrapper. I can then create the trips table. The above sets the maximum row count per GPU device at 38 million and creates an index on the cab_type column. The cab_type column has very low cardinality so the indexing will be less beneficial versus a column with a higher cardinality of values. Some of the data types used in the above table aren't the types I normally use in my benchmarks. The reason for this is that there is still limited data type support available in this early version of BrytlytDB. I've had to substitute DATETIME with DATE which will truncate the timestamps in the dataset to just the date rather than the full date and time. There is no SMALLINT support yet so I've had to use the larger INT type as an replacement for those fields. DECIMAL types aren't yet supported so I'll be using DOUBLE PRECISION as a replacement there. The above replacements could speed up or slow down the queries I benchmark with so I'm hoping to do another benchmark when data type support is widened in the future. With the table created I'll launch 32 simultaneous load jobs across the two EC2 instances to load the data into BrytlytDB's trips table. This is the import script I ran on the first EC2 instance: And this is the load script for the second instance. The first EC2 instance loaded its half of the dataset in 1 hour, 12 minutes and 21 seconds. The second instance loaded its half in 1 hour, 9 minutes and 57 seconds. After the data was loaded in I could see the PostgreSQL data directory filled up with reasonably evenly-sized data folders. Here's what they look like on the first EC2 instance: I was then able to use PostgreSQL's CLI tool to make sure I can see the table and all 1.1 billion records. The times quoted below are the lowest query times seen during a series of runs. As with all my benchmarks, I use the lowest query time as a way of indicating \"top speed\". The following completed in 0.762 seconds. The following completed in 2.472 seconds. The following completed in 4.131 seconds. The following completed in 6.041 seconds. Given all the features of PostgreSQL are still available I'm blown away at how fast BrytlytDB is able to aggregate data. The cluster I used cost around $30 / hour which means this system out performed other Cloud-based and PostgreSQL-based data warehousing solutions both in terms of wall clock time and in terms of cost per hour of running the cluster. That being said I know Nvidia's K80 chips use extremely fast memory and have thousands of compute cores so I expect further optimisations from BrytlytDB to drive down these already fast query times. \u2190 Back to Index Copyright \u00a9 2014 - 2017 Mark Litwintschik. This site's template is based off a template by Giulio Fidente.", "links": ["https://twitter.com/marklit82", "http://tech.marksblogg.com/theme/cv.pdf", "https://uk.linkedin.com/in/marklitwintschik/", "http://tech.marksblogg.com/feeds/all.atom.xml", "http://www.brytlyt.com/", "https://uk.linkedin.com/in/marklitwintschik/", "https://github.com/giulivo/pelican-svbhack"]}, "229": {"url": "https://blog.datazar.com/why-we-focus-on-design-just-as-much-as-functionality-f6ccb3b2c926", "title": "Why We Focus on Design Just as Much as Functionality", "text": "It\u2019s no secret that scientific software isn\u2019t the most beautiful software in terms of design/aesthetics (generally speaking here). It is very effective though; it gets the job done. It does exactly what you expect it to do. Being a huge movie buff, I\u2019ve always wondered why the software we use in the laboratories or classrooms looks nothing like what we see in the movies. Movies do this thing where tech looks exciting and futuristic and super fun to use. And then I go back to reality and it\u2019s nothing like what I just watched. It\u2019s worth mentioning that not everything in the movies looks practical, especially when it comes to software or even more ridiculous: computer hacking. But that\u2019s another subject. The fact there\u2019s some art missing is something I noticed not only in the software we use in the lab, but also the instruments we build and the papers we write. It seems as if the art has been completely taken out of the sciences. Before you say it, we obviously can\u2019t incorporate art into somethings like highly sensitive instruments as unnecessary additions area big no because then you\u2019re interfering. Another thing to note might be the fact that hundreds or even some thousands years ago, the scientists were also artists. Art came naturally to them as they were also discovering mother nature. At that time, science was also only available to the rich which meant a couple of things: they could afford to spend a little bit of time incorporating art into their scientific work as they weren\u2019t rushing to meet grant deadlines, but it also meant they were already educated in the arts from an early age. These days anyone can be whatever they want to be. To the very least, it\u2019s easier now than it was in that era by an order of magnitude. Today we\u2019re taught that art almost has no place in the sciences and we must be efficient and to the point. Anything extra is generally referred to as fluff. If you like to romanticize science, you\u2019re not really a scientist. But I strongly believe that it\u2019s the artistic and I might add\u200a\u2014\u200aphilosophical\u200a\u2014\u200afluff that gets people interested in science. The hard math and core scientific values obviously being there regardless. All of this results in the absence of art in the sciences which makes art seem dry and boring. Science in its purest form is absolutely beautiful if communicated right. That\u2019s why we love to do our part at Datazar. We\u2019re far, far from perfect or even close to what we want to be. But putting just as much thought in design and UX as we put in functionality is a core principle. There\u2019s a misconception that incorporating art means adding more stuff. Incorporating art can also mean doing less, using less and making the most of what you already have in a beautiful and simple way. An example would be removing that obscene amount of jargon from your paper. So I ask everyone designing software for scientists, design for humans not robots. Scientists are humans too. So go ahead add that CSS animation.", "links": ["https://blog.datazar.com?source=logo-lo_2026b0e216f0---e2c7e6e1c75", "https://twitter.com/DatazarHQ", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.datazar.com%2Fwhy-we-focus-on-design-just-as-much-as-functionality-f6ccb3b2c926", "https://blog.datazar.com", "https://blog.datazar.com/tagged/r-language", "https://blog.datazar.com/tagged/open-data", "https://blog.datazar.com/tagged/how-to", "https://www.r-bloggers.com/", "https://blog.datazar.com/search", "https://blog.datazar.com/@ata_aman?source=post_header_lockup", "https://blog.datazar.com/@ata_aman?source=post_header_lockup", "https://blog.datazar.com/tagged/science?source=post", "https://blog.datazar.com/tagged/art?source=post", "https://blog.datazar.com/tagged/research?source=post", "https://blog.datazar.com/tagged/software-development?source=post", "https://blog.datazar.com/tagged/design?source=post", "https://blog.datazar.com/@ata_aman?source=footer_card", "https://blog.datazar.com/@ata_aman", "http://twitter.com/datazarhq", "https://blog.datazar.com?source=footer_card", "https://blog.datazar.com?source=footer_card", "https://blog.datazar.com", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "230": {"url": "https://news.ycombinator.com/item?id=14950255", "title": "", "text": "", "links": ["https://news.ycombinator.com", "https://edgylabs.com/machine-learning-to-enhance-smartphone-pictures/", "https://hn.algolia.com/?query=Machine%20Learning%20to%20Enhance%20Smartphone%20Pictures&sort=byDate&dateRange=all&type=story&storyText=false&prefix&page=0", "https://www.google.com/search?q=Machine%20Learning%20to%20Enhance%20Smartphone%20Pictures", "https://github.com/HackerNews/API", "http://www.ycombinator.com/apply/"]}, "231": {"url": "http://starmine.ai/datasets/ds02.html", "title": "", "text": "reddit Slack Twitter Instagram Facebook LinkedIn", "links": ["http://starmine.ai", "http://reddit.com/r/datasets", "https://join.slack.com/t/starmineai/shared_invite/MjIwNjg5OTUyODAzLTE1MDE2MTk1MzQtMmJhYTExNDViMA", "https://twitter.com/starmineAI", "https://www.instagram.com/starmine.ai/", "https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Fstarmine.ai/index.html&t=starmine%20ICO", "https://www.linkedin.com/cws/share?url=http%3A%2F%2Fstarmine.ai/index.html&token=&isFramed=true", "http://research.kraeutli.com/index.php/2013/11/the-tate-collection-on-github", "http://54.174.116.134/recommend/datasets/subscribers/free/supercolumns/supercolumns-elements-CMDB-nasdaq-nyse-otcbb-general-2017/supercolumns-elements-nasdaq-nyse-otcbb-general-UPDATE-2017-03-01.csv", "http://54.174.116.134/recommend/datasets/subscribers/free/supercolumns/supercolumns-elements-CMDB-nasdaq-nyse-otcbb-general-2017/supercolumns-elements-nasdaq-nyse-otcbb-general-UPDATE-2017-03-01.csv", "http://54.174.116.134/recommend/app/ai_connect-finance-historicaltrends_api?ccl1=playstation&ccl2=helium&ccl3=korea&ccl4=shampoo&ccl5=coffee&query=concept_column_labels&themesource=theme_CMDB-nasdaq-nyse-otcbb&month=01&year=all&db=CMDB-nasdaq-nyse-otcbb-general", "https://www0.gsb.columbia.edu/mygsb/faculty/research/pubfiles/1555/contagious.pdf", "https://trends.google.com/trends/explore?q=playstation,helium,korea,shampoo,coffee", "https://trends.google.com/trends/explore?q=playstation,helium,korea,shampoo,coffee"]}, "232": {"url": "https://github.com/maciejkula/spotlight/blob/master/examples/movielens_sequence/readme.rst", "title": "", "text": "Using sequences of user-item interactions as an input for recommender models has a number of attractive properties. Firstly, it recognizes that recommending the next item that a user may want to buy or see is precisely the goal we are trying to achieve. Secondly, it's plausible that the ordering of users' interactions carries additional information over and above just the identities of items they have interacted with. For example, a user is more likely to watch the next episode of a given TV series if they've just finished the previous episode. Finally, when the sequence of past interactions rather than the identity of the user is the input to a model, online systems can incorporate new users (and old users' new actions) in real time. They are fed to the existing model, and do not require a new model to be fit to incorporate new information (unlike factorization models). Recurrent neural networks are the most natural way of modelling such sequence problems. In recommendations, gated recurrent units (GRUs) have been used with success in the Session-based recommendations with recurrent neural networks paper. Spotlight implements a similar model using LSTM units as one of its sequence representations. But recurrent neural networks are not the only way of effectively representing sequences: convolutions can also do the job. In particular, we can use causal convolutions: convolutional filters applied to the sequence in a left-to-right fashion, emitting a representation at each step. They are causal in that the their output at time t is conditional on input up to t-1: this is necessary to ensure that they do not have access to the elements of the sequence we are trying to predict. Like LSTMs, causal convolutions can model sequences with long-term dependencies. This is achieved in two ways: stacking convolutional layers (with padding, every convolutional layer preserves the shape of the input), and dilation: insertion of gaps into the convolutional filters (otherwise known as atrous convolutions). Causal convolutions have been used in several recent high-profile papers: Using convolutional rather than recurrent networks for representing sequences has a couple of advantages, as described in this blog post: Spotlight implements causal convolution models as part of its sequence models package, alongside more traditional recurrent and pooling models. The Spotlight implementation has the following characteristics: The model is trained using one of Spotlight's implicit feedback losses, including pointwise (logistic and hinge) and pairwise (BPR as well as WARP-like adaptive hinge) losses. As with other Spotlight sequence models, the loss is computed for all the time steps of the sequence in one pass: for all timesteps t in the sequence, a prediction using elements up to t-1 is made, and the loss is averaged along both the time and the minibatch axis. This leads to siginficant training speed-ups relative to only computing the loss for the last element in the sequence. To see how causal CNNs compare to more traditional sequence models we can have a look at how they perform at predicting the next rated movie on the Movielens 1M dataset. With 1 million interactions spread among 6000 users and around 4000 movies it should be small enough to run quick experiments, but large enough to yield meaningful results. I chose to split the dataset into 80% train, and 10% test and validation sets. I construct 200-long sequences by splitting each user's item sequence into 200-long chunks; if a chunk is shorter than 200 elements, it's padded with zeros. I use mean reciprocal rank (MRR) as the evaluation metric. To choose hyperparameters, I run a quick, coarse grained hyperparameter search, using random sampling to draw 100 hyperparameter sets. With the data and hyperparameters ready, fitting and evaluating the model is relatively simple: Fitting the models is fairly quick, taking at most two or three minutes on a single K80 GPU. The code for the experiments is available in the experiments folder of the Spotlight repo. The results are as follows: It's difficult to draw clear-cut conclusions about the effect of each hyperparameter, but it looks like: To compare causal convolutions with more traditional sequence models I run similar hyperparameter searches for LSTM-based representations and pooling representations. The pooling representation is a simple averaging of item embedding across the sequence; the LSTM-based model runs an LSTM along a user's interactions, using the hidden state for prediction of the next element at each step. The results are as follows: A single layer LSTM seems to outperform causal convolutions, by an over 10% margin, helped by the adaptive hinge loss. Simple pooling performs quite badly. It looks like causal convolutions need some more work before beating recurrent networks. There are a couple of possible avenues for making them better: I'd love to get some input on these. If you have suggestions, let me know on Twitter or open an issue or PR in Spotlight.", "links": ["https://github.com/", "https://arxiv.org/abs/1511.06939", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.LSTMNet", "https://travis-ci.org/maciejkula/spotlight", "https://arxiv.org/pdf/1609.03499.pdf", "http://papers.nips.cc/paper/6527-conditional-image-generation-with-pixelcnn-decoders", "https://arxiv.org/abs/1610.10099", "https://medium.com/@TalPerry/convolutional-methods-for-text-d5260fd5675f", "https://arxiv.org/pdf/1610.10099.pdf", "https://maciejkula.github.io/spotlight/sequence/sequence.html", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.CNNNet", "https://maciejkula.github.io/spotlight/losses.html", "https://grouplens.org/datasets/movielens/1m/", "https://en.wikipedia.org/wiki/Mean_reciprocal_rank", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.LSTMNet", "https://maciejkula.github.io/spotlight/sequence/representations.html#spotlight.sequence.representations.PoolNet", "https://maciejkula.github.io/spotlight/losses.html#spotlight.losses.adaptive_hinge_loss", "https://twitter.com/Maciej_Kula", "https://github.com/maciejkula/spotlight", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "233": {"url": "https://github.com/ynqa/word-embedding", "title": "", "text": "GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together.  Sign up             Use Git or checkout with SVN using the web URL.             This is an implementation of word embedding (also referred to as word representation) models in Golang. Word embedding makes words' meaning, structure, and concept mapping into vector space (and low dimension). For representative instance: Like this example, it could calculate word meaning by arithmetic operations between vectors. Listed models for word embedding, and checked it already implemented. Downloading text8 corpus, and training by Skip-Gram with negative sampling.", "links": ["https://github.com/", "https://github.com/ynqa/word-embedding/blob/master/LICENSE", "https://help.github.com/articles/which-remote-url-should-i-use", "https://travis-ci.org/ynqa/word-embedding", "https://godoc.org/github.com/ynqa/word-embedding", "https://goreportcard.com/report/github.com/ynqa/word-embedding", "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf", "http://nlp.stanford.edu/pubs/glove.pdf", "https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf", "http://mattmahoney.net/dc/textdata", "https://github.com/cjlin1/libsvm", "http://www.aclweb.org/anthology/Q15-1016", "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.648.8023&rep=rep1&type=pdf", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "234": {"url": "https://github.com/brannondorsey/keras_weight_animator", "title": "", "text": "GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together.  Sign up             Use Git or checkout with SVN using the web URL.          Save Keras weight matrices as short animated videos to better understand what and how your neural network models are learning. Below are examples of the first LSTM layer and the final output layer of a six-class RNN model trained over one epoch. Blue represents low values and red represents high values.   In order to render videos from the saved weight images you must also have the following packages installed on your machine: This module is named keras_weight_animator. It exposes a Keras callback function that you can include in any model fit(...) method. The two required parameters to image_saver_callback(...) are the Keras model and an output_directory to periodically save weight images to. By default, keras_weight_animator saves layer weights every 100 batches to output_directory as PNGs in folders named epoch_XXX-layer_NAME-weights_YY. Once training is complete, you can optionally create short animated video clips from the image sequences saved in output_directory using [bin/create_image_sequence.sh](bin/create_image_sequence.sh) path/to/output_directory. This will use parallel, mogrify, and ffmpeg to create a .mp4 from the image sequences located in each folder of output_directory. Video files will be named like epoch_XXX-layer_NAME-weights_YY.mp4. You can run this script automatically from your training script by passing the render_videos=True parameter to image_saver_callback(...). weight_image_sequences(...) takes a variety of optional keyword arguments. I've included an example usage of the module in examples/wisdm.py. This example uses smartphone accelerometer data from WISDM to classify human activity tasks like walking, standing, sitting, walking upstairs, etc... This example uses a one layer LSTM to classify a set of 60 data points (representing three seconds of data sampled at 20hz) as belonging to one of six classes. It outputs image sequences and videos to data/wisdm. Using a bash script to leverage parallel, ImageMagick, and FFMPEG isn't necessarily the most elegant solution, but its the one I had time for. The goal of this here lil' project was to write a quick tool that allows me to better understand how weights change over mini-batch updates in a variety of neural networks. Perhaps in the future I will come back and clean up some of the inelegancies. If you have interest in contributing or maintaining a cleaner version of this lib, please reach out at brannon@brannondorsey.com. This module is \u00a9 Brannon Dorsey 2017, released under an \u2665 MIT License \u2665. You are free to use, modify, distribute, sell, etc... this software under those terms. Example data is from the WIreless Sensor Datamining (WISDM) Actitracker dataset published by Fordham University: This idea is tangentially inspired by much of the work on Visualizing and Understanding Recurrent Networks by Andrej Karpathy, Justin Johnson, and Fei-Fei Li. GNU Parallel is adamant about citation to the point of excess IMHO, but for what its worth, here is their bibtex:", "links": ["https://github.com/", "https://github.com/brannondorsey/keras_weight_animator/blob/master/LICENSE", "https://help.github.com/articles/which-remote-url-should-i-use", "https://github.com/brannondorsey/keras_weight_animator/pull/1", "http://keras.io", "https://www.gnu.org/software/parallel/", "https://www.imagemagick.org/script/index.php", "https://ffmpeg.org/download.html", "https://keras.io/callbacks/", "http://matplotlib.org/users/colormaps.html", "http://www.cis.fordham.edu/wisdm/dataset.php", "https://arxiv.org/abs/1506.02078", "https://github.com/contact", "https://developer.github.com", "https://training.github.com", "https://shop.github.com", "https://github.com/blog", "https://github.com/about", "https://github.com", "https://github.com/site/terms", "https://github.com/site/privacy", "https://github.com/security", "https://status.github.com/", "https://help.github.com"]}, "235": {"url": "https://axibase.github.io/atsd-use-cases/Expatriation_Q2/", "title": "", "text": "Expatriation is the temporary or permanent relocation of a person by choice or by force from their native country to any other country in the world for work, pleasure, or purpose. History has seen expatriation from nearly every country in the world up to and including the current epoch. Famous American expatriates throughout history have included American founding father Benjamin Franklin, who worked as the first United States Minister to France for almost a decade after appointment by the Continental Congress in 1778. The  sixth President of the United States John Quincy Adams, who served as a U.S. Diplomat in four countries before winning the Presidency as a Democratic-Republican in 1825, most notably opening American diplomatic ties as the first United States Minister to the Russian Empire, a position he loved  so much he declined a subsequent offer to serve as an Associate Justice on the Supreme Court of the United States a few years later. American expatriates have come from all walks of life: authors, such as Ernest Hemingway who lived in China, Spain, Cuba, and was even present on the beach codenamed Omaha in Normandy during the allied landing. Musicians, like Jimi Hendrix who rose to international fame in London after leaving his native Seattle, directors like Stanley Kubrick, singers like Tina Turner, and even basketball players like  Allen Iverson, who played an incomplete season for a Turkish basketball team in 2010 after retiring from the NBA. For those who decide to make the change permanent, there comes a time when the inconvenience of living abroad as a resident alien outweighs the novelty and many decide to renounce their citizenship for one reason or another. Unsurprisingly, this is a long and complicated process, one of the main features of which is settling up with the Internal Revenue Service (IRS) via the eponymous Expatriation Tax. The Federal Register is a publishing outlet for the United States Government where the IRS officially releases quarterly information about American citizens who renounce their passport in lieu of a new one and other long-term residents who have decided to repatriate, view the raw data here and use the Axibase Data Crawler designed specifically for collecting, parsing, organzing and inserting historical expatriation data and new data as it becomes available.  Axibase covered last quarter\u2019s expatriation release here and is using this quarter\u2019s release to follow-up. Every election cycle countless celebrities and netizens issue their promise to leave if their candidate fails to get elected and the growing notoriety surrounding the 2016 Presidental Election has proven to  be no different. This type of social theater is almost as old as the country itself as there have been a number of similarly divisive elections throughout American history: Andrew Jackson\u2019s victory in 1828 was seen as a populace uprising against the corrupt political elite and a return to the authority of the common man, Abraham Lincoln\u2019s second term was viewed as the tightening of federal authority to excessive levels that ultimately led to the American Civil War, Franklin Roosevelt was called a dangerous communist whose infamous court-packing plan favorably increased the number of seats on the Supreme Court to his  advantage and was seen as abhorrent and traitorous by many, and of course who can forget recent elections that have featured scandal after scandal and two modern Presidents who served after losing the popular vote, but winning in the electorate. Figure 1   Query 1 Data queried in the SQL Console. Table 1 To give the issue further context, Figure 1 and Table 1 use the most current Department of Homeland Security (DHS) data  available to display current naturalization numbers in the United States. Naturalization is the sister process to expatriation, because as a person expatriates from one country they must be naturalized in another or end up a stateless person. Naturalization numbers in America have remained fairly constant throughout the last decade because there are a number of federal regulations that control the amount of long-term immigrants that America accepts each year and that quota is met without fail. As noted in the our Q1 article, the sister value and subject of this examination, expatriation numbers, has been steadily growing for several consecutive years. Figure 2.1   Query 2 The above query features robust syntax and calculated values. See the following tutorials  to understand more about SQL Console. Table 2 Figure 2.2   Expatriation has been increasing each year by roughly 30% since 2010, which featured abnormally high expatriation rates, most likely attributable to the economic turndown of the Great Recession which began in the United States as a result of the sub-prime mortgage crisis. American citizenship is often sought after for the economic opportunity that comes along with the passport, as the ability to work and do business in the country is heavily restricted or regulated, and with the value of that investment or  opportunity in question, it is unsurprising that the number of investors in the system, that is, new citizens, would fluctuate. It seems appropriate to call naturalization an investment because of the nature of the process, which is long, complicated, and often quite expensive similar to a long-position that will cost more at purchasing time but promises high returns after reaching maturity. The peak, or more appropriately, valley of the global recession occurred in 2009 when the global GDP contracted causing a decline in the median familty income of about five percent. Figure 3   Query 3 Table 3 Reducing the timespan to look at data over the last 5 years captures the repetition of this trend for this period of time. Of the eighteen quarters included in this query, only six of them, roughly 33% showed negative growth for this statistic. Growing expatriation numbers provide interesting insight into the perception of America both domestically and abroad, as these numbers not only include U.S. citizens who renounce their citizenship, but also long-term resident aliens who have returned home. Almost unbelievably, the IRS does not strip characteristic information about expatriates from their publications meaning that first and last names are included in the data. Using Social Security data concerning the most common first names chosen throughout the last century and United States Census data concerning the most common last names to compare, the following queries and visualizations show  the number of expatriates by common first and common last name throughout the entire observed period, ranked by their frequency in the IRS publication. The goal of these queries is purely demonstrative and lighthearted. There is no intention to invade privacy, defame people, or otherwise cause harm. Figure 4  Query 4 Table 4 All of these names appear on the list of most common American first names, linked here Figure 5  Query 5 Table 5 Only the name Smith appears on the list of most common American last names, linked here Use the Axibase Data Crawler to collect the data used in this artcle and examine it yourself in ChartLab.", "links": ["https://github.com/axibase/atsd-use-cases", "https://www.irs.gov/", "https://www.irs.gov/individuals/international-taxpayers/expatriation-tax", "https://www.federalregister.gov/", "https://www.federalregister.gov/documents/2017/05/10/2017-09475/quarterly-publication-of-individuals-who-have-chosen-to-expatriate-as-required-by-section-6039g", "https://github.com/axibase/atsd-data-crawlers/tree/irs-expatriation-data-crawler", "https://axibase.com", "https://apps.axibase.com/chartlab/654b9945#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql", "https://www.uscis.gov/news/fact-sheets/naturalization-fact-sheet", "http://www.unhcr.org/stateless-people.html", "https://apps.axibase.com/chartlab/7fa5b643#fullscreen", "https://github.com/axibase/atsd/tree/master/api/sql#examples", "https://apps.axibase.com/chartlab/7fa5b643/5/#fullscreen", "https://web.stanford.edu/group/recessiontrends/cgi-bin/web/sites/all/themes/barron/pdf/IncomeWealthDebt_fact_sheet.pdf", "https://apps.axibase.com/chartlab/7fa5b643/2/#fullscreen", "https://www.ssa.gov/oact/babynames/decades/century.html", "http://www.census.gov/main/www/cen2000.html", "https://www.ssa.gov/oact/babynames/decades/century.html", "http://www.census.gov/main/www/cen2000.html", "https://github.com/axibase/atsd-data-crawlers/tree/irs-expatriation-data-crawler", "https://apps.axibase.com", "https://github.com/axibase/atsd-use-cases", "https://github.com/axibase", "https://pages.github.com"]}, "236": {"url": "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/", "title": "", "text": "Machine learning is eating the world right now. Everyone and their mother are learning about machine learning models, classification, neural networks, and Andrew Ng. You\u2019ve decided you want to be a part of it, but where to start? In this article we\u2019ll cover some important characteristics of Python and why it\u2019s great for machine learning. We\u2019ll also cover some of the most important libraries it has for ML, and if it piques your interest, some places where you can learn more. Python is a great choice for machine learning for several reasons. First and foremost, it\u2019s a simple language\u00a0on the surface; even if you\u2019re not familiar with Python, getting up to speed is very quick if you\u2019ve ever used any other language with C-like syntax (i.e. every language out there). Second, Python has a great community, which results in good documentation and friendly, comprehensive answers in StackOverflow (fundamental!). Third, also stemming from the great community, there are plenty of useful libraries for Python (both as \u201cbatteries included\u201d and third party), which solve basically any problem that you can have (including machine learning). Yeah and it\u2019s true. Python isn\u2019t the fastest language out there: all those handy abstractions come at a cost. But here\u2019s the trick: libraries can and do offload the expensive calculations to the much more performant (but harder to use) C and C++. For instance, there\u2019s NumPy, which is a library for numerical computation. It\u2019s written in C, and it\u2019s fast. Practically every library out there that involves intensive calculations uses it \u2014 almost all the libraries listed next use it in some form. So if you read NumPy, think fast. Therefore, you can make your scripts run basically as fast as straight up writing them in a lower level language. So there\u2019s really nothing to worry about when it comes to speed. Are you starting out in machine learning? Want something that covers everything from feature engineering to training and testing a model? Look no further than scikit-learn! This fantastic piece of free software provides every tool necessary for machine learning and data mining. It\u2019s the de facto standard library for machine learning in Python, recommended for most of the \u2018old\u2019 ML algorithms. This library does both classification and regression, supporting basically every algorithm out there (support vector machines, random forest, naive bayes, and so on). It\u2019s built in such a way that allows easy switching of algorithms, so experimentation is easy. These \u2018older\u2019 algorithms are surprisingly resilient and work very well in a lot of cases. But that\u2019s not all! Scikit-learn also does dimensionality reduction, clustering, you name it. It\u2019s also blazingly fast since it runs on NumPy and SciPy (meaning that all the heavy number crunching is run on C instead of Python). Check out some examples to see everything this library is capable of, and the tutorials if you want to learn how it works. While not a machine learning library per se, NLTK is a must when working with natural language processing (NLP). It comes with a bundle of datasets and other lexical resources (useful for training models) in addition to libraries for working with text \u2014 for functions such as classification, tokenization, stemming, tagging, parsing and more. The usefulness of having all of this stuff neatly packaged can\u2019t be overstated. So if you are interested in NLP, check out some tutorials! Used widely in research and academia, Theano is the grandfather of all deep learning frameworks. Written in Python, it\u2019s tightly integrated with NumPy. Theano allows you to create neural networks, which are represented as mathematical expressions with multi-dimensional arrays. Theano handles this for you so you don\u2019t have to worry about the actual implementation of the math involved. It supports offloading calculations to the much faster GPU, which is a feature that everyone supports today, but back when they introduced it this wasn\u2019t the case. The library is very mature at this point and supports a very wide range of operations, which is a great plus when it comes to comparing it with other similar libraries. The biggest complaint out there is that the API may be unwieldy for some, making the library hard to use for beginners. However, there are wrappers that ease the pain and make working with Theano simple, such as Keras, Blocks and Lasagne. Interested in learning about Theano? Check out this Jupyter Notebook tutorial. The Google Brain team created TensorFlow for internal use in machine learning applications, and open sourced it in late 2015. They wanted something that could replace their older, closed source machine learning framework, DistBelief, which they said wasn\u2019t flexible enough and too tightly coupled to their infrastructure to be shared with other researchers around the world. And so TensorFlow was created. Learning from the mistakes of the past, many consider this library to be an improvement over Theano, claiming more flexibility and a more intuitive API. Not only can it be used for research but also for production environments, supporting huge clusters of GPUs for training. While it doesn\u2019t support as wide a range of operations as Theano, it has better computational graph visualizations. TensorFlow is very popular nowadays. In fact, if you\u2019ve heard about a single library on this list, it\u2019s probably this one: there isn\u2019t a day that goes by without a new blog post or paper mentioning TensorFlow gets published. This popularity translates into a lot of new users and a lot of tutorials, making it very welcoming to beginners. Keras is a fantastic library that provides a high-level API for neural networks and is capable of running on top of either Theano or TensorFlow. It makes harnessing the full power of these complex pieces of software much easier than using them directly. It\u2019s very user-friendly, putting user experience as a top priority. They manage this by using simple APIs and excellent feedback on errors. It\u2019s also modular, meaning that different models (neural layers, cost functions, and so on) can be plugged together with little restrictions. This also makes it very easy to extend, since it\u2019s simple to add new modules and connect them with the existing ones. Some people have called Keras so good that it is effectively cheating in machine learning. So if you\u2019re starting out with deep learning, go through the examples and documentation to get a feel for what you can do with it. And if you want to learn, start out with this tutorial and see where you can go from there. Two similar alternatives are Lasagne and Blocks, but they only run on Theano. So if you tried Keras and are unhappy with it, maybe try out one of these alternatives to see if they work out for you. Another popular deep learning framework is Torch, which is written in Lua. Facebook open-sourced a Python implementation of Torch called PyTorch, which allows you to conveniently use the same low-level libraries that Torch uses, but from Python instead of Lua. PyTorch is much better for debugging since one of the biggest differences between Theano/TensorFlow and PyTorch is that the former use symbolic computation while the latter doesn\u2019t. Symbolic computation means that coding an operation (say, \u2018x + y\u2019), it\u2019s not computed when that line is interpreted. Before getting executed it has to be compiled (translated to CUDA or C). This makes debugging harder in Theano/TensorFlow, since an error is much harder to associate with the line of code that caused it. Of course, doing things this way has its advantages, but debugging isn\u2019t one of them. If you want to start out with PyTorch the official tutorials are very friendly to beginners but get to advanced topics as well. Alright, you\u2019ve presented me with a lot of alternatives for machine learning libraries in Python. What should I choose? How do I compare these things? Where do I start? Our Ape Advice\u2122 for beginners is to try and not get bogged down by details. If you\u2019ve never done anything machine learning related, try out scikit-learn. You\u2019ll get an idea of how the cycle of tagging, training and testing work and how a model is developed. Now, if you want to try out deep learning, start out with Keras \u2014 which is widely agreed to be the easiest framework \u2014 and see where that takes you. After you have more experience, you will start to see what it is that you actually want from the framework: greater speed, a different API, or maybe something else, and you\u2019ll be able to make a more informed decision. And even then, there is an endless supply of articles out there comparing Theano, Torch, and TensorFlow. There\u2019s no real way to tell which one is the good one. It\u2019s important to take into account that all of them have wide support and are improving constantly, making comparisons harder to make. A six month old benchmark may be outdated, and year old claims of framework X doesn\u2019t support operation Y could no longer be valid. Finally, if you\u2019re interested in doing machine learning specifically applied to NLP, why not check out MonkeyLearn! Our platform provides a unique UX that makes it super easy to build, train and improve NLP models. You can either use pre-trained models for common use cases (like sentiment analysis, topic detection or keyword extraction) or train custom algorithms using your particular data. Also, you don\u2019t have to worry about the underlying infrastructure or deploying your models, our scalable cloud does this for you. You can start for free and integrate right away with our beautiful API. There are plenty of online resources out there to learn about machine learning ! Here are a few: So that was a brief intro to machine learning in Python and some of its libraries. The important part is not getting bogged down by details and just trying stuff out. Follow your curiosity, and don\u2019t be afraid to experiment. Know about a python library that was left out? Share it in the comments below!", "links": ["http://www.monkeylearn.com", "http://monkeylearn.com/use-cases/", "http://monkeylearn.com/docs/", "http://www.monkeylearn.com/pricing", "http://www.monkeylearn.com/blog/", "https://app.monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/?utm_source=ml-blog&utm_medium=header&utm_campaign=blog", "https://monkeylearn.com/blog", "http://monkeylearn.com/use-cases/", "http://monkeylearn.com/docs/", "http://www.monkeylearn.com/pricing", "http://www.monkeylearn.com/blog/", "https://app.monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/?utm_source=ml-blog&utm_medium=header&utm_campaign=blog", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/07/Post_2c.png", "http://scikit-learn.org/", "http://scikit-learn.org/stable/auto_examples/index.html#general-examples", "http://scikit-learn.org/stable/tutorial/index.html", "http://www.nltk.org/", "http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk", "http://deeplearning.net/software/theano/", "https://keras.io/", "https://github.com/mila-udem/blocks", "https://github.com/Lasagne/Lasagne", "http://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/Theano%20Tutorial.ipynb", "https://www.tensorflow.org/", "https://www.tensorflow.org/get_started/graph_viz", "https://www.tensorflow.org/tutorials/", "https://keras.io/", "https://news.ycombinator.com/item?id=13872764", "https://github.com/fchollet/keras#getting-started-30-seconds-to-keras", "https://keras.io/getting-started/functional-api-guide/", "https://elitedatascience.com/keras-tutorial-deep-learning-in-python", "https://github.com/Lasagne/Lasagne", "https://github.com/mila-udem/blocks", "http://torch.ch/", "http://pytorch.org/", "http://pytorch.org/tutorials/", "http://scikit-learn.org/stable/tutorial/basic/tutorial.html", "https://elitedatascience.com/keras-tutorial-deep-learning-in-python", "https://www.quora.com/Is-TensorFlow-better-than-other-leading-libraries-such-as-Torch-Theano", "http://www.ccri.com/2016/12/09/torch-vs-tensorflow-vs-theano/", "https://www.reddit.com/r/MachineLearning/comments/5w3q74/d_so_pytorch_vs_tensorflow_whats_the_verdict_on/", "https://github.com/zer0n/deepframeworks/blob/master/README.md", "https://monkeylearn.com/", "https://app.monkeylearn.com/accounts/register/", "http://nbviewer.jupyter.org/github/rhiever/Data-Analysis-and-Machine-Learning-Projects/blob/master/example-data-science-notebook/Example%20Machine%20Learning%20Notebook.ipynb", "https://blog.monkeylearn.com/a-gentle-guide-to-machine-learning/", "https://www.coursera.org/learn/machine-learning", "http://course.fast.ai/", "https://monkeylearn.com/blog/author/bruno/", "https://monkeylearn.com/blog/category/news/", "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/#respond", "https://monkeylearn.com/blog/author/bruno/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/monkeylearn_zapier.png", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/wp-content/uploads/2017/06/Post_1e.png", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/naive-bayes-classifier.png", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/wp-content/uploads/2017/05/monkey-startup.png", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/getting-started-with-python-machine-learning/", "https://monkeylearn.com/blog/zapier-integration-machine-learning/", "https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/", "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/", "https://monkeylearn.com/blog/analyzing-startup-news-with-machine-learning/", "https://monkeylearn.com/blog/category/applications/", "https://monkeylearn.com/blog/category/guides/", "https://monkeylearn.com/blog/category/howto/", "https://monkeylearn.com/blog/category/news/", "https://monkeylearn.com/blog/category/text-classification/", "https://www.monkeylearn.com", "https://www.monkeylearn.com", "http://slack.monkeylearn.com/", "https://twitter.com/monkeylearn", "https://github.com/monkeylearn/"]}, "237": {"url": "https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4", "title": "Machine Learning Translation and the Google Translate Algorithm", "text": "Every day we use different technologies without even knowing how exactly they work. In fact, it\u2019s not very easy to understand engines powered by machine learning. The Statsbot team wants to make machine learning clear by telling data stories in this blog. Today, we\u2019ve decided to explore machine translators and explain how the Google Translate algorithm works. Years ago, it was very time consuming to translate the text from an unknown language. Using simple vocabularies with word-for-word translation was hard for two reasons: 1) the reader had to know the grammar rules and 2) needed to keep in mind all language versions while translating the whole sentence. Now, we don\u2019t need to struggle so much\u2013 we can translate phrases, sentences, and even large texts just by putting them in Google Translate. But most people don\u2019t actually care how the engine of machine learning translation works. This post is for those who do care. If the Google Translate engine tried to kept the translations for even short sentences, it wouldn\u2019t work because of the huge number of possible variations. The best idea can be to teach the computer sets of grammar rules and translate the sentences according to them. If only it were as easy as it sounds. If you have ever tried learning a foreign language, you know that there are always a lot of exceptions to rules. When we try to capture all these rules, exceptions and exceptions to the exceptions in the program, the quality of translation breaks down. Creating your own simple machine translator would be a great project for any data science resume. Let\u2019s try to investigate what hides in the \u201cblack boxes\u201d that we call machine translators. Deep neural networks can achieve excellent results in very complicated tasks (speech/visual object recognition), but despite their flexibility, they can be applied only for tasks where the input and target have fixed dimensionality. Here is where Long Short-Term Memory networks (LSTMs) come into play, helping us to work with sequences whose length we can\u2019t know a priori. LSTMs are a special kind of recurrent neural network (RNN), capable of learning long-term dependencies. All RNNs look like a chain of repeating modules. So the LSTM transmits data from module to module and, for example, for generating Ht we use not only Xt, but all previous input values X. To learn more about structure and mathematical models of LSTM, you can read the great article \u201cUnderstanding LSTM Networks.\u201d Our next step is bidirectional recurrent neural networks (BRNNs). What a BRNN does, is split the neurons of a regular RNN into two directions. One direction is for positive time, or forward states. The other direction is for negative time, or backward states. The output of these two states are not connected to inputs of the opposite direction states. To understand why BRNNs can work better than a simple RNN, imagine that we have a sentence of 9 words and we want to predict the 5th word. We can make it know either only the first 4 words, or the first 4 words and last 4 words. Of course, the quality in the second case would be better. Now we\u2019re ready to move to sequence to sequence models (also called seq2seq). The basic seq2seq model consist of two RNNs: an encoder network that processes the input and a decoder network that generates the output. Finally, we can make our first machine translator! However, let\u2019s think about one trick. Google Translate currently supports 103 languages, so we should have 103x102 different models for each pair of languages. Of course, the quality of these models varies according to the popularity of languages and the amount of documents needed for training this network. The best that we can do is to make one NN to take any language as input and translate into any language. That very idea was realized by Google engineers at the end of 2016. Architecture of NN was build on the seq2seq model, which we have already studied. The only exception is that between the encoder and decoder there are 8 layers of LSTM-RNN that have residual connections between layers with some tweaks for accuracy and speed. If you want to go deeper with that, take a look at the article Google\u2019s Neural Machine Translation System. The system requires a \u201ctoken\u201d at the beginning of the input sentence which specifies the language you\u2019re trying to translate the phrase into. This improves translation quality and enables translations even between two languages which the system hasn\u2019t seen yet, a method termed \u201cZero-Shot Translation.\u201d When we\u2019re talking about improvements and better results from Google Translate algorithms, how can we correctly evaluate that the first candidate for translation is better than the second? It\u2019s not a trivial problem, because for some commonly used sentences we have the sets of reference translations from the professional translators, that have, of course, some differences. There are a lot of approaches that partly solve this problem, but the most popular and effective metric is BLEU (bilingual evaluation understudy). Imagine, we have two candidates from machine translators: Although they have the same meaning they differ in quality and have different structure. Let\u2019s look at two human translations: Obviously, Candidate 1 is better, sharing more words and phrases compared to Candidate 2. This is a key idea of the simple BLEU approach. We can compare n-grams of the candidate with n-grams of the reference translation and count the number of matches (independent from their position). We use only n-gram precisions, because calculating recall is difficult with multiple refs and the result is the geometric average of n-gram scores. Now you can evaluate the complex engine of machine learning translation. Next time when you translate something with Google Translate, imagine how many millions of documents it analyzed before giving you the best language version. Clapping shows how much you appreciated Daniil Korbut\u2019s story.", "links": ["https://blog.statsbot.co?source=logo-lo_164dbb90c48e---cfc9f21a543a", "https://twitter.com/statsbotco", "https://medium.com/m/signin?redirect=https%3A%2F%2Fblog.statsbot.co%2Fmachine-learning-translation-96f0ed8f19e4", "https://blog.statsbot.co", "https://blog.statsbot.co/analytics/home", "https://blog.statsbot.co/datascience/home", "https://blog.statsbot.co/design/home", "https://blog.statsbot.co/startups/home", "https://blog.statsbot.co/bots/home", "https://blog.statsbot.co/news/home", "https://blog.statsbot.co/statsbot-digest-b0d7372f842a", "http://statsbot.co?utm_source=blog&utm_medium=navigation&utm_campaign=robot_face", "https://blog.statsbot.co/search", "https://blog.statsbot.co/@daniilkorbut?source=post_header_lockup", "https://blog.statsbot.co/@daniilkorbut?source=post_header_lockup", "https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html", "http://statsbot.co?utm_source=blog&utm_medium=article&utm_campaign=machine_translation", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6?utm_source=blog&utm_medium=article&utm_campaign=machine_translation", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6", "https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6", "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "https://www.semanticscholar.org/paper/Hybrid-speech-recognition-with-Deep-Bidirectional-Graves-Jaitly/5807664af8e63d5207f59fb263c9e7bd3673be79", "https://research.googleblog.com/2016/09/a-neural-network-for-machine.html", "https://www.newscientist.com/article/2114748-google-translate-ai-invents-its-own-language-to-translate-with/", "https://research.googleblog.com/2016/09/a-neural-network-for-machine.html", "https://arxiv.org/abs/1609.08144", "https://en.wikipedia.org/wiki/BLEU", "https://en.wikipedia.org/wiki/N-gram", "https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3", "https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278", "https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2", "https://blog.statsbot.co/tagged/machine-learning?source=post", "https://blog.statsbot.co/tagged/machine-translation?source=post", "https://blog.statsbot.co/tagged/google-translate?source=post", "https://blog.statsbot.co/tagged/data-science?source=post", "https://blog.statsbot.co/tagged/neural-networks?source=post", "https://blog.statsbot.co/@daniilkorbut?source=footer_card", "https://blog.statsbot.co/@daniilkorbut", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co?source=footer_card", "https://blog.statsbot.co", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "238": {"url": "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "title": "Yanir SeroussiMy 10-step path to becoming a remote data scientist with\u00a0Automattic", "text": "About two years ago, I read the book The Year without Pants, which describes the author\u2019s experience leading a team at Automattic (the company behind WordPress.com, among other products). Automattic is a fully-distributed company, which means that all of its employees work remotely (hence pants are optional). While the book discusses some of the challenges of working remotely, the author\u2019s general experience was very positive. A few months after reading the book, I decided to look for a full-time position after a period of independent work. Ideally, I wanted a well-paid data science-y remote job with an established distributed tech company that offers a good life balance and makes products I care about. Automattic seemed to tick all my boxes, so I decided to apply for a job with them. This post describes my application steps, which ultimately led to me becoming a data scientist with Automattic. Before jumping in, it\u2019s worth noting that this post describes my personal experience. If you apply for a job with Automattic, your experience is likely to be different, as the process varies across teams, and evolves over time. I decided to apply for a data wrangler position with Automattic in October 2015. While data wrangler may sound less sexy than data scientist, reading the job ad led me to believe that the position may involve interesting data science work. This impression was strengthened by some LinkedIn stalking, which included finding current data wranglers and reading through their profiles and websites. I later found out that all the people on the data division start out as data wranglers, and then they may pick their own title. Some data wranglers do data science work, while others are more focused on data engineering, and there are some projects that require a broad range of skills. As the usefulness of the term data scientist is questionable, I\u2019m not too fussed about fancy job titles. It\u2019s more important to do interesting work in a supportive environment. Applying for the job was fairly straightforward. I simply followed the instructions from the ad:  Does this sound interesting? If yes, please send a short email to jobs @ this domain telling us about yourself and attach a resum\u00e9. Let us know what you can contribute to the team. Include the title of the position you\u2019re applying for and your name in the subject. Proofread! Make sure you spell and capitalize WordPress and Automattic correctly. We are lucky to receive hundreds of applications for every position, so try to make your application stand out. If you apply for multiple positions or send multiple emails there will be one reply.  Having been on the receiving side of job applications, I find it surprising that many people don\u2019t bother writing a cover letter, addressing the selection criteria in the ad, or even applying for a job they\u2019re qualified to do. Hence, my cover letter was fairly short, comprising of several bullet points that highlight the similarities between the job requirements and my experience. It was nothing fancy, but simple cover letters have worked well for me in the past. The initial application was followed by a long wait. From my research, this is the typical scenario. This is unsurprising, as Automattic is a fairly small company with a large footprint, which is both distributed and known as a great place to work (e.g., its Glassdoor rating is 4.9). Therefore, it attracts many applicants from all over the world, which take a while to process. In addition, Matt Mullenweg (Automattic\u2019s CEO) reviews job applications before passing them on to the team leads. As I didn\u2019t know that Matt reviewed job applications, I decided to try to shorten the wait by getting introduced to someone in the data division. My first attempt was via a second-degree LinkedIn connection who works for Automattic. He responded quickly when I reached out to him, saying that his experience working with the company is in line with the Glassdoor reviews \u2013 it\u2019s the best job he\u2019s had in his 15-year-long career. However, he couldn\u2019t help me with an intro, because there is no simple way around Automattic\u2019s internal processes. Nonetheless, he reassured me that it is worth waiting patiently, as the strict process means that you end up working with great people. I wasn\u2019t in a huge rush to find a job, but in December 2015 I decided to accept an offer to become the head of data science at Car Next Door. This was a good decision at the time, as I believe in the company\u2019s original vision of reducing the number of cars on the road through car sharing, and it seemed like there would be many interesting projects for me to work on. The position wasn\u2019t completely remote, but as the company was already spread across several cities, I was able to work from home for a day or two every week. In addition, it was a pleasant commute by bike from my Sydney home to the office, so putting the fully-remote job search on hold didn\u2019t seem like a major sacrifice. As I haven\u2019t heard anything from Automattic at that stage, it seemed unwise to reject a good offer, so I started working full-time with Car Next Door in January 2016. I successfully attracted Automattic\u2019s attention with a post I published on the misuse of the word insights by many tech companies, which included an example from WordPress.com. Greg Ichneumon Brown, one of the data wranglers, commented on the post, and invited me to apply to join Automattic and help them address the issues I raised. This happened after I accepted the offer from Car Next Door, and hasn\u2019t resulted in any speed up of the process, so I just gave up on Automattic and carried on with my life. I finally heard back from Automattic in February 2016 (four months after my initial application and a month into my employment with Car Next Door). Martin Remy, who leads the data division, emailed me to enquire if I\u2019m still interested in the position. I informed him that I was no longer looking for a job, but we agreed to have an informal chat, as I\u2019ve been waiting for such a long time. As is often the case with Automattic interviews, the chat with Martin was completely text-based. Working with a distributed team means that voice and video calls can be hard to schedule. Hence, Automattic relies heavily on textual channels, and text-based interviews allow the company to test the written communication skills of candidates. The chat revolved around my past work experience, and Martin also took the time to answer my questions about the company and the data division. At the conclusion of the chat, Martin suggested I contact him directly if I was ever interested in continuing the application process. While I was happy with my position at the time, the chat strengthened my positive impression of Automattic, and I decided that I would reapply if I were to look for a full-time position again. My next job search started earlier than I had anticipated. In October 2016, I decided to leave Car Next Door due to disagreements with the founders over the general direction of the company. In addition, I had more flexibility in choosing where to live, as my personal circumstances had changed. As I\u2019ve always been curious about life outside the capital cities of Australia, I wanted to move away from Sydney. While I could have probably continued working remotely with Car Next Door, I felt that it would be better to find a job with a fully-distributed team. Therefore, I messaged Martin and we scheduled another chat. The second chat with Martin took place in early November. Similarly to the first chat, it was conducted via Skype text messages, and revolved around my work in the time that has passed since the first chat. This time, as I was keen on continuing with the process, I asked more specific questions about what kind of work I\u2019m likely to end up doing and what the next steps would be. The answers were that I\u2019d be joining the data science team, and that the next steps are a pre-trial test, a paid trial, and a final interview with Matt. While this sounds straightforward, it took another six months until I finally became an Automattic employee (but I wasn\u2019t in a rush). The pre-trial test consisted of a data analysis task, where I was given a dataset and a set of questions to answer by Carly Stambaugh, the data science lead. The goal of the test is to evaluate the candidate\u2019s approach to a problem, and assess organisational and communication skills. As such, the focus isn\u2019t on obtaining a specific result, so candidates are given a choice of several potential avenues to explore. The open-ended nature of the task is reminiscent of many real-world data science projects, where you don\u2019t always have a clear idea of what you\u2019re going to discover. While some people may find this kind of uncertainty daunting, I find it interesting, as it is one of the things that makes data science a science. I spent a few days analysing the data and preparing a report, which was submitted as a Jupyter Notebook. After submitting my initial report, there were a few follow-up questions, which I answered by email. The report was reviewed by Carly and Martin, and as they were satisfied with my work, I was invited to proceed to the next stage: A paid trial project. The main part of the application process with Automattic is the paid trial project. The rationale behind doing paid trials was explained a few years ago by Matt in Hire by Auditions, Not Resumes:  Before we hire anyone, they go through a trial process first, on contract. They can do the work at night or over the weekend, so they don\u2019t have to leave their current job in the meantime. We pay a standard rate of $25 per hour, regardless of whether you\u2019re applying to be an engineer or the chief financial officer. During the trials, we give the applicants actual work. If you\u2019re applying to work in customer support, you\u2019ll answer tickets. If you\u2019re an engineer, you\u2019ll work on engineering problems. If you\u2019re a designer, you\u2019ll design. There\u2019s nothing like being in the trenches with someone, working with them day by day. It tells you something you can\u2019t learn from resumes, interviews, or reference checks. At the end of the trial, everyone involved has a great sense of whether they want to work together going forward. And, yes, that means everyone \u2014 it\u2019s a mutual tryout. Some people decide we\u2019re not the right fit for them.  The goal of my trial project was to improve the Elasticsearch language detection algorithm. This took about a month, and ultimately resulted in a pull request that got merged into the language detection plugin. I find this aspect of the process pretty exciting: While the plugin is used to classify millions of documents internally by Automattic, its impact extends beyond the company, as Elasticsearch is used by many other organisations and projects. This stands in contrast to many other technical job interviews, which consist of unpaid work on toy problems under stressful conditions, where the work performed is ultimately thrown away. While the monetary compensation for the trial work is lower than the market rate for data science consulting, I valued the opportunity to work on a real open source project, even if this hadn\u2019t led to me getting hired. There was much more to the trial project than what\u2019s shown in the final pull request. Most of the discussions were held on an internal project thread, primarly under the guidance of Carly (the data science lead), and Greg (the data wrangler who replied to my post a year earlier). The project was kicked off with a general problem statement: There was some evidence that the Elasticsearch language detection plugin doesn\u2019t perform well on short texts, and my mission was to improve it. As the plugin didn\u2019t include any tests for short texts, one of the main contributions of my work was the creation of datasets and tests to measure its accuracy on texts of different lengths. This was followed by some tweaks that improved the plugin\u2019s performance, as summarised in the pull request. Internally, this work consisted of several iterations where I came up with ideas, asked questions, implemented the ideas, shared the results, and discussed further steps. There are still many possible improvements to the work done in the trial. However, as trials generally last around a month, we decided to end it after a few iterations. I enjoyed the trial process, but it is definitely not for everyone. Most notably, there is a strong emphasis on asynchronous text-based communication, which is the main mode by which projects are coordinated at Automattic. People who don\u2019t enjoy written communication may find this aspect challenging, but I have always found that writing helps me organise my thoughts, and that I retain information better when reading than when listening to people speak. That being said, Automatticians do meet in person several times a year, and some teams have video chats for some discussions. While doing the trial, I had a video chat with Carly, which was the first (and last) time in the process that I got to see and hear a live human. However, this was not an essential part of the trial project, as our chat was mostly on the data scientist role and my job expectations. I finished working on the trial project just before Christmas. The feedback I received throughout the trial was positive, but Martin, Carly, and Greg had to go through the work and discuss it among themselves before making a final decision. This took about a month, due to the holiday period, various personal circumstances, and the data science team meetup that was scheduled for January 2017. Eventually, Martin got back to me with positive news: They were satisfied with my trial work, which meant there was only one stage left \u2013 the final interview with Matt Mullenweg, Automattic\u2019s CEO. Like other parts of the process, the interview with Matt is text-based. The way it works is fairly simple: I was instructed to message Matt on Slack and wait for a response, which may take days or weeks. I sent Matt a message on January 25, and was surprised to hear back from him the following morning. However, that day was Australia Day, which is a public holiday here. Therefore, I only got back to him two hours after he messaged me that morning, and by that time he was probably already busy with other things. This was the start of a pretty long wait. I left Car Next Door at the end of January, as I figured that I would be able to line up some other work even if things didn\u2019t work out with Automattic. My plan was to take some time off, and then move up to the Northern Rivers area of New South Wales. I had two Reef Life Survey trips planned, so I wasn\u2019t going to start working again before mid-April. I assumed that I would hear back from Matt before then, which would have allowed me to make an informed decision whether to look for another job or not. After two weeks of waiting, the time for my dive trips was nearing. As I was going to be without mobile reception for a while, I thought it\u2019d be worth letting Matt know my schedule. After discussing the matter with Martin, I messaged Matt. He responded, saying that we might as well do the interview at the beginning of April, as I won\u2019t be starting work before that time anyway. I would have preferred to be done with the interview earlier, but was happy to have some certainty and not worry about missing more chat messages before April. In early April, I returned from my second dive trip (which included a close encounter with Cyclone Debbie), and was hoping to sort out my remote work situation while completing the move up north. Unfortunately, while the move was successful, I was ready to give up on Automattic because I haven\u2019t heard back from Matt at all in April. However, Martin remained optimistic and encouraged me to wait patiently, which I did as I was pretty busy with the move and with some casual freelancing projects. The chat with Matt finally happened on May 2. As is often the case, it took a few hours and covered my background, the trial process, and some other general questions. I asked him about my long wait for the final chat, and he apologised for me being an outlier, as most chats happen within two weeks of a candidate being passed over to him. As the chat was about to conclude, we got to the topic of salary negotiation (which went well), and then the process was finally over! Within a few hours of the chat I was sent an offer letter and an employment contract. As Automattic has an entity in Australia (called Ausomattic), it\u2019s a fairly standard contract. I signed the contract and started work the following week \u2013 over a year and a half after my initial application. Even before I started working, I booked tickets to meet the data division in Montr\u00e9al \u2013 a fairly swift transition from the long wait for the final interview. As noted above, Automatticians get to choose their own job titles, so to become a data scientist with Automattic, I had to set my job title to Data Scientist. This is generally how many people become data scientists these days, even outside Automattic. However, job titles don\u2019t matter as much as job satisfaction. And after 2.5 months with Automattic, I\u2019m very satisfied with my decision to join the company. My first three weeks were spent doing customer support, like all new Automattic employees. Since then, I\u2019ve been involved in projects to make engagement measurement more consistent (harder than it sounds, as counting things is hard), and to improve the data science codebase (e.g., moving away from Legacy Python). Besides that, I also went to Montr\u00e9al for the data division meetup, and have started getting into chatbot work. I\u2019m looking forward to doing more work and sharing my experience here and on data.blog. Very enlightening post! It was very awesome to see that the insights you saw to Elasticsearch went to a PR. I bet that was worth the whole thing!  That\u2019s very exciting, I wanted to ask are you a self learner or do you have a degree,can you please share your background. Thank you  Thanks Mostafa. Yes, I have a BSc in computer science, and a PhD in what you would now call data science. See: https://www.linkedin.com/in/yanirseroussi/  Fill in your details below or click an icon to log in:     Connecting to %s         Enter your email address to follow this blog and receive notifications of new posts by email.", "links": ["https://yanirseroussi.com/", "https://yanirseroussi.com/", "https://yanirseroussi.com/about/", "https://yanirseroussi.com/presentations/", "https://yanirseroussi.com/phd-work/", "https://yanirseroussi.com/2014/04/05/kaggle-competition-summaries/", "http://scottberkun.com/yearwithoutpants/", "https://automattic.com/", "https://yanirseroussi.com/2015/03/22/the-long-road-to-a-lifestyle-business/", "http://web.archive.org/web/20150908140923/https://automattic.com/work-with-us/data-wrangler/", "https://yanirseroussi.com/2016/08/04/is-data-scientist-a-useless-job-title/", "https://automattic.com/about/", "https://www.glassdoor.com.au/Reviews/Automattic-Reviews-E751107.htm", "http://davemart.in/remote-hiring/", "https://www.carnextdoor.com.au/", "https://yanirseroussi.com/2015/12/08/this-holiday-season-give-me-real-insights/", "https://yanirseroussi.com/2015/12/08/this-holiday-season-give-me-real-insights/#comment-957", "http://jupyter.org/", "https://hbr.org/2014/01/hire-by-auditions-not-resumes", "https://www.elastic.co/products/elasticsearch", "https://github.com/jprante/elasticsearch-langdetect/pull/69", "https://github.com/jprante/elasticsearch-langdetect/pull/69", "https://yanirseroussi.com/2016/01/24/the-joys-of-offline-data-collection/", "https://www.whitsundaytimes.com.au/news/boat-caught-in-eye-of-cyclone-cruises-home/3164170/", "https://data.blog/2017/06/29/data-coalesce-automattic-data-division-meets-in-montreal/", "http://daynebatten.com/2016/06/counting-hard-data-science/", "http://powerfulpython.com/blog/magic-word-legacy-python/", "https://data.blog/2017/05/24/may-the-bot-be-with-you-how-algorithms-are-supporting-happiness-at-wordpress-com/", "https://data.blog/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=twitter", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=facebook", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=google-plus-1", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=linkedin", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?share=reddit", "https://yanirseroussi.com/category/data-science-2/", "https://yanirseroussi.com/tag/automattic/", "https://yanirseroussi.com/tag/career/", "https://yanirseroussi.com/tag/data-science/", "https://yanirseroussi.com/tag/elasticsearch/", "https://yanirseroussi.com/tag/personal/", "https://yanirseroussi.com/tag/wordpress/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "https://yanirseroussi.com/author/yanirseroussi/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comments", "https://yanirseroussi.com/2017/06/03/exploring-and-visualising-reef-life-survey-data/", "https://dotnetmeditations.com/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1698", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1698&_wpnonce=6c197b2684", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1698#respond", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1700", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1700&_wpnonce=350fa40ca9", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1700#respond", "http://yanirseroussi.com", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/#comment-1705", "https://www.linkedin.com/in/yanirseroussi/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?like_comment=1705&_wpnonce=7f1e8d7061", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/?replytocom=1705#respond", "https://gravatar.com/site/signup/", "https://yanirseroussi.com/2017/07/29/my-10-step-path-to-becoming-a-remote-data-scientist-with-automattic/", "https://yanirseroussi.com/2017/06/03/exploring-and-visualising-reef-life-survey-data/", "https://yanirseroussi.com/2017/01/08/customer-lifetime-value-and-the-proliferation-of-misinformation-on-the-internet/", "https://yanirseroussi.com/2016/09/19/ask-why-finding-motives-causes-and-purpose-in-data-science/", "https://yanirseroussi.com/2016/08/21/seven-ways-to-be-data-driven-off-a-cliff/", "https://yanirseroussi.com/category/bcrecommender/", "https://yanirseroussi.com/category/data-science-2/", "https://yanirseroussi.com/category/environment/", "https://yanirseroussi.com/category/general/", "https://yanirseroussi.com/category/kaggle-2/", "https://yanirseroussi.com/category/machine-intelligence/", "https://yanirseroussi.com/category/phd-work/", "https://wordpress.com/?ref=footer_blog"]}, "239": {"url": "https://medium.com/airbnb-engineering/using-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d", "title": "Using Machine Learning to Predict Value of Homes On\u00a0Airbnb", "text": "by Robert Chang Data products have always been an instrumental part of Airbnb\u2019s service. However, we have long recognized that it\u2019s costly to make data products. For example, personalized search ranking enables guests to more easily discover homes, and smart pricing allows hosts to set more competitive prices according to supply and demand. However, these projects each required a lot of dedicated data science and engineering time and effort. Recently, advances in Airbnb\u2019s machine learning infrastructure have lowered the cost significantly to deploy new machine learning models to production. For example, our ML Infra team built a general feature repository that allows users to leverage high quality, vetted, reusable features in their models. Data scientists have started to incorporate several AutoML tools into their workflows to speed up model selection and performance benchmarking. Additionally, ML infra created a new framework that will automatically translate Jupyter notebooks into Airflow pipelines. In this post, I will describe how these tools worked together to expedite the modeling process and hence lower the overall development costs for a specific use case of LTV modeling\u200a\u2014\u200apredicting the value of homes on Airbnb. Customer Lifetime Value (LTV), a popular concept among e-commerce and marketplace companies, captures the projected value of a user for a fixed time horizon, often measured in dollar terms. At e-commerce companies like Spotify or Netflix, LTV is often used to make pricing decisions like setting subscription fees. At marketplace companies like Airbnb, knowing users\u2019 LTVs enable us to allocate budget across different marketing channels more efficiently, calculate more precise bidding prices for online marketing based on keywords, and create better listing segments. While one can use past data to calculate the historical value of existing listings, we took one step further to predict LTV of new listings using machine learning. Data scientists are typically accustomed to machine learning related tasks such as feature engineering, prototyping, and model selection. However, taking a model prototype to production often requires an orthogonal set of data engineering skills that data scientists might not be familiar with. Luckily, At Airbnb we have machine learning tools that abstract away the engineering work behind productionizing ML models. In fact, we could not have put our model into production without these amazing tools. The remainder of this post is organized into four topics, along with the tools we used to tackle each task: One of the first steps of any supervised machine learning project is to define relevant features that are correlated with the chosen outcome variable, a process called feature engineering. For example, in predicting LTV, one might compute the percentage of the next 180 calendar dates that a listing is available or a listing\u2019s price relative to comparable listings in the same market. At Airbnb, feature engineering often means writing Hive queries to create features from scratch. However, this work is tedious and time consuming as it requires specific domain knowledge and business logic, which means the feature pipelines are often not easily sharable or even reusable. To make this work more scalable, we developed Zipline\u200a\u2014\u200aa training feature repository that provides features at different levels of granularity, such as at the host, guest, listing, or market level. The crowdsourced nature of this internal tool allows data scientists to use a wide variety of high quality, vetted features that others have prepared for past projects. If a desired feature is not available, a user can create her own feature with a feature configuration file like the following: When multiple features are required for the construction of a training set, Zipline will automatically perform intelligent key joins and backfill the training dataset behind the scenes. For the listing LTV model, we used existing Zipline features and also added a handful of our own. In sum, there were over 150 features in our model, including: With our features and outcome variable defined, we can now train a model to learn from our historical data. As in the example training dataset above, we often need to perform additional data processing before we can fit a model: In this step, we don\u2019t quite know what is the best set of features to use, so writing code that allows us to rapidly iterate is essential. The pipeline construct, commonly available in open-source tools like Scikit-Learn and Spark, is a very convenient tool for prototyping. Pipelines allow data scientists to specify high-level blueprints that describe how features should be transformed, and which models to train. To make it more concrete, below is a code snippet from our LTV model pipeline: At a high level, we use pipelines to specify data transformations for different types of features, depending on whether those features are of type binary, categorical, or numeric. FeatureUnion at the end simply combines the features column-wise to create the final training dataset. The advantage of writing prototypes with pipelines is that it abstracts away tedious data transformations using data transforms. Collectively, these transforms ensure that data will be transformed consistently across training and scoring, which solves a common problem of data transformation inconsistency when translating a prototype into production. Furthermore, pipelines also separates data transformations from model fitting. While not shown in the code above, data scientists can add a final step to specify an estimator for model fitting. By exploring different estimators, data scientists can perform model selection to pick the best model to improve the model\u2019s out of sample error. As mentioned in the previous section, we need to decide which candidate model is the best to put into production. To make such a decision, we need to weigh the tradeoffs between model interpretability and model complexity. For example, a sparse linear model might be very interpretable but not complex enough to generalize well. A tree based model might be flexible enough to capture non-linear patterns but not very interpretable. This is known as the Bias-Variance tradeoff. In applications such as insurance or credit screening, a model needs to be interpretable because it\u2019s important for the model to avoid inadvertently discriminating against certain customers. In applications such as image classification, however, it is much more important to have a performant classifier than an interpretable model. Given that model selection can be quite time consuming, we experimented with using various AutoML tools to speed up the process. By exploring a wide variety of models, we found which types of models tended to perform best. For example, we learned that eXtreme gradient boosted trees (XGBoost) significantly outperformed benchmark models such as mean response models, ridge regression models, and single decision trees. Given that our primary goal was to predict listing values, we felt comfortable productionizing our final model using XGBoost, which favors flexibility over interpretability. As we alluded to earlier, building a production pipeline is quite different from building a prototype on a local laptop. For example, how can we perform periodic re-training? How do we score a large number of examples efficiently? How do we build a pipeline to monitor model performance over time? At Airbnb, we built a framework called ML Automator that automagically translates a Jupyter notebook into an Airflow machine learning pipeline. This framework is designed specifically for data scientists who are already familiar with writing prototypes in Python, and want to take their model to production with limited experience in data engineering. Here is a code snippet demonstrating how the fit and transform functions are defined in our LTV model. The fit function tells the framework that a XGBoost model will be trained, and that data transformations will be carried out according to the pipeline we defined previously. Once the notebook is merged, ML Automator will wrap the trained model inside a Python UDF and create an Airflow pipeline like the one below. Data engineering tasks such as data serialization, scheduling of periodic re-training, and distributed scoring are all encapsulated as a part of this daily batch job. As a result, this framework significantly lowers the cost of model development for data scientists, as if there was a dedicated data engineer working alongside the data scientists to take the model into production! Note: Beyond productionization, there are other topics, such as tracking model performance over time or leveraging elastic compute environment for modeling, which we will not cover in this post. Rest assured, these are all active areas under development. In the past few months, data scientists have partnered very closely with ML Infra, and many great patterns and ideas arose out of this collaboration. In fact, we believe that these tools will unlock a new paradigm for how to develop machine learning models at Airbnb. We are very excited about the future of this framework and the new paradigm it brought along. By bridging the gap between prototyping and productionization, we can truly enable data scientists and engineers to pursue end-to-end machine learning projects and make our product better. Want to use or build these ML tools? We\u2019re always looking for talented people to join our Data Science and Analytics team! Special thanks to members of Data Science & ML Infra team who were involved in this work: Aaron Keys, Brad Hunter, Hamel Husain, Jiaying Shi, Krishna Puttaswamy, Michael Musson, Nick Handel, Varant Zanoyan, Vaughn Quoss et al. Additional thanks to Gary Tang, Jason Goodman, Jeff Feng, Lindsay Pettingill for reviewing this blog post.", "links": ["https://medium.com/", "https://medium.com/airbnb-engineering?source=logo-lo_1c4bac4bfcb2---53c7c27702d5", "https://twitter.com/AirbnbEng", "https://medium.com/m/signin?redirect=https%3A%2F%2Fmedium.com%2Fairbnb-engineering%2Fusing-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d", "https://medium.com/airbnb-engineering", "https://medium.com/airbnb-engineering/ai/home", "https://medium.com/airbnb-engineering/airbnb-engineering-backend/home", "https://medium.com/airbnb-engineering/data/home", "https://medium.com/airbnb-engineering/airbnb-engineering-infrastructure/home", "https://medium.com/airbnb-engineering/tagged/mobile", "https://medium.com/airbnb-engineering/web/home", "http://airbnb.io/projects/", "https://medium.com/airbnb-engineering/search", "https://medium.com/@rchang?source=post_header_lockup", "https://medium.com/@rchang?source=post_header_lockup", "https://twitter.com/_rchang", "https://medium.com/swlh/diligence-at-social-capital-part-3-cohorts-and-revenue-ltv-ab65a07464e1", "http://scikit-learn.org/stable/", "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html", "https://www.kaggle.com/general/16927", "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html", "https://spark.apache.org/docs/latest/ml-pipeline.html", "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html", "http://scikit-learn.org/stable/data_transforms.html", "http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html", "https://medium.com/airbnb-engineering/automated-machine-learning-a-paradigm-shift-that-accelerates-data-scientist-productivity-airbnb-f1f8a10d61f8", "http://scott.fortmann-roe.com/docs/BiasVariance.html", "https://medium.com/airbnb-engineering/automated-machine-learning-a-paradigm-shift-that-accelerates-data-scientist-productivity-airbnb-f1f8a10d61f8", "https://github.com/dmlc/xgboost", "https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8", "http://www.florianwilhelm.info/2016/10/python_udf_in_hive/", "https://airflow.incubator.apache.org/", "https://www.airbnb.com/careers/departments/data-science-analytics", "https://www.linkedin.com/in/aaronkeys/", "https://www.linkedin.com/in/brad-hunter-497621a/", "https://www.linkedin.com/in/hamelhusain/", "https://www.linkedin.com/in/jiaying-shi-a2142733/", "https://www.linkedin.com/in/krishnaputtaswamy/", "https://www.linkedin.com/in/michael-m-a37b1932/", "https://www.linkedin.com/in/nicholashandel/", "https://www.linkedin.com/in/vzanoyan/", "https://www.linkedin.com/in/vquoss/", "https://www.linkedin.com/in/thegarytang/", "https://medium.com/@jasonkgoodman", "https://twitter.com/jtfeng", "https://medium.com/@lpettingill", "https://medium.com/@jtfeng?source=post_page", "https://medium.com/@jasonkgoodman?source=post_page", "https://medium.com/@gary.tang_94319?source=post_page", "https://medium.com/@lpettingill?source=post_page", "https://medium.com/@vquoss?source=post_page", "https://medium.com/@eddie.santos.3?source=post_page", "https://medium.com/@NicholasHandel?source=post_page", "https://medium.com/tag/machine-learning?source=post", "https://medium.com/tag/data-science?source=post", "https://medium.com/tag/airbnb?source=post", "https://medium.com/tag/technology?source=post", "https://medium.com/@rchang?source=footer_card", "https://medium.com/@rchang", "http://twitter.com/Airbnb", "http://twitter.com/Twitter", "https://medium.com/airbnb-engineering?source=footer_card", "https://medium.com/airbnb-engineering?source=footer_card", "http://airbnb.io", "https://medium.com/airbnb-engineering", "https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg"]}, "240": {"url": "https://www.ayasdi.com/blog/artificial-intelligence/why_ai_is_the_new_bigdata/", "title": "The \u201cBig Data-ization\u201d of Artificial Intelligence", "text": "\u2039 Back to Blog  Artificial Intelligence, Data, Machine Intelligence, Machine Learning It seems like it was only a few years ago that the term \u201cbig data\u201d went from a promising area of research and interest to something so ubiquitous that it lost all meaning, descending ultimately into the butt of jokes.  As everyone piled onto the big data bandwagon, it became impossible to separate truth from fiction. Every executive and entrepreneur that I ran into was doing a \u201cbig data\u201d thing. I recall meeting someone whose company shot videos for enterprise customers and was pitching it as a \u2018Big Data play\u2019 \u2013 because video files, you know, are huge \u2013 they take up lots of space.  Thankfully, the noise associated with \u201cbig data\u201d is abating as sophistication and common sense take hold. In fact, in many circles the term actually exposes the user as someone who doesn\u2019t really understand the space.  Unfortunately, the same malady has now afflicted AI. Everyone I meet is doing an \u2018AI play\u2019 \u2013 even if all they did was to build a simple linear regressor in Excel.  AI is, unfortunately, the new \u201cbig data.\u201d While not good, it is not all bad either.  After all, the data ecosystem benefited from all of the \u201cbig data\u201d attention and investment \u2013 creating some amazing software and producing some exceptional productivity gains.  The same will happen with AI \u2013 with the increased attention comes investment dollars which in turn will drive adoption \u2013 enhancing the ecosystem. Having said that we need to stop calling regression on excel AI \u2013 it\u2019s ridiculous and undermines some incredible work being done in the space.  Regression has been around for 200+ years. Gauss and Legendre didn\u2019t don the AI cloak when they discovered regression. Neither should the recent stats graduate trying to raise money for his/her startup.  I will tell you what I think qualifies as AI in a moment \u2013 but here are some thoughts that influence that discussion. First, the AI definition I use is focused on the narrow, application specific AI, not the more general problem of artificial general intelligence (AGI) where simulating a person using software is the equivalent of intelligence.  Second, the vast, vast majority of the data that exists in the world is unlabeled. It is not practical to label that data manually and doing so would likely create bias anyway. One can argue that the Internet as a whole is simply a mechanism for humans to entertain ourselves while providing label data for machines.  Unlabeled data presents a different challenge, one we will address shortly, but the key point here is that it is everywhere and represents the key to extracting business value (or any value).  Third, we are not producing data scientists at a rate that can keep pace with the growth of data. Even with the moniker as a the \u201csexiest job of the 21st century\u201d the pace at which data scientists are created doesn\u2019t begin to approach the growth rate we are seeing in data.  Fourth, data scientists, for the most part, are not UX designers or product managers or, in many cases even engineers. As a result, the subject matter experts, those that sit in the business, don\u2019t have effective interfaces to the data science outputs. The interfaces that they have \u2013 powerpoint, excel, or PDF reports have limited utility in transforming the behavior of a company. What is required is something to shape behavior is something more \u2013 applications.  So what does qualify as intelligence? In \u2018On Intelligence\u2019 Jeff Hawkins says that all AI boils down to memory and prediction. The argument is very persuasive, but a bit reductive. We have a slightly different take for what an AI should display and it encompasses a framework: Here are characteristics that I think any AI should display. While some of these elements may seem self-evident that is because they are taken as a single item. Intelligence has a broader context. All the elements must work in conjunction with each other to qualify as AI. \u00a0\u00a0\u00a0 The five elements are: Let\u2019s take each of these concepts in turn. Discovery is the ability of an intelligent system to learn from data without upfront human intervention. Often, this needs to be done without being presented with an explicit target. It relies on the use of unsupervised and semi-supervised machine learning techniques (such as segmentation, dimensionality reduction, anomaly detection, etc.), as well as more supervised techniques where there is an outcome or there are several outcomes of interest. Usually, in enterprise software, the term discovery refers to the ability of ETL/MDM solutions to discover the various schemas of tables in large databases and automatically find join keys etc. This is not what we mean by discovery. We use of the term very differently and has this has important implications. In complex datasets, it is nearly impossible to ask the \u201cright\u201d questions. To discover what value lies within the data one must \u00a0understand all the relationships that are inherent and important in the data. That requires a principled approach to hypothesis generation. \u00a0 One technique, topological data analysis (TDA), is exceptional at surfacing hidden relationships that exist in the data and identifying those relationships that are meaningful without having to ask specific questions of the data. The result is an output that is able to represent complex phenomena, and is therefore able to surface weaker signals as well as the stronger signals. \u00a0 This permits the detection of emergent phenomena. \u00a0 As a result, enterprises can now discover answers to questions they didn\u2019t even know to ask and do so with data that is unlabeled. Once the data set is understood through intelligent discovery, supervised approaches are applied to predict what will happen in the future. These types of problems include classification, regression and ranking.  For this pillar, most companies use a standard set of supervised machine learning algorithms including random forests, gradient boosting, linear/sparse learners. It should be noted, however, that the unsupervised work from the previous step is highly useful in many ways. For example, it can generate relevant features for use in prediction tasks or finding local patches of data where supervised algorithms may struggle (systematic errors). The predict phase is an important part of the business value associated with data science, however, generally, in predictive analytics, there exists a notion that this is the sum total of machine learning.  This is not the case by far.  Prediction, while important, is pretty well understood and does not, on its own qualify as \u201cintelligence.\u201d \u00a0It goes back to calling a maxed out Excel table and a linear regressor AI. It is just not the case. Further, Prediction can go wrong along a number of dimensions, particularly if the groups on which you are predicting are racked with some type of bias (algorithmic, sampling etc.)  Again, Prediction is key and provides tremendous business value done correctly, but in and of itself it is not AI. We need to stop calling it as such. Applications need to support interaction with humans in a way which makes outcomes recognizable and believable. For example, when one builds a predictive model, it is important to have an explanation of how the model is doing what it is doing, i.e. what the features in the model are doing in terms that are familiar to the users of the model. \u00a0This level of familiarity is important in generating trust and intuition.  Similarly, in the same way that automobiles have mechanisms not just for detecting the presence of a malfunction, but also for specifying the nature of the malfunction and suggesting a method for correcting it, so one needs to have a \u201cnuts and bolts\u201d understanding of how an application is working in order to \u201crepair\u201d it when it goes awry. \u00a0 Transparency AND Justification. There is a difference. Transparency tells you what algorithms and parameters were used, while, Justification tells you why. For intelligence to be meaningful, it must be able to justify and explain its assertions, as well as to be able to diagnose failures.  No business leader should deploy intelligent and autonomous applications against critical business problems without a thorough understanding of what variables power the model.  Enterprises cannot move to a model of intelligent applications without trust and transparency. \u00a0 AI without UX is of limited utility.  UX is what distributes that intelligence across the organization and pushes it to the edge \u2013 where it can consumed by practitioners and subject matter experts.  Ultimately, the process of operationalizing an intelligent application within the enterprise requires some change in the organization, an acceptance that the application will evolve over time and that will demand downstream changes \u2013 automated or otherwise.  \u00a0For this to happen, intelligent applications need to be \u201clive\u201d in the business process, seeing new data and automatically executing the loop of Discover, Predict, Justify on a frequency that makes sense for that business process. For some processes that may be quarterly, for others daily. That loop can even be measured in seconds. \u00a0 Intelligent systems are designed to detect and react as the data evolves. An intelligent system is one that is always learning, live in the workflow and constantly improving. \u00a0In the modern data world, an application that is not getting more intelligent is getting dumber.  Intelligent applications are designed to detect and react when data distributions evolve. As a result, they need to be \u201con the wire\u201d in order to detect that phenomena before it becomes a problem. Too many solutions provide an answer in a point of time, an intelligent system is one that is always learning through the framework outlined here. This is what defines intelligence \u2013 not a machine learning algorithm kicking out predictions or the results of a data scientist\u2019s work as a PDF. For the industry to continue to grow and evolve we need to start doing a better job of recognizing what is truly AI and what is imposter-ware.  Let\u2019s make it a point of celebrating the real AI work.", "links": ["https://www.ayasdi.com", "https://www.ayasdi.com/platform/", "https://www.ayasdi.com/industry-overview/", "https://www.ayasdi.com/financial-services/", "https://www.ayasdi.com/healthcare/", "https://www.ayasdi.com/public-sector/", "https://www.ayasdi.com/solutions/anti-money-laundering/", "https://www.ayasdi.com/solutions/clinical-variation-management/", "https://www.ayasdi.com/solutions/denials/", "https://www.ayasdi.com/solutions/regulatory-risk/", "https://www.ayasdi.com/solutions/population-health/", "https://www.ayasdi.com/company/", "https://www.ayasdi.com/company/leadership/", "https://www.ayasdi.com/company/careers/", "https://www.ayasdi.com/company/collaborators/", "https://www.ayasdi.com/company/customers/", "https://www.ayasdi.com/company/news-and-events/", "https://www.ayasdi.com/company/contact-us/", "https://www.ayasdi.com/blog/", "https://www.ayasdi.com/request-a-demo/", "https://www.ayasdi.com/blog/author/gurjeet-singh/", "https://www.ayasdi.com/blog/category/artificial-intelligence/", "https://www.ayasdi.com/blog/category/bigdata/", "https://www.ayasdi.com/blog/category/machine-intelligence/", "https://www.ayasdi.com/blog/category/machine-learning/", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/08/02144138/BG2.png", "https://priceonomics.com/the-discovery-of-statistical-regression/", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/05/12132340/Machine_Intelligence_Apps_WP_051617v01.pdf", "https://s3.amazonaws.com/cdn.ayasdi.com/wp-content/uploads/2017/08/02144141/DS1.png", "https://www.wired.com/brandlab/2015/05/jeff-hawkins-firing-silicon-brain/", "http://cdn.ayasdi.com/wp-content/uploads/2015/02/wp-tda-and-machine-learning.pdf", "https://www.ayasdi.com/blog/aml/intelligent-segmentation-as-the-attack-point-for-aml/", "http://feedburner.google.com/fb/a/mailverify?uri=AyasdiBlog", "https://ayasdicommunity.force.com/login", "https://www.ayasdi.com/terms-of-service/", "https://www.ayasdi.com/privacy-policy/", "https://www.ayasdi.com/pilot-terms/", "http://www.youtube.com/user/ayasdi", "http://feeds.feedburner.com/AyasdiBlog", "http://www.linkedin.com/company/ayasdi", "https://www.facebook.com/ayasdi", "https://twitter.com/ayasdi"]}, "241": {"url": "https://www.mapd.com/blog/2017/08/03/whos-got-the-best-mobile-network-in-the-us/", "title": "", "text": "Based on the deluge of commercials and ads in this competitive industry, it seems every major player claims that they do. The competition is apparently so fierce that even their spokesmen are being headhunted. Admittedly, it can be a bit confusing when you find these results in one city:  And these in another:  By the way, how do you even define \u2018best\u2019 network? It\u2019s a loaded question for sure. We\u2019ll get back to that in a minute. It\u2019s a question with relevance to nearly every American, considering the vast majority of us own a cellphone of some kind (over 95% according to Pew). And we don\u2019t leave them alone; we touch them thousands of times each day. In fact, a good mobile network can affect our very quality of life. Ever travelled any significant distance with children and poor mobile data speeds? The same question is also relevant to those very telecommunications providers themselves, of course, and particularly in light of the fact that obtaining new subscribers will increasingly mean winning them away from competitors. Ensuring you are outpacing your rivals with superior network metrics is a key ingredient to both marketing and market share success. Our friends and partners at Tutela have created the world\u2019s largest mobile information platform, collecting billions of crowdsourced data points anonymously. These span device, network, and application information to allow us to get from data to insights. They\u2019ve graciously given us (and now you) a slice of their treasure trove. Our demo contains a few months of US data (~112M rows), although Tutela has data for more metrics, for nearly every country, and for a much greater timeframe. There are obviously many variables to consider when it comes to a great network, and one carrier will almost certainly not take the top spot in each category. But we can identify areas of significance where we hope to find favorable results: Spoiler alert: we\u2019re not actually going to attempt to find a winner. Those are just some of the metrics you can see at a glance on our MapD Immerse demo dashboard:  This is immediately useful. Coverage, speed, signal strength, and latency all matter, so MapD allows you to visualize and correlate them for effective consumption. Not coincidentally, the MapD Immerse visualization tool is purpose-built to leverage the MapD Core backend GPU-powered, in-memory, columnar SQL database at scale. This allows  interactive analysis over multi-billion row data sets, with the ability to drill down to an individual row...in milliseconds. It\u2019s also why Tutela has selected to partner with MapD; no need for indexing, sampling, or pre-computation. This means you can zoom in and out, filter, and more in an ad hoc fashion. Let\u2019s take it for a spin. As a consumer, you might be interested in how well each service provider performs in a certain geographic region (i.e., where you live and work) to aid in making a purchasing decision.  For example, to zoom from country level to the Washington, DC area, just type the city name into the searchbox of the pointmap. MapD Immerse will automatically zoom in when you click enter. It looks like in this region, you might lean toward Verizon:  But it may not be as clear cut if you live in San Francisco:  In either case, Sprint better have some compelling discounts or incentives to entice new subscribers. Further, both Sprint and T-Mobile might leverage this information to prioritize buildout or upgrade of cell towers. If they\u2019re analyzing subscriber loss in these areas, this also may give them insight into why that could be occurring. Another valuable feature of the MapD platform is the ability to easily identify trends, anomalies, or outliers. Visualization can often be an invaluable enabler for these situations. Sometimes new revenue opportunities or cost savings can be had by capitalizing on correlations in data that you didn\u2019t even realize existed! See the spike in data downloads in the line chart? By highlighting the time frame with your mouse, MapD will cross filter all charts using the selected time slice. Doing the same with the line chart for data uploads further reveals that the largest spikes occur on May 28 and 30. Ah, that\u2019s Memorial Day weekend in the United States. So this makes sense.  We can get even more granular and note that the bulk of data transfer occurred between 12-1am local time (4-5am UTC) as the heatmap indicates. I wonder if Uber and Lyft also experienced a spike around that time, as celebrations came to an end?  When data exploration at scale is so easy, what other interesting insights will you find? Explore this demo using Tutela's dataset on MapD\u2019s platform on your own here.  If you\u2019d like to learn more about MapD\u2019s recently announced partnership with Tutela, watch our on-demand webinar, introducing and demonstrating Tutela Explorer, a new mobile data analytics solution that provides real-time, interactive and highly visual insight into the performance of mobile networks and device usage. Tutela Explorer is available now as-a-service with global data coverage from Tutela.", "links": ["http://www.mapd.com/blog", "https://www.nytimes.com/2016/10/15/business/sprint-verizon-hear-me-now-paul-marcarelli.html", "http://www.pewinternet.org/fact-sheet/mobile/", "http://www.networkworld.com/article/3092446/smartphones/we-touch-our-phones-2617-times-a-day-says-study.html", "https://www.tutela.com/explorer/", "https://www.mapd.com/demos/telecom", "https://www.mapd.com/demos/telecom", "https://www.mapd.com/platform/core/", "https://www.mapd.com/demos/telecom", "https://www.brighttalk.com/webcast/14525/267851", "https://www.tutela.com/explorer"]}, "242": {"url": "https://www.oreilly.com/learning/building-a-simple-graphql-server-with-neo4j", "title": "Building a simple GraphQL server with Neo4j", "text": "How to implement a GraphQL API that queries Neo4j for a simple movie app. If you're interested in making the move from REST to GraphQL, check out this OSCON video from the team at Github. GraphQL is a powerful new tool for building APIs that allows clients to ask for only the data they need. Originally designed at Facebook to minimize data sent over the wire and reduce round-trip API requests for rendering views in native mobile apps, GraphQL has since been open sourced to a healthy community that is building developer tools. There are also a number of large companies and startups such as GitHub, Yelp, Coursera, Shopify, and Mattermark building public and internal GraphQL APIs. Despite what the name seems to imply, GraphQL is not a query language for graph databases, it is instead an API query language and runtime for building APIs. The \u201cGraph\u201d component of the name comes from the graph data model that GraphQL uses in the frontend. GraphQL itself is simply a specification, and there are many great tools available for building GraphQL APIs in almost every language. In this post we'll make use of graphql-tools by Apollo to build a simple GraphQL API in JavaScript that queries a Neo4j graph database for movies and movie recommendations. We will follow a recipe approach: first, exploring the problem in more detail, then developing our solution, and finally we discuss our approach. Good resources for learning more about GraphQL are GraphQL.org and the Apollo Dev Blog. GraphQL by design can work with any database or backend system, however in this example we'll be using the Neo4j graph database as our persistence layer. Why use a graph database with GraphQL? The idea of application data as a graph is an underpinning design choice of GraphQL. For example, think of customers who have placed orders that contain products\u2014that's a graph! GraphQL enables developers to translate their backend data into the application data graph on the frontend, but if we use a graph database on the backend we can do away with this impedance mismatch and we have graphs all the way down. We'd like to build a simple GraphQL API for a movie app that can do two things: In a traditional REST-ish API approach we might create two separate endpoints, perhaps /movies/search and /movies/similar. One endpoint to search for movies by substring of the title, and another to return a list of similar movies. As we add additional features we might keep adding endpoints. With GraphQL, our API is served from a single endpoint /graphql that takes one or more GraphQL queries and returns JSON data in a shape that is specified by the GraphQL query - only the data requested by the client is returned. Let's see how we can build this simple API. We'll be building a simple node.js JavaScript web server using Express.js to serve our GraphQL endpoint1. First of all we\u2019ll need a Neo4j database (with data) for our GraphQL server to query. For this example we\u2019ll make use of a Neo4j Sandbox instance. Neo4j Sandbox allows us to quickly spin up a hosted Neo4j instance, optionally with existing datasets focused around specific use cases. We\u2019ll use the Recommendations Neo4j Sandbox which includes data about movies and movie reviews and is designed to be used for generating personalized recommendations (for example, by using collaborative filtering to recommend movies based on similar users\u2019 reviews). We'll be making use of a few dependencies for this GraphQL server. I won't list them all here (you can see the full list in the package.json for this project, but there a few worth noting: We\u2019ll follow the \u201cGraphQL First\u201d development paradigm. In this approach, we start by defining a GraphQL schema. This schema defines the types and queries available in our API and then becomes the specification for the API. If we were building a complete application, the frontend developers could use this schema to build out the frontend while the backend team builds the backend in parallel, speeding development. Once we\u2019ve defined our schema we\u2019ll need to create resolver functions that are responsible for fetching data from Neo4j. This schema will define the types and GraphQL queries that we'll be able to use in our API. You can think of the schema as the API blueprint. schema.js Each GraphQL field is resolved by a resolver function. The resolver function defines how data is fetched for that field. resolvers.js Now that we've defined the GraphQL schema and resolver functions we are ready to serve the GraphQL endpoint, using Express.js. server.js If you've ever used Express this should look familiar, you'll notice that we're creating two endpoints and serving them on localhost:8080 Let's take a look at what we just did: Now that we have our API running, let's use the GraphiQL in-browser IDE for GraphQL to search for movies whose titles contain \"Matrix\" and find similar movies for each matching movie. We can load GraphiQL by opening http://loalhost:8080/graphiql in our browser and querying with this GraphQL query:   Querying our GraphQL movies API using GraphiQL, the in-browser IDE for GraphQL GraphQL is still new to almost everyone with the exception of Facebook, where it has been used since 2012, so many conventions and best practices are still being developed. However, GraphQL First Development is a philosophy the community has adopted which gives structure to the process of building a GraphQL API. By defining a contract for the API, expressed as a GraphQL schema, the frontend and backend developer teams can independently implement their applications, using the schema as a guide. To make GraphQL First Development even easier with Neo4j, the Neo4j team has built neo4j-graphql-cli, a command line tool for easily spinning up a Neo4j backed GraphQL endpoint based on a user defined GraphQL schema that can optionally be annotated with Cypher queries - exposing the power of Cypher from within GraphQL. You can learn more about this here. 1 All code for this project is available on GitHub . This example is also available as an Apollo Launchpad Pad, which you can run live in the browser.\u21a9 William Lyon is an engineer on the Developer Relations team at Neo4j, the open source graph database, where he builds tools for integrating Neo4j with other technologies and helps users be successful with graphs. He also leads Neo4j's Data Journalism Accelerator Program. Prior to Neo4j, he worked as a software engineer for a variety of startups, building mobile apps for the real estate industry, trading tools for quantitative finance, and predictive APIs. William holds a masters degree in Computer Science from the University of Montana. You can... How to package up your own repositories for use via PHP's Composer. Will content-blocking change the Web? Learn how to use Node.js and Express.js to quickly bootstrap a web server, tie in MongoDB, and display it all using dynamic HTML templates Diversity and inclusion in the tech industry has experienced a severe failure. Erica Baker approaches the topic like we would for any other service failure: with a postmortem. \u00a9 2017 O'Reilly Media, Inc. All trademarks and registered trademarks appearing on oreilly.com are the property of their respective owners.", "links": ["https://www.safaribooksonline.com/home/?utm_medium=content&utm_source=oreilly.com&utm_campaign=lgen&utm_content=20170601+nav", "http://www.oreilly.com/conferences/", "http://shop.oreilly.com/", "https://www.safaribooksonline.com/home/?utm_medium=content&utm_source=oreilly.com&utm_campaign=lgen&utm_content=20170601+nav", "http://www.oreilly.com/conferences/", "http://shop.oreilly.com/", "https://pixabay.com/en/louvre-pyramid-mesh-perspective-2189967/", "https://www.safaribooksonline.com/library/view/oscon-2017-/9781491976227/video306664.html?utm_source=oreilly&utm_medium=newsite&utm_campaign=building-a-simple-graphql-server-with-neo4j", "https://github.com/apollographql/graphql-tools", "http://www.apollodata.com/", "http://graphql.org/", "https://dev-blog.apollodata.com/", "https://neo4j.com/sandbox-v2/", "https://github.com/johnymontana/movies-graphql-neo4j-server/blob/master/package.json", "http://dev.apollodata.com/tools/graphql-tools/index.html", "https://expressjs.com/", "https://github.com/neo4j/neo4j-javascript-driver", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://www.graph.cool/docs/faq/graphql-sdl-schema-definition-language-kr84dktnp0/", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "http://loalhost:8080/graphiql", "https://www.npmjs.com/package/neo4j-graphql-cli", "http://neo4j.com/developer/graphql", "https://github.com/johnymontana/movies-graphql-neo4j-server", "https://launchpad.graphql.com/3wzp7qnjv", "https://pixabay.com/en/louvre-pyramid-mesh-perspective-2189967/", "https://twitter.com/share", "http://oreilly.com/about/", "http://oreilly.com/work-with-us.html", "http://oreilly.com/careers/", "http://shop.oreilly.com/category/customer-service.do", "http://shop.oreilly.com/category/customer-service.do", "http://fb.co/OReilly", "http://twitter.com/oreillymedia", "https://www.youtube.com/user/OreillyMedia", "https://plus.google.com/+oreillymedia", "https://www.linkedin.com/company/oreilly-media", "http://oreilly.com/terms/", "http://oreilly.com/privacy.html", "http://www.oreilly.com/about/editorial_independence.html"]}, "243": {"url": "http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "title": "", "text": "", "links": ["http://www.getrevue.co/?ref=Revue+Profile", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profileimage&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profiledate&utm_medium=email&utm_source=Data+Science+Digest", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=profilename&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/products/flyelephant-cloud?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/lXPZA?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/lXPZA?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blogs.technet.microsoft.com/machinelearning/2017/07/25/lessons-learned-benchmarking-fast-machine-learning-algorithms/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/DM9WJ?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/DM9WJ?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://medium.com/@stathis/design-by-evolution-393e41863f98?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/XYWKo?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/XYWKo?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.datasciencelearner.com/complete-overview-learning-python-data-analysis/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/6nba0?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/6nba0?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://reinforce.io/blog/introduction-to-tensorforce/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xekd3?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xekd3?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.kdnuggets.com/2017/05/top-10-machine-learning-videos-on-youtube-updated.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Ka8JK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Ka8JK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://arxiv.org/abs/1704.01568?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/96KED?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/96KED?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://datascienceplus.com/random-forests-in-r/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Vd1wK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Vd1wK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://blog.keras.io/the-future-of-deep-learning.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/eKr5e?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/eKr5e?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://medium.com/airbnb-engineering/using-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/odRNK?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/odRNK?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://machinelearning.apple.com/2017/07/07/GAN.html?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/xekoa?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/xekoa?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://arxiv.org/abs/1707.06642?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/mxRw2?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/mxRw2?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "http://www.techleer.com/articles/200-naive-bayes-machine-learning-algorithm-for-classification-problems/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/3K1nx?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/3K1nx?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463?gi=7fe02170b69e&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/OnXK7?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/OnXK7?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://aiukraine.com/en/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/nqRVq?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/nqRVq?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.bonaccorso.eu/2017/07/23/machine-learning-algorithms/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/Z4bBj?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/Z4bBj?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/jobs", "http://bit.ly/2uqHV4J", "https://flyelephant.net/jobs/Spinbackup-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/jobs/Spinbackup-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/yJRmd?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/yJRmd?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://flyelephant.net/jobs/Snap-Ukraine-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "https://flyelephant.net/jobs/Snap-Ukraine-Data-Scientist?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=Data%20Science%20Digest", "http://rev.vu/jAR3B?utm_campaign=Issue&utm_content=domain&utm_medium=email&utm_source=Data+Science+Digest", "http://rev.vu/jAR3B?utm_campaign=Issue&utm_content=share&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%238%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "https://www.getrevue.co/?utm_source=Data Science Digest&utm_medium=email&utm_content=footerlink&utm_campaign=Issue", "http://datasciencedigest.flyelephant.net/?utm_campaign=Issue&utm_content=forwarded&utm_medium=email&utm_source=Data+Science+Digest", "https://www.twitter.com/share?url=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450&via=revue&text=Data%20Science%20Digest%20-%20Issue%20%238%20by%20%40FlyElephantNet&related=revue", "http://www.facebook.com/sharer/sharer.php?u=http://datasciencedigest.flyelephant.net/issues/data-science-digest-issue-8-56450", "http://www.getrevue.co/?ref=Revue+Profile"]}}}